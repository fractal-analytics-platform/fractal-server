{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fractal Analytics Framework","text":"<p>Fractal is a framework developed at the BioVisionCenter to process bioimaging data at scale in the OME-Zarr format and prepare the images for interactive visualization.</p> <p>This is the server component of the fractal analytics platform (find more information about Fractal in general and the other repositories at the Fractal home page). The source code is available on the fractal-server GitHub repository.</p>"},{"location":"#licence-and-copyright","title":"Licence and Copyright","text":"<p>Fractal was conceived in the Liberali Lab at the Friedrich Miescher Institute for Biomedical Research and in the Pelkmans Lab at the University of Zurich by @jluethi and @gusqgm. The Fractal project is now developed at the BioVisionCenter at the University of Zurich and the project lead is with @jluethi. The core development is done under contract by eXact lab S.r.l..</p> <p>Unless otherwise specified, Fractal components are released under the BSD 3-Clause License, and copyright is with the BioVisionCenter at the University of Zurich.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>Note: Numbers like (#1234) point to closed Pull Requests on the fractal-server repository.</p>"},{"location":"changelog/#2170-prereleases","title":"2.17.0 - prereleases","text":"<p>This version requires a data-migration script (<code>fractalctl update-db-data</code>), see instructions at TBD.</p> <p>The main content of this release is the introduction of the computational resource&amp;profile concepts, and a review of the application settings.</p> <ul> <li>API (main PRs: #2809, #2870, #2877, #2884, #2911, #2915, #2925, #2940, #2941, #2943):<ul> <li>Introduce API for <code>Resource</code> and <code>Profile</code> models.</li> <li>Drop API for user settings.</li> <li>Drop handling of deprecated <code>DatasetV2.filters</code> attribute when creating dataset dumps (#2917).</li> <li>Enable querying users by <code>resource_id</code> (#2877).</li> <li>Check matching-<code>resource_id</code> upon job submission (#2896).</li> <li>Treat <code>TaskGroupV2.resource_id</code> as not nullable (#2896).</li> <li>Split <code>/api/settings/</code> into smaller-scope endpoints.</li> <li>Update rules for read access to task groups with resource information (#2941).</li> <li>Only show tasks and task groups associated to current user's <code>Resource</code> (#2906, #2943).</li> </ul> </li> <li>Task-group lifecycle:<ul> <li>Rely on resource and profile rather than user settings (#2809).</li> </ul> </li> <li>Runner<ul> <li>Rely on resource and profile rather than user settings (#2809).</li> <li>Make <code>extra_lines</code> a non-optional list in SLURM configuration (#2893).</li> <li>Enable <code>user_local_exports</code> on SLURM-SSH runner.</li> </ul> </li> <li>Database and models (also #2931):<ul> <li>Introduce <code>Resource</code> and <code>Profile</code> models (#2809).</li> <li>Introduce <code>resource_id</code> foreign key for task-group and project models (#).</li> <li>Move <code>project_dir</code> and <code>slurm_accounts</code> from <code>UserSettings</code> to <code>UserOAuth</code>.</li> <li>Make <code>project_dir</code> required.</li> <li>Discontinue usage of <code>UserSettings</code> table.</li> </ul> </li> <li>Authentication API:<ul> <li>Drop OAuth-based self registration (#2890).</li> </ul> </li> <li>App settings (#2874, #2882, #2895, #2898, #2916, #2922):<ul> <li>Remove all configuration variables that are now part of <code>Resource</code>s.</li> <li>Split main <code>Settings</code> model into smaller-scope models.</li> <li>Remove email-password encryption.</li> <li>Introduce <code>init-db-data</code> command.</li> <li>Set default <code>FRACTAL_API_MAX_JOB_LIST_LENGTH</code> to 25 (#2928).</li> <li>Introduce <code>FRACTAL_DEFAULT_GROUP_NAME</code>, set to either <code>\"All\"</code> or <code>None</code> (#2939).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>fastapi</code> to v0.120 (#2921).</li> <li>Bump <code>uvicorn</code> to v0.38 (#2921).</li> <li>Bump <code>fastapi-users</code> to v15 (#2907).</li> </ul> </li> <li>Testing and GitHub actions:<ul> <li>Simplify Python environment in documentation GitHub action (#2919).</li> <li>Simplify PyPI-publish GitHub action (#2927).</li> <li>Drop explicit dependency on `python-dotenv (#2921).</li> </ul> </li> <li>Testing:<ul> <li>Introduce <code>pytest-env</code> dependency.</li> <li>Update testing database to version 2.16.6 (#2909).</li> <li>Test OAuth flow with <code>pytest</code> and remove OAuth GHA (#2929).</li> </ul> </li> </ul>"},{"location":"changelog/#2166","title":"2.16.6","text":"<ul> <li>API:<ul> <li>Fix bug in import-workflow endpoint, leading to the wrong task-group being selected (#2863).</li> </ul> </li> <li>Models:<ul> <li>Fix use of custom <code>AttributeFilters</code> type in SQLModel model (#2830).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>pydantic</code> to 2.12.0 (#2830).</li> <li>Bump <code>poetry</code> to 2.2.1 in GitHub actions (#2830).</li> </ul> </li> <li>Testing:<ul> <li>Add pre-commit rule to prevent custom types is models (#2830).</li> </ul> </li> </ul>"},{"location":"changelog/#2165","title":"2.16.5","text":"<ul> <li>Dependencies:<ul> <li>Bump <code>fastapi</code>, <code>sqlmodel</code>, <code>uvicorn</code>, <code>pydantic-settings</code> versions (#2827).</li> </ul> </li> </ul>"},{"location":"changelog/#2164","title":"2.16.4","text":"<ul> <li>Task life cycle:<ul> <li>Switch to PyPI Index API for finding latest package versions (#2790).</li> </ul> </li> <li>SSH:<ul> <li>Bump default lock-acquisition timeout from 250 to 500 seconds (#XXX).</li> <li>Introduce structured logs for SSH-lock dynamics (#XXX).</li> </ul> </li> <li>API:<ul> <li>Replace <code>HTTP_422_UNPROCESSABLE_CONTENT</code> with <code>HTTP_422_UNPROCESSABLE_CONTENT</code> (#2790).</li> </ul> </li> <li>Internal:<ul> <li>Move <code>app.runner</code> into <code>runner</code> (#2814).</li> </ul> </li> <li>Testing:<ul> <li>Use function-scoped base folder for backend (#2793).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>fastapi</code> version (#2790).</li> <li>Bump <code>uvicorn-worker</code> and <code>sqlmodel</code> versions (#2792).</li> </ul> </li> </ul>"},{"location":"changelog/#2163","title":"2.16.3","text":"<ul> <li>Task life cycle:<ul> <li>Move post-pixi-installation logic into the same SLURM job as <code>pixi install</code>, for SSH/SLURM deployment (#2787).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>cryptography</code> (#2788).</li> </ul> </li> </ul>"},{"location":"changelog/#2162","title":"2.16.2","text":"<ul> <li>Task life cycle:<ul> <li>Move <code>pixi install</code> execution to SLURM jobs, for SSH/SLURM deployment (#2784).</li> <li>Add <code>SLURM_CONFIG</code> to <code>PixiSettings</code> (#2784).</li> </ul> </li> </ul>"},{"location":"changelog/#2161","title":"2.16.1","text":"<ul> <li>Runner:<ul> <li>Drop top-level executor-error-related logs (#2783).</li> <li>Drop obsolete <code>FRACTAL_SLURM_SBATCH_SLEEP</code> configuration variable (#2785).</li> </ul> </li> </ul>"},{"location":"changelog/#2160","title":"2.16.0","text":"<ul> <li>Runner:<ul> <li>Record SLURM-job stderr in <code>JobV2.executor_error_log</code> (#2750, #2771, #2773).</li> </ul> </li> <li>Task life cycle:<ul> <li>Support both pre-pinning and post-pinning of dependencies (#2761).</li> <li>Drop <code>FRACTAL_MAX_PIP_VERSION</code> configuration variable (#2766).</li> <li>Make TaskGroup deletion a lifecycle operation (#2759).</li> </ul> </li> <li>Testing:<ul> <li>Add <code>out_of_memory</code> mock task (#2770).</li> <li>Make fixtures sync when possible (#2776).</li> <li>Bump pytest-related dependencies (#2776).</li> </ul> </li> </ul>"},{"location":"changelog/#2159","title":"2.15.9","text":"<ul> <li>API:<ul> <li>In <code>POST /api/v2/project/{project_id}/status/images/</code>, include all available types&amp;attributes (#2762).</li> </ul> </li> <li>Internal:<ul> <li>Optimize <code>fractal_server.images.tools.aggregate_attributes</code> (#2762).</li> </ul> </li> </ul>"},{"location":"changelog/#2158","title":"2.15.8","text":"<ul> <li>Runner:<ul> <li>Split <code>SlurmJob</code> submission into three steps, reducing SSH connections (#2749).</li> <li>Deprecate experimental <code>pre_submission_commands</code> feature (#2749).</li> </ul> </li> <li>Documentation:<ul> <li>Fix <code>Fractal Users</code> documentation page (#2738).</li> <li>Improve documentation for Pixi task collection (#2742).</li> </ul> </li> <li>Task life cycle:<ul> <li>Add test to lock <code>FRACTAL_MAX_PIP_VERSION</code> with latest version on PyPI (#2752).</li> <li>Use pixi v0.54.1 in tests (#2758).</li> </ul> </li> <li>Internal<ul> <li>Improve type hints (#2739).</li> </ul> </li> <li>SSH:<ul> <li>Add a decorator to open a new connection (socket) if the decorated function hits a <code>NoValidConnectionError</code> or a <code>OSError</code> (#2747).</li> <li>Remove multiple-attempts logic (#2747).</li> </ul> </li> </ul>"},{"location":"changelog/#2157","title":"2.15.7","text":"<ul> <li>API:<ul> <li>Capture SSH related failure in submit-job endpoint (#2734).</li> </ul> </li> <li>Task life cycle:<ul> <li>Also edit <code>tool.pixi.project.platforms</code> section of <code>pyproject.toml</code>, for <code>pixi</code>-based task collection (#2711).</li> </ul> </li> <li>Internal:<ul> <li>Extend use of <code>UnreachableBranchError</code> in API and runner (#2726).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>fastapi</code> to version <code>0.116.*</code> (#2718).</li> <li>Bump all <code>docs</code> optional dependencies (#2722).</li> <li>Add support for Python 3.13 (#2716).</li> </ul> </li> <li>Documentation:<ul> <li>Remove/fix several obsolete docs pages (#2722).</li> </ul> </li> <li>Testing:<ul> <li>Improve/optimize several tests (#2727).</li> <li>Use <code>poetry</code> 2.1.3 in all GitHub Actions and always install it using <code>pipx</code> (#2731).</li> <li>Move all GitHub Action runners to <code>ubuntu-24.04</code> (#2732).</li> <li>Avoid using <code>threading</code> private method (#2733).</li> </ul> </li> </ul>"},{"location":"changelog/#2156","title":"2.15.6","text":"<ul> <li>Runner:<ul> <li>Remove obsolete <code>JobExecutionError</code> attributes and <code>TaskExecutionError</code> handling (#2708).</li> <li>Always interpret <code>needs_gpu</code> string as a boolean (#2706).</li> </ul> </li> <li>Task life cycle:<ul> <li>Use <code>bash --login</code> for <code>pixi install</code> execution over SSH (#2709).</li> </ul> </li> <li>Development:<ul> <li>Move <code>example</code> folder to root directory (#2720).</li> </ul> </li> </ul>"},{"location":"changelog/#2155","title":"2.15.5","text":"<ul> <li>API:<ul> <li>Update <code>HistoryRun</code> and <code>HistoryUnit</code>s statuses when <code>Workflow</code>s are manually labeled as failed (#2705).</li> </ul> </li> </ul>"},{"location":"changelog/#2154","title":"2.15.4","text":"<ul> <li>Task lifecycle:<ul> <li>Edit <code>pyproject.toml</code> in-place before running <code>pixi install</code> (#2696).</li> </ul> </li> </ul>"},{"location":"changelog/#2153","title":"2.15.3","text":"<ul> <li>API:<ul> <li>Ongoing <code>WorkflowTask</code>s are marked as submitted during <code>Job</code> execution (#2692).</li> </ul> </li> </ul>"},{"location":"changelog/#2152","title":"2.15.2","text":"<ul> <li>API:<ul> <li>Improve logging for <code>PATCH /admin/v2/job/{job_id}/</code> (#2686).</li> <li>Prevent deletion and reordering of <code>WorkflowTask</code>s in <code>Workflow</code>s associated to submitted <code>Job</code>s (#2689).</li> </ul> </li> <li>Database:<ul> <li>Set <code>pool_pre_ping=True</code> for sync db engine (#2676).</li> </ul> </li> <li>Runner:<ul> <li>Update <code>chmod ... -R</code> to <code>chmod -R ...</code> (#2681).</li> <li>Add custom handling of some <code>slurm_load_jobs</code> socket-timeout error (#2683).</li> <li>Remove redundant <code>mkdir</code> in SLURM SSH runner (#2671).</li> <li>Do not write to SLURM stderr from remote worker (#2691).</li> <li>Fix spurious version-mismatch warning in remote worker (#2691).</li> </ul> </li> <li>SSH:<ul> <li>Always set <code>in_stream=False</code> for <code>fabric.Connection.run</code> (#2694).</li> </ul> </li> <li>Testing:<ul> <li>Fix <code>test_FractalSSH.py::test_folder_utils</code> for MacOS (#2678).</li> <li>Add pytest marker <code>fails_on_macos</code> (#2681).</li> <li>Remove patching of <code>sys.stdin</code>, thanks to updates to <code>fabric.Connection.run</code> arguments (#2694).</li> </ul> </li> </ul>"},{"location":"changelog/#2151","title":"2.15.1","text":"<p>This release fixes the reason for yanking 2.15.0.</p> <ul> <li>Database:<ul> <li>Modify <code>JSON-&gt;JSONB</code> database schema migration in-place, so that columns which represent JSON Schemas or <code>meta_*</code> fields remain in JSON form rather than JSONB (#2664, #2666).</li> </ul> </li> </ul>"},{"location":"changelog/#2150-yanked","title":"2.15.0 [yanked]","text":"<p>This release was yanked on PyPI, because its conversion of all JSON columns into JSONB changes the key order, see https://github.com/fractal-analytics-platform/fractal-server/issues/2663.</p> <ul> <li>Database:<ul> <li>Rename <code>TaskGroupV2.wheel_path</code> into <code>TaskGroupV2.archive_path</code> (#2627).</li> <li>Rename <code>TaskGroupV2.pip_freeze</code> into <code>TaskGroupV2.env_info</code> (#2627).</li> <li>Add <code>TaskGroupV2.pixi_version</code> (#2627).</li> <li>Transform every JSON column to JSONB (#2662).</li> </ul> </li> <li>API:<ul> <li>Introduce new value <code>TaskGroupV2OriginEnum.PIXI</code> (#2627).</li> <li>Exclude <code>TaskGroupV2.env_info</code> from API responses (#2627).</li> <li>Introduce <code>POST /api/v2/task/collect/pixi/</code> (#2627).</li> <li>Extend deactivation/reactivation endpoints to pixi task groups (#2627).</li> <li>Block <code>DELETE /api/v2/task-group/{task_group_id}/</code> if a task-group activity is ongoing (#2642).</li> <li>Introduce <code>_verify_non_duplication_group_path</code> auxiliary function (#2643).</li> </ul> </li> <li>Task lifecycle:<ul> <li>Introduce full support for pixi task-group lifecycle (#2627, #2651, #2652, #2654).</li> </ul> </li> <li>SSH:<ul> <li>Introduce <code>FractalSSH.read_remote_text_file</code> (#2627).</li> </ul> </li> <li>Runner:<ul> <li>Fix use of <code>worker_init/extra_lines</code> for multi-image job execution (#2660).</li> <li>Support SLURM configuration options <code>nodelist</code> and <code>exclude</code> (#2660).</li> </ul> </li> <li>App configuration:<ul> <li>Introduce new configuration variable <code>FRACTAL_PIXI_CONFIG_FILE</code> and new attribute <code>Settings.pixi</code> (#2627, #2650).</li> </ul> </li> </ul>"},{"location":"changelog/#21416","title":"2.14.16","text":"<ul> <li>Internal:<ul> <li>Refactor and optimize enrich-image functions (#2620).</li> </ul> </li> <li>Database:<ul> <li>Add indices to <code>HistoryImageCache</code> table (#2620).</li> </ul> </li> <li>SSH:<ul> <li>Increase Paramiko <code>banner_timeout</code> from the default 15 seconds to 30 seconds (#2632).</li> <li>Re-include <code>check_connection</code> upon <code>SlurmSSHRunner</code> startup (#2636).</li> </ul> </li> <li>Testing:<ul> <li>Introduce benchmarks for database operations (#2620).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>cryptography</code>, <code>packaging</code> and <code>python-dotenv</code> dependencies (#2630).</li> </ul> </li> </ul>"},{"location":"changelog/#21415","title":"2.14.15","text":"<ul> <li>API:<ul> <li>Add required <code>workflowtask_id</code> query parameter to <code>verify-unique-types</code> endpoint (#2619).</li> <li>Enrich images with status within <code>verify-unique-types</code> endpoint, when necessary (#2619).</li> </ul> </li> </ul>"},{"location":"changelog/#21414","title":"2.14.14","text":"<ul> <li>API:<ul> <li>Fix <code>GET /api/v2/task-group/</code> by adding missing sorting before <code>itertools.groupby</code> (#2614).</li> </ul> </li> <li>Internal:<ul> <li>Drop <code>execute_command_async</code> function (#2611).</li> <li>Introduce <code>TaskType</code> enum (#2612).</li> </ul> </li> </ul>"},{"location":"changelog/#21413","title":"2.14.13","text":"<ul> <li>API:<ul> <li>Group response items of <code>GET /api/v2/task-group/</code> by <code>pkg_name</code> (#2596).</li> <li>Disambiguate response items of <code>GET /api/v2/task-group/</code> (#2596).</li> </ul> </li> <li>Internal:<ul> <li>Introduce <code>UnreachableBranchError</code> (#2596).</li> </ul> </li> <li>Testing:<ul> <li>Enforce task-group non-duplication constraints in <code>task_factory_v2</code> fixture (#2596).</li> </ul> </li> </ul>"},{"location":"changelog/#21412","title":"2.14.12","text":"<ul> <li>Runner:<ul> <li>Enable status-based selection of images to process (#2588).</li> </ul> </li> <li>API:<ul> <li>Remove <code>unit_status</code> query parameter from <code>/project/{project_id}/status/images/</code> (#2588).</li> <li>Remove default type filters from <code>/project/{project_id}/status/images/</code> (#2588).</li> <li>Sort lists of existing attribute values in <code>aggregate_attributes</code> (#2588).</li> </ul> </li> <li>Task-group lifecycle:<ul> <li>Split <code>pip install</code> command into two steps (#2600).</li> </ul> </li> </ul>"},{"location":"changelog/#21411","title":"2.14.11","text":"<ul> <li>Task-group lifecycle:<ul> <li>Support version-pinning for two dependencies in task collection (#2590, #2599).</li> <li>Support version-pinning for previously-missing dependencies in task collection (#2590, #2599).</li> </ul> </li> <li>Development:<ul> <li>Improve <code>mypy</code> configuration in <code>pyproject.toml</code> (#2595).</li> </ul> </li> </ul>"},{"location":"changelog/#21410","title":"2.14.10","text":"<p>This version requires a data-migration script (<code>fractalctl update-db-data</code>).</p> <ul> <li>Database:<ul> <li>Improve data-migration script that is necessary for 2.14.8 (#2594).</li> </ul> </li> </ul>"},{"location":"changelog/#2149","title":"2.14.9","text":"<p>WARNING: Do not release this version, but go directly to 2.14.10.</p> <ul> <li>Task-group lifecycle:<ul> <li>Improve handling of SSH-related errors (#2589).</li> </ul> </li> <li>Database:<ul> <li>Rename data-migration script that is necessary for 2.14.8 (#2592).</li> </ul> </li> </ul>"},{"location":"changelog/#2148","title":"2.14.8","text":"<p>WARNING: Do not release this version, but go directly to 2.14.10.</p> <ul> <li>API:<ul> <li>Update <code>POST /project/{project_id}/workflow/{workflow_id}/wftask/replace-task/</code> so that it re-uses existing workflow task (#2565).</li> </ul> </li> <li>Database:<ul> <li>Add <code>HistoryRun.task_id</code> column (#2565).</li> </ul> </li> <li>Internal:<ul> <li>Refactor: extract <code>enrich_image_list</code> function from <code>/project/{project_id}/status/images/</code> endpoint (#2585).</li> </ul> </li> </ul>"},{"location":"changelog/#2147","title":"2.14.7","text":"<ul> <li>Runner:<ul> <li>Re-include SLURM accounts for both sudo-slurm and ssh-slurm runners (#2580)</li> <li>Re-include use of <code>worker_init</code> for both sudo-slurm and ssh-slurm runners (#2580)</li> </ul> </li> <li>Testing:<ul> <li>Use <code>Optional</code> for argument type hints in mock tasks (#2575).</li> </ul> </li> </ul>"},{"location":"changelog/#2146","title":"2.14.6","text":"<ul> <li>API:<ul> <li>Introduce <code>api/v2/project/{project.id}/workflow/{workflow.id}/version-update-candidates/</code> endpoint (#2556).</li> </ul> </li> <li>Task lifecycle:<ul> <li>Use dedicated SSH connections for lifecycle background tasks (#2569).</li> <li>Also set <code>TaskGroup.version</code> for custom task collections (#2573).</li> </ul> </li> <li>Internal:<ul> <li>Inherit from <code>StrEnum</code> rather than <code>str, Enum</code> (#2561).</li> <li>Run <code>pyupgrade</code> on codebase (#2563).</li> <li>Remove init file of obsolete folder (#2571).</li> </ul> </li> <li>Dependencies:<ul> <li>Deprecate Python 3.10 (#2561).</li> </ul> </li> </ul>"},{"location":"changelog/#2145","title":"2.14.5","text":"<p>This version introduces an important internal refactor of the runner component, with the goal of simplifying the SLURM version.</p> <ul> <li>Runner:<ul> <li>Make <code>submit/multisubmit</code> method take static arguments, rather than a callable (#2549).</li> <li>Replace input/output pickle files with JSON files (#2549).</li> <li>Drop possibility of non-<code>utf-8</code> encoding for <code>_run_command_as_user</code> function (#2549).</li> <li>Avoid local-remote-local round trip for task parameters, by writing the local file first (#2549).</li> <li>Stop relying on positive/negative return codes to produce either <code>TaskExecutionError</code>/<code>JobExecutionError</code>, in favor of only producing <code>TaskExecutionError</code> at the lowest task-execution level (#2549).</li> <li>Re-implement <code>run_single_task</code> within SLURM remote <code>worker</code> function (#2549).</li> <li>Drop <code>TaskFiles.remote_files_dict</code> (#2549).</li> <li>Drop obsolete <code>FRACTAL_SLURM_ERROR_HANDLING_INTERVAL</code> config variable (#2549).</li> <li>Drop obsolete <code>utils_executors.py</code> module (#2549).</li> </ul> </li> <li>Dependencies:<ul> <li>Drop <code>cloudpickle</code> dependency (#2549).</li> <li>Remove copyright references to <code>clusterfutures</code> (#2549).</li> </ul> </li> <li>Testing:<ul> <li>Introduce <code>slurm_alive</code> fixture that checks <code>scontrol ping</code> results (#2549).</li> </ul> </li> </ul>"},{"location":"changelog/#2144","title":"2.14.4","text":"<ul> <li>API:<ul> <li>Replace most <code>field_validator</code>s with <code>Annotated</code> types, and review <code>model_validator</code>s (#2504).</li> </ul> </li> <li>SSH:<ul> <li>Remove Python-wrapper layer for <code>tar</code> commands (#2554).</li> <li>Add <code>elapsed</code> information to SSH-lock-was-acquired log (#2558).</li> </ul> </li> </ul>"},{"location":"changelog/#2143","title":"2.14.3","text":"<ul> <li>Runner:<ul> <li>Skip creation/removal of folder copy in compress-folder module (#2553).</li> <li>Drop obsolete <code>--extra-import-paths</code> option from SLURM remote worker (#2550).</li> </ul> </li> </ul>"},{"location":"changelog/#2142","title":"2.14.2","text":"<ul> <li>API:<ul> <li>Handle inaccessible <code>python_interpreter</code> or <code>package_root</code> in custom task collection  (#2536).</li> <li>Do not raise non-processed-images warning if the previous task is a converter (#2546).</li> </ul> </li> <li>App:<ul> <li>Use <code>Enum</code> values in f-strings, for filenames and error messages (#2540).</li> </ul> </li> <li>Runner:<ul> <li>Handle exceptions in post-task-execution runner code (#2543).</li> </ul> </li> </ul>"},{"location":"changelog/#2141","title":"2.14.1","text":"<ul> <li>API:<ul> <li>Add <code>POST /project/{project_id}/dataset/{dataset_id}/images/non-processed/</code> endpoint (#2524, #2533).</li> </ul> </li> <li>Runner:<ul> <li>Do not create temporary output-pickle files (#2539).</li> <li>Set logging level to <code>DEBUG</code> within <code>compress_folder</code> and <code>extract_archive</code> modules (#2539).</li> <li>Transform job-error log into warning (#2539).</li> <li>Drop <code>FRACTAL_SLURM_INTERVAL_BEFORE_RETRIEVAL</code> (#2525, #2531).</li> <li>Increase <code>MAX_NUM_THREADS</code> from 4 to 12 (#2520).</li> <li>Support re-deriving an existing image with a non-trivial <code>origin</code> (#2527).</li> </ul> </li> <li>Testing:<ul> <li>Adopt ubuntu24 containers for CI (#2530).</li> <li>Do not run Python3.11 container CI for PRs, but only for merges (#2519).</li> <li>Add mock wheel file and update assertion for pip 25.1 (#2523).</li> <li>Optimize <code>test_reactivate_local_fail</code> (#2511).</li> <li>Replace <code>fractal-tasks-core</code> with <code>testing-tasks-mock</code> in tests (#2511).</li> <li>Improve flaky test (#2513).</li> </ul> </li> </ul>"},{"location":"changelog/#2140","title":"2.14.0","text":"<p>This release mostly concerns the new database/runner integration in view of providing more granular history/status information. This includes a full overhaul of the runner.</p> <ul> <li>API:<ul> <li>Add all new status endpoints.</li> <li>Add <code>GET /job/latest/</code> endpoint (#2389).</li> <li>Make request body required for <code>replace-task</code> endpoint (#2355).</li> <li>Introduce shared tools for pagination.</li> <li>Remove <code>valstr</code> validator and introduce <code>NonEmptyString</code> in schemas (#2352).</li> </ul> </li> <li>Database<ul> <li>New tables <code>HistoryRun</code>, <code>HistoryUnit</code> and <code>HistoryImageCache</code> tables.</li> <li>Drop attribute/type filters from dataset table.</li> <li>Add <code>type_filters</code> column to job table.</li> <li>Use <code>ondelete</code> flag in place of custom DELETE-endpoint logics.</li> </ul> </li> <li>Runner<ul> <li>Full overhaul of runners. Among the large number of changes, this includes:<ul> <li>Fully drop the <code>concurrent.futures</code> interface.</li> <li>Fully drop the multithreaded nature of SLURM runners, in favor of a more linear submission/retrieval flow.</li> <li>New <code>BaseRunner</code>, <code>LocalRunner</code>, <code>BaseSlurmRunner</code>, <code>SlurmSSHRunner</code> and <code>SlurmSudoRunner</code> objects.</li> <li>The two SLURM runners now share a large part of base logic.</li> <li>Database updates to <code>HistoryRun</code>, <code>HistoryUnit</code> and <code>HistoryImageCache</code> tables.</li> <li>We do not fill <code>Dataset.history</code> any more.</li> </ul> </li> </ul> </li> <li>Task lifecycle:<ul> <li>Drop hard-coded use of <code>--no-cache-dir</code> for <code>pip install</code> command (#2357).</li> </ul> </li> <li>App:<ul> <li>Obfuscate sensitive information from settings using <code>SecretStr</code> (#2333).</li> <li>Drop <code>FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE</code> obsolete configuration variable (#2359).</li> </ul> </li> <li>Testing:<ul> <li>Use <code>fractal-task-tools</code> to build <code>fractal-tasks-mock</code> manifest (#2374).</li> </ul> </li> <li>Development:<ul> <li>Add <code>codespell</code> to precommit (#2358).</li> <li>Drop obsolete <code>examples</code> folder (#2405).</li> </ul> </li> </ul>"},{"location":"changelog/#2131","title":"2.13.1","text":"<ul> <li>API:<ul> <li>Add <code>AccountingRecord</code> and <code>AccountingRecordSlurm</code> tables (#2267).</li> <li>Add <code>/admin/v2/impersonate</code> endpoint (#2280).</li> <li>Replace <code>_raise_if_naive_datetime</code> with <code>AwareDatetime</code> (#2283).</li> </ul> </li> <li>Database:<ul> <li>Add <code>/admin/v2/accounting/</code> and <code>/admin/v2/accounting/slurm/</code> endpoints (#2267).</li> </ul> </li> <li>Runner:<ul> <li>Populate <code>AccountingRecord</code> from runner (#2267).</li> </ul> </li> <li>App:<ul> <li>Review configuration variables for email-sending (#2269).</li> <li>Reduce error-level log records(#2282).</li> </ul> </li> <li>Testing:<ul> <li>Drop obsolete files/folders from <code>tests/data</code> (#2281).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>httpx</code> to version <code>0.28.*</code> (#2284).</li> </ul> </li> </ul>"},{"location":"changelog/#2130","title":"2.13.0","text":"<p>With this release we switch to Pydantic v2.</p> <ul> <li>Runner:<ul> <li>Deprecate <code>FRACTAL_BACKEND_RUNNER=\"local_experimental\"</code> (#2273).</li> <li>Fully replace <code>clusterfutures</code> classes with custom ones (#2272).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>pydantic</code> to v2 (#2270).</li> <li>Drop <code>clusterfutures</code> dependency (#2272).</li> <li>Drop <code>psutil</code> dependency (#2273).</li> <li>Bump <code>cryptography</code> to version <code>44.0.*</code> (#2274).</li> <li>Bump <code>sqlmodel</code> to version <code>0.0.22</code> (#2275).</li> <li>Bump <code>packaging</code> to version <code>24.*.*</code> (#2275).</li> <li>Bump <code>cloudpickle</code> to version <code>3.1.*</code> (#2275).</li> <li>Bump <code>uvicorn-workers</code> to version <code>0.3.0</code> (#2275).</li> <li>Bump <code>gunicorn</code> to version <code>23.*.*</code> (#2275).</li> <li>Bump <code>httpx</code> to version <code>0.27.*</code> (#2275).</li> </ul> </li> </ul>"},{"location":"changelog/#2121","title":"2.12.1","text":"<p>Note: this version requires a manual update of email-related configuration variables.</p> <ul> <li>API:<ul> <li>Deprecate <code>use_dataset_filters</code> query parameter for <code>/project/{project_id}/dataset/{dataset_id}/images/query/</code> (#2231).</li> </ul> </li> <li>App:<ul> <li>Add fractal-server version to logs (#2228).</li> <li>Review configuration variables for email-sending (#2241).</li> </ul> </li> <li>Database:<ul> <li>Remove <code>run_migrations_offline</code> from <code>env.py</code> and make <code>run_migrations_online</code> sync (#2239).</li> </ul> </li> <li>Task lifecycle:<ul> <li>Reset logger handlers upon success of a background lifecycle operation, to avoid open file descriptors (#2256).</li> </ul> </li> <li>Runner<ul> <li>Sudo/SLURM executor checks the fractal-server version using <code>FRACTAL_SLURM_WORKER_PYTHON</code> config variable, if set (#2240).</li> <li>Add <code>uname -n</code> to SLURM submission scripts (#2247).</li> <li>Handle <code>_COMPONENT_KEY_</code>-related errors in sudo/SLURM executor, to simplify testing (#2245).</li> <li>Drop obsolete <code>SlurmJob.workflow_task_file_prefix</code> for both SSH/sudo executors (#2245).</li> <li>Drop obsolete <code>keep_pickle_files</code> attribute from slurm executors (#2246).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>uvicorn</code> version (#2242).</li> </ul> </li> <li>Testing:<ul> <li>Improve testing of sudo-Slurm executor (#2245, #2246).</li> <li>Introduce <code>container</code> pytest marker (#2249).</li> <li>Split CI GitHub Actions in three jobs: API, not API and Containers (#2249).</li> </ul> </li> </ul>"},{"location":"changelog/#2120","title":"2.12.0","text":"<p>WARNING: The database schema update introduced via this version is non-reversible.</p> <ul> <li>API:<ul> <li>Drop V1 endpoints (#2230).</li> </ul> </li> <li>Database:<ul> <li>Drop V1 tables (#2230).</li> </ul> </li> <li>Runner:<ul> <li>Drop V1 runners (#2230).</li> </ul> </li> <li>Testing:<ul> <li>Drop V1 tests (#2230).</li> <li>Update V2 tests to keep coverage stable (#2230).</li> </ul> </li> </ul>"},{"location":"changelog/#2111","title":"2.11.1","text":"<ul> <li>Database<ul> <li>Drop columns <code>DatasetV2.filters</code> and <code>WorkflowTaskV2.input_filters</code> (#2232).</li> </ul> </li> </ul>"},{"location":"changelog/#2110","title":"2.11.0","text":"<p>This version revamps the filters data structure, and it introduces complex attribute filters.</p> <p>Note: This release requires running <code>fractalctl update-db-data</code>. Some legacy columns will be removed from the database, either as part of the <code>2.11.0</code> data-migration or as part of the <code>2.11.1</code> schema migration. Please make sure you have a database dump.</p> <ul> <li>API:<ul> <li>Align API with new database schemas for filters-related columns (#2168, #2196, #2202).</li> <li>Support importing workflows or datasets with legacy (pre-<code>2.11.0</code>) filters-related fields (#2185, #2227).</li> <li>Avoid blocking operations from the download-job-logs endpoint, when the zip archive of a running job is requested (#2225).</li> <li>Update and simplify <code>/api/v2/project/{project_id}/status/</code>, dropping use of temporary job files (#2169).</li> <li>Add new (experimental) <code>/project/{project_id}/workflow/{workflow_id}/type-filters-flow/</code> endpoint (#2208).</li> </ul> </li> <li>Database:<ul> <li>Update table schemas for all filters-related columns:<ul> <li>Always handle attribute- and type-filters in different columns (#2168).</li> <li>Update attribute-filter-values type from scalar to list (#2168, #2196).</li> <li>Deprecate attribute filters for <code>WorkflowTaskV2</code> (#2168).</li> <li>Add attribute filters to <code>JobV2</code> (#2168).</li> </ul> </li> <li><code>2.11.0</code> data-migration script (#2168, #2202, #2208, #2209).</li> </ul> </li> <li>Runner:<ul> <li>Introduce database writes in runner component, to replace the use of temporary files (#2169).</li> <li>Use <code>TaskV2.input_types</code> for filtering, rather than validation (#2191, #2196).</li> <li>Make job-execution background-task function sync, to make it transparent that it runs on a thread (#2220).</li> <li>Remove all filters from <code>TaskOutput</code> (#2190).</li> </ul> </li> <li>Task Collection:<ul> <li>Improve logs handling for failed task collections (#2192)</li> </ul> </li> <li>Testing:<ul> <li>Speed up CI by splitting it into more jobs (#2210).</li> </ul> </li> </ul>"},{"location":"changelog/#2106","title":"2.10.6","text":"<ul> <li>Task lifecycle:<ul> <li>Use unique logger names for task-lifecycle operations (#2204).</li> </ul> </li> </ul>"},{"location":"changelog/#2105","title":"2.10.5","text":"<ul> <li>App:<ul> <li>Add missing space in \"To\" field for email settings (#2173).</li> </ul> </li> <li>Testing:<ul> <li>Improve configuration for coverage GitHub Action step (#2175).</li> <li>Add <code>persist-credentials: false</code> to <code>actions/checkout@v4</code> GitHub Action steps (#2176).</li> </ul> </li> <li>Dependencies:<ul> <li>Require <code>bumpver&gt;2024.0</code> (#2179).</li> </ul> </li> </ul>"},{"location":"changelog/#2104","title":"2.10.4","text":"<ul> <li>Switch to poetry v2 (#2165).</li> <li>Require Python &lt;3.13 (#2165).</li> </ul>"},{"location":"changelog/#2103","title":"2.10.3","text":"<p>Note: this version fixes a bug introduced in version 2.10.1.</p> <ul> <li>API:<ul> <li>Fix bug in <code>POST /api/v2/project/{p_id}/workflow/{w_id}/wftask/replace-task/</code> endpoint (#2163).</li> <li>Add validation for <code>.whl</code> filename (#2147).</li> <li>Trim whitespaces in <code>DatasetCreateV2.zarr_dir</code> (#2138).</li> <li>Support sending emails upon new OAuth signup (#2150).</li> </ul> </li> <li>App:<ul> <li>Introduce configuration for email settings (#2150).</li> </ul> </li> <li>Command-line interface:<ul> <li>Add <code>fractalctl email-settings</code> (#2150).</li> </ul> </li> <li>Dependencies:<ul> <li>Add direct dependency on <code>cryptography</code> (#2150).</li> </ul> </li> <li>Testing:<ul> <li>Introduce <code>mailpit</code>-based end-to-end test of email sending (#2150).</li> </ul> </li> </ul>"},{"location":"changelog/#2102","title":"2.10.2","text":"<ul> <li>App:<ul> <li>Add <code>FRACTAL_PIP_CACHE_DIR</code> configuration variable (#2141).</li> </ul> </li> <li>Tasks life cycle:<ul> <li>Prevent deactivation of task groups with <code>\"github.com\"</code> in pip-freeze information (#2144).</li> </ul> </li> <li>Runner:<ul> <li>Handle early shutdown for sudo SLURM executor (#2132).</li> <li>Fix repeated setting of <code>timestamp_ended</code> in task-group reactivation (#2140).</li> </ul> </li> </ul>"},{"location":"changelog/#2101","title":"2.10.1","text":"<ul> <li>API:<ul> <li>Add <code>POST /api/v2/project/{p_id}/workflow/{w_id}/wftask/replace-task/</code> endpoint (#2129).</li> </ul> </li> <li>Testing:<ul> <li>Use system postgresql in GitHub actions, rather than independent container (#2199).</li> </ul> </li> </ul>"},{"location":"changelog/#2100","title":"2.10.0","text":"<ul> <li>API:<ul> <li>Major update of <code>POST /api/v2/task/collect/pip/</code>, to support wheel-file upload (#2113).</li> </ul> </li> <li>Testing:<ul> <li>Add test of private task collection (#2126).</li> </ul> </li> </ul>"},{"location":"changelog/#292","title":"2.9.2","text":"<ul> <li>API<ul> <li>Remove <code>cache_dir</code> and use <code>project_dir/.fractal_cache</code> (#2121).</li> </ul> </li> <li>Docs<ul> <li>Improve docstrings and reduce mkdocs warnings (#2122).</li> </ul> </li> </ul>"},{"location":"changelog/#291","title":"2.9.1","text":"<ul> <li>Task collection:<ul> <li>Fix bug in wheel-based SSH task-collection (#2119).</li> </ul> </li> <li>Testing:<ul> <li>Re-include a specific test previously skipped for Python 3.12 (#2114).</li> <li>Add metadata to <code>fractal-tasks-mock</code> package (#2117).</li> </ul> </li> <li>Docs:<ul> <li>Add info about working versions.</li> </ul> </li> </ul>"},{"location":"changelog/#290","title":"2.9.0","text":"<p>WARNING 1: This version drops support for sqlite, and removes the configuration variables <code>DB_ENGINE</code> and <code>SQLITE_PATH</code>.</p> <p>WARNING 2: This version removes the <code>CollectionStateV2</code> database table. Make sure you have a database dump before running <code>fractalctl set-db</code>, since this operation cannot be undone.</p> <ul> <li>API<ul> <li>Remove <code>GET /api/v2/task/collect/{state_id}/</code> endpoint (#2010).</li> <li>Remove <code>active</code> property from <code>PATCH /api/v2/task-group/{task_group_id}/</code> (#2033).</li> <li>Add <code>GET /api/v2/task-group/activity/</code> endpoint (#2005, #2027).</li> <li>Add <code>GET /api/v2/task-group/activity/{task_group_activity_id}/</code> endpoint (#2005).</li> <li>Add <code>GET /admin/v2/task-group/activity/</code> endpoint (#2005, #2027).</li> <li>Add <code>POST /api/v2/task-group/{task_group_id}/{deactivate|reactivate}</code> endpoints (#2033, #2066, #2078).</li> <li>Add <code>POST /admin/v2/task-group/{task_group_id}/{deactivate|reactivate}</code> endpoints (#2062, #2078).</li> <li>Remove <code>GET /auth/current-user/viewer-paths/</code> (#2096).</li> <li>Add <code>GET /auth/current-user/allowed-viewer-paths/</code>, with logic for <code>fractal-vizarr-viewer</code> authorization (#2096).</li> <li>Add <code>category</code>, <code>modality</code> and <code>author</code> query parameters to <code>GET /admin/v2/task/</code> (#2102).</li> <li>Add <code>POST /auth/group/{group_id}/add-user/{user_id}/</code> (#2101).</li> <li>Add <code>POST /auth/group/{group_id}/remove-user/{user_id}/</code> (#2101, #2111).</li> <li>Add <code>POST /auth/users/{user_id}/set-groups/</code> (#2106).</li> <li>Remove <code>new_user_ids</code> property from <code>PATCH /auth/group/{group_id}/</code> (#2101).</li> <li>Remove <code>new_group_ids</code> property from <code>PATCH /auth/users/{user_id}/</code> (#2106).</li> <li>Internals:</li> <li>Fix bug in <code>_get_collection_task_group_activity_status_message</code> (#2047).</li> <li>Remove <code>valutc</code> validator for timestamps from API schemas, since it does not match with <code>psycopg3</code> behavior (#2064).</li> <li>Add query parameters <code>timestamp_last_used_{min|max}</code> to <code>GET /admin/v2/task-group/</code> (#2061).</li> <li>Remove <code>_convert_to_db_timestamp</code> and add <code>_raise_if_naive_datetime</code>: now API only accepts timezone-aware datetimes as query parameters (#2068).</li> <li>Remove <code>_encode_as_utc</code>: now timestamps are serialized in JSONs with their own timezone (#2081).</li> </ul> </li> <li>Database<ul> <li>Drop support for sqlite, and remove the <code>DB_ENGINE</code> and <code>SQLITE_PATH</code> configuration variables (#2052).</li> <li>Add <code>TaskGroupActivityV2</code> table (#2005).</li> <li>Drop <code>CollectionStateV2</code> table (#2010).</li> <li>Add <code>TaskGroupV2.pip_freeze</code> nullable column (#2017).</li> <li>Add  <code>venv_size_in_kB</code> and <code>venv_file_number</code> to <code>TaskGroupV2</code> (#2034).</li> <li>Add <code>TaskGroupV2.timestamp_last_used</code> column, updated on job submission (#2049, #2061, #2086).</li> </ul> </li> <li>Task-lifecycle internals:<ul> <li>Refactor task collection and database-session management in background tasks (#2030).</li> <li>Update <code>TaskGroupActivityV2</code> objects (#2005).</li> <li>Update filename and path for task-collection scripts (#2008).</li> <li>Copy wheel file into <code>task_group.path</code> and update <code>task_group.wheel_path</code>, for local task collection (#2020).</li> <li>Set <code>TaskGroupActivityV2.timestamp_ended</code> when collections terminate (#2026).</li> <li>Refactor bash templates and add <code>install_from_freeze.sh</code> (#2029).</li> <li>Introduce background operations for local reactivate/deactivate (#2033).</li> <li>Introduce background operations for SSH reactivate/deactivate (#2066).</li> <li>Fix escaping of newlines within f-strings, in logs (#2028).</li> <li>Improve handling of task groups created before 2.9.0 (#2050).</li> <li>Add <code>TaskGroupCreateV2Strict</code> for task collections (#2080).</li> <li>Always create <code>script_dir_remote</code> in SSH lifecycle background tasks (#2089).</li> <li>Postpone setting <code>active=False</code> in task-group deactivation to after all preliminary checks (#2100).</li> </ul> </li> <li>Runner:<ul> <li>Improve error handling in <code>_zip_folder_to_file_and_remove</code> (#2057).</li> <li>Improve error handling in <code>FractalSlurmSSHExecutor</code> <code>handshake</code> method (#2083).</li> <li>Use the \"spawn\" start method for the multiprocessing context, for the <code>ProcessPoolExecutor</code>-based runner (#2084).</li> <li>Extract common functionalities from SLURM/sudo and SLURM/SSH executors (#2107).</li> </ul> </li> <li>SSH internals:<ul> <li>Add <code>FractalSSH.remote_exists</code> method (#2008).</li> <li>Drop <code>FractalSSH.{_get,_put}</code> wrappers of <code>SFTPClient</code> methods (#2077).</li> <li>Try re-opening the connection in <code>FractalSSH.check_connection</code> when an error occurs (#2035).</li> <li>Move <code>NoValidConnectionError</code> exception handling into <code>FractalSSH.log_and_raise</code> method (#2070).</li> <li>Improve closed-socket testing (#2076).</li> </ul> </li> <li>App:</li> <li>Add <code>FRACTAL_VIEWER_AUTHORIZATION_SCHEME</code> and <code>FRACTAL_VIEWER_BASE_FOLDER</code> configuration variables (#2096).</li> <li>Testing:<ul> <li>Drop <code>fetch-depth</code> from <code>checkout</code> in GitHub actions (#2039).</li> </ul> </li> <li>Scripts:<ul> <li>Introduce <code>scripts/export_v1_workflows.py</code> (#2043).</li> </ul> </li> <li>Dependencies:<ul> <li>Remove <code>passlib</code> dependency (#2112).</li> <li>Bump <code>fastapi-users</code> to v14, which includes switch to <code>pwdlib</code> (#2112).</li> </ul> </li> </ul>"},{"location":"changelog/#281","title":"2.8.1","text":"<ul> <li>API:<ul> <li>Validate all user-provided strings that end up in pip-install commands (#2003).</li> </ul> </li> </ul>"},{"location":"changelog/#280","title":"2.8.0","text":"<ul> <li>Task collection<ul> <li>Now both the local and SSH versions of the task collection use the bash templates (#1980).</li> <li>Update task-collections database logs incrementally (#1980).</li> <li>Add <code>TaskGroupV2.pinned_package_versions_string</code> property (#1980).</li> <li>Support pinned-package versions for SSH task collection (#1980).</li> <li>Now <code>pip install</code> uses <code>--no-cache</code> (#1980).</li> </ul> </li> <li>API<ul> <li>Deprecate the <code>verbose</code> query parameter in <code>GET /api/v2/task/collect/{state_id}/</code> (#1980).</li> <li>Add <code>project_dir</code> attribute to <code>UserSettings</code> (#1990).</li> <li>Set a default for <code>DatasetV2.zarr_dir</code> (#1990).</li> <li>Combine the <code>args_schema_parallel</code> and <code>args_schema_non_parallel</code> query parameters in <code>GET /api/v2/task/</code> into a single parameter <code>args_schema</code> (#1998).</li> </ul> </li> </ul>"},{"location":"changelog/#271","title":"2.7.1","text":"<p>WARNING: As of this version, all extras for <code>pip install</code> are deprecated and the corresponding dependencies become required.</p> <ul> <li>Database:<ul> <li>Drop <code>TaskV2.owner</code> column (#1977).</li> <li>Make <code>TaskV2.taskgroupv2_id</code> column required (#1977).</li> </ul> </li> <li>Dependencies:<ul> <li>Make <code>psycopg[binary]</code> dependency required, and drop <code>postgres-pyscopg-binary</code> extra (#1970).</li> <li>Make <code>gunicorn</code> dependency required, and drop <code>gunicorn</code> extra (#1970).</li> </ul> </li> <li>Testing:<ul> <li>Switch from SQLite to Postgres in the OAuth Github action (#1981).</li> </ul> </li> </ul>"},{"location":"changelog/#270","title":"2.7.0","text":"<p>WARNING: This release comes with several specific notes:</p> <ol> <li>It requires running <code>fractalctl update-db-data</code> (after <code>fractalctl set-db</code>).</li> <li>When running <code>fractalctl update-db-data</code>, the environment variable    <code>FRACTAL_V27_DEFAULT_USER_EMAIL</code> must be set, e.g. as in    <code>FRACTAL_V27_DEFAULT_USER_EMAIL=admin@fractal.yx fractalctl    update-db-data</code>. This user must exist, and they will own all    previously-common tasks/task-groups.</li> <li>The pip extra <code>postgres</code> is deprecated, in favor of <code>postgres-psycopg-binary</code>.</li> <li>The configuration variable <code>DB_ENGINE=\"postgres\"</code> is deprecated, in favor of <code>DB_ENGINE=\"postgres-psycopg\"</code>.</li> <li>Python3.9 is deprecated.</li> </ol> <ul> <li>API:<ul> <li>Users and user groups:<ul> <li>Replace <code>UserRead.group_names</code> and <code>UserRead.group_ids</code> with <code>UserRead.group_ids_names</code> ordered list (#1844, #1850).</li> <li>Deprecate <code>GET /auth/group-names/</code> (#1844).</li> <li>Add <code>DELETE /auth/group/{id}/</code> endpoint (#1885).</li> <li>Add <code>PATCH auth/group/{group_id}/user-settings/</code> bulk endpoint (#1936).</li> </ul> </li> <li>Task groups:<ul> <li>Introduce <code>/api/v2/task-group/</code> routes (#1817, #1847, #1852, #1856, #1943).</li> <li>Respond with 422 error when any task-creating endpoint would break a non-duplication constraint (#1861).</li> <li>Enforce non-duplication constraints on <code>TaskGroupV2</code> (#1865).</li> <li>Fix non-duplication check in <code>PATCH /api/v2/task-group/{id}/</code> (#1911).</li> <li>Add cascade operations to <code>DELETE /api/v2/task-group/{task_group_id}/</code> and to <code>DELETE /admin/v2/task-group/{task_group_id}/</code> (#1867).</li> <li>Expand use and validators for <code>TaskGroupCreateV2</code> schema (#1861).</li> <li>Do not process task <code>source</code>s in task/task-group CRUD operations (#1861).</li> <li>Do not process task <code>owner</code>s in task/task-group CRUD operations (#1861).</li> </ul> </li> <li>Tasks:<ul> <li>Drop <code>TaskCreateV2.source</code> (#1909).</li> <li>Drop <code>TaskUpdateV2.version</code> (#1905).</li> <li>Revamp access-control for <code>/api/v2/task/</code> endpoints, based on task-group attributes (#1817).</li> <li>Update <code>/api/v2/task/</code> endpoints and schemas with new task attributes (#1856).</li> <li>Forbid changing <code>TaskV2.name</code> (#1925).</li> </ul> </li> <li>Task collection:<ul> <li>Improve preliminary checks in task-collection endpoints (#1861).</li> <li>Refactor split between task-collection endpoints and background tasks (#1861).</li> <li>Create <code>TaskGroupV2</code> object within task-collection endpoints (#1861).</li> <li>Fix response of task-collection endpoint (#1902).</li> <li>Automatically discover PyPI package version if missing or invalid (#1858, #1861, #1902).</li> <li>Use appropriate log-file path in collection-status endpoint (#1902).</li> <li>Add task <code>authors</code> to manifest schema (#1856).</li> <li>Do not use <code>source</code> for custom task collection (#1893).</li> <li>Rename custom-task-collection request-body field from <code>source</code> to <code>label</code> (#1896).</li> <li>Improve error messages from task collection (#1913).</li> <li>Forbid non-unique task names in <code>ManifestV2</code> (#1925).</li> </ul> </li> <li>Workflows and workflow tasks:<ul> <li>Introduce additional checks in POST-workflowtask endpoint, concerning non-active or non-accessible tasks (#1817).</li> <li>Introduce additional intormation in GET-workflow endpoint, concerning non-active or non-accessible tasks (#1817).</li> <li>Introduce additional intormation in PATCH-workflow endpoint, concerning non-active or non-accessible tasks (#1868, #1869).</li> <li>Stop logging warnings for non-common tasks in workflow export (#1893).</li> <li>Drop <code>WorkflowTaskCreateV2.order</code> (#1906).</li> <li>Update endpoints for workflow import/export  (#1925, #1939, #1960).</li> </ul> </li> <li>Datasets:<ul> <li>Remove <code>TaskDumpV2.owner</code> attribute (#1909).</li> </ul> </li> <li>Jobs:<ul> <li>Prevent job submission if includes non-active or non-accessible tasks (#1817).</li> <li>Remove rate limit for <code>POST /project/{project_id}/job/submit/</code> (#1944).</li> </ul> </li> <li>Admin:<ul> <li>Remove <code>owner</code> from <code>GET admin/v2/task/</code> (#1909).</li> <li>Deprecate <code>kind</code> query parameter for <code>/admin/v2/task/</code> (#1893).</li> <li>Add <code>origin</code> and <code>pkg_name</code> query parameters to <code>GET /admin/v2/task-group/</code> (#1979).</li> </ul> </li> <li>Schemas:<ul> <li>Forbid extras in <code>TaskCollectPipV2</code> (#1891).</li> <li>Forbid extras in all Create/Update/Import schemas (#1895).</li> <li>Deprecate internal <code>TaskCollectPip</code> schema in favor of <code>TaskGroupV2</code> (#1861).</li> </ul> </li> </ul> </li> <li>Database:<ul> <li>Introduce <code>TaskGroupV2</code> table (#1817, #1856).</li> <li>Add  <code>timestamp_created</code> column to <code>LinkUserGroup</code> table (#1850).</li> <li>Add <code>TaskV2</code> attributes <code>authors</code>, <code>tags</code>, <code>category</code> and <code>modality</code> (#1856).</li> <li>Add <code>update-db-data</code> script (#1820, #1888).</li> <li>Add <code>taskgroupv2_id</code> foreign key to <code>CollectionStateV2</code> (#1867).</li> <li>Make <code>TaskV2.source</code> nullable and drop its uniqueness constraint (#1861).</li> <li>Add <code>TaskGroupV2</code> columns <code>wheel_path</code>, <code>pinned_package_versions</code> (#1861).</li> <li>Clean up <code>alembic</code> migration scripts (#1894).</li> <li>Verify task-group non-duplication constraint in <code>2.7.0</code> data-migration script (#1927).</li> <li>Normalize <code>pkg_name</code> in <code>2.7.0</code> data-migration script (#1930).</li> <li>Deprecate <code>DB_ENGINE=\"postgres\"</code> configuration variable (#1946).</li> </ul> </li> <li>Runner:<ul> <li>Do not create local folders with 755 permissions unless <code>FRACTAL_BACKEND_RUNNER=\"slurm\"</code> (#1923).</li> <li>Fix bug of SSH/SFTP commands not acquiring lock (#1949).</li> <li>Fix bug of unhandled exception in SSH/SLURM executor (#1963).</li> <li>Always remove task-subfolder compressed archive (#1949).</li> </ul> </li> <li>Task collection:<ul> <li>Create base directory (in SSH mode), if missing (#1949).</li> <li>Fix bug of SSH/SFTP commands not acquiring lock (#1949).</li> </ul> </li> <li>SSH:<ul> <li>Improve logging for SSH-connection-locking flow (#1949).</li> <li>Introduce <code>FractalSSH.fetch_file</code> and <code>FractalSSH.read_remote_json_file</code> (#1949).</li> <li>Use <code>paramiko.sftp_client.SFTPClient</code> methods directly rathen than <code>fabric</code> wrappers (#1949).</li> <li>Disable prefetching for <code>SFTPClient.get</code> (#1949).</li> </ul> </li> <li>Internal:<ul> <li>Update <code>_create_first_group</code> so that it only searches for <code>UserGroups</code> with a given name (#1964).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump fastapi to <code>0.115</code> (#1942).</li> <li>Remove pip extra <code>postgres</code>, corresponding to <code>psycopg2+asyncpg</code> (#1946).</li> <li>Deprecate python3.9 (#1946).</li> </ul> </li> <li>Testing:<ul> <li>Benchmark <code>GET /api/v2/task-group/</code> (#1922).</li> <li>Use new <code>ubuntu22-slurm-multipy</code> image, with Python3.12 and with Python-version specific venvs (#1946, #1969).</li> <li>Get <code>DB_ENGINE</code> variable from <code>os.environ</code> rather than from installed packages (#1968).</li> </ul> </li> </ul>"},{"location":"changelog/#264","title":"2.6.4","text":"<ul> <li>Database<ul> <li>Fix use of naming convention for database schema-migration scripts (#1819).</li> </ul> </li> <li>Testing:<ul> <li>Test <code>alembic downgrade base</code> (#1819).</li> <li>Add <code>GET /api/v2/task/</code> to benchmarks (#1825).</li> </ul> </li> </ul>"},{"location":"changelog/#263","title":"2.6.3","text":"<ul> <li>API:<ul> <li>Introduce <code>GET /auth/current-user/viewer-paths/</code> endpoint (#1816).</li> <li>Add <code>viewer_paths</code> attribute to <code>UserGroup</code> endpoints (#1816).</li> </ul> </li> <li>Database:<ul> <li>Add  <code>viewer_paths</code> column to <code>UserGroup</code> table (#1816).</li> </ul> </li> <li>Runner:<ul> <li>Anticipate <code>wait_thread.shutdown_callback</code> assignment in <code>FractalSlurmExecutor</code>, to avoid an uncaught exception (#1815).</li> </ul> </li> </ul>"},{"location":"changelog/#262","title":"2.6.2","text":"<ul> <li>Allow setting <code>UserSettings</code> attributes to <code>None</code> in standard/strict PATCH endpoints (#1814).</li> </ul>"},{"location":"changelog/#261","title":"2.6.1","text":"<ul> <li>App (internal):<ul> <li>Remove <code>FRACTAL_SLURM_SSH_HOST</code>, <code>FRACTAL_SLURM_SSH_USER</code>, <code>FRACTAL_SLURM_SSH_PRIVATE_KEY_PATH</code> and <code>FRACTAL_SLURM_SSH_WORKING_BASE_DIR</code> from <code>Settings</code>  (#1804).</li> </ul> </li> <li>Database:<ul> <li>Drop <code>slurm_user</code>, <code>slurm_accounts</code> and <code>cache_dir</code> columns from <code>UserOAuth</code> (#1804)</li> </ul> </li> </ul>"},{"location":"changelog/#260","title":"2.6.0","text":"<p>WARNING: This release requires running <code>fractalctl update-db-data</code> (after <code>fractalctl set-db</code>).</p> <ul> <li>API:<ul> <li>Introduce user-settings API, in <code>/auth/users/{user_id}/settings/</code> and <code>/auth/current-user/settings/</code> (#1778, #1807).</li> <li>Add the creation of empty settings to <code>UserManager.on_after_register</code> hook (#1778).</li> <li>Remove deprecated user's attributes (<code>slurm_user</code>, <code>cache_dir</code>, <code>slurm_accounts</code>) from API, in favor of new <code>UserSetting</code> ones (#1778).</li> <li>Validate user settings in endpoints that rely on them (#1778).</li> <li>Propagate user settings to background tasks when needed (#1778).</li> </ul> </li> <li>Database:<ul> <li>Introduce new <code>user_settings</code> table, and link it to <code>user_oauth</code> (#1778).</li> </ul> </li> <li>Internal:</li> <li>Remove redundant string validation in <code>FractalSSH.remove_folder</code> and <code>TaskCollectCustomV2</code> (#1810).</li> <li>Make <code>validate_cmd</code> more strict about non-string arguments (#1810).</li> </ul>"},{"location":"changelog/#252","title":"2.5.2","text":"<ul> <li>App:<ul> <li>Replace <code>fractal_ssh</code> attribute with <code>fractal_ssh_list</code>, in <code>app.state</code> (#1790).</li> <li>Move creation of SSH connections from app startup to endpoints (#1790).</li> </ul> </li> <li>Internal<ul> <li>Introduce <code>FractalSSHList</code>, in view of support for multiple SSH/Slurm service users (#1790).</li> <li>Make <code>FractalSSH.close()</code> more aggressively close <code>Transport</code> attribute (#1790).</li> <li>Set <code>look_for_keys=False</code> for paramiko/fabric connection (#1790).</li> </ul> </li> <li>Testing:<ul> <li>Add fixture to always test that threads do not accumulate during tests (#1790).</li> </ul> </li> </ul>"},{"location":"changelog/#251","title":"2.5.1","text":"<ul> <li>API:<ul> <li>Make <code>WorkflowTaskDumpV2</code> attributes <code>task_id</code> and <code>task</code> optional (#1784).</li> <li>Add validation for user-provided strings that execute commands with subprocess or remote-shell (#1767).</li> </ul> </li> <li>Runner and task collection:<ul> <li>Validate commands before running them via <code>subprocess</code> or <code>fabric</code> (#1767).</li> </ul> </li> </ul>"},{"location":"changelog/#250","title":"2.5.0","text":"<p>WARNING: This release has a minor API bug when displaying a V2 dataset with a history that contains legacy tasks. It's recommended to update to 2.5.1.</p> <p>This release removes support for including V1 tasks in V2 workflows. This comes with changes to the database (data and metadata), to the API, and to the V2 runner.</p> <ul> <li>Runner:<ul> <li>Deprecate running v1 tasks within v2 workflows (#1721).</li> </ul> </li> <li>Database:<ul> <li>Remove <code>Task.is_v2_compatible</code> column (#1721).</li> <li>For table <code>WorkflowTaskV2</code>, drop <code>is_legacy_task</code> and <code>task_legacy_id</code> columns, remove <code>task_legacy</code> ORM attribute, make <code>task_id</code> required, make <code>task</code> required (#1721).</li> </ul> </li> <li>API:<ul> <li>Drop v1-v2-task-compatibility admin endpoint (#1721).</li> <li>Drop <code>/task-legacy/</code> endpoint (#1721).</li> <li>Remove legacy task code branches from <code>WorkflowTaskV2</code> CRUD endpoints (#1721).</li> <li>Add OAuth accounts info to <code>UserRead</code> at <code>.oauth_accounts</code> (#1765).</li> </ul> </li> <li>Testing:<ul> <li>Improve OAuth Github Action to test OAuth account flow (#1765).</li> </ul> </li> </ul>"},{"location":"changelog/#242","title":"2.4.2","text":"<ul> <li>App:<ul> <li>Improve logging in <code>fractalctl set-db</code> (#1764).</li> </ul> </li> <li>Runner:<ul> <li>Add <code>--set-home</code> to <code>sudo -u</code> impersonation command, to fix Ubuntu18 behavior (#1762).</li> </ul> </li> <li>Testing:<ul> <li>Start tests of migrations from valid v2.4.0 database (#1764).</li> </ul> </li> </ul>"},{"location":"changelog/#241","title":"2.4.1","text":"<p>This is mainly a bugfix release, re-implementing a check that was removed in 2.4.0.</p> <ul> <li>API:<ul> <li>Re-introduce check for existing-user-email in <code>PATCH /auth/users/{id}/</code> (#1760).</li> </ul> </li> </ul>"},{"location":"changelog/#240","title":"2.4.0","text":"<p>This release introduces support for user groups, but without linking it to any access-control rules (which will be introduced later).</p> <p>NOTE: This release requires running the <code>fractalctl update-db-data</code> script.</p> <ul> <li>App:<ul> <li>Move creation of first user from application startup into <code>fractalctl set-db</code> command (#1738, #1748).</li> <li>Add creation of default user group into <code>fractalctl set-db</code> command (#1738).</li> <li>Create <code>update-db-script</code> for current version, that adds all users to default group (#1738).</li> </ul> </li> <li>API:<ul> <li>Added <code>/auth/group/</code> and <code>/auth/group-names/</code> routers (#1738, #1752).</li> <li>Implement <code>/auth/users/{id}/</code> POST/PATCH routes in <code>fractal-server</code> (#1738, #1747, #1752).</li> <li>Introduce <code>UserUpdateWithNewGroupIds</code> schema for <code>PATCH /auth/users/{id}/</code> (#1747, #1752).</li> <li>Add <code>UserManager.on_after_register</code> hook to add new users to default user group (#1738).</li> </ul> </li> <li>Database:<ul> <li>Added new <code>usergroup</code> and <code>linkusergroup</code> tables (#1738).</li> </ul> </li> <li>Internal<ul> <li>Refactored <code>fractal_server.app.auth</code> and <code>fractal_server.app.security</code> (#1738)/</li> <li>Export all relevant modules in <code>app.models</code>, since it matters e.g. for <code>autogenerate</code>-ing migration scripts (#1738).</li> </ul> </li> <li>Testing<ul> <li>Add <code>UserGroup</code> validation to <code>scripts/validate_db_data_with_read_schemas.py</code> (#1746).</li> </ul> </li> </ul>"},{"location":"changelog/#2311","title":"2.3.11","text":"<ul> <li>SSH runner:<ul> <li>Move remote-folder creation from <code>submit_workflow</code> to more specific <code>_process_workflow</code> (#1728).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add <code>GET /auth/token/login/</code> to tested endpoints (#1720).</li> </ul> </li> <li>Testing:<ul> <li>Update GitHub actions <code>upload-artifact</code> and <code>download-artifact</code> to <code>v4</code> (#1725).</li> </ul> </li> </ul>"},{"location":"changelog/#2310","title":"2.3.10","text":"<ul> <li>Fix minor bug in zipping-job logging (#1716).</li> </ul>"},{"location":"changelog/#239","title":"2.3.9","text":"<ul> <li>Add logging for zipping-job-folder operations (#1714).</li> </ul>"},{"location":"changelog/#238","title":"2.3.8","text":"<p>NOTE: <code>FRACTAL_API_V1_MODE=\"include_without_submission\"</code> is now transformed into <code>FRACTAL_API_V1_MODE=\"include_read_only\"</code>.</p> <ul> <li>API:<ul> <li>Support read-only mode for V1 (#1701).</li> <li>Improve handling of zipped job-folder in download-logs endpoints (#1702).</li> </ul> </li> <li>Runner:<ul> <li>Improve database-error handling in V2 job execution (#1702).</li> <li>Zip job folder after job execution (#1702).</li> </ul> </li> <li>App:<ul> <li><code>UvicornWorker</code> is now imported from <code>uvicorn-worker</code> (#1690).</li> </ul> </li> <li>Testing:<ul> <li>Remove <code>HAS_LOCAL_SBATCH</code> variable and related if-branches (#1699).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add <code>GET /auth/current-user/</code> to tested endpoints (#1700).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>mkdocstrings</code> to <code>^0.25.2</code> (#1707).</li> <li>Update <code>fastapi</code> to <code>^0.112.0</code> (#1705).</li> </ul> </li> </ul>"},{"location":"changelog/#237","title":"2.3.7","text":"<ul> <li>SSH SLURM executor:<ul> <li>Handle early shutdown in SSH executor (#1696).</li> </ul> </li> <li>Task collection:<ul> <li>Introduce a new configuration variable <code>FRACTAL_MAX_PIP_VERSION</code> to pin task-collection pip (#1675).</li> </ul> </li> </ul>"},{"location":"changelog/#236","title":"2.3.6","text":"<ul> <li>API:<ul> <li>When creating a WorkflowTask, do not pre-populate its top-level arguments based on JSON Schema default values (#1688).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>sqlmodel</code> to <code>^0.0.21</code> (#1674).</li> <li>Add <code>uvicorn-worker</code> (#1690).</li> </ul> </li> </ul>"},{"location":"changelog/#235","title":"2.3.5","text":"<p>WARNING: The <code>pre_submission_commands</code> SLURM configuration is included as an experimental feature, since it is still not useful for its main intended goal (calling <code>module load</code> before running <code>sbatch</code>).</p> <ul> <li>SLURM runners:<ul> <li>Expose <code>gpus</code> SLURM parameter (#1678).</li> <li>For SSH executor, add <code>pre_submission_commands</code> (#1678).</li> <li>Removed obsolete arguments from <code>get_slurm_config</code> function (#1678).</li> </ul> </li> <li>SSH features:<ul> <li>Add <code>FractalSSH.write_remote_file</code> method (#1678).</li> </ul> </li> </ul>"},{"location":"changelog/#234","title":"2.3.4","text":"<ul> <li>SSH SLURM runner:<ul> <li>Refactor <code>compress_folder</code> and <code>extract_archive</code> modules, and stop using <code>tarfile</code> library (#1641).</li> </ul> </li> <li>API:<ul> <li>Introduce <code>FRACTAL_API_V1_MODE=include_without_submission</code> to include V1 API but forbid job submission (#1664).</li> </ul> </li> <li>Testing:<ul> <li>Do not test V1 API with <code>DB_ENGINE=\"postgres-psycopg\"</code> (#1667).</li> <li>Use new Fractal SLURM containers in CI (#1663).</li> <li>Adapt tests so that they always refer to the current Python version (the one running <code>pytest</code>), when needed; this means that we don't require the presence of any additional Python version in the development environment, apart from the current one (#1633).</li> <li>Include Python3.11 in some tests (#1669).</li> <li>Simplify CI SLURM Dockerfile after base-image updates (#1670).</li> <li>Cache <code>ubuntu22-slurm-multipy</code> Docker image in CI (#1671).</li> <li>Add <code>oauth.yaml</code> GitHub action to test OIDC authentication (#1665).</li> </ul> </li> </ul>"},{"location":"changelog/#233","title":"2.3.3","text":"<p>This release fixes a SSH-task-collection bug introduced in version 2.3.1.</p> <ul> <li>API:<ul> <li>Expose new superuser-restricted endpoint <code>GET /api/settings/</code> (#1662).</li> </ul> </li> <li>SLURM runner:<ul> <li>Make <code>FRACTAL_SLURM_SBATCH_SLEEP</code> configuration variable <code>float</code> (#1658).</li> </ul> </li> <li>SSH features:<ul> <li>Fix wrong removal of task-package folder upon task-collection failure (#1649).</li> <li>Remove <code>FractalSSH.rename_folder</code> method (#1654).</li> </ul> </li> <li>Testing:<ul> <li>Refactor task-collection fixtures (#1637).</li> </ul> </li> </ul>"},{"location":"changelog/#232","title":"2.3.2","text":"<p>WARNING: The remove-remote-venv-folder in the SSH task collection is broken (see issue 1633). Do not deploy this version in an SSH-based <code>fractal-server</code> instance.</p> <ul> <li>API:<ul> <li>Fix incorrect zipping of structured job-log folders (#1648).</li> </ul> </li> </ul>"},{"location":"changelog/#231","title":"2.3.1","text":"<p>This release includes a bugfix for task names with special characters.</p> <p>WARNING: The remove-remote-venv-folder in the SSH task collection is broken (see issue 1633). Do not deploy this version in an SSH-based <code>fractal-server</code> instance.</p> <ul> <li>Runner:<ul> <li>Improve sanitization of subfolder names (commits from 3d89d6ba104d1c6f11812bc9de5cbdff25f81aa2 to 426fa3522cf2eef90d8bd2da3b2b8a5b646b9bf4).</li> </ul> </li> <li>API:<ul> <li>Improve error message when task-collection Python is not defined (#1640).</li> <li>Use a single endpoint for standard and SSH task collection (#1640).</li> </ul> </li> <li>SSH features:<ul> <li>Remove remote venv folder upon failed task collection in SSH mode (#1634, #1640).</li> <li>Refactor <code>FractalSSH</code> (#1635).</li> <li>Set <code>fabric.Connection.forward_agent=False</code> (#1639).</li> </ul> </li> <li>Testing:<ul> <li>Improved testing of SSH task-collection API (#1640).</li> <li>Improved testing of <code>FractalSSH</code> methods (#1635).</li> <li>Stop testing SQLite database for V1 in CI (#1630).</li> </ul> </li> </ul>"},{"location":"changelog/#230","title":"2.3.0","text":"<p>This release includes two important updates: 1. An Update update to task-collection configuration variables and logic. 2. The first released version of the experimental SSH features.</p> <p>Re: task-collection configuration, we now support two main use cases:</p> <ol> <li> <p>When running a production instance (including on a SLURM cluster), you    should set e.g. <code>FRACTAL_TASKS_PYTHON_DEFAULT_VERSION=3.10</code>, and make sure    that <code>FRACTAL_TASKS_PYTHON_3_10=/some/python</code> is an absolute path. Optionally,    you can define other variables like <code>FRACTAL_TASKS_PYTHON_3_9</code>,    <code>FRACTAL_TASKS_PYTHON_3_11</code> or <code>FRACTAL_TASKS_PYTHON_3_12</code>.</p> </li> <li> <p>If you leave <code>FRACTAL_TASKS_PYTHON_DEFAULT_VERSION</code> unset, then only the    Python interpreter that is currently running <code>fractal-server</code> can be used    for task collection.</p> </li> </ol> <p>WARNING: If you don't set <code>FRACTAL_TASKS_PYTHON_DEFAULT_VERSION</code>, then you will only have a single Python interpreter available for tasks (namely the one running <code>fractal-server</code>).</p> <ul> <li>API:<ul> <li>Introduce <code>api/v2/task/collect/custom/</code> endpoint (#1607, #1613, #1617, #1629).</li> </ul> </li> <li>Task collection:<ul> <li>Introduce task-collection Python-related configuration variables (#1587).</li> <li>Always set Python version for task collection, and only use <code>FRACTAL_TASKS_PYTHON_X_Y</code> variables (#1587).</li> <li>Refactor task-collection functions and schemas (#1587, #1617).</li> <li>Remove <code>TaskCollectStatusV2</code> and <code>get_collection_data</code> internal schema/function (#1598).</li> <li>Introduce <code>CollectionStatusV2</code> enum for task-collection status (#1598).</li> <li>Reject task-collection request if it includes a wheel file and a version (#1608). SSH features:</li> <li>Introduce <code>fractal_server/ssh</code> subpackage (#1545, #1599, #1611).</li> <li>Introduce SSH executor and runner (#1545).</li> <li>Introduce SSH task collection (#1545, #1599, #1626).</li> <li>Introduce SSH-related configuration variables (#1545).</li> <li>Modify app lifespan to handle SSH connection (#1545).</li> <li>Split <code>app/runner/executor/slurm</code> into <code>sudo</code> and <code>ssh</code> subfolders (#1545).</li> <li>Introduce FractalSSH object which is a wrapper class around fabric.Connection object. It provides a <code>lock</code> to avoid loss of ssh instructions and a custom timeout (#1618)</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>sqlmodel</code> to <code>^0.0.19</code> (#1584).</li> <li>Update <code>pytest-asyncio</code> to <code>^0.23</code> (#1558).</li> </ul> </li> <li>Testing:<ul> <li>Test the way <code>FractalProcessPoolExecutor</code> spawns processes and threads (#1579).</li> <li>Remove <code>event_loop</code> fixture: every test will run on its own event loop (#1558).</li> <li>Test task collection with non-canonical package name (#1602).</li> </ul> </li> </ul>"},{"location":"changelog/#220","title":"2.2.0","text":"<p>This release streamlines options for the Gunicorn startup command, and includes two new experimental features.</p> <p>NOTE 1: you can now enable custom Gunicorn worker/logger by adding the following options to the <code>gunicorn</code> startup command: - <code>--worker-class fractal_server.gunicorn_fractal.FractalWorker</code> - <code>--logger-class fractal_server.gunicorn_fractal.FractalGunicornLogger</code></p> <p>NOTE 2: A new experimental local runner is available, which uses processes instead of threads and support shutdown. You can try it out with the configuration variable <code>FRACTAL_BACKEND_RUNNER=local_experimental</code></p> <p>NOTE 3: A new PostgreSQL database adapter is available, fully based on <code>psycopg3</code> (rather than <code>pyscopg2</code>+<code>asyncpg</code>). You can try it out with the configuration variable <code>DB_ENGINE=postgres-psycopg</code> (note that this requires the <code>pip install</code> extra <code>postgres-psycopg-binary</code>).</p> <ul> <li>API:<ul> <li>Add extensive logs to <code>DELETE /api/v2/project/{project_id}</code> (#1532).</li> <li>Remove catch of <code>IntegrityError</code> in <code>POST /api/v1/project</code> (#1530).</li> </ul> </li> <li>App and deployment:<ul> <li>Move <code>FractalGunicornLogger</code> and <code>FractalWorker</code> into <code>fractal_server/gunicorn_fractal.py</code> (#1535).</li> <li>Add custom gunicorn/uvicorn worker to handle SIGABRT signal (#1526).</li> <li>Store list of submitted jobs in app state (#1538).</li> <li>Add logic for graceful shutdown for job slurm executors (#1547).</li> </ul> </li> <li>Runner:<ul> <li>Change structure of job folders, introducing per-task subfolders (#1523).</li> <li>Rename internal <code>workflow_dir</code> and <code>workflow_dir_user</code> variables to local/remote (#1534).</li> <li>Improve handling of errors in <code>submit_workflow</code> background task (#1556, #1566).</li> <li>Add new <code>local_experimental</code> runner, based on <code>ProcessPoolExecutor</code> (#1544, #1566).</li> </ul> </li> <li>Database:<ul> <li>Add new Postgres adapter <code>psycopg</code> (#1562).</li> </ul> </li> <li>Dependencies<ul> <li>Add <code>fabric</code> to <code>dev</code> dependencies (#1518).</li> <li>Add new <code>postgres-psycopg-binary</code> extra (#1562).</li> </ul> </li> <li>Testing:<ul> <li>Extract <code>pytest-docker</code> fixtures into a dedicated module (#1516).</li> <li>Rename SLURM containers in CI (#1516).</li> <li>Install and run SSH daemon in CI containers (#1518).</li> <li>Add unit test of SSH connection via fabric/paramiko (#1518).</li> <li>Remove obsolete folders from <code>tests/data</code> (#1517).</li> </ul> </li> </ul>"},{"location":"changelog/#210","title":"2.1.0","text":"<p>This release fixes a severe bug where SLURM-executor auxiliary threads are not joined when a Fractal job ends.</p> <ul> <li>App:<ul> <li>Add missing join for <code>wait_thread</code> upon <code>FractalSlurmExecutor</code> exit (#1511).</li> <li>Replace <code>startup</code>/<code>shutdown</code> events with <code>lifespan</code> event (#1501).</li> </ul> </li> <li>API:<ul> <li>Remove <code>Path.resolve</code> from the submit-job endpoints and add validator for <code>Settings.FRACTAL_RUNNER_WORKING_BASE_DIR</code> (#1497).</li> </ul> </li> <li>Testing:<ul> <li>Improve dockerfiles for SLURM (#1495, #1496).</li> <li>Set short timeout for <code>docker compose down</code> (#1500).</li> </ul> </li> </ul>"},{"location":"changelog/#206","title":"2.0.6","text":"<p>NOTE: This version changes log formats. For <code>uvicorn</code> logs, this change requires no action. For <code>gunicorn</code>, logs formats are only changed by adding the following command-line option: <code>gunicorn ... --logger-class fractal_server.logger.gunicorn_logger.FractalGunicornLogger</code>.</p> <ul> <li>API:<ul> <li>Add <code>FRACTAL_API_V1_MODE</code> environment variable to include/exclude V1 API (#1480).</li> <li>Change format of uvicorn loggers (#1491).</li> <li>Introduce <code>FractalGunicornLogger</code> class (#1491).</li> </ul> </li> <li>Runner:<ul> <li>Fix missing <code>.log</code> files in server folder for SLURM jobs (#1479).</li> </ul> </li> <li>Database:<ul> <li>Remove <code>UserOAuth.project_list</code> and <code>UserOAuth.project_list_v2</code> relationships (#1482).</li> </ul> </li> <li>Dev dependencies:<ul> <li>Bump <code>pytest</code> to <code>8.1.*</code> (#1486).</li> <li>Bump <code>coverage</code> to <code>7.5.*</code> (#1486).</li> <li>Bump <code>pytest-docker</code> to <code>3.1.*</code> (#1486).</li> <li>Bump <code>pytest-subprocess</code> to <code>^1.5</code> (#1486).</li> </ul> </li> <li>Benchmarks:<ul> <li>Move <code>populate_db</code> scripts into <code>benchmark</code> folder (#1489).</li> </ul> </li> </ul>"},{"location":"changelog/#205","title":"2.0.5","text":"<ul> <li>API:<ul> <li>Add <code>GET /admin/v2/task/</code> (#1465).</li> <li>Improve error message in DELETE-task endpoint (#1471).</li> </ul> </li> <li>Set <code>JobV2</code> folder attributes from within the submit-job endpoint (#1464).</li> <li>Tests:<ul> <li>Make SLURM CI work on MacOS (#1476).</li> </ul> </li> </ul>"},{"location":"changelog/#204","title":"2.0.4","text":"<ul> <li>Add <code>FRACTAL_SLURM_SBATCH_SLEEP</code> configuration variable (#1467).</li> </ul>"},{"location":"changelog/#203","title":"2.0.3","text":"<p>WARNING: This update requires running a fix-db script, via <code>fractalctl update-db-data</code>.</p> <ul> <li>Database:<ul> <li>Create fix-db script to remove <code>images</code> and <code>history</code> from dataset dumps in V1/V2 jobs (#1456).</li> </ul> </li> <li>Tests:<ul> <li>Split <code>test_full_workflow_v2.py</code> into local/slurm files (#1454).</li> </ul> </li> </ul>"},{"location":"changelog/#202","title":"2.0.2","text":"<p>WARNING: Running this version on a pre-existing database (where the <code>jobsv2</code> table has some entries) is broken. Running this version on a freshly-created database works as expected.</p> <ul> <li>API:<ul> <li>Fix bug in status endpoint (#1449).</li> <li>Improve handling of out-of-scope scenario in status endpoint (#1449).</li> <li>Do not include dataset <code>history</code> in <code>JobV2.dataset_dump</code> (#1445).</li> <li>Forbid extra arguments in <code>DumpV2</code> schemas (#1445).</li> </ul> </li> <li>API V1:<ul> <li>Do not include dataset <code>history</code> in <code>ApplyWorkflow.{input,output}_dataset_dump</code> (#1453).</li> </ul> </li> <li>Move settings logs to <code>check_settings</code> and use fractal-server <code>set_logger</code> (#1452).</li> <li>Benchmarks:<ul> <li>Handle some more errors in benchmark flow (#1445).</li> </ul> </li> <li>Tests:<ul> <li>Update testing database to version 2.0.1 (#1445).</li> </ul> </li> </ul>"},{"location":"changelog/#201","title":"2.0.1","text":"<ul> <li>Database/API:<ul> <li>Do not include <code>dataset_dump.images</code> in <code>JobV2</code> table (#1441).</li> </ul> </li> <li>Internal functions:<ul> <li>Introduce more robust <code>reset_logger_handlers</code> function (#1425).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add <code>POST /api/v2/project/project_id/dataset/dataset_id/images/query/</code> in bechmarks  to evaluate the impact of the number of images during the query (#1441).</li> </ul> </li> <li>Development:<ul> <li>Use <code>poetry</code> 1.8.2 in GitHub actions and documentation.</li> </ul> </li> </ul>"},{"location":"changelog/#200","title":"2.0.0","text":"<p>Major update.</p>"},{"location":"changelog/#1410","title":"1.4.10","text":"<p>WARNING: Starting from this version, the dependencies for the <code>slurm</code> extra are required; commands like <code>pip install fractal-server[slurm,postgres]</code> must be replaced by <code>pip install fractal-server[postgres]</code>.</p> <ul> <li>Dependencies:<ul> <li>Make <code>clusterfutures</code> and <code>cloudpickle</code> required dependencies (#1255).</li> <li>Remove <code>slurm</code> extra from package (#1255).</li> </ul> </li> <li>API:<ul> <li>Handle invalid history file in <code>GET /project/{project_id}/dataset/{dataset_id}/status/</code> (#1259).</li> </ul> </li> <li>Runner:<ul> <li>Add custom <code>_jobs_finished</code> function to check the job status and to avoid squeue errors (#1266)</li> </ul> </li> </ul>"},{"location":"changelog/#149","title":"1.4.9","text":"<p>This release is a follow-up of 1.4.7 and 1.4.8, to mitigate the risk of job folders becoming very large.</p> <ul> <li>Runner:<ul> <li>Exclude <code>history</code> from <code>TaskParameters</code> object for parallel tasks, so that it does not end up in input pickle files (#1247).</li> </ul> </li> </ul>"},{"location":"changelog/#148","title":"1.4.8","text":"<p>This release is a follow-up of 1.4.7, to mitigate the risk of job folders becoming very large.</p> <ul> <li>Runner:<ul> <li>Exclude <code>metadata[\"image\"]</code> from <code>TaskParameters</code> object for parallel tasks, so that it does not end up in input pickle files (#1245).</li> <li>Exclude components list from <code>workflow.log</code> logs (#1245).</li> </ul> </li> <li>Database:<ul> <li>Remove spurious logging of <code>fractal_server.app.db</code> string (#1245).</li> </ul> </li> </ul>"},{"location":"changelog/#147","title":"1.4.7","text":"<p>This release provides a bugfix (PR 1239) and a workaround (PR 1238) for the SLURM runner, which became relevant for the use case of processing a large dataset (300 wells with 25 cycles each).</p> <ul> <li>Runner:<ul> <li>Do not include <code>metadata[\"image\"]</code> in JSON file with task arguments (#1238).</li> <li>Add <code>FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE</code> configuration variable, to define exceptions where tasks still require <code>metadata[\"image\"]</code> (#1238).</li> <li>Fix bug in globbing patterns, when copying files from user-side to server-side job folder in SLURM executor (#1239).</li> </ul> </li> <li>API:<ul> <li>Fix error message for rate limits in apply-workflow endpoint (#1231).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add more scenarios, as per issue #1184 (#1232).</li> </ul> </li> </ul>"},{"location":"changelog/#146","title":"1.4.6","text":"<ul> <li>API:<ul> <li>Add <code>GET /admin/job/{job_id}</code> (#1230).</li> <li>Handle <code>FileNotFound</code> in <code>GET /project/{project_id}/job/{job_id}/</code> (#1230).</li> </ul> </li> </ul>"},{"location":"changelog/#145","title":"1.4.5","text":"<ul> <li>Remove CORS middleware (#1228).</li> <li>Testing:<ul> <li>Fix <code>migrations.yml</code> GitHub action (#1225).</li> </ul> </li> </ul>"},{"location":"changelog/#144","title":"1.4.4","text":"<ul> <li>API:<ul> <li>Add rate limiting to <code>POST /{project_id}/workflow/{workflow_id}/apply/</code> (#1199).</li> <li>Allow users to read the logs of ongoing jobs with <code>GET /project/{project_id}/job/{job_id}/</code>, using <code>show_tmp_logs</code> query parameter (#1216).</li> <li>Add <code>log</code> query parameter in <code>GET {/api/v1/job/,/api/v1/{project.id}/job/,/admin/job/}</code>, to trim response body (#1218).</li> <li>Add <code>args_schema</code> query parameter in <code>GET /api/v1/task/</code> to trim response body (#1218).</li> <li>Add <code>history</code> query parameter in <code>GET {/api/v1/dataset/,/api/v1/project/{project.id}/dataset/}</code> to trim response body (#1219).</li> <li>Remove <code>task_list</code> from <code>job.workflow_dump</code> creation in <code>/api/v1/{project_id}/workflow/{workflow_id}/apply/</code>(#1219)</li> <li>Remove <code>task_list</code> from <code>WorkflowDump</code> Pydantic schema (#1219)</li> </ul> </li> <li>Dependencies:<ul> <li>Update fastapi to <code>^0.109.0</code> (#1222).</li> <li>Update gunicorn to <code>^21.2.0</code> (#1222).</li> <li>Update aiosqlite to <code>^0.19.0</code> (#1222).</li> <li>Update uvicorn to <code>^0.27.0</code> (#1222).</li> </ul> </li> </ul>"},{"location":"changelog/#143","title":"1.4.3","text":"<p>WARNING:</p> <p>This update requires running a fix-db script, via <code>fractalctl update-db-data</code>.</p> <ul> <li>API:<ul> <li>Improve validation of <code>UserCreate.slurm_accounts</code> (#1162).</li> <li>Add <code>timestamp_created</code> to <code>WorkflowRead</code>, <code>WorkflowDump</code>, <code>DatasetRead</code> and <code>DatasetDump</code> (#1152).</li> <li>Make all dumps in <code>ApplyWorkflowRead</code> non optional (#1175).</li> <li>Ensure that timestamps in <code>Read</code> schemas are timezone-aware, regardless of <code>DB_ENGINE</code> (#1186).</li> <li>Add timezone-aware timestamp query parameters to all <code>/admin</code> endpoints (#1186).</li> </ul> </li> <li>API (internal):<ul> <li>Change the class method <code>Workflow.insert_task</code> into the auxiliary function <code>_workflow_insert_task</code> (#1149).</li> </ul> </li> <li>Database:<ul> <li>Make <code>WorkflowTask.workflow_id</code> and <code>WorkflowTask.task_id</code> not nullable (#1137).</li> <li>Add <code>Workflow.timestamp_created</code> and <code>Dataset.timestamp_created</code> columns (#1152).</li> <li>Start a new <code>current.py</code> fix-db script (#1152, #1195).</li> <li>Add to <code>migrations.yml</code> a new script (<code>validate_db_data_with_read_schemas.py</code>) that validates test-DB data with Read schemas (#1187).</li> <li>Expose <code>fix-db</code> scripts via command-line option <code>fractalctl update-db-data</code> (#1197).</li> </ul> </li> <li>App (internal):<ul> <li>Check in <code>Settings</code> that <code>psycopg2</code>, <code>asyngpg</code> and <code>cfut</code>, if required, are installed (#1167).</li> <li>Split <code>DB.set_db</code> into sync/async methods (#1165).</li> <li>Rename <code>DB.get_db</code> into <code>DB.get_async_db</code> (#1183).</li> <li>Normalize names of task packages (#1188).</li> </ul> </li> <li>Testing:<ul> <li>Update <code>clean_db_fractal_1.4.1.sql</code> to <code>clean_db_fractal_1.4.2.sql</code>, and change <code>migrations.yml</code> target version (#1152).</li> <li>Reorganise the test directory into subdirectories, named according to the order in which we want the CI to execute them (#1166).</li> <li>Split the CI into two independent jobs, <code>Core</code> and <code>Runner</code>, to save time through parallelisation (#1204).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>python-dotenv</code> to version 0.21.0 (#1172).</li> </ul> </li> <li>Runner:<ul> <li>Remove <code>JobStatusType.RUNNING</code>, incorporating it into <code>JobStatusType.SUBMITTED</code> (#1179).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add <code>fractal_client.py</code> and <code>populate_script_v2.py</code> for creating different database status scenarios (#1178).</li> <li>Add a custom benchmark suite in <code>api_bench.py</code>.</li> <li>Remove locust.</li> </ul> </li> <li>Documentation:<ul> <li>Add the minimum set of environment variables required to set the database and start the server (#1198).</li> </ul> </li> </ul>"},{"location":"changelog/#142","title":"1.4.2","text":"<p>WARNINGs:</p> <ol> <li>This update requires running a fix-db script, available at https://raw.githubusercontent.com/fractal-analytics-platform/fractal-server/1.4.2/scripts/fix_db/current.py.</li> <li>Starting from this version, non-verified users have limited access to <code>/api/v1/</code> endpoints. Before the upgrade, all existing users must be manually set to verified.</li> </ol> <ul> <li>API:<ul> <li>Prevent access to <code>GET/PATCH</code> task endpoints for non-verified users (#1114).</li> <li>Prevent access to task-collection and workflow-apply endpoints for non-verified users (#1099).</li> <li>Make first-admin-user verified (#1110).</li> <li>Add the automatic setting of <code>ApplyWorkflow.end_timestamp</code> when patching <code>ApplyWorkflow.status</code> via <code>PATCH /admin/job/{job_id}</code> (#1121).</li> <li>Change <code>ProjectDump.timestamp_created</code> type from <code>datetime</code> to <code>str</code> (#1120).</li> <li>Change <code>_DatasetHistoryItem.workflowtask</code> type into <code>WorkflowTaskDump</code> (#1139).</li> <li>Change status code of stop-job endpoints to 202 (#1151).</li> </ul> </li> <li>API (internal):<ul> <li>Implement cascade operations explicitly, in <code>DELETE</code> endpoints for datasets, workflows and projects (#1130).</li> <li>Update <code>GET /project/{project_id}/workflow/{workflow_id}/job/</code> to avoid using <code>Workflow.job_list</code> (#1130).</li> <li>Remove obsolete sync-database dependency from apply-workflow endpoint (#1144).</li> </ul> </li> <li>Database:<ul> <li>Add <code>ApplyWorkflow.project_dump</code> column (#1070).</li> <li>Provide more meaningful names to fix-db scripts (#1107).</li> <li>Add <code>Project.timestamp_created</code> column, with timezone-aware default (#1102, #1131).</li> <li>Remove <code>Dataset.list_jobs_input</code> and <code>Dataset.list_jobs_output</code> relationships (#1130).</li> <li>Remove <code>Workflow.job_list</code> (#1130).</li> </ul> </li> <li>Runner:<ul> <li>In SLURM backend, use <code>slurm_account</code> (as received from apply-workflow endpoint) with top priority (#1145).</li> <li>Forbid setting of SLURM account from <code>WorkflowTask.meta</code> or as part of <code>worker_init</code> variable (#1145).</li> <li>Include more info in error message upon <code>sbatch</code> failure (#1142).</li> <li>Replace <code>sbatch</code> <code>--chdir</code> option with <code>-D</code>, to support also slurm versions before 17.11 (#1159).</li> </ul> </li> <li>Testing:<ul> <li>Extended systematic testing of database models (#1078).</li> <li>Review <code>MockCurrentUser</code> fixture, to handle different kinds of users (#1099).</li> <li>Remove <code>persist</code> from <code>MockCurrentUser</code> (#1098).</li> <li>Update <code>migrations.yml</code> GitHub Action to use up-to-date database and also test fix-db script (#1101).</li> <li>Add more schema-based validation to fix-db current script (#1107).</li> <li>Update <code>.dict()</code> to <code>.model_dump()</code> for <code>SQLModel</code> objects, to fix some <code>DeprecationWarnings</code>(##1133).</li> <li>Small improvement in schema coverage (#1125).</li> <li>Add unit test for <code>security</code> module (#1036).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>sqlmodel</code> to version 0.0.14 (#1124).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add automatic benchmark system for API's performances (#1123)</li> </ul> </li> <li>App (internal):<ul> <li>Move <code>_create_first_user</code> from <code>main</code> to <code>security</code> module, and allow it to create multiple regular users (#1036).</li> </ul> </li> </ul>"},{"location":"changelog/#141","title":"1.4.1","text":"<ul> <li>API:<ul> <li>Add <code>GET /admin/job/{job_id}/stop/</code> and <code>GET /admin/job/{job_id}/download/</code> endpoints (#1059).</li> <li>Use <code>DatasetDump</code> and <code>WorkflowDump</code> models for \"dump\" attributes of <code>ApplyWorkflowRead</code> (#1049, #1082).</li> <li>Add <code>slurm_accounts</code> to <code>User</code> schemas and add <code>slurm_account</code> to <code>ApplyWorkflow</code> schemas (#1067).</li> <li>Prevent providing a <code>package_version</code> for task collection from a <code>.whl</code> local package (#1069).</li> <li>Add <code>DatasetRead.project</code> and <code>WorkflowRead.project</code> attributes (#1082).</li> </ul> </li> <li>Database:<ul> <li>Make <code>ApplyWorkflow.workflow_dump</code> column non-nullable (#1049).</li> <li>Add <code>UserOAuth.slurm_accounts</code> and <code>ApplyWorkflow.slurm_account</code> columns (#1067).</li> <li>Add script for adding <code>ApplyWorkflow.user_email</code> (#1058).</li> <li>Add <code>Dataset.project</code> and <code>Workflow.project</code> relationships (#1082).</li> <li>Avoid using <code>Project</code> relationships <code>dataset_list</code> or <code>workflow_list</code> within some <code>GET</code> endpoints (#1082).</li> <li>Fully remove <code>Project</code> relationships <code>dataset_list</code>, <code>workflow_list</code> and <code>job_list</code> (#1091).</li> </ul> </li> <li>Testing:<ul> <li>Only use ubuntu-22.04 in GitHub actions (#1061).</li> <li>Improve unit testing of database models (#1082).</li> </ul> </li> <li>Dependencies:<ul> <li>Pin <code>bcrypt</code> to 4.0.1 to avoid warning in passlib (#1060).</li> </ul> </li> <li>Runner:<ul> <li>Set SLURM-job working directory to <code>job.working_dir_user</code> through <code>--chdir</code> option (#1064).</li> </ul> </li> </ul>"},{"location":"changelog/#140","title":"1.4.0","text":"<ul> <li>API:<ul> <li>Major endpoint changes:<ul> <li>Add trailing slash to all endpoints' paths (#1003).</li> <li>Add new admin-area endpoints restricted to superusers at <code>/admin</code> (#947, #1009, #1032).</li> <li>Add new <code>GET</code> endpoints <code>api/v1/job/</code> and <code>api/v1/project/{project_id}/workflow/{workflow_id}/job/</code> (#969, #1003).</li> <li>Add new <code>GET</code> endpoints <code>api/v1/dataset/</code> and <code>api/v1/workflow/</code> (#988, #1003).</li> <li>Add new <code>GET</code> endpoint <code>api/v1/project/{project_id}/dataset/</code> (#993).</li> <li>Add <code>PATCH /admin/job/{job_id}/</code> endpoint (#1030, #1053).</li> <li>Move <code>GET /auth/whoami/</code> to <code>GET /auth/current-user/</code> (#1013).</li> <li>Move <code>PATCH /auth/users/me/</code> to <code>PATCH /auth/current-user/</code> (#1013, #1035).</li> <li>Remove <code>DELETE /auth/users/{id}/</code> endpoint (#994).</li> <li>Remove <code>GET /auth/users/me/</code> (#1013).</li> <li>Remove <code>POST</code> <code>/auth/forgot-password/</code>, <code>/auth/reset-password/</code>, <code>/auth/request-verify-token/</code>, <code>/auth/verify/</code> (#1033).</li> <li>Move <code>GET /auth/userlist/</code> to <code>GET /auth/users/</code> (#1033).</li> </ul> </li> <li>New behaviors or responses of existing endpoints:<ul> <li>Change response of <code>/api/v1/project/{project_id}/job/{job_id}/stop/</code> endpoint to 204 no-content (#967).</li> <li>Remove <code>dataset_list</code> attribute from <code>ProjectRead</code>, which affects all <code>GET</code> endpoints that return some project (#993).</li> <li>Make it possible to delete a <code>Dataset</code>, <code>Workflow</code> or <code>Project</code>, even when it is in relationship to an <code>ApplyWorkflow</code> - provided that the <code>ApplyWorkflow</code> is not pending or running (#927, #973).</li> <li>Align <code>ApplyWorkflowRead</code> with new <code>ApplyWorkflow</code>, which has optional foreign keys <code>project_id</code>, <code>workflow_id</code>, <code>input_dataset_id</code>, and <code>output_dataset_id</code> (#984).</li> <li>Define types for <code>ApplyWorkflowRead</code> \"dump\" attributes (#990). WARNING: reverted with #999.</li> </ul> </li> <li>Internal changes:<ul> <li>Move all routes definitions into <code>fractal_server/app/routes</code> (#976).</li> <li>Fix construction of <code>ApplyWorkflow.workflow_dump</code>, within apply endpoint (#968).</li> <li>Fix construction of <code>ApplyWorkflow</code> attributes <code>input_dataset_dump</code> and <code>output_dataset_dump</code>, within apply endpoint (#990).</li> <li>Remove <code>asyncio.gather</code>, in view of SQLAlchemy2 update (#1004).</li> </ul> </li> </ul> </li> <li>Database:<ul> <li>Make foreign-keys of <code>ApplyWorkflow</code> (<code>project_id</code>, <code>workflow_id</code>, <code>input_dataset_id</code>, <code>output_dataset_id</code>) optional (#927).</li> <li>Add columns <code>input_dataset_dump</code>, <code>output_dataset_dump</code> and <code>user_email</code> to <code>ApplyWorkflow</code> (#927).</li> <li>Add relations <code>Dataset.list_jobs_input</code> and <code>Dataset.list_jobs_output</code> (#927).</li> <li>Make <code>ApplyWorkflow.start_timestamp</code> non-nullable (#927).</li> <li>Remove <code>\"cascade\": \"all, delete-orphan\"</code> from <code>Project.job_list</code> (#927).</li> <li>Add <code>Workflow.job_list</code> relation (#927).</li> <li>Do not use <code>Enum</code>s as column types (e.g. for <code>ApplyWorkflow.status</code>), but only for (de-)serialization (#974).</li> <li>Set <code>pool_pre_ping</code> option to <code>True</code>, for asyncpg driver (#1037).</li> <li>Add script for updating DB from 1.4.0 to 1.4.1 (#1010)</li> <li>Fix missing try/except in sync session (#1020).</li> </ul> </li> <li>App:<ul> <li>Skip creation of first-superuser when one superuser already exists (#1006).</li> </ul> </li> <li>Dependencies:<ul> <li>Update sqlalchemy to version <code>&gt;=2.0.23,&lt;2.1</code> (#1044).</li> <li>Update sqlmodel to version 0.0.12 (#1044).</li> <li>Upgrade asyncpg to version 0.29.0 (#1036).</li> </ul> </li> <li>Runner:<ul> <li>Refresh DB objects within <code>submit_workflow</code> (#927).</li> </ul> </li> <li>Testing:<ul> <li>Add <code>await db_engine.dispose()</code> in <code>db_create_tables</code> fixture (#1047).</li> <li>Set <code>debug=False</code> in <code>event_loop</code> fixture (#1044).</li> <li>Improve <code>test_full_workflow.py</code> (#971).</li> <li>Update <code>pytest-asyncio</code> to v0.21 (#1008).</li> <li>Fix CI issue related to event loop and asyncpg (#1012).</li> <li>Add GitHub Action testing database migrations (#1010).</li> <li>Use greenlet v3 in <code>poetry.lock</code> (#1044).</li> </ul> </li> <li>Documentation:<ul> <li>Add OAuth2 example endpoints to Web API page (#1034, #1038).</li> </ul> </li> <li>Development:<ul> <li>Use poetry 1.7.1 (#1043).</li> </ul> </li> </ul>"},{"location":"changelog/#1314-do-not-use","title":"1.3.14 (do not use!)","text":"<p>WARNING: This version introduces a change that is then reverted in 1.4.0, namely it sets the <code>ApplyWorkflow.status</code> type to <code>Enum</code>, when used with PostgreSQL. It is recommended to not use it, and upgrade to 1.4.0 directly.</p> <ul> <li>Make <code>Dataset.resource_list</code> an <code>ordering_list</code>, ordered by <code>Resource.id</code> (#951).</li> <li>Expose <code>redirect_url</code> for OAuth clients (#953).</li> <li>Expose JSON Schema for the <code>ManifestV1</code> Pydantic model (#942).</li> <li>Improve delete-resource endpoint (#943).</li> <li>Dependencies:<ul> <li>Upgrade sqlmodel to 0.0.11 (#949).</li> </ul> </li> <li>Testing:<ul> <li>Fix bug in local tests with Docker/SLURM (#948).</li> </ul> </li> </ul>"},{"location":"changelog/#1313","title":"1.3.13","text":"<ul> <li>Configure sqlite WAL to avoid \"database is locked\" errors (#860).</li> <li>Dependencies:<ul> <li>Add <code>sqlalchemy[asyncio]</code> extra, and do not directly require <code>greenlet</code> (#895).</li> <li>Fix <code>cloudpickle</code>-version definition in <code>pyproject.toml</code> (#937).</li> <li>Remove obsolete <code>sqlalchemy_utils</code> dependency (#939).</li> </ul> </li> <li>Testing:<ul> <li>Use ubuntu-22 for GitHub CI (#909).</li> <li>Run GitHub CI both with SQLite and Postgres (#915).</li> <li>Disable <code>postgres</code> service in GitHub action when running tests with SQLite (#931).</li> <li>Make <code>test_commands.py</code> tests stateless, also when running with Postgres (#917).</li> </ul> </li> <li>Documentation:<ul> <li>Add information about minimal supported SQLite version (#916).</li> </ul> </li> </ul>"},{"location":"changelog/#1312","title":"1.3.12","text":"<ul> <li>Project creation:<ul> <li>Do not automatically create a dataset upon project creation (#897).</li> <li>Remove <code>ProjectCreate.default_dataset_name</code> attribute (#897).</li> </ul> </li> <li>Dataset history:<ul> <li>Create a new (non-nullable) history column in <code>Dataset</code> table (#898, #901).</li> <li>Deprecate history handling in <code>/project/{project_id}/job/{job_id}</code> endpoint (#898).</li> <li>Deprecate <code>HISTORY_LEGACY</code> (#898).</li> </ul> </li> <li>Testing:<ul> <li>Remove obsolete fixture <code>slurm_config</code> (#903).</li> </ul> </li> </ul>"},{"location":"changelog/#1311","title":"1.3.11","text":"<p>This is mainly a bugfix release for the <code>PermissionError</code> issue.</p> <ul> <li>Fix <code>PermissionError</code>s in parallel-task metadata aggregation for the SLURM backend (#893).</li> <li>Documentation:<ul> <li>Bump <code>mkdocs-render-swagger-plugin</code> to 0.1.0 (#889).</li> </ul> </li> <li>Testing:<ul> <li>Fix <code>poetry install</code> command and <code>poetry</code> version in GitHub CI (#889).</li> </ul> </li> </ul>"},{"location":"changelog/#1310","title":"1.3.10","text":"<p>Warning: updating to this version requires changes to the configuration variable</p> <ul> <li>Updates to SLURM interface:<ul> <li>Remove <code>sudo</code>-requiring <code>ls</code> calls from <code>FractalFileWaitThread.check</code> (#885);</li> <li>Change default of <code>FRACTAL_SLURM_POLL_INTERVAL</code> to 5 seconds (#885);</li> <li>Rename <code>FRACTAL_SLURM_OUTPUT_FILE_GRACE_TIME</code> configuration variables into <code>FRACTAL_SLURM_ERROR_HANDLING_INTERVAL</code> (#885);</li> <li>Remove <code>FRACTAL_SLURM_KILLWAIT_INTERVAL</code> variable and corresponding logic (#885);</li> <li>Remove <code>_multiple_paths_exist_as_user</code> helper function (#885);</li> <li>Review type hints and default values of SLURM-related configuration variables (#885).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>fastapi</code> to version <code>^0.103.0</code> (#877);</li> <li>Update <code>fastapi-users</code> to version <code>^12.1.0</code> (#877).</li> </ul> </li> </ul>"},{"location":"changelog/#139","title":"1.3.9","text":"<ul> <li>Make updated-metadata collection robust for metadiff files consisting of a single <code>null</code> value (#879).</li> <li>Automate procedure for publishing package to PyPI (#881).</li> </ul>"},{"location":"changelog/#138","title":"1.3.8","text":"<ul> <li>Backend runner:<ul> <li>Add aggregation logic for parallel-task updated metadata (#852);</li> <li>Make updated-metadata collection robust for missing files (#852, #863).</li> </ul> </li> <li>Database interface:</li> <li>API:<ul> <li>Prevent user from bypassing workflow-name constraint via the PATCH endpoint (#867).</li> <li>Handle error upon task collection, when tasks exist in the database but not on-disk (#874).</li> <li>Add <code>_check_project_exists</code> helper function (#872).</li> </ul> </li> <li>Configuration variables:<ul> <li>Remove <code>DEPLOYMENT_TYPE</code> variable and update <code>alive</code> endpoint (#875);</li> <li>Introduce <code>Settings.check_db</code> method, and call it during inline/offline migrations (#855);</li> <li>Introduce <code>Settings.check_runner</code> method (#875);</li> <li>Fail if <code>FRACTAL_BACKEND_RUNNER</code> is <code>\"local\"</code> and <code>FRACTAL_LOCAL_CONFIG_FILE</code> is set but missing on-disk (#875);</li> <li>Clean up <code>Settings.check</code> method and improve its coverage (#875);</li> </ul> </li> <li>Package, repository, documentation:<ul> <li>Change <code>fractal_server.common</code> from being a git-submodule to being a regular folder (#859).</li> <li>Pin documentation dependencies (#865).</li> <li>Split <code>app/models/project.py</code> into two modules for dataset and project (#871).</li> <li>Revamp documentation on database interface and on the corresponding configuration variables (#855).</li> </ul> </li> </ul>"},{"location":"changelog/#137","title":"1.3.7","text":"<ul> <li>Oauth2-related updates (#822):<ul> <li>Update configuration of OAuth2 clients, to support OIDC/GitHub/Google;</li> <li>Merge <code>SQLModelBaseOAuthAccount</code> and <code>OAuthAccount</code> models;</li> <li>Update <code>UserOAuth.oauth_accounts</code> relationship and fix <code>list_users</code> endpoint accordingly;</li> <li>Introduce dummy <code>UserManager.on_after_login</code> method;</li> <li>Rename <code>OAuthClient</code> into <code>OAuthClientConfig</code>;</li> <li>Revamp users-related parts of documentation.</li> </ul> </li> </ul>"},{"location":"changelog/#136","title":"1.3.6","text":"<ul> <li>Update <code>output_dataset.meta</code> also when workflow execution fails (#843).</li> <li>Improve error message for unknown errors in job execution (#843).</li> <li>Fix log message incorrectly marked as \"error\" (#846).</li> </ul>"},{"location":"changelog/#135","title":"1.3.5","text":"<ul> <li>Review structure of dataset history (#803):<ul> <li>Re-define structure for <code>history</code> property of <code>Dataset.meta</code>;</li> <li>Introduce <code>\"api/v1/project/{project_id}/dataset/{dataset_id}/status/\"</code> endpoint;</li> <li>Introduce <code>\"api/v1/project/{project_id}/dataset/{dataset_id}/export_history/\"</code> endpoint;</li> <li>Move legacy history to <code>Dataset.meta[\"HISTORY_LEGACY\"]</code>.</li> </ul> </li> <li>Make <code>first_task_index</code> and <code>last_task_index</code> properties of <code>ApplyWorkflow</code> required (#803).</li> <li>Add <code>docs_info</code> and <code>docs_link</code> to Task model (#814)</li> <li>Accept <code>TaskUpdate.version=None</code> in task-patch endpoint (#818).</li> <li>Store a copy of the <code>Workflow</code> into the optional column <code>ApplyWorkflow.workflow_dump</code> at the time of submission (#804, #834).</li> <li>Prevent execution of multiple jobs with the same output dataset (#801).</li> <li>Transform non-absolute <code>FRACTAL_TASKS_DIR</code> into absolute paths, relative to the current working directory (#825).</li> <li>Error handling:<ul> <li>Raise an appropriate error if a task command is not executable (#800).</li> <li>Improve handling of errors raised in <code>get_slurm_config</code> (#800).</li> </ul> </li> <li>Documentation:<ul> <li>Clarify documentation about <code>SlurmConfig</code> (#798).</li> <li>Update documentation configuration and GitHub actions (#811).</li> </ul> </li> <li>Tests:<ul> <li>Move <code>tests/test_common.py</code> into <code>fractal-common</code> repository (#808).</li> <li>Switch to <code>docker compose</code> v2 and unpin <code>pyyaml</code> version (#816).</li> </ul> </li> </ul>"},{"location":"changelog/#134","title":"1.3.4","text":"<ul> <li>Support execution of a workflow subset (#784).</li> <li>Fix internal server error for invalid <code>task_id</code> in <code>create_workflowtask</code> endpoint (#782).</li> <li>Improve logging in background task collection (#776).</li> <li>Handle failures in <code>submit_workflow</code> without raising errors (#787).</li> <li>Simplify internal function for execution of a list of task (#780).</li> <li>Exclude <code>common/tests</code> and other git-related files from build (#795).</li> <li>Remove development dependencies <code>Pillow</code> and <code>pytest-mock</code> (#795).</li> <li>Remove obsolete folders from <code>tests/data</code> folder (#795).</li> </ul>"},{"location":"changelog/#133","title":"1.3.3","text":"<ul> <li>Pin Pydantic to v1 (#779).</li> </ul>"},{"location":"changelog/#132","title":"1.3.2","text":"<ul> <li>Add sqlalchemy naming convention for DB constraints, and add <code>render_as_batch=True</code> to <code>do_run_migrations</code> (#757).</li> <li>Fix bug in job-stop endpoint, due to missing default for <code>FractalSlurmExecutor.wait_thread.shutdown_file</code> (#768, #769).</li> <li>Fix bug upon inserting a task with <code>meta=None</code> into a Workflow (#772).</li> </ul>"},{"location":"changelog/#131","title":"1.3.1","text":"<ul> <li>Fix return value of stop-job endpoint (#764).</li> <li>Expose new GET <code>WorkflowTask</code> endpoint (#762).</li> <li>Clean up API modules (#762):<ul> <li>Split workflow/workflowtask modules;</li> <li>Split tasks/task-collection modules.</li> </ul> </li> </ul>"},{"location":"changelog/#130","title":"1.3.0","text":"<ul> <li>Refactor user model:<ul> <li>Switch from UUID4 to int for IDs (#660, #684).</li> <li>Fix many-to-many relationship between users and project (#660).</li> <li>Rename <code>Project.user_member_list</code> into <code>Project.user_list</code> (#660).</li> <li>Add <code>username</code> column (#704).</li> </ul> </li> <li>Update endpoints (see also 1.2-&gt;1.3 upgrade info in the documentation):<ul> <li>Review endpoint URLs (#669).</li> <li>Remove foreign keys from payloads (#669).</li> </ul> </li> <li>Update <code>Task</code> models, task collection and task-related endpoints:<ul> <li>Add <code>version</code> and <code>owner</code> columns to <code>Task</code> model (#704).</li> <li>Set <code>Task.version</code> during task collection (#719).</li> <li>Set <code>Task.owner</code> as part of create-task endpoint (#704).</li> <li>For custom tasks, prepend <code>owner</code> to user-provided <code>source</code> (#725).</li> <li>Remove <code>default_args</code> from <code>Tasks</code> model and from manifest tasks (#707).</li> <li>Add <code>args_schema</code> and <code>args_schema_version</code> to <code>Task</code> model (#707).</li> <li>Expose <code>args_schema</code> and <code>args_schema_version</code> in task POST/PATCH endpoints (#749).</li> <li>Make <code>Task.source</code> task-specific rather than package-specific (#719).</li> <li>Make <code>Task.source</code> unique (#725).</li> <li>Update <code>_TaskCollectPip</code> methods, attributes and properties (#719).</li> <li>Remove private/public options for task collection (#704).</li> <li>Improve error message for missing package manifest (#704).</li> <li>Improve behavior when task-collection folder already exists (#704).</li> <li>Expose <code>pinned_package_version</code> for tasks collection (#744).</li> <li>Restrict Task editing to superusers and task owners (#733).</li> <li>Implement <code>delete_task</code> endpoint (#745).</li> </ul> </li> <li>Update <code>Workflow</code> and <code>WorkflowTask</code> endpoints:<ul> <li>Always merge new <code>WorkflowTask.args</code> with defaults from <code>Task.args_schema</code>, in <code>update_workflowtask</code> endpoint (#759).</li> <li>Remove <code>WorkflowTask.overridden_meta</code> property and on-the-fly overriding of <code>meta</code> (#752).</li> <li>Add warning when exporting workflows which include custom tasks (#728).</li> <li>When importing a workflow, only use tasks' <code>source</code> values, instead of <code>(source,name)</code> pairs (#719).</li> </ul> </li> <li>Job execution:<ul> <li>Add <code>FractalSlurmExecutor.shutdown</code> and corresponding endpoint (#631, #691, #696).</li> <li>In <code>FractalSlurmExecutor</code>, make <code>working_dir*</code> attributes required (#679).</li> <li>Remove <code>ApplyWorkflow.overwrite_input</code> column (#684, #694).</li> <li>Make <code>output_dataset_id</code> a required argument of apply-workflow endpoint (#681).</li> <li>Improve error message related to out-of-space disk (#699).</li> <li>Include timestamp in job working directory, to avoid name clashes (#756).</li> </ul> </li> <li>Other updates to endpoints and database:<ul> <li>Add <code>ApplyWorkflow.end_timestamp</code> column (#687, #684).</li> <li>Prevent deletion of a <code>Workflow</code>/<code>Dataset</code> in relationship with existing <code>ApplyWorkflow</code> (#703).</li> <li>Add project-name uniqueness constraint in project-edit endpoint (#689).</li> </ul> </li> <li>Other updates to internal logic:<ul> <li>Drop <code>WorkflowTask.arguments</code> property and <code>WorkflowTask.assemble_args</code> method (#742).</li> <li>Add test for collection of tasks packages with tasks in a subpackage (#743).</li> <li>Expose <code>FRACTAL_CORS_ALLOW_ORIGIN</code> environment variable (#688).</li> <li>Expose <code>FRACTAL_DEFAULT_ADMIN_USERNAME</code> environment variable (#751).</li> </ul> </li> <li>Package and repository:<ul> <li>Remove <code>fastapi-users-db-sqlmodel</code> dependency (#660).</li> <li>Make coverage measure more accurate (#676) and improve coverage (#678).</li> <li>Require pydantic version to be <code>&gt;=1.10.8</code> (#711, #713).</li> <li>Include multiple <code>fractal-common</code> updates (#705, #719).</li> <li>Add test equivalent to <code>alembic check</code> (#722).</li> <li>Update <code>poetry.lock</code> to address security alerts (#723).</li> <li>Remove <code>sqlmodel</code> from <code>fractal-common</code>, and declare database models with multiple inheritance (#710).</li> <li>Make email generation more robust in <code>MockCurrentUser</code> (#730).</li> <li>Update <code>poetry.lock</code> to <code>cryptography=41</code>, to address security alert (#739).</li> <li>Add <code>greenlet</code> as a direct dependency (#748).</li> <li>Removed tests for <code>IntegrityError</code> (#754).</li> </ul> </li> </ul>"},{"location":"changelog/#125","title":"1.2.5","text":"<ul> <li>Fix bug in task collection when using sqlite (#664, #673).</li> <li>Fix bug in task collection from local package, where package extras were not considered (#671).</li> <li>Improve error handling in workflow-apply endpoint (#665).</li> <li>Fix a bug upon project removal in the presence of project-related jobs (#666). Note: this removes the <code>ApplyWorkflow.Project</code> attribute.</li> </ul>"},{"location":"changelog/#124","title":"1.2.4","text":"<ul> <li>Review setup for database URLs, especially to allow using UNIX-socket connections for postgresql (#657).</li> </ul>"},{"location":"changelog/#123","title":"1.2.3","text":"<ul> <li>Fix bug that was keeping multiple database conection open (#649).</li> </ul>"},{"location":"changelog/#122","title":"1.2.2","text":"<ul> <li>Fix bug related to <code>user_local_exports</code> in SLURM-backend configuration (#642).</li> </ul>"},{"location":"changelog/#121","title":"1.2.1","text":"<ul> <li>Fix bug upon creation of first user when using multiple workers (#632).</li> <li>Allow both ports 5173 and 4173 as CORS origins (#637).</li> </ul>"},{"location":"changelog/#120","title":"1.2.0","text":"<ul> <li>Drop <code>project.project_dir</code> and replace it with <code>user.cache_dir</code> (#601).</li> <li>Update SLURM backend (#582, #612, #614); this includes (1) combining several tasks in a single SLURM job, and (2) offering more granular sources for SLURM configuration options.</li> <li>Expose local user exports in SLURM configuration file (#625).</li> <li>Make local backend rely on custom <code>FractalThreadPoolExecutor</code>, where <code>parallel_tasks_per_job</code> can affect parallelism (#626).</li> <li>Review logging configuration (#619, #623).</li> <li>Update to fastapi <code>0.95</code> (#587).</li> <li>Minor improvements in dataset-edit endpoint (#593) and tests (#589).</li> <li>Include test of non-python task (#594).</li> <li>Move dummy tasks from package to tests (#601).</li> <li>Remove deprecated parsl backend (#607).</li> <li>Improve error handling in workflow-import endpoint (#595).</li> <li>Also show logs for successful workflow execution (#635).</li> </ul>"},{"location":"changelog/#111","title":"1.1.1","text":"<ul> <li>Include <code>reordered_workflowtask_ids</code> in workflow-edit endpoint payload, to reorder the task list of a workflow (#585).</li> </ul>"},{"location":"changelog/#110","title":"1.1.0","text":"<ul> <li>Align with new tasks interface in <code>fractal-tasks-core&gt;=0.8.0</code>, and remove <code>glob_pattern</code> column from <code>resource</code> database table (#544).</li> <li>Drop python 3.8 support (#527).</li> <li>Improve validation of API request payloads (#545).</li> <li>Improve request validation in project-creation endpoint (#537).</li> <li>Update the endpoint to patch a <code>Task</code> (#526).</li> <li>Add new project-update endpoint, and relax constraints on <code>project_dir</code> in new-project endpoint (#563).</li> <li>Update <code>DatasetUpdate</code> schema (#558 and #565).</li> <li>Fix redundant task-error logs in slurm backend (#552).</li> <li>Improve handling of task-collection errors (#559).</li> <li>If <code>FRACTAL_BACKEND_RUNNER=slurm</code>, include some configuration checks at server startup (#529).</li> <li>Fail if <code>FRACTAL_SLURM_WORKER_PYTHON</code> has different versions of <code>fractal-server</code> or <code>cloudpickle</code> (#533).</li> </ul>"},{"location":"changelog/#108","title":"1.0.8","text":"<ul> <li>Fix handling of parallel-tasks errors in <code>FractalSlurmExecutor</code> (#497).</li> <li>Add test for custom tasks (#500).</li> <li>Improve formatting of job logs (#503).</li> <li>Improve error handling in workflow-execution server endpoint (#515).</li> <li>Update <code>_TaskBase</code> schema from fractal-common (#517).</li> </ul>"},{"location":"changelog/#107","title":"1.0.7","text":"<ul> <li>Update endpoints to import/export a workflow (#495).</li> </ul>"},{"location":"changelog/#106","title":"1.0.6","text":"<ul> <li>Add new endpoints to import/export a workflow (#490).</li> </ul>"},{"location":"changelog/#105","title":"1.0.5","text":"<ul> <li>Separate workflow-execution folder into two (server- and user-owned) folders, to avoid permission issues (#475).</li> <li>Explicitly pin sqlalchemy to v1 (#480).</li> </ul>"},{"location":"changelog/#104","title":"1.0.4","text":"<ul> <li>Add new POST endpoint to create new Task (#486).</li> </ul>"},{"location":"changelog/#103","title":"1.0.3","text":"<p>Missing due to releasing error.</p>"},{"location":"changelog/#102","title":"1.0.2","text":"<ul> <li>Add <code>FRACTAL_RUNNER_MAX_TASKS_PER_WORKFLOW</code> configuration variable (#469).</li> </ul>"},{"location":"changelog/#101","title":"1.0.1","text":"<ul> <li>Fix bug with environment variable names (#468).</li> </ul>"},{"location":"changelog/#100","title":"1.0.0","text":"<ul> <li>First release listed in CHANGELOG.</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>To configure the Fractal Server one must define some environment variables. Some of them are required, and the server will not start unless they are set. Some are optional and sensible defaults are provided.</p> <p>The required variables are the following</p> <pre><code>JWT_SECRET_KEY\nPOSTGRES_DB\n</code></pre>"},{"location":"development/","title":"Development","text":"<p>The development of Fractal Server takes place on the fractal-server Github repository.  To ask questions or to inform us of a bug or unexpected behavior, please feel free to open an issue.</p> <p>To contribute code, please fork the repository and submit a pull request.</p>"},{"location":"development/#set-up-the-development-environment","title":"Set up the development environment","text":""},{"location":"development/#install-poetry","title":"Install poetry","text":"<p>Fractal uses poetry to manage the development environment and dependencies, and to streamline the build and release operations; at least version 2.0.0 is recommended.</p> <p>A simple way to install <code>poetry</code> is <pre><code>pipx install poetry==2.2.1`\n</code></pre> while other options are described here.</p>"},{"location":"development/#clone-repository","title":"Clone repository","text":"<p>You can clone the <code>fractal-server</code> repository via <pre><code>git clone https://github.com/fractal-analytics-platform/fractal-server.git\n</code></pre></p>"},{"location":"development/#install-package","title":"Install package","text":"<p>Running <pre><code>poetry install --with dev --with docs\n</code></pre> will initialise a Python virtual environment and install Fractal Server and all its dependencies, including optional dependencies. Note that to run commands from within this environment you should prepend them with <code>poetry run</code> (as in <code>poetry run fractalctl set-db</code>).</p>"},{"location":"development/#update-database-schema-during-development","title":"Update database schema during development","text":"<p>Whenever the models are modified (either in <code>app/models</code> or in <code>app/schemas</code>), you should update them via a migration. To check whether this is needed, run <pre><code>poetry run alembic check\n</code></pre></p> <p>If needed, the simplest procedure is to use <code>alembic --autogenerate</code> to create an incremental migration script, as in <pre><code>$ export POSTGRES_DB=\"autogenerate-fractal-revision\"\n$ dropdb --if-exist \"$POSTGRES_DB\"\n$ createdb \"$POSTGRES_DB\"\n$ poetry run fractalctl set-db --skip-init-data\n$ poetry run alembic revision --autogenerate -m \"Some migration message\"\n</code></pre></p>"},{"location":"development/#release","title":"Release","text":"<ol> <li>Checkout to branch <code>main</code>.</li> <li>Check that the current HEAD of the <code>main</code> branch passes all the tests (note: make sure that you are using the poetry-installed local package).</li> <li>Update the <code>CHANGELOG.md</code> file (e.g. remove <code>(unreleased)</code> from the upcoming version).</li> <li>If you have modified the models, then you must also create a new migration script (note: in principle the CI will fail if you forget this step).</li> <li>Use one of the following <pre><code>poetry run bumpver update --tag-num --tag-commit --commit --dry\npoetry run bumpver update --patch --tag-commit --commit --dry\npoetry run bumpver update --minor --tag-commit --commit --dry\npoetry run bumpver update --set-version X.Y.Z --tag-commit --commit --dry\n</code></pre> to test updating the version bump.</li> <li>If the previous step looks OK, remove <code>--dry</code> and re-run to actually bump the version, commit and push the changes.</li> <li>Approve (or have approved) the new version at Publish package to PyPI.</li> <li>After the release: If the release was a stable one (e.g. <code>X.Y.Z</code>, not <code>X.Y.Za1</code> or <code>X.Y.Zrc2</code>), move <code>fractal_server/data_migrations/X_Y_Z.py</code> to <code>fractal_server/data_migrations/old</code>.</li> </ol>"},{"location":"development/#run-tests","title":"Run tests","text":"<p>Unit and integration testing of Fractal Server uses the pytest testing framework.</p> <p>To test the SLURM backend, we use a custom version of a  Docker local SLURM cluster. The pytest plugin pytest-docker is then used to spin up the Docker containers for the duration of the tests.</p> <p>Important: this requires docker being installed on the development system, and the current user being in the <code>docker</code> group. A simple check for this requirement is to run a command like <code>docker ps</code>, and verify that it does not raise any permission-related error. Note that also <code>docker compose</code> must be available..</p> <p>If you installed the development dependencies, you may run the test suite by invoking <pre><code>poetry run pytest\n</code></pre> from the main directory of the <code>fractal-server</code> repository. It is sometimes useful to specify additional arguments, e.g. <pre><code>poetry run pytest -s -v --log-cli-level info --full-trace\n</code></pre></p> <p>Tests are also run as part of GitHub Actions Continuous Integration for the <code>fractal-server</code> repository.</p>"},{"location":"development/#documentation","title":"Documentation","text":"<p>The documentations is built with mkdocs and the Material theme. Docstrings should be formatted as in the Google Python Style Guide.</p> <p>To build the documentation</p> <ol> <li>Setup a python environment and install the requirements from <code>docs/doc-requirements.txt</code>.</li> <li>Run <pre><code>poetry run mkdocs serve --config-file mkdocs.yml\n</code></pre> and browse the documentation at <code>http://127.0.0.1:8000</code>.</li> </ol>"},{"location":"install_and_deploy/","title":"Install and deploy","text":"<p>Fractal Server is the core ingredient of more deployments of the Fractal framework, which also includes several other Fractal components (e.g. a web client) and also relies on external resources being availble (e.g. a PostgreSQL database and a SLURM cluster).</p> <p>Here we do not describe the full procedure for a full-fledged Fractal deployment in detail. Some examples of typical deployment are available as container-based demos at https://github.com/fractal-analytics-platform/fractal-containers/tree/main/examples.</p>"},{"location":"install_and_deploy/#how-to-install","title":"How to install","text":"<p>\u26a0\ufe0f  The minimum supported Python version for fractal-server is 3.11.</p> <p>Fractal Server is hosted on the PyPI index, and it can be installed with <code>pip</code> via <pre><code>pip install fractal-server\n</code></pre></p> <p>For details on how to install Fractal Server in a development environment, see the Development page.</p>"},{"location":"install_and_deploy/#how-to-deploy","title":"How to deploy","text":"<p>Here we describe the basic steps for running Fractal Server.</p>"},{"location":"install_and_deploy/#1-set-up-configuration-variables","title":"1. Set up configuration variables","text":"<p>For this command to work properly, a set of variables need to be specified, either as enviromnent variables or in a file like <code>.fractal_server.env</code>. An example of such file is <pre><code>JWT_SECRET_KEY=XXX\nFRACTAL_RUNNER_BACKEND=local\nPOSTGRES_DB=fractal-database-name\n</code></pre></p> <p>\u26a0\ufe0f  <code>JWT_SECRET_KEY=XXX</code> must be replaced with a more secure string, that should not be disclosed. \u26a0\ufe0f</p> <p>More details (including default values) are available in the Configuration page.</p>"},{"location":"install_and_deploy/#2-set-up-the-database","title":"2. Set up the database","text":"<p>After creating a PostgreSQL database for <code>fractal-server</code>, and after setting the proper <code>fractal-server</code> configuration variables (see the database page), the command <pre><code>fractalctl set-db\n</code></pre> applies the appropriate schema migrations.</p>"},{"location":"install_and_deploy/#3-start-the-server","title":"3. Start the server","text":"<p>In the environment where Fractal Server is installed, you can run it via <code>gunicorn</code> with a command like <pre><code>gunicorn fractal_server.main:app \\\n    --workers 2 \\\n    --bind \"0.0.0.0:8000\" \\\n    --access-logfile - \\\n    --error-logfile - \\\n    --worker-class fractal_server.gunicorn_fractal.FractalWorker \\\n    --logger-class fractal_server.gunicorn_fractal.FractalGunicornLogger\n</code></pre> To verify that the server is up, you can use the <code>/api/alive/</code> endpoint - as in <pre><code>$ curl http://localhost:8000/api/alive/\n{\"alive\":true,\"version\":\"2.15.6\"}\n</code></pre></p>"},{"location":"openapi/","title":"Web API","text":""},{"location":"internals/","title":"Fractal Server internal components","text":"<p>On top of exposing a web API to clients, <code>fractal-server</code> includes several internal subsystems:</p> <ul> <li>Database interface;</li> <li>Basic logging;</li> <li>Automated task collection for Fractal-compatible task packages;</li> <li>User management, authentication and authorization;</li> <li>Computational backends to execute workflows;</li> </ul>"},{"location":"internals/database_interface/","title":"Database Interface","text":"<p>Fractal Server only allows PostgreSQL to be used as database; the database-related configuration variables are described below (and in the configuration page).</p> <p>To make database operations verbose, set <code>DB_ECHO</code> equal to <code>true</code>, <code>True</code> or <code>1</code>.</p>"},{"location":"internals/database_interface/#requirements","title":"Requirements","text":"<p>To use PostgreSQL as a database, Fractal Server relies on <code>sqlalchemy</code> and <code>psycopg[binary]</code>.</p>"},{"location":"internals/database_interface/#setup","title":"Setup","text":"<p>We assume that a PostgreSQL is active, with some host (this can be e.g. <code>localhost</code> or a UNIX socket like <code>/var/run/postgresql/</code>), a port (we use the default 5432 in the examples below) and a user (e.g. <code>postgres</code> or <code>fractal</code>).</p> <p>\u26a0\ufe0f Notes:</p> <ol> <li>The postgres user must be created from outside <code>fractal-server</code>.</li> <li>A given machine user may or may not require a password (e.g. depending on    whether the machine username matches with the PostgreSQL username, and on    whether connection happens via a UNIX socket). See documentation here:    https://www.postgresql.org/docs/current/auth-pg-hba-conf.html.</li> </ol> <p>Here we create a database called <code>fractal_db</code>, through the <code>createdb</code> command:</p> <pre><code>$ createdb \\\n    --host=localhost \\\n    --port=5432 \\\n    --username=postgres \\\n    --no-password \\\n    --owner=fractal \\\n    fractal_db\n</code></pre> <p>All options of this command (and of the ones below) should be aligned with the configuration of a specific PostgreSQL instance. Within <code>fractal-server</code>, this is done by setting the following configuration variables (before running <code>fractalctl set-db</code> or <code>fractalctl start</code>):</p> <ul> <li> <p>Required:</p> <pre><code>POSTGRES_DB=fractal_db\n</code></pre> </li> <li> <p>Optional:</p> <pre><code>POSTGRES_HOST=localhost             # default: localhost\nPOSTGRES_PORT=5432                  # default: 5432\nPOSTGRES_USER=fractal               # example: fractal\nPOSTGRES_PASSWORD=\n</code></pre> </li> </ul> <p><code>fractal-server</code> will then use the <code>URL.create</code> function from <code>SQLalchemy</code> to generate the appropriate URL to connect to:</p> <p><pre><code>URL.create(\n    drivername=\"postgresql+psycopg\",\n    username=self.POSTGRES_USER,\n    password=self.POSTGRES_PASSWORD,\n    host=self.POSTGRES_HOST,\n    port=self.POSTGRES_PORT,\n    database=self.POSTGRES_DB,\n)\n</code></pre> Note that <code>POSTGRES_HOST</code> can be either a URL or the path to a UNIX domain socket (e.g. <code>/var/run/postgresql</code>).</p>"},{"location":"internals/database_interface/#backup-and-restore","title":"Backup and restore","text":"<p>To backup and restore data, one can use the utilities <code>pg_dump</code> and <code>psql</code>.</p> <p>It is possible to dump/restore data in various formats (see documentation of <code>pg_dump</code>), but in this example we stick with the default plain-text format.</p> <pre><code>$ pg_dump \\\n    --host=localhost \\\n    --port=5432\\\n    --username=fractal \\\n    --format=plain \\\n    --file=fractal_dump.sql \\\n    fractal_db\n</code></pre> <p>In order to restore a database from a dump, we first create a new empty one (<code>new_fractal_db</code>): <pre><code>$ createdb \\\n    --host=localhost \\\n    --port=5432\\\n    --username=postgres \\\n    --no-password \\\n    --owner=fractal \\\n    new_fractal_db\n</code></pre> and then we populate it using the dumped data:</p> <pre><code>$ psql \\\n    --host=localhost \\\n    --port=5432\\\n    --username=fractal \\\n    --dbname=new_fractal_db &lt; fractal_dump.sql\n</code></pre> <p>One of the multiple ways to compress data is to use <code>gzip</code>, by adapting the commands above as in: <pre><code>$ pg_dump ... | gzip -c fractal_dump.sql.gz\n$ gzip --decompress --keep fractal_dump.sql.gz\n$ createdb ...\n$ psql ... &lt; fractal_dump.sql\n</code></pre></p>"},{"location":"internals/logs/","title":"Logs","text":"<p>Logging in <code>fractal-server</code> is based on the standard <code>logging</code> library, and its logging levels are defined here. For a more detailed view on <code>fractal-server</code> logging, see the logger module documentation.</p> <p>The logger module exposes the functions to set/get/close a logger, and it defines where the records are sent to (e.g. the <code>fractal-server</code> console or a specific file). The logging levels of a logger created with <code>set_logger</code> are defined as follows:</p> <ul> <li>The minimum logging level for logs to appear in the console is set by   <code>FRACTAL_LOGGING_LEVEL</code>;</li> <li>The <code>FileHandler</code> logger handlers are alwasy set at the <code>DEBUG</code> level, that   is, they write all log records.</li> </ul> <p>This means that the <code>FRACTAL_LOGGING_LEVEL</code> offers a quick way to switch to very verbose console logging (setting it e.g. to <code>10</code>, that is, <code>DEBUG</code> level) and to switch back to less verbose logging (e.g. <code>FRACTAL_LOGGING_LEVEL=20</code> or <code>30</code>), without ever modifying the on-file logs. Note that the typical reason for having on-file logs in <code>fractal-server</code> is to log information about background tasks, that are not executed as part of an API endpoint.</p>"},{"location":"internals/logs/#example-use-cases","title":"Example use cases","text":"<ol> <li> <p>Module-level logs that should only appear in the <code>fractal-server</code> console <pre><code>from fractal_server.logger import set_logger\n\nmodule_logger = set_logger(__name__)\n\ndef my_function():\n    module_logger.debug(\"This is an DEBUG log, from my_function\")\n    module_logger.info(\"This is an INFO log, from my_function\")\n    module_logger.warning(\"This is a WARNING log, from my_function\")\n</code></pre> Note that only logs with level equal or higher to <code>FRACTAL_LOGGING_LEVEL</code> will be shown.</p> </li> <li> <p>Function-level logs that should only appear in the <code>fractal-server</code> console <pre><code>from fractal_server.logger import set_logger\n\ndef my_function():\n    function_logger = set_logger(\"my_function\")\n    function_logger.debug(\"This is an DEBUG log, from my_function\")\n    function_logger.info(\"This is an INFO log, from my_function\")\n    function_logger.warning(\"This is a WARNING log, from my_function\")\n</code></pre> Note that only logs with level equal or higher to <code>FRACTAL_LOGGING_LEVEL</code> will be shown.</p> </li> <li> <p>Custom logs that should appear both in the fractal-server console and in a    log file <pre><code>from fractal_server.logger import set_logger\nfrom fractal_server.logger import close_logger\n\ndef my_function():\n    this_logger = set_logger(\"this_logger\", log_file_path=\"/tmp/this.log\")\n    this_logger.debug(\"This is an DEBUG log, from my_function\")\n    this_logger.info(\"This is an INFO log, from my_function\")\n    this_logger.warning(\"This is a WARNING log, from my_function\")\n    close_logger(this_logger)\n</code></pre> Note that only logs with level equal or higher to <code>FRACTAL_LOGGING_LEVEL</code> will be shown in the console, but all logs will be written to <code>\"/tmp/this.log\"</code>.</p> </li> </ol>"},{"location":"internals/logs/#future-plans","title":"Future plans","text":"<p>The current page concerns the logs that are emitted from <code>fractal-sever</code>, but not the ones coming from other sources (e.g. <code>fastapi</code> or <code>uvicorn/gunicorn</code>). In a future refactor we may address this point, with the twofold goal of</p> <ol> <li>Integrating different log sources, so that they can be shown in a    homogeneous way (e.g. all with same format);</li> <li>Redirecting all console logs (from different sources) to a rotating file    (e.g. via a RotatingFileHandler).</li> </ol>"},{"location":"internals/task_collection/","title":"Task Collection","text":"<p>In-progress (for the moment, refer to the tasks module).</p>"},{"location":"internals/users/","title":"Fractal Users","text":"<p>Fractal Server's user model and authentication/authorization systems are powered by the FastAPI Users library, and most of the components described below can be identified in the corresponding overview.</p>"},{"location":"internals/users/#user-model","title":"User Model","text":"<p>A Fractal user corresponds to an instance of the <code>UserOAuth</code> class, with the following attributes:</p> Attribute Type Nullable Default id integer incremental email email - hashed_password string - is_active bool true is_superuser bool false is_verified bool false <p>Most attributes are the default ones from <code>fastapi-users</code>.</p> <p>In the startup phase, <code>fractal-server</code> creates a default user, who also has the superuser privileges that are necessary for managing other users. The credentials for this user are defined via the environment variables <code>FRACTAL_ADMIN_DEFAULT_EMAIL</code> (default: <code>admin@example.org</code>) and <code>FRACTAL_ADMIN_DEFAULT_PASSWORD</code> (default: <code>1234</code>).</p> <p>\u26a0\ufe0f You should always modify the password of the default user after it's created; this can be done with API calls to the <code>PATCH /auth/users/{id}</code> endpoint of the <code>fractal-server</code> API, e.g. through the <code>curl</code> command or the Fractal command-line client. When the API instance is exposed to multiple users, skipping the default-user password update leads to a severe vulnerability! </p> <p>The most common use cases for <code>fractal-server</code> are:</p> <ol> <li>The server is used by a single user (e.g. on their own machine, with the local backend); in this case you may simply customize and use the default user.</li> <li>The server has multiple users, and it is connected to a SLURM cluster. For <code>fractal-server</code> to execute jobs on the SLURM cluster each Fractal must be associated to a cluster user via additional properties defined in the <code>UserSettings</code> table. See here for more details about SLURM users.</li> </ol> <p>More details about user management are provided in the User Management section below.</p>"},{"location":"internals/users/#authentication","title":"Authentication","text":""},{"location":"internals/users/#login","title":"Login","text":"<p>An authentication backend is composed of two parts:</p> <ul> <li>the transport, that manages how the token will be carried over the request,</li> <li>the strategy, which manages how the token is generated and secured.</li> </ul> <p>Fractal Server provides two authentication backends (Bearer and Cookie), both based the JWT strategy. Each backend produces both <code>/auth/login</code> and <code>/auth/logout</code> routes.</p> <p>FastAPI Users provides the <code>logout</code> endpoint by default, but this is not relevant in <code>fractal-server</code> since we do not store tokens in the database.</p>"},{"location":"internals/users/#bearer","title":"Bearer","text":"<p>The Bearer transport backend provides login at <code>/auth/token/login</code> <pre><code>$ curl \\\n    -X POST \\\n    -H \"Content-Type: application/x-www-form-urlencoded\" \\\n    -d \"username=admin@example.org&amp;password=1234\" \\\n    http://127.0.0.1:8000/auth/token/login/\n\n{\n    \"access_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxIiwiYXVkIjpbImZyYWN0YWwiXSwiZXhwIjoxNjkzNTc1MzM1fQ.UmkhBKxgBM2mxXlrTlt5HXqtDDOe_mMYiMkKUS5jbXU\",\n    \"token_type\":\"bearer\"\n}\n</code></pre></p>"},{"location":"internals/users/#cookie","title":"Cookie","text":"<p>The Cookie transport backend provides login at <code>/auth/login</code></p> <pre><code>$ curl \\\n    -X POST \\\n    -H \"Content-Type: application/x-www-form-urlencoded\" \\\n    -d \"username=admin@example.org&amp;password=1234\" \\\n    --cookie-jar - \\\n    http://127.0.0.1:8000/auth/login/\n\n\n#HttpOnly_127.0.0.1 FALSE   /   TRUE    0   fastapiusersauth    eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxIiwiYXVkIjpbImZyYWN0YWwiXSwiZXhwIjoxNjkzNjQ4MDI5fQ.UKRdbVjwys4grQrhpGyxcxcVbNSNJ29RQiFubpGYYUk\n</code></pre>"},{"location":"internals/users/#authenticated-calls","title":"Authenticated calls","text":"<p>Once you have the token, you can use it to identify yourself by sending it along in the header of an API request. Here is an example with an API request to <code>/auth/current-user/</code>: <pre><code>$ curl \\\n    -X GET \\\n    -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIyIiwiYXVkIjpbImZyYWN0YWwiXSwiZXhwIjoxNjkzOTI2MTM4fQ.MqWhW0xRgCV9ZgZr1HcdynrIJ7z46IBzO7pyfTUaTU8\" \\\n    http://127.0.0.1:8000/auth/current-user/\n\n{\n    \"id\":1,\n    \"email\":\"admin@example.org\",\n    \"is_active\":true,\n    \"is_superuser\":true,\n    \"is_verified\":false,\n    \"slurm_user\":null,\n    \"cache_dir\":null\n}\n</code></pre></p>"},{"location":"internals/users/#oauth2","title":"OAuth2","text":"<p>Fractal Server also allows a different authentication procedure, not based on the knowledge of a user's password but on external <code>OAuth2</code> authentication clients.</p> <p>Through the <code>httpx-oauth</code> library, we currently support <code>OpenID Connect</code> (aka <code>OIDC</code>), <code>GitHub</code> and <code>Google</code> (and many more clients can be readily included).</p>"},{"location":"internals/users/#configuration","title":"Configuration","text":"<p>To use a certain <code>OAuth2</code> client, you must first register the <code>fractal-server</code> application (see instructions for GitHub and Google).</p> <p>During app registration, you should provide two endpoints:</p> <ul> <li>the <code>Homepage URL</code> (e.g. <code>http://127.0.0.1:8000/</code>),</li> <li>the <code>Authorization callback URL</code> (e.g. <code>http://127.0.0.1:8000/auth/github/callback/</code>, where <code>github</code> could be any client name).</li> </ul> <p>and at the end of this procedure, you will kwnow the Client ID and Client Secret for the app.</p> <p>Note 1: You have to enable the \"Email addresses\" permission for your GitHub registered app, at https://github.com/settings/apps/{registered-app}/permissions. . And a similar setting may be required for Google.</p> <p>Note 2: You can have just one <code>GitHub</code> client and one <code>Google</code> client, but as many <code>OIDC</code> client as you want, as long as you call them with different names.</p> <p>To add an <code>OAuth2</code> client, the following environment variables must be added to <code>fractal-server</code> configuration:</p> OIDC (single client)OIDC (multiple clients)GitHubGoogle <pre><code>OAUTH_MYCLIENT_CLIENT_ID=...\nOAUTH_MYCLIENT_CLIENT_SECRET=...\nOAUTH_MYCLIENT_OIDC_CONFIGURATION_ENDPOINT=https://client.com/.well-known/openid-configuration\nOAUTH_MYCLIENT_REDIRECT_URL=...   # e.g. https://fractal-web.example.org/auth/login/oauth2\n</code></pre> <pre><code>OAUTH_MYCLIENT1_CLIENT_ID=...\nOAUTH_MYCLIENT1_CLIENT_SECRET=...\nOAUTH_MYCLIENT1_OIDC_CONFIGURATION_ENDPOINT=https://client1.com/.well-known/openid-configuration\nOAUTH_MYCLIENT1_REDIRECT_URL=...   # e.g. https://fractal-web.1.example.org/auth/login/oauth2\n\nOAUTH_MYCLIENT2_CLIENT_ID=...\nOAUTH_MYCLIENT2_CLIENT_SECRET=...\nOAUTH_MYCLIENT2_OIDC_CONFIGURATION_ENDPOINT=https://client2.com/.well-known/openid-configuration\nOAUTH_MYCLIENT2_REDIRECT_URL=...   # e.g. https://fractal-web.2.example.org/auth/login/oauth2\n</code></pre> <pre><code>OAUTH_GITHUB_CLIENT_ID=...\nOAUTH_GITHUB_CLIENT_SECRET=...\nOAUTH_GITHUB_REDIRECT_URL=...   # e.g. https://fractal-web.example.org/auth/login/oauth2\n</code></pre> <pre><code>OAUTH_GOOGLE_CLIENT_ID=...\nOAUTH_GOOGLE_CLIENT_SECRET=...\nOAUTH_GOOGLE_REDIRECT_URL=...   # e.g. https://fractal-web.example.org/auth/login/oauth2\n</code></pre> <p>When <code>fractal-server</code> starts, two new routes will be generated for each client:</p> <ul> <li><code>/auth/client-name/authorize</code> ,</li> <li><code>/auth/client-name/callback</code> (the <code>Authorization callback URL</code> of the client).</li> </ul> <p>For <code>GitHub</code> and <code>Google</code> clients the <code>client-name</code> is <code>github</code> or <code>google</code>, while for <code>OIDC</code> clients it comes from the environment variables (e.g. for <code>OAUTH_MYCLIENT_CLIENT_ID</code> the <code>client-name</code> is <code>MYCLIENT</code>).</p> <p>Note that the <code>OAUTH_*_REDIRECT_URL</code> environment variable is optional. It is not relevant for the examples described in this page, since they are all in the command-line interface. However, it is required when OAuth authentication is performed starting from a browser (e.g. through the <code>fractal-web</code> client), since the callback URL should be opened in the browser itself.</p>"},{"location":"internals/users/#authorization-code-flow","title":"Authorization Code Flow","text":"<p>Authentication via OAuth2 client is based on the Authorizion Code Flow, as described in this diagram</p> <p> </p> <p>(adapted from https://auth0.com/docs/get-started/authentication-and-authorization-flow/authorization-code-flow, \u00a9 2023 Okta, Inc.)   </p> <p>We can now review how <code>fractal-server</code> handles these steps:</p> <ul> <li> <p>Steps 1 \u2192 4</p> <ul> <li>The starting point is <code>/auth/client-name/authorize</code>;</li> <li>Here an <code>authorization_url</code> is generated and provided to the user;</li> <li>This URL will redirect the user to the Authorization Server (which is e.g. GitHub or Google, and not related to <code>fractal-server</code>), together with a <code>state</code> code for increased security;</li> <li>The user must authenticate and grant <code>fractal-server</code> the permissions it requires.</li> </ul> </li> <li> <p>Steps 5 \u2192 8</p> <ul> <li>The flow comes back to <code>fractal-server</code> at <code>/auth/client-name/callback</code>, together with the Authorization Code.</li> <li>A FastAPI dependency of the callback endpoint, <code>oauth2_authorize_callback</code>, takes care of exchanging this code for the Access Token.</li> </ul> </li> <li> <p>Steps 9 \u2192 10</p> <ul> <li>The callback endpoint uses the Access Token to obtain the user's email address and an account identifier from the Resource Server (which, depending on the client, may coincide with the Authorization Server).</li> </ul> </li> </ul> <p>After that, the callback endpoint performs some extra operations, which are not stricly part of the <code>OAuth2</code> protocol:</p> <ul> <li>It checks that <code>state</code> is still valid;</li> <li>If a user with the given email address doesn't already exist, it creates one with a random password;</li> <li>If the user has never authenticated with this <code>OAuth2</code> client before, it adds in the database a new entry to the <code>oauthaccount</code> table, properly linked to the <code>user_oauth</code> table`; at subsequent logins that entry will just be updated;</li> <li>It prepares a JWT token for the user and serves it in the Response Cookie.</li> </ul>"},{"location":"internals/users/#full-example","title":"Full example","text":"<p>A given <code>fractal-server</code> instance is registered as a GitHub App, and <code>fractal-server</code> is configured accordingly. A new user comes in, who wants to sign up using her GitHub account (associated to <code>person@university.edu</code>).</p> <p>First, she makes a call to <code>/auth/github/authorize</code>: <pre><code>$ curl \\\n    -X GET \\\n    http://127.0.0.1:8000/auth/github/authorize/\n\n{\n    \"authorization_url\":\"https://github.com/login/oauth/authorize/?\n        response_type=code&amp;\n        client_id=...&amp;\n        redirect_uri=...&amp;\n        state=...&amp;\n        scope=user+user%3Aemail\"\n}\n</code></pre></p> <p>Now the <code>authorization_url</code> must be visited using a browser. After logging in to GitHub, she is asked to grant the app the permissions it requires.</p> <p>After that, she is redirected back to <code>fractal-server</code> at <code>/auth/github/callback</code>, together with two query parameters: <pre><code>http://127.0.0.1:8000/auth/github/callback/?\n    code=...&amp;\n    state=...\n</code></pre></p> <p>The callback function does not return anything, but the response cookie contains a JWT token <pre><code>\"fastapiusersauth\": {\n    \"httpOnly\": true,\n    \"path\": \"/\",\n    \"samesite\": \"None\",\n    \"secure\": true,\n    \"value\": \"ey...\"     &lt;----- This is the JWT token\n}\n</code></pre></p> <p>The user can now make authenticated calls using this token, as in <pre><code>curl \\\n    -X GET \\\n    -H \"Authorization: Bearer ey...\" \\\n    http://127.0.0.1:8000/auth/current-user/\n\n{\n    \"id\":3,\n    \"email\":\"person@university.edu\",\n    \"is_active\":true,\n    \"is_superuser\":false,\n    \"is_verified\":false,\n    \"slurm_user\":null,\n    \"cache_dir\":null\n}\n</code></pre></p>"},{"location":"internals/users/#authorization","title":"Authorization","text":"<p>On top of being authenticated, a user must be authorized in order to perform specific actions in <code>fratal-server</code>:</p> <ol> <li>Some endpoints require the user to have a specific attribute (e.g. being <code>active</code> or being <code>superuser</code>);</li> <li>Access control is in-place for some database resources, and encode via database relationships with the User table (e.g. for `Project``);</li> <li>Additional business logic to regulate access may be defined within specific endpoints (e.g. for patching or removing a Task).</li> </ol> <p>The three cases are described more in detail below.</p>"},{"location":"internals/users/#user-attributes","title":"User attributes","text":"<p>Some endpoints require the user to have a specific attribute. This is implemented through a FastAPI dependencies, e.g. using fastapi_users.current_user: <pre><code>current_active_user = fastapi_users.current_user(active=True)\n\n# fake endpoint\n@router.get(\"/am/i/active/\")\nasync def am_i_active(\n    user: UserOAuth = Depends(current_active_user)\n):\n    return {f\"User {user.id}\":  \"you are active\"}\n</code></pre></p> <p>Being an active user (i.e. <code>user.is_active==True</code>) is required by</p> <ul> <li>all <code>/api/v1/...</code> endpoints</li> <li>all <code>/auth/users/...</code>,</li> <li>POST <code>/auth/register/</code>,</li> <li>GET <code>/auth/userlist/</code>,</li> <li>GET <code>/auth/current-user/</code>.</li> </ul> <p>Being a superuser (i.e. <code>user.is_superuser==True</code>) is required by</p> <ul> <li>all <code>/auth/users/...</code>,</li> <li>POST <code>/auth/register/</code>,</li> <li>GET <code>/auth/userlist/</code>.</li> </ul> <p>and it also gives full access (without further checks) to</p> <ul> <li>PATCH <code>/api/v1/task/{task_id}/</code></li> <li>DELETE <code>/api/v1/task/{task_id}/</code></li> </ul> <p>No endpoint currently requires the user to be verified (i.e. having <code>user.is_verified==True</code>).</p>"},{"location":"internals/users/#database-relationships","title":"Database relationships","text":"<p>The following resources in the <code>fractal-server</code> database are always related to a single <code>Project</code> (via their foreign key <code>project_id</code>):</p> <ul> <li><code>Dataset</code>,</li> <li><code>Workflow</code>,</li> <li><code>WorkflowTask</code> (through <code>Workflow</code>).</li> <li><code>ApplyWorkflow</code> (i.e. a workflow-execution job),</li> </ul> <p>Each endpoint that operates on one of these resources (or directly on a <code>Project</code>) requires the user to be in the <code>Project.user_list</code>.</p> <p>The <code>fractal-server</code> database structure is general, and the user/project relationships is a many-to-many one. However the API does not currently expose a feature to easily associate multiple users to the same project.</p>"},{"location":"internals/users/#endpoint-logic","title":"Endpoint logic","text":"<p>The User Model includes additional attributes <code>username</code> and <code>slurm_user</code>, which are optional and default to <code>None</code>. Apart from <code>slurm_user</code> being needed for User Impersonation in SLURM, these two attributes are also used for additional access control to <code>Task</code> resources.</p> <p>\u26a0\ufe0f This is an experimental feature, which will likely evolve in the future (possibly towards the implementation of user groups/roles).</p> <p>When a <code>Task</code> is created, the attribute <code>Task.owner</code> is set equal to <code>username</code> or, if not present, to <code>slurm_user</code> (there must be at least one to create a Task). With a similar logic, we consider a user to be the owner of a Task if <code>username==Task.owner</code> or, if <code>username</code> is <code>None</code>, we check that <code>slurm_user==Task.owner</code>. The following endpoints require a non-superuser user to be the owner of the Task:</p> <ul> <li>PATCH <code>/api/v1/task/{task_id}/</code>,</li> <li>DELETE <code>/api/v1/task/{task_id}/</code>.</li> </ul>"},{"location":"internals/users/#user-management","title":"User Management","text":"<p>The endpoints to manage users can be found under the route <code>/auth/</code>. On top of the <code>login/logout</code> ones (described above), several other endpoints are available, including all the ones exposed by FastAPI Users (see here). Here are more details for the most relevant endpoints.</p>"},{"location":"internals/users/#post-authregister","title":"POST <code>/auth/register</code>","text":"<p>\ud83d\udd10 Restricted to superusers.</p> <p>New users can be registred by a superuser at <code>/auth/register</code>:</p> <pre><code>$ curl \\\n    -X POST \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer ey...\" \\\n    -d '{\"email\": \"user@example.com\", \"password\": \"password\"}' \\\n    http://127.0.0.1:8000/auth/register/\n\n{\n    \"id\":2,\n    \"email\":\"user@example.com\",\n    \"is_active\":true,\n    \"is_superuser\":false,\n    \"is_verified\":false,\n    \"slurm_user\":null,\n    \"cache_dir\":null,\n    \"username\":null\n}\n</code></pre> <p>Here we provided <code>email</code> and <code>password</code>, which are the only required fields of <code>UserCreate</code>; we could also provide the following attributes: <code>is_active</code>, <code>is_superuser</code>, <code>is_verified</code>, <code>slurm_user</code>, <code>cache_dir</code>, <code>username</code>.</p>"},{"location":"internals/users/#get-authuserlist","title":"GET <code>/auth/userlist</code>","text":"<p>\ud83d\udd10 Restricted to superusers.</p> <p>The route <code>/auth/userlist</code> returns the list of all registred users:</p> <pre><code>$ curl \\\n    -X GET \\\n    -H \"Authorization: Bearer ey...\" \\\n    http://127.0.0.1:8000/auth/userlist/\n\n[\n    {\n        \"id\":1,\n        \"email\":\"admin@example.org\",\n        \"is_active\":true,\n        \"is_superuser\":true,\n        \"is_verified\":false,\n        \"slurm_user\":null,\n        \"cache_dir\":null,\n        \"username\":\"admin\"\n    },\n    {\n        \"id\":2,\n        \"email\":\"user@example.com\",\n        \"is_active\":true,\n        \"is_superuser\":false,\n        \"is_verified\":false,\n        \"slurm_user\":null,\n        \"cache_dir\":null,\n        \"username\":null\n    }\n]\n</code></pre>"},{"location":"internals/users/#get-authcurrent-user","title":"GET <code>/auth/current-user/</code>","text":"<p>At <code>/auth/current-user/</code>, authenticated users can get informations about themself: <pre><code>curl \\\n    -X GET \\\n    -H \"Authorization: Bearer ey...\" \\\n    http://127.0.0.1:8000/auth/current-user/\n\n{\n    \"id\":2,\n    \"email\":\"user@example.com\",\n    \"is_active\":true,\n    \"is_superuser\":false,\n    \"is_verified\":false,\n    \"slurm_user\":null,\n    \"cache_dir\":null,\n    \"username\":null\n}\n</code></pre></p>"},{"location":"internals/users/#patch-authcurrent-user","title":"PATCH <code>/auth/current-user/</code>","text":"<p>At <code>/auth/current-user/</code>, authenticated users can modify some of their attributes (namely <code>cache_dir</code>, as of fractal-server 1.4.0): <pre><code>curl \\\n    -X PATCH \\\n    -H \"Authorization: Bearer ey...\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"cache_dir\": \"/tmp/somewhere\"}' \\\n    http://127.0.0.1:8000/auth/current-user/\n\n{\n    \"id\":2,\n    \"email\":\"user@example.com\",\n    \"is_active\":true,\n    \"is_superuser\":false,\n    \"is_verified\":false,\n    \"slurm_user\":null,\n    \"cache_dir\":\"/tmp/somewhere\",\n    \"username\":null\n}\n</code></pre></p>"},{"location":"internals/users/#users-endpoints","title":"<code>/users</code> endpoints","text":"<p>\ud83d\udd10 Restricted to superusers.</p> <p>The additional user-management routes exposed by FastAPI Users in <code>/users</code> (see here) are available in <code>fractal-server</code> at  <code>/auth/users/</code>. For the moment all these routes are all restricted to superusers.</p> <p>GET <code>/{id}/</code></p> <p>Return the user with a given <code>id</code>.</p> <p>PATCH <code>/{id}/</code></p> <p>Update the user with a given <code>id</code>.</p> <p>Requires a <code>UserUpdate</code> payload.</p> <p>DELETE <code>/{id}/</code></p> <p>Delete the user with the given <code>id</code>.</p>"},{"location":"internals/runners/","title":"Runner Backends","text":"<p>Runner backends are responsible for scheduling and applying (running) tasks on your data. Fractal currently supports the integration with a SLURM cluster, with user impersonation handled either via <code>sudo -u</code> or by <code>ssh</code>. More details here.</p> <p>Moreover, a <code>local</code> backend exists for development and testing. This backend runs tasks locally, on the same host where <code>fractal-server</code> is running, and through a concurrent.futures <code>ThreadPoolExecutor</code>.</p>"},{"location":"internals/runners/local/","title":"Local backend","text":""},{"location":"internals/runners/local/#configuration","title":"Configuration","text":"<p>The logic for setting up the local-backend configuration of a given <code>WorkflowTask</code> is implemented in the get_local_config submodule.</p> <p>This configuration includes a single (optional) parameter, namely the integer variable <code>parallel_tasks_per_job</code>. This parameter is related to tasks that needs to be run in parallel over several inputs: When <code>parallel_tasks_per_job</code> is set, it will represent the maximum number of tasks that the backend will run at the same time.</p> <p>The typical intended use case is that setting <code>parallel_tasks_per_job</code> to a small number (e.g. <code>1</code>) will limit parallelism when executing tasks requiring a large amount of resources (e.g. memory).</p> <p>The different sources for <code>parallel_tasks_per_job</code> are:</p> <ol> <li>If the <code>WorkflowTask.meta</code> field has a <code>parallel_tasks_per_job</code> key, the corresponding value takes highest priority;</li> <li>Next priority goes to a <code>parallel_tasks_per_job</code> entry in <code>WorkflowTask.task.meta</code>;</li> <li>Next priority goes to the configuration in <code>FRACTAL_LOCAL_CONFIG_FILE_zzz</code>, a JSON file that may contain a definition of a    <code>LocalBackendConfig</code> object like <pre><code>{\n  \"parallel_tasks_per_job\": 1\n}\n</code></pre></li> <li>Lowest-priority (that is, the default) is to set <code>parallel_tasks_per_job=None</code>, which corresponds to not limiting parallelism at all.</li> </ol>"},{"location":"internals/runners/slurm/","title":"SLURM backends","text":"<p>NOTE: <code>SlurmConfig</code> objects are created internally in <code>fractal-server</code>, and they are not meant to be initialized by the user; the same holds for <code>SlurmConfig</code> attributes (e.g. <code>mem_per_task_MB</code>) which are not meant to be part of the <code>FRACTAL_SLURM_CONFIG_FILE_zzz</code> JSON file (details on the expected file content are defined in the <code>SlurmConfigFile</code> model).</p>"},{"location":"internals/runners/slurm/#slurm-configuration","title":"SLURM configuration","text":"<p>The logic for setting up the SLURM configuration of a given <code>WorkflowTask</code> is implemented in the <code>slurm_config.py</code> submodule.</p> <p>The different sources for SLURM configuration options (like <code>partition</code>, <code>cpus_per_task</code>, ...) are:</p> <ol> <li>All attributes that are explicitly set in the <code>WorkflowTask.meta</code> dictionary attribute take highest priority;</li> <li>Next priority goes to all attributes that are explicitly set in the <code>WorkflowTask.task.meta</code> dictionary attribute;</li> <li>Lowest-priority (that is default) values come from the configuration in <code>FRACTAL_SLURM_CONFIG_FILE_zzz</code>.</li> </ol>"},{"location":"internals/runners/slurm/#example","title":"Example","text":"<p>The configuration file could be the one defined here, while a certain <code>WorkflowTask</code> could have <pre><code>workflow_task.meta = {\"cpus_per_task\": 3}\nworkflow_task.task.meta = {\"cpus_per_task\": 2, \"mem\": \"10G\"}\n</code></pre> In this case, the SLURM configuration for this <code>WorkflowTask</code> will correspond to <pre><code>partition=main\ncpus_per_task=3\nmem=10G\n</code></pre></p>"},{"location":"internals/runners/slurm/#exporting-environment-variables","title":"Exporting environment variables","text":"<p>The <code>fractal-server</code> admin may need to set some global variables that need to be included in all SLURM submission scripts; this can be achieved via the <code>extra_lines</code> field in the SLURM configuration file, for instance as in <pre><code>{\n  \"default_slurm_config\": {\n    \"partition\": \"main\",\n    \"extra_lines\": [\n      \"export SOMEVARIABLE=123\",\n      \"export ANOTHERVARIABLE=ABC\"\n    ]\n  }\n}\n</code></pre></p> <p>There exists another use case where the value of a variable depends on the user who runs a certain task. A relevant example is that user A (who will run the task via SLURM) needs to define the cache-directory paths for some libraries they use (and those must be paths where user A can write).  This use case is also supported in the specs of <code>fractal-server</code> SLURM configuration file: If this file includes a block like <pre><code>{\n  ...\n  \"user_local_exports\": {\n    \"LIBRARY_1_CACHE_DIR\": \"somewhere/library_1\",\n    \"LIBRARY_2_FILE\": \"somewhere/else/library_2.json\"\n  }\n}\n</code></pre> then the SLURM submission script will include the lines <pre><code>...\nexport LIBRARY_1_CACHE_DIR=/my/cache/somewhere/library_1\nexport LIBRARY_2_FILE=/my/cache/somewhere/else/library_2.json\n...\n</code></pre> Note that all paths in the values of <code>user_local_exports</code> are interpreted as relative to a base directory which is user-specific (for instance <code>/my/cache/</code>, in the example above), and which is defined in the <code>User.settings.cache_dir</code> attribute. Also note that in this case <code>fractal-server</code> only compiles the configuration options into lines of the SLURM submission script, without performing any check on the validity of the given paths.</p>"},{"location":"internals/runners/slurm/#slurm-batching","title":"SLURM batching","text":"<p>The SLURM backend in <code>fractal-server</code> may combine multiple tasks in the same SLURM job (AKA batching), in order to reduce the total number of SLURM jobs that are submitted. This is especially relevant for clusters with constraints on the number of jobs that a user is allowed to submit over a certain timespan.</p> <p>The logic for handling the batching parameters (that is, how many tasks can be combined in the same SLURM job, and how many of them can run in parallel) is implemented in this submodule.</p>"},{"location":"internals/runners/slurm/#user-impersonation","title":"User impersonation","text":""},{"location":"internals/runners/slurm/#sudo-based-impersonation","title":"<code>sudo</code>-based impersonation","text":"<p>The user who runs <code>fractal-server</code> must have sufficient priviliges for running some commands via <code>sudo -u</code> to impersonate other users of the SLURM cluster without any password. The required commands include <code>sbatch</code>, <code>scancel</code>, <code>cat</code>, <code>ls</code> and <code>mkdir</code>. An example of how to achieve this is to add this block to the <code>sudoers</code> file: <pre><code>Runas_Alias FRACTAL_IMPERSONATE_USERS = fractal, user1, user2, user3\nCmnd_Alias FRACTAL_CMD = /usr/bin/sbatch, /usr/bin/scancel, /usr/bin/cat, /usr/bin/ls, /usr/bin/mkdir\nfractal ALL=(FRACTAL_IMPERSONATE_USERS) NOPASSWD:FRACTAL_CMD\n</code></pre> where <code>fractal</code> is the user running <code>fractal-server</code>, and <code>{user1,user2,user3}</code> are the users who can be impersonated. Note that one could also grant <code>fractal</code> the option of impersonating a whole UNIX group, instead of listing users one by one.</p>"},{"location":"internals/runners/slurm/#ssh-based-impersonation","title":"SSH-based impersonation","text":"<p>In this scenario, one or many service users exist on the SLURM cluster, which will be the one running all jobs. The user settings for each Fractal user determines which service user will be impersonated (through SSH) when connecting to the cluster to run jobs.</p>"},{"location":"internals/version_upgrades/","title":"Version upgrades","text":"<p>Here are additional details about some version upgrades:</p> <ul> <li>Upgrade from fractal-server 1.2.5 to 1.3.0.</li> </ul>"},{"location":"internals/version_upgrades/upgrade_1_2_5_to_1_3_0/","title":"Upgrade from 1.2.5 to 1.3.0","text":"<p>A large part of endpoints were updated, mostly to move the foreign-key IDs from the Pydantic models used to validate the request payload to the endpoint path. When necessary, some of those same foreign-key IDs are now passed as query parameters (rather than body or path parameters). The rationale behind this refactor is that endpoints paths are now more consistent:</p> <ul> <li>They have a hierarchical structure, when appropriate (e.g. a project may   contain a datasets, workflows and jobs, while a workflow may contain   workflowtasks, ...).</li> <li>They are more consistently defined and more transparently related to CRUD   operations (see e.g.   here).</li> </ul> <p>The list of updated endpoints is below. Note that the IDs that are now part of the path/query parameters are not required any more as part of the body parameters.</p> <pre><code>OLD GET /api/v1/job/download/{job_id}\nNEW GET /api/v1/project/{project_id}/job/{job_id}/download/\n\nOLD GET /api/v1/job/{job_id}\nNEW GET /api/v1/project/{project_id}/job/{job_id}\n\nOLD GET /api/v1/project/{project_id}/jobs/\nNEW GET /api/v1/project/{project_id}/job/\n\nOLD POST /api/v1/project/{project_id}/{dataset_id}\nNEW POST /api/v1/project/{project_id}/dataset/{dataset_id}/resource/\n\nOLD GET /api/v1/project/{project_id}/{dataset_id}\nNEW GET /api/v1/project/{project_id}/dataset/{dataset_id}\n\nOLD GET /api/v1/project/{project_id}/{dataset_id}/resources/\nNEW GET /api/v1/project/{project_id}/dataset/{dataset_id}/resource/\n\nOLD PATCH /api/v1/project/{project_id}/{dataset_id}\nNEW PATCH /api/v1/project/{project_id}/dataset/{dataset_id}\n\nOLD PATCH /api/v1/project/{project_id}/{dataset_id}/{resource_id}\nNEW PATCH /api/v1/project/{project_id}/dataset/{dataset_id}/resource/{resource_id}\n\nOLD DELETE /api/v1/project/{project_id}/{dataset_id}\nNEW DELETE /api/v1/project/{project_id}/dataset/{dataset_id}\n\nOLD DELETE /api/v1/project/{project_id}/{dataset_id}/{resource_id}\nNEW DELETE /api/v1/project/{project_id}/dataset/{dataset_id}/resource/{resource_id}\n\nOLD PATCH /api/v1/workflow/{workflow_id}/edit-task/{workflow_task_id}\nNEW PATCH /api/v1/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}\n\nOLD DELETE /api/v1/workflow/{workflow_id}/rm-task/{workflow_task_id}\nNEW DELETE /api/v1/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}\n\nOLD POST /api/v1/workflow/{workflow_id}/add-task/\nNEW POST /api/v1/project/{project_id}/workflow/{workflow_id}/wftask/\nNEW QUERY PARAMETERS: ['task_id']\n\nOLD POST /api/v1/project/{project_id}/import-workflow/\nNEW POST /api/v1/project/{project_id}/workflow/import/\n\nOLD POST /api/v1/project/apply/\nNEW POST /api/v1/project/{project_id}/workflow/{workflow_id}/apply/\nNEW QUERY PARAMETERS: ['input_dataset_id', 'output_dataset_id']\n\nOLD POST /api/v1/workflow/\nNEW POST /api/v1/project/{project_id}/workflow/\n\nOLD GET /api/v1/project/{project_id}/workflows/\nNEW GET /api/v1/project/{project_id}/workflow/\n\nOLD GET /api/v1/workflow/{workflow_id}\nNEW GET /api/v1/project/{project_id}/workflow/{workflow_id}\n\nOLD GET /api/v1/workflow/{workflow_id}/export/\nNEW GET /api/v1/project/{project_id}/workflow/{workflow_id}/export/\n\nOLD PATCH /api/v1/workflow/{workflow_id}\nNEW PATCH /api/v1/project/{project_id}/workflow/{workflow_id}\n\nOLD DELETE /api/v1/workflow/{workflow_id}\nNEW DELETE /api/v1/project/{project_id}/workflow/{workflow_id}\n\nOLD POST /api/v1/project/{project_id}/\nNEW POST /api/v1/project/{project_id}/dataset/\n\nNEW GET /api/v1/project/{project_id}/job/{job_id}/stop/\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>fractal_server<ul> <li>app<ul> <li>db</li> <li>models<ul> <li>linkusergroup</li> <li>linkuserproject</li> <li>security</li> <li>user_settings</li> <li>v2<ul> <li>accounting</li> <li>dataset</li> <li>history</li> <li>job</li> <li>profile</li> <li>project</li> <li>resource</li> <li>task</li> <li>task_group</li> <li>workflow</li> <li>workflowtask</li> </ul> </li> </ul> </li> <li>routes<ul> <li>admin<ul> <li>v2<ul> <li>_aux_functions</li> <li>accounting</li> <li>impersonate</li> <li>job</li> <li>profile</li> <li>project</li> <li>resource</li> <li>task</li> <li>task_group</li> <li>task_group_lifecycle</li> </ul> </li> </ul> </li> <li>api<ul> <li>v2<ul> <li>_aux_functions</li> <li>_aux_functions_history</li> <li>_aux_functions_task_lifecycle</li> <li>_aux_functions_task_version_update</li> <li>_aux_functions_tasks</li> <li>_aux_task_group_disambiguation</li> <li>dataset</li> <li>history</li> <li>images</li> <li>job</li> <li>pre_submission_checks</li> <li>project</li> <li>status_legacy</li> <li>submit</li> <li>task</li> <li>task_collection</li> <li>task_collection_custom</li> <li>task_collection_pixi</li> <li>task_group</li> <li>task_group_lifecycle</li> <li>task_version_update</li> <li>workflow</li> <li>workflow_import</li> <li>workflowtask</li> </ul> </li> </ul> </li> <li>auth<ul> <li>_aux_auth</li> <li>current_user</li> <li>group</li> <li>login</li> <li>oauth</li> <li>register</li> <li>router</li> <li>users</li> </ul> </li> <li>aux<ul> <li>_job</li> <li>_runner</li> <li>validate_user_profile</li> </ul> </li> <li>pagination</li> </ul> </li> <li>schemas<ul> <li>user</li> <li>user_group</li> <li>v2<ul> <li>accounting</li> <li>dataset</li> <li>dumps</li> <li>history</li> <li>job</li> <li>manifest</li> <li>profile</li> <li>project</li> <li>resource</li> <li>status_legacy</li> <li>task</li> <li>task_collection</li> <li>task_group</li> <li>workflow</li> <li>workflowtask</li> </ul> </li> </ul> </li> <li>security<ul> <li>signup_email</li> </ul> </li> <li>shutdown</li> </ul> </li> <li>config<ul> <li>_data</li> <li>_database</li> <li>_email</li> <li>_main</li> <li>_oauth</li> <li>_settings_config</li> </ul> </li> <li>exceptions</li> <li>gunicorn_fractal</li> <li>images<ul> <li>models</li> <li>status_tools</li> <li>tools</li> </ul> </li> <li>logger</li> <li>main</li> <li>runner<ul> <li>components</li> <li>config<ul> <li>_local</li> <li>_slurm</li> <li>slurm_mem_to_MB</li> </ul> </li> <li>exceptions</li> <li>executors<ul> <li>base_runner</li> <li>call_command_wrapper</li> <li>local<ul> <li>get_local_config</li> <li>runner</li> </ul> </li> <li>slurm_common<ul> <li>_batching</li> <li>_job_states</li> <li>base_slurm_runner</li> <li>get_slurm_config</li> <li>remote</li> <li>slurm_config</li> <li>slurm_job_task_models</li> </ul> </li> <li>slurm_ssh<ul> <li>run_subprocess</li> <li>runner</li> <li>tar_commands</li> </ul> </li> <li>slurm_sudo<ul> <li>_subprocess_run_as_user</li> <li>runner</li> </ul> </li> </ul> </li> <li>filenames</li> <li>set_start_and_last_task_index</li> <li>task_files</li> <li>v2<ul> <li>_local</li> <li>_slurm_ssh</li> <li>_slurm_sudo</li> <li>db_tools</li> <li>deduplicate_list</li> <li>merge_outputs</li> <li>runner</li> <li>runner_functions</li> <li>submit_workflow</li> <li>task_interface</li> </ul> </li> <li>versions</li> </ul> </li> <li>ssh<ul> <li>_fabric</li> </ul> </li> <li>string_tools</li> <li>syringe</li> <li>tasks<ul> <li>config<ul> <li>_pixi</li> <li>_python</li> </ul> </li> <li>utils</li> <li>v2<ul> <li>local<ul> <li>_utils</li> <li>collect</li> <li>collect_pixi</li> <li>deactivate</li> <li>deactivate_pixi</li> <li>delete</li> <li>reactivate</li> <li>reactivate_pixi</li> </ul> </li> <li>ssh<ul> <li>_pixi_slurm_ssh</li> <li>_utils</li> <li>collect</li> <li>collect_pixi</li> <li>deactivate</li> <li>deactivate_pixi</li> <li>delete</li> <li>reactivate</li> <li>reactivate_pixi</li> </ul> </li> <li>utils_background</li> <li>utils_database</li> <li>utils_package_names</li> <li>utils_pixi</li> <li>utils_python_interpreter</li> <li>utils_templates</li> </ul> </li> </ul> </li> <li>types<ul> <li>validators<ul> <li>_common_validators</li> <li>_filter_validators</li> <li>_workflow_task_arguments_validators</li> </ul> </li> </ul> </li> <li>urls</li> <li>utils</li> <li>zip_tools</li> </ul> </li> </ul>"},{"location":"reference/fractal_server/","title":"fractal_server","text":""},{"location":"reference/fractal_server/exceptions/","title":"exceptions","text":""},{"location":"reference/fractal_server/gunicorn_fractal/","title":"gunicorn_fractal","text":""},{"location":"reference/fractal_server/gunicorn_fractal/#fractal_server.gunicorn_fractal.FractalWorker","title":"<code>FractalWorker</code>","text":"<p>               Bases: <code>UvicornWorker</code></p> <p>Subclass of uvicorn workers, which also captures SIGABRT and handles it within the <code>custom_handle_abort</code> method.</p> Source code in <code>fractal_server/gunicorn_fractal.py</code> <pre><code>class FractalWorker(UvicornWorker):\n    \"\"\"\n    Subclass of uvicorn workers, which also captures SIGABRT and handles\n    it within the `custom_handle_abort` method.\n    \"\"\"\n\n    def init_signals(self) -&gt; None:\n        super().init_signals()\n        signal.signal(signal.SIGABRT, self.custom_handle_abort)\n        logger.info(\n            f\"[FractalWorker.init_signals - pid={self.pid}] \"\n            \"Set `custom_handle_abort` for SIGABRT\"\n        )\n\n    def custom_handle_abort(self, sig, frame):\n        \"\"\"\n        Custom version of `gunicorn.workers.base.Worker.handle_abort`,\n        transforming SIGABRT into SIGTERM.\n        \"\"\"\n        self.alive = False\n        logger.info(\n            f\"[FractalWorker.custom_handle_abort - pid={self.pid}] \"\n            \"Now send SIGTERM to process.\"\n        )\n        os.kill(self.pid, signal.SIGTERM)\n</code></pre>"},{"location":"reference/fractal_server/gunicorn_fractal/#fractal_server.gunicorn_fractal.FractalWorker.custom_handle_abort","title":"<code>custom_handle_abort(sig, frame)</code>","text":"<p>Custom version of <code>gunicorn.workers.base.Worker.handle_abort</code>, transforming SIGABRT into SIGTERM.</p> Source code in <code>fractal_server/gunicorn_fractal.py</code> <pre><code>def custom_handle_abort(self, sig, frame):\n    \"\"\"\n    Custom version of `gunicorn.workers.base.Worker.handle_abort`,\n    transforming SIGABRT into SIGTERM.\n    \"\"\"\n    self.alive = False\n    logger.info(\n        f\"[FractalWorker.custom_handle_abort - pid={self.pid}] \"\n        \"Now send SIGTERM to process.\"\n    )\n    os.kill(self.pid, signal.SIGTERM)\n</code></pre>"},{"location":"reference/fractal_server/logger/","title":"logger","text":"<p>This module provides logging utilities</p>"},{"location":"reference/fractal_server/logger/#fractal_server.logger.close_logger","title":"<code>close_logger(logger)</code>","text":"<p>Close all handlers associated to a <code>logging.Logger</code> object</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>The actual logger</p> required Source code in <code>fractal_server/logger.py</code> <pre><code>def close_logger(logger: logging.Logger) -&gt; None:\n    \"\"\"\n    Close all handlers associated to a `logging.Logger` object\n\n    Args:\n        logger: The actual logger\n    \"\"\"\n    for handle in logger.handlers:\n        handle.close()\n</code></pre>"},{"location":"reference/fractal_server/logger/#fractal_server.logger.config_uvicorn_loggers","title":"<code>config_uvicorn_loggers()</code>","text":"<p>Change the formatter for the uvicorn access/error loggers.</p> <p>This is similar to https://stackoverflow.com/a/68864979/19085332. See also https://github.com/tiangolo/fastapi/issues/1508.</p> <p>This function is meant to work in two scenarios:</p> <ol> <li>The most relevant case is for a <code>gunicorn</code> startup command, with    <code>--access-logfile</code> and <code>--error-logfile</code> options set.</li> <li>The case of <code>fractalctl start</code> (directly calling <code>uvicorn</code>).</li> </ol> <p>Because of the second use case, we need to check whether uvicorn loggers already have a handler. If not, we skip the formatting.</p> Source code in <code>fractal_server/logger.py</code> <pre><code>def config_uvicorn_loggers():\n    \"\"\"\n    Change the formatter for the uvicorn access/error loggers.\n\n    This is similar to https://stackoverflow.com/a/68864979/19085332. See also\n    https://github.com/tiangolo/fastapi/issues/1508.\n\n    This function is meant to work in two scenarios:\n\n    1. The most relevant case is for a `gunicorn` startup command, with\n       `--access-logfile` and `--error-logfile` options set.\n    2. The case of `fractalctl start` (directly calling `uvicorn`).\n\n    Because of the second use case, we need to check whether uvicorn loggers\n    already have a handler. If not, we skip the formatting.\n    \"\"\"\n\n    access_logger = logging.getLogger(\"uvicorn.access\")\n    if len(access_logger.handlers) &gt; 0:\n        access_logger.handlers[0].setFormatter(LOG_FORMATTER)\n\n    error_logger = logging.getLogger(\"uvicorn.error\")\n    if len(error_logger.handlers) &gt; 0:\n        error_logger.handlers[0].setFormatter(LOG_FORMATTER)\n</code></pre>"},{"location":"reference/fractal_server/logger/#fractal_server.logger.get_logger","title":"<code>get_logger(logger_name=None)</code>","text":"<p>Wrap the <code>logging.getLogger</code> function.</p> <p>The typical use case for this function is to retrieve a logger that was already defined, as in the following example: <pre><code>def function1(logger_name):\n    logger = get_logger(logger_name)\n    logger.info(\"Info from function1\")\n\ndef funtion2():\n    logger_name = \"my_logger\"\n    logger = set_logger(logger_name)\n    logger.info(\"Info from function2\")\n    function1(logger_name)\n    close_logger(logger)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>logger_name</code> <code>str | None</code> <p>Name of logger</p> <code>None</code> <p>Returns:     Logger with name <code>logger_name</code></p> Source code in <code>fractal_server/logger.py</code> <pre><code>def get_logger(logger_name: str | None = None) -&gt; logging.Logger:\n    \"\"\"\n    Wrap the\n    [`logging.getLogger`](https://docs.python.org/3/library/logging.html#logging.getLogger)\n    function.\n\n    The typical use case for this function is to retrieve a logger that was\n    already defined, as in the following example:\n    ```python\n    def function1(logger_name):\n        logger = get_logger(logger_name)\n        logger.info(\"Info from function1\")\n\n    def funtion2():\n        logger_name = \"my_logger\"\n        logger = set_logger(logger_name)\n        logger.info(\"Info from function2\")\n        function1(logger_name)\n        close_logger(logger)\n    ```\n\n    Args:\n        logger_name: Name of logger\n    Returns:\n        Logger with name `logger_name`\n    \"\"\"\n    return logging.getLogger(logger_name)\n</code></pre>"},{"location":"reference/fractal_server/logger/#fractal_server.logger.reset_logger_handlers","title":"<code>reset_logger_handlers(logger)</code>","text":"<p>Close and remove all handlers associated to a <code>logging.Logger</code> object</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>The actual logger</p> required Source code in <code>fractal_server/logger.py</code> <pre><code>def reset_logger_handlers(logger: logging.Logger) -&gt; None:\n    \"\"\"\n    Close and remove all handlers associated to a `logging.Logger` object\n\n    Args:\n        logger: The actual logger\n    \"\"\"\n    close_logger(logger)\n    logger.handlers.clear()\n</code></pre>"},{"location":"reference/fractal_server/logger/#fractal_server.logger.set_logger","title":"<code>set_logger(logger_name, *, log_file_path=None, default_logging_level=None)</code>","text":"<p>Set up a <code>fractal-server</code> logger</p> <p>The logger (a <code>logging.Logger</code> object) will have the following properties:</p> <ul> <li>The attribute <code>Logger.propagate</code> set to <code>False</code>;</li> <li>One and only one <code>logging.StreamHandler</code> handler, with severity level set to <code>FRACTAL_LOGGING_LEVEL</code> (or <code>default_logging_level</code>, if set), and formatter set as in the <code>logger.LOG_FORMAT</code> variable from the current module;</li> <li>One or many <code>logging.FileHandler</code> handlers, including one pointint to <code>log_file_path</code> (if set); all these handlers have severity level set to <code>logging.DEBUG</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>logger_name</code> <code>str</code> <p>The identifier of the logger.</p> required <code>log_file_path</code> <code>str | Path | None</code> <p>Path to the log file.</p> <code>None</code> <code>default_logging_level</code> <code>int | None</code> <p>Override for <code>settings.FRACTAL_LOGGING_LEVEL</code></p> <code>None</code> <p>Returns:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>The logger, as configured by the arguments.</p> Source code in <code>fractal_server/logger.py</code> <pre><code>def set_logger(\n    logger_name: str,\n    *,\n    log_file_path: str | Path | None = None,\n    default_logging_level: int | None = None,\n) -&gt; logging.Logger:\n    \"\"\"\n    Set up a `fractal-server` logger\n\n    The logger (a `logging.Logger` object) will have the following properties:\n\n    * The attribute `Logger.propagate` set to `False`;\n    * One and only one `logging.StreamHandler` handler, with severity level set\n    to `FRACTAL_LOGGING_LEVEL` (or `default_logging_level`, if set), and\n    formatter set as in the `logger.LOG_FORMAT`\n    variable from the current module;\n    * One or many `logging.FileHandler` handlers, including one pointint to\n    `log_file_path` (if set); all these handlers have severity level set to\n    `logging.DEBUG`.\n\n    Args:\n        logger_name: The identifier of the logger.\n        log_file_path: Path to the log file.\n        default_logging_level: Override for `settings.FRACTAL_LOGGING_LEVEL`\n\n    Returns:\n        logger: The logger, as configured by the arguments.\n    \"\"\"\n\n    logger = logging.getLogger(logger_name)\n    logger.propagate = False\n    logger.setLevel(logging.DEBUG)\n\n    current_stream_handlers = [\n        handler\n        for handler in logger.handlers\n        if isinstance(handler, logging.StreamHandler)\n    ]\n\n    if not current_stream_handlers:\n        stream_handler = logging.StreamHandler()\n        if default_logging_level is None:\n            settings = Inject(get_settings)\n            default_logging_level = settings.FRACTAL_LOGGING_LEVEL\n        stream_handler.setLevel(default_logging_level)\n        stream_handler.setFormatter(LOG_FORMATTER)\n        logger.addHandler(stream_handler)\n\n    if log_file_path is not None:\n        file_handler = logging.FileHandler(log_file_path, mode=\"a\")\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(LOG_FORMATTER)\n        file_handler.setFormatter(LOG_FORMATTER)\n        logger.addHandler(file_handler)\n        current_file_handlers = [\n            handler\n            for handler in logger.handlers\n            if isinstance(handler, logging.FileHandler)\n        ]\n        if len(current_file_handlers) &gt; 1:\n            logger.warning(\n                f\"Logger {logger_name} has multiple file handlers: \"\n                f\"{current_file_handlers}\"\n            )\n\n    return logger\n</code></pre>"},{"location":"reference/fractal_server/main/","title":"main","text":""},{"location":"reference/fractal_server/main/#fractal_server.main.check_settings","title":"<code>check_settings()</code>","text":"<p>Check and register the settings</p> <p>Verify the consistency of the settings, in particular that required variables are set.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the configuration is invalid.</p> Source code in <code>fractal_server/main.py</code> <pre><code>def check_settings() -&gt; None:\n    \"\"\"\n    Check and register the settings\n\n    Verify the consistency of the settings, in particular that required\n    variables are set.\n\n    Raises:\n        ValidationError: If the configuration is invalid.\n    \"\"\"\n    settings = Inject(get_settings)\n    db_settings = Inject(get_db_settings)\n    email_settings = Inject(get_email_settings)\n    data_settings = Inject(get_data_settings)\n    logger = set_logger(\"fractal_server_settings\")\n    logger.debug(\"Fractal Settings:\")\n    for key, value in chain(\n        db_settings.model_dump().items(),\n        settings.model_dump().items(),\n        email_settings.model_dump().items(),\n        data_settings.model_dump().items(),\n    ):\n        if any(s in key.upper() for s in [\"PASSWORD\", \"SECRET\", \"KEY\"]):\n            value = \"*****\"\n        logger.debug(f\"  {key}: {value}\")\n    reset_logger_handlers(logger)\n</code></pre>"},{"location":"reference/fractal_server/main/#fractal_server.main.collect_routers","title":"<code>collect_routers(app)</code>","text":"<p>Register the routers to the application</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>FastAPI</code> <p>The application to register the routers to.</p> required Source code in <code>fractal_server/main.py</code> <pre><code>def collect_routers(app: FastAPI) -&gt; None:\n    \"\"\"\n    Register the routers to the application\n\n    Args:\n        app:\n            The application to register the routers to.\n    \"\"\"\n    from .app.routes.api import router_api\n    from .app.routes.api.v2 import router_api_v2\n    from .app.routes.admin.v2 import router_admin_v2\n    from .app.routes.auth.router import router_auth\n\n    app.include_router(router_api, prefix=\"/api\")\n    app.include_router(router_api_v2, prefix=\"/api/v2\")\n    app.include_router(\n        router_admin_v2, prefix=\"/admin/v2\", tags=[\"V2 Admin area\"]\n    )\n    app.include_router(router_auth, prefix=\"/auth\", tags=[\"Authentication\"])\n</code></pre>"},{"location":"reference/fractal_server/main/#fractal_server.main.start_application","title":"<code>start_application()</code>","text":"<p>Create the application, initialise it and collect all available routers.</p> <p>Returns:</p> Name Type Description <code>app</code> <code>FastAPI</code> <p>The fully initialised application.</p> Source code in <code>fractal_server/main.py</code> <pre><code>def start_application() -&gt; FastAPI:\n    \"\"\"\n    Create the application, initialise it and collect all available routers.\n\n    Returns:\n        app:\n            The fully initialised application.\n    \"\"\"\n    app = FastAPI(lifespan=lifespan)\n    collect_routers(app)\n    return app\n</code></pre>"},{"location":"reference/fractal_server/string_tools/","title":"string_tools","text":""},{"location":"reference/fractal_server/string_tools/#fractal_server.string_tools.interpret_as_bool","title":"<code>interpret_as_bool(value)</code>","text":"<p>Interpret a boolean or a string representation of a boolean as a boolean value.</p> <p>Accepts either a boolean (<code>True</code> or <code>False</code>) or a case-insensitive string representation of a boolean (\"true\" or \"false\"). Returns the corresponding boolean value.</p> Source code in <code>fractal_server/string_tools.py</code> <pre><code>def interpret_as_bool(value: bool | str) -&gt; bool:\n    \"\"\"\n    Interpret a boolean or a string representation of a boolean as a boolean\n    value.\n\n    Accepts either a boolean (`True` or `False`) or a case-insensitive string\n    representation of a boolean (\"true\" or \"false\").\n    Returns the corresponding boolean value.\n    \"\"\"\n    if isinstance(value, bool):\n        return value\n    elif isinstance(value, str):\n        value_lower = value.lower()\n        if value_lower == \"true\":\n            return True\n        elif value_lower == \"false\":\n            return False\n        else:\n            raise ValueError(\"String must be 'true' or 'false'.\")\n    else:\n        raise TypeError(f\"Expected bool or str, got {type(value)}: '{value}'.\")\n</code></pre>"},{"location":"reference/fractal_server/string_tools/#fractal_server.string_tools.sanitize_string","title":"<code>sanitize_string(value)</code>","text":"<p>Make string safe to be used in file/folder names and subprocess commands.</p> <p>Make the string lower-case, and replace any special character with an underscore, where special characters are:</p> <pre><code>&gt;&gt;&gt; string.punctuation\n'!\"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~'\n&gt;&gt;&gt; string.whitespace\n' \\t\\n\\r\\x0b\\x0c'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Input string</p> required <p>Returns:</p> Type Description <code>str</code> <p>Sanitized value</p> Source code in <code>fractal_server/string_tools.py</code> <pre><code>def sanitize_string(value: str) -&gt; str:\n    \"\"\"\n    Make string safe to be used in file/folder names and subprocess commands.\n\n    Make the string lower-case, and replace any special character with an\n    underscore, where special characters are:\n\n\n        &gt;&gt;&gt; string.punctuation\n        '!\"#$%&amp;\\'()*+,-./:;&lt;=&gt;?@[\\\\\\\\]^_`{|}~'\n        &gt;&gt;&gt; string.whitespace\n        ' \\\\t\\\\n\\\\r\\\\x0b\\\\x0c'\n\n    Args:\n        value: Input string\n\n    Returns:\n        Sanitized value\n    \"\"\"\n    new_value = value.lower()\n    for character in __SPECIAL_CHARACTERS__:\n        new_value = new_value.replace(character, \"_\")\n    return new_value\n</code></pre>"},{"location":"reference/fractal_server/string_tools/#fractal_server.string_tools.validate_cmd","title":"<code>validate_cmd(command, *, allow_char=None, attribute_name='Command')</code>","text":"<p>Assert that the provided <code>command</code> does not contain any of the forbidden characters for commands (fractal_server.string_tools.NOT_ALLOWED_FOR_COMMANDS)</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>command to validate.</p> required <code>allow_char</code> <code>str | None</code> <p>chars to accept among the forbidden ones</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute, to be used in error message.</p> <code>'Command'</code> Source code in <code>fractal_server/string_tools.py</code> <pre><code>def validate_cmd(\n    command: str,\n    *,\n    allow_char: str | None = None,\n    attribute_name: str = \"Command\",\n):\n    \"\"\"\n    Assert that the provided `command` does not contain any of the forbidden\n    characters for commands\n    (fractal_server.string_tools.__NOT_ALLOWED_FOR_COMMANDS__)\n\n    Args:\n        command: command to validate.\n        allow_char: chars to accept among the forbidden ones\n        attribute_name: Name of the attribute, to be used in error message.\n    \"\"\"\n    if not isinstance(command, str):\n        raise ValueError(f\"{command=} is not a string.\")\n    forbidden = set(__NOT_ALLOWED_FOR_COMMANDS__)\n    if allow_char is not None:\n        forbidden = forbidden - set(allow_char)\n    if not forbidden.isdisjoint(set(command)):\n        raise ValueError(\n            f\"{attribute_name} must not contain any of this characters: \"\n            f\"'{forbidden}'\\n\"\n            f\"Provided {attribute_name.lower()}: '{command}'.\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/syringe/","title":"syringe","text":"<p>This module provides an extremely simple utility for dependency injection.</p> <p>It's made up of a single singleton class that provides a directory for the dependencies. The dependencies are stored in a dictionary and can be overridden or popped from the directory.</p>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe--usage","title":"Usage:","text":"<pre><code>&gt;&gt;&gt; from syringe import Inject\n&gt;&gt;&gt; def foo():\n&gt;&gt;&gt;     return 42\n&gt;&gt;&gt;\n&gt;&gt;&gt; def oof():\n&gt;&gt;&gt;     return 24\n&gt;&gt;&gt;\n&gt;&gt;&gt; def bar():\n&gt;&gt;&gt;     return Inject(foo)\n&gt;&gt;&gt;\n&gt;&gt;&gt; bar()\n42\n&gt;&gt;&gt; Inject.override(foo, oof)\n&gt;&gt;&gt; bar()\n24\n&gt;&gt;&gt; Inject.pop(foo)\n&gt;&gt;&gt; bar()\n42\n</code></pre>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe.Inject","title":"<code>Inject = _Inject()</code>  <code>module-attribute</code>","text":"<p>The singleton instance of <code>_Inject</code>, the only public member of this module.</p>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe._Inject","title":"<code>_Inject</code>","text":"<p>Injection class</p> <p>This is a private class that is never directly instantiated.</p> <p>Attributes:</p> Name Type Description <code>_dependencies</code> <code>dict[Any, Any]</code> <p>The dependency directory</p> Source code in <code>fractal_server/syringe.py</code> <pre><code>class _Inject:\n    \"\"\"\n    Injection class\n\n    This is a private class that is never directly instantiated.\n\n    Attributes:\n        _dependencies:\n            The dependency directory\n    \"\"\"\n\n    _dependencies: dict[Any, Any] = {}\n\n    def __init__(self):\n        global _instance_count\n        if _instance_count == 1:\n            raise RuntimeError(\"You must only instance this class once\")\n        _instance_count += 1\n\n    @classmethod\n    def __call__(cls, _callable: Callable[..., T]) -&gt; T:\n        \"\"\"\n        Call the dependency\n\n        Args:\n            _callable:\n                Callable dependency object\n\n        Returns:\n            The output of calling `_callalbe` or its dependency override.\n        \"\"\"\n        try:\n            return cls._dependencies[_callable]()\n        except KeyError:\n            return _callable()\n\n    @classmethod\n    def pop(cls, _callable: Callable[..., T]) -&gt; T:\n        \"\"\"\n        Remove the dependency from the directory\n\n        Args:\n            _callable:\n                Callable dependency object\n        \"\"\"\n        try:\n            return cls._dependencies.pop(_callable)\n        except KeyError:\n            raise RuntimeError(f\"No dependency override for {_callable}\")\n\n    @classmethod\n    def override(\n        cls, _callable: Callable[..., T], value: Callable[..., T]\n    ) -&gt; None:\n        \"\"\"\n        Override dependency\n\n        Substitute a dependency with a different arbitrary callable.\n\n        Args:\n            _callable:\n                Callable dependency object\n            value:\n                Callable override\n        \"\"\"\n        cls._dependencies[_callable] = value\n</code></pre>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe._Inject.__call__","title":"<code>__call__(_callable)</code>  <code>classmethod</code>","text":"<p>Call the dependency</p> <p>Parameters:</p> Name Type Description Default <code>_callable</code> <code>Callable[..., T]</code> <p>Callable dependency object</p> required <p>Returns:</p> Type Description <code>T</code> <p>The output of calling <code>_callalbe</code> or its dependency override.</p> Source code in <code>fractal_server/syringe.py</code> <pre><code>@classmethod\ndef __call__(cls, _callable: Callable[..., T]) -&gt; T:\n    \"\"\"\n    Call the dependency\n\n    Args:\n        _callable:\n            Callable dependency object\n\n    Returns:\n        The output of calling `_callalbe` or its dependency override.\n    \"\"\"\n    try:\n        return cls._dependencies[_callable]()\n    except KeyError:\n        return _callable()\n</code></pre>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe._Inject.override","title":"<code>override(_callable, value)</code>  <code>classmethod</code>","text":"<p>Override dependency</p> <p>Substitute a dependency with a different arbitrary callable.</p> <p>Parameters:</p> Name Type Description Default <code>_callable</code> <code>Callable[..., T]</code> <p>Callable dependency object</p> required <code>value</code> <code>Callable[..., T]</code> <p>Callable override</p> required Source code in <code>fractal_server/syringe.py</code> <pre><code>@classmethod\ndef override(\n    cls, _callable: Callable[..., T], value: Callable[..., T]\n) -&gt; None:\n    \"\"\"\n    Override dependency\n\n    Substitute a dependency with a different arbitrary callable.\n\n    Args:\n        _callable:\n            Callable dependency object\n        value:\n            Callable override\n    \"\"\"\n    cls._dependencies[_callable] = value\n</code></pre>"},{"location":"reference/fractal_server/syringe/#fractal_server.syringe._Inject.pop","title":"<code>pop(_callable)</code>  <code>classmethod</code>","text":"<p>Remove the dependency from the directory</p> <p>Parameters:</p> Name Type Description Default <code>_callable</code> <code>Callable[..., T]</code> <p>Callable dependency object</p> required Source code in <code>fractal_server/syringe.py</code> <pre><code>@classmethod\ndef pop(cls, _callable: Callable[..., T]) -&gt; T:\n    \"\"\"\n    Remove the dependency from the directory\n\n    Args:\n        _callable:\n            Callable dependency object\n    \"\"\"\n    try:\n        return cls._dependencies.pop(_callable)\n    except KeyError:\n        raise RuntimeError(f\"No dependency override for {_callable}\")\n</code></pre>"},{"location":"reference/fractal_server/urls/","title":"urls","text":""},{"location":"reference/fractal_server/utils/","title":"utils","text":"<p>This module provides general purpose utilities that are not specific to any subsystem.</p>"},{"location":"reference/fractal_server/utils/#fractal_server.utils.execute_command_sync","title":"<code>execute_command_sync(*, command, logger_name=None, allow_char=None)</code>","text":"<p>Execute arbitrary command</p> <p>If the command returns a return code different from zero, a <code>RuntimeError</code> is raised.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>Command to be executed.</p> required <code>logger_name</code> <code>str | None</code> <p>Name of the logger.</p> <code>None</code> <code>allow_char</code> <code>str | None</code> <p>Argument propagated to <code>validate_cmd</code>.</p> <code>None</code> Source code in <code>fractal_server/utils.py</code> <pre><code>def execute_command_sync(\n    *,\n    command: str,\n    logger_name: str | None = None,\n    allow_char: str | None = None,\n) -&gt; str:\n    \"\"\"\n    Execute arbitrary command\n\n    If the command returns a return code different from zero, a `RuntimeError`\n    is raised.\n\n    Args:\n        command: Command to be executed.\n        logger_name: Name of the logger.\n        allow_char: Argument propagated to `validate_cmd`.\n    \"\"\"\n    logger = get_logger(logger_name)\n    logger.debug(f\"START subprocess call to '{command}'\")\n    validate_cmd(command=command, allow_char=allow_char)\n    res = subprocess.run(  # nosec\n        shlex.split(command),\n        capture_output=True,\n        encoding=\"utf-8\",\n    )\n    returncode = res.returncode\n    stdout = res.stdout\n    stderr = res.stderr\n    if res.returncode != 0:\n        logger.debug(f\"ERROR in subprocess call to '{command}'\")\n        raise RuntimeError(\n            f\"Command {command} failed.\\n\"\n            f\"returncode={res.returncode}\\n\"\n            \"STDOUT:\\n\"\n            f\"{stdout}\\n\"\n            \"STDERR:\\n\"\n            f\"{stderr}\\n\"\n        )\n    logger.debug(f\"{returncode=}\")\n    logger.debug(\"STDOUT:\")\n    logger.debug(stdout)\n    logger.debug(\"STDERR:\")\n    logger.debug(stderr)\n    logger.debug(f\"END   subprocess call to '{command}'\")\n    return stdout\n</code></pre>"},{"location":"reference/fractal_server/utils/#fractal_server.utils.get_timestamp","title":"<code>get_timestamp()</code>","text":"<p>Get timezone aware timestamp.</p> Source code in <code>fractal_server/utils.py</code> <pre><code>def get_timestamp() -&gt; datetime:\n    \"\"\"\n    Get timezone aware timestamp.\n    \"\"\"\n    return datetime.now(tz=timezone.utc)\n</code></pre>"},{"location":"reference/fractal_server/zip_tools/","title":"zip_tools","text":""},{"location":"reference/fractal_server/zip_tools/#fractal_server.zip_tools._create_zip","title":"<code>_create_zip(folder, output)</code>","text":"<p>Zip a folder into a zip-file or into a BytesIO.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Folder to be zipped.</p> required <code>output</code> <code>T</code> <p>Either a string with the path of the zip file, or a BytesIO object.</p> required <p>Returns:</p> Type Description <code>T</code> <p>Either the zip-file path string, or the modified BytesIO object.</p> Source code in <code>fractal_server/zip_tools.py</code> <pre><code>def _create_zip(folder: str, output: T) -&gt; T:\n    \"\"\"\n    Zip a folder into a zip-file or into a BytesIO.\n\n    Args:\n        folder: Folder to be zipped.\n        output: Either a string with the path of the zip file, or a BytesIO\n            object.\n\n    Returns:\n        Either the zip-file path string, or the modified BytesIO object.\n    \"\"\"\n    if isinstance(output, str) and os.path.exists(output):\n        raise FileExistsError(f\"Zip file '{output}' already exists\")\n    if isinstance(output, BytesIO) and output.getbuffer().nbytes &gt; 0:\n        raise ValueError(\"BytesIO is not empty\")\n\n    with ZipFile(output, mode=\"w\", compression=ZIP_DEFLATED) as zipfile:\n        for root, dirs, files in os.walk(folder):\n            for file in files:\n                file_path = os.path.join(root, file)\n                archive_path = os.path.relpath(file_path, folder)\n                zipfile.write(file_path, archive_path)\n    return output\n</code></pre>"},{"location":"reference/fractal_server/zip_tools/#fractal_server.zip_tools._folder_can_be_deleted","title":"<code>_folder_can_be_deleted(folder)</code>","text":"<p>Given the path of a folder as string, returns <code>False</code> if either: - the related zip file <code>{folder}.zip</code> does already exists, - the folder and the zip file have a different number of internal files, - the zip file has a very small size. Otherwise returns <code>True</code>.</p> Source code in <code>fractal_server/zip_tools.py</code> <pre><code>def _folder_can_be_deleted(folder: str) -&gt; bool:\n    \"\"\"\n    Given the path of a folder as string, returns `False` if either:\n    - the related zip file `{folder}.zip` does already exists,\n    - the folder and the zip file have a different number of internal files,\n    - the zip file has a very small size.\n    Otherwise returns `True`.\n    \"\"\"\n    # CHECK 1: zip file exists\n    zip_file = f\"{folder}.zip\"\n    if not os.path.exists(zip_file):\n        logger.info(\n            f\"Folder '{folder}' won't be deleted because file '{zip_file}' \"\n            \"does not exist.\"\n        )\n        return False\n\n    # CHECK 2: folder and zip file have the same number of files\n    folder_files_count = sum(1 for f in Path(folder).rglob(\"*\") if f.is_file())\n    with ZipFile(zip_file, \"r\") as zip_ref:\n        zip_files_count = len(zip_ref.namelist())\n    if folder_files_count != zip_files_count:\n        logger.info(\n            f\"Folder '{folder}' won't be deleted because it contains \"\n            f\"{folder_files_count} files while '{zip_file}' contains \"\n            f\"{zip_files_count}.\"\n        )\n        return False\n\n    # CHECK 3: zip file size is &gt;= than `THRESHOLD_ZIP_FILE_SIZE_MB`\n    zip_size = os.path.getsize(zip_file)\n    if zip_size &lt; THRESHOLD_ZIP_FILE_SIZE_MB * (1024**2):\n        logger.info(\n            f\"Folder '{folder}' won't be deleted because '{zip_file}' is too \"\n            f\"small ({zip_size / (1024**2):.5f} MB, whereas the minimum limit \"\n            f\"for deletion is {THRESHOLD_ZIP_FILE_SIZE_MB}).\"\n        )\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/fractal_server/zip_tools/#fractal_server.zip_tools._zip_folder_to_byte_stream_iterator","title":"<code>_zip_folder_to_byte_stream_iterator(folder)</code>","text":"<p>Returns byte stream with the zipped log folder of a job.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>the folder to zip</p> required Source code in <code>fractal_server/zip_tools.py</code> <pre><code>def _zip_folder_to_byte_stream_iterator(folder: str) -&gt; Iterator:\n    \"\"\"\n    Returns byte stream with the zipped log folder of a job.\n\n    Args:\n        folder: the folder to zip\n    \"\"\"\n    zip_file = Path(f\"{folder}.zip\")\n\n    if os.path.exists(zip_file):\n\n        def iterfile():\n            \"\"\"\n            https://fastapi.tiangolo.com/advanced/custom-response/#using-streamingresponse-with-file-like-objects\n            \"\"\"\n            with open(zip_file, mode=\"rb\") as file_like:\n                yield from file_like\n\n        return iterfile()\n\n    else:\n        byte_stream = _create_zip(folder, output=BytesIO())\n        return iter([byte_stream.getvalue()])\n</code></pre>"},{"location":"reference/fractal_server/zip_tools/#fractal_server.zip_tools._zip_folder_to_file_and_remove","title":"<code>_zip_folder_to_file_and_remove(folder)</code>","text":"<p>Creates a ZIP archive of the specified folder and removes the original folder (if it can be deleted).</p> <p>This function performs the following steps: 1. Creates a ZIP archive of the <code>folder</code> and names it with a temporary    suffix <code>_tmp.zip</code>. 2. Renames the ZIP removing the suffix (this would possibly overwrite a     file with the same name already present). 3. Checks if the folder can be safely deleted using the     <code>_folder_can_be_deleted</code> function. If so, deletes the original folder.</p> Source code in <code>fractal_server/zip_tools.py</code> <pre><code>def _zip_folder_to_file_and_remove(folder: str) -&gt; None:\n    \"\"\"\n    Creates a ZIP archive of the specified folder and removes the original\n    folder (if it can be deleted).\n\n    This function performs the following steps:\n    1. Creates a ZIP archive of the `folder` and names it with a temporary\n       suffix `_tmp.zip`.\n    2. Renames the ZIP removing the suffix (this would possibly overwrite a\n        file with the same name already present).\n    3. Checks if the folder can be safely deleted using the\n        `_folder_can_be_deleted` function. If so, deletes the original folder.\n    \"\"\"\n\n    tmp_zipfile = f\"{folder}_tmp.zip\"\n    zipfile = f\"{folder}.zip\"\n\n    try:\n        logger.info(f\"Start creating temporary zip file at '{tmp_zipfile}'.\")\n        _create_zip(folder, tmp_zipfile)\n        logger.info(\"Zip file created.\")\n    except Exception as e:\n        logger.error(\n            f\"Error while creating temporary zip file. Original error: '{e}'.\"\n        )\n        Path(tmp_zipfile).unlink(missing_ok=True)\n        return\n\n    logger.info(f\"Moving temporary zip file to {zipfile}.\")\n    shutil.move(tmp_zipfile, zipfile)\n    logger.info(\"Zip file moved.\")\n\n    if _folder_can_be_deleted(folder):\n        logger.info(f\"Removing folder '{folder}'.\")\n        shutil.rmtree(folder)\n        logger.info(\"Folder removed.\")\n</code></pre>"},{"location":"reference/fractal_server/app/","title":"app","text":""},{"location":"reference/fractal_server/app/shutdown/","title":"shutdown","text":""},{"location":"reference/fractal_server/app/db/","title":"db","text":"<p><code>db</code> module, loosely adapted from https://testdriven.io/blog/fastapi-sqlmodel/#async-sqlmodel</p>"},{"location":"reference/fractal_server/app/db/#fractal_server.app.db.DB","title":"<code>DB</code>","text":"<p>DB class</p> Source code in <code>fractal_server/app/db/__init__.py</code> <pre><code>class DB:\n    \"\"\"\n    DB class\n    \"\"\"\n\n    @classmethod\n    def engine_async(cls):\n        try:\n            return cls._engine_async\n        except AttributeError:\n            cls.set_async_db()\n            return cls._engine_async\n\n    @classmethod\n    def engine_sync(cls):\n        try:\n            return cls._engine_sync\n        except AttributeError:\n            cls.set_sync_db()\n            return cls._engine_sync\n\n    @classmethod\n    def set_async_db(cls):\n        db_settings = Inject(get_db_settings)\n\n        cls._engine_async = create_async_engine(\n            db_settings.DATABASE_URL,\n            echo=db_settings.DB_ECHO,\n            future=True,\n            pool_pre_ping=True,\n        )\n        cls._async_session_maker = sessionmaker(\n            cls._engine_async,\n            class_=AsyncSession,\n            expire_on_commit=False,\n            future=True,\n        )\n\n    @classmethod\n    def set_sync_db(cls):\n        db_settings = Inject(get_db_settings)\n\n        cls._engine_sync = create_engine(\n            db_settings.DATABASE_URL,\n            echo=db_settings.DB_ECHO,\n            future=True,\n            pool_pre_ping=True,\n        )\n\n        cls._sync_session_maker = sessionmaker(\n            bind=cls._engine_sync,\n            autocommit=False,\n            autoflush=False,\n            future=True,\n        )\n\n    @classmethod\n    async def get_async_db(cls) -&gt; AsyncGenerator[AsyncSession, None]:\n        \"\"\"\n        Get async database session\n        \"\"\"\n        try:\n            session_maker = cls._async_session_maker()\n        except AttributeError:\n            cls.set_async_db()\n            session_maker = cls._async_session_maker()\n        async with session_maker as async_session:\n            yield async_session\n\n    @classmethod\n    def get_sync_db(cls) -&gt; Generator[DBSyncSession, None, None]:\n        \"\"\"\n        Get sync database session\n        \"\"\"\n        try:\n            session_maker = cls._sync_session_maker()\n        except AttributeError:\n            cls.set_sync_db()\n            session_maker = cls._sync_session_maker()\n        with session_maker as sync_session:\n            yield sync_session\n</code></pre>"},{"location":"reference/fractal_server/app/db/#fractal_server.app.db.DB.get_async_db","title":"<code>get_async_db()</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get async database session</p> Source code in <code>fractal_server/app/db/__init__.py</code> <pre><code>@classmethod\nasync def get_async_db(cls) -&gt; AsyncGenerator[AsyncSession, None]:\n    \"\"\"\n    Get async database session\n    \"\"\"\n    try:\n        session_maker = cls._async_session_maker()\n    except AttributeError:\n        cls.set_async_db()\n        session_maker = cls._async_session_maker()\n    async with session_maker as async_session:\n        yield async_session\n</code></pre>"},{"location":"reference/fractal_server/app/db/#fractal_server.app.db.DB.get_sync_db","title":"<code>get_sync_db()</code>  <code>classmethod</code>","text":"<p>Get sync database session</p> Source code in <code>fractal_server/app/db/__init__.py</code> <pre><code>@classmethod\ndef get_sync_db(cls) -&gt; Generator[DBSyncSession, None, None]:\n    \"\"\"\n    Get sync database session\n    \"\"\"\n    try:\n        session_maker = cls._sync_session_maker()\n    except AttributeError:\n        cls.set_sync_db()\n        session_maker = cls._sync_session_maker()\n    with session_maker as sync_session:\n        yield sync_session\n</code></pre>"},{"location":"reference/fractal_server/app/models/","title":"models","text":"<p>Note that this module is imported from <code>fractal_server/migrations/env.py</code>, thus we should always export all relevant database models from here or they will not be picked up by alembic.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.OAuthAccount","title":"<code>OAuthAccount</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for OAuth accounts (<code>oauthaccount</code> database table).</p> <p>This class is based on fastapi_users_db_sqlmodel::SQLModelBaseOAuthAccount. Original Copyright: 2021 Fran\u00e7ois Voron, released under MIT licence.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int | None</code> <code>user_id</code> <code>int</code> <code>user</code> <code>Optional[UserOAuth]</code> <code>oauth_name</code> <code>str</code> <code>access_token</code> <code>str</code> <code>expires_at</code> <code>int | None</code> <code>refresh_token</code> <code>str | None</code> <code>account_id</code> <code>str</code> <code>account_email</code> <code>str</code> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class OAuthAccount(SQLModel, table=True):\n    \"\"\"\n    ORM model for OAuth accounts (`oauthaccount` database table).\n\n    This class is based on fastapi_users_db_sqlmodel::SQLModelBaseOAuthAccount.\n    Original Copyright: 2021 Fran\u00e7ois Voron, released under MIT licence.\n\n    Attributes:\n        id:\n        user_id:\n        user:\n        oauth_name:\n        access_token:\n        expires_at:\n        refresh_token:\n        account_id:\n        account_email:\n    \"\"\"\n\n    __tablename__ = \"oauthaccount\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    user: Optional[\"UserOAuth\"] = Relationship(back_populates=\"oauth_accounts\")\n    oauth_name: str = Field(index=True, nullable=False)\n    access_token: str = Field(nullable=False)\n    expires_at: int | None = Field(nullable=True, default=None)\n    refresh_token: str | None = Field(nullable=True, default=None)\n    account_id: str = Field(index=True, nullable=False)\n    account_email: str = Field(nullable=False)\n    model_config = ConfigDict(from_attributes=True)\n</code></pre>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/resource.py</code> <pre><code>class Resource(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    type: str\n    \"\"\"\n    One of `local`, `slurm_sudo` or `slurm_ssh` - matching with\n    `settings.FRACTAL_RUNNER_BACKEND`.\n    \"\"\"\n\n    name: str = Field(unique=True)\n    \"\"\"\n    Resource name.\n    \"\"\"\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    \"\"\"\n    Creation timestamp (autogenerated).\n    \"\"\"\n\n    host: str | None = None\n    \"\"\"\n    Address for ssh connections, when `type=\"slurm_ssh\"`.\n    \"\"\"\n\n    jobs_local_dir: str\n    \"\"\"\n    Base local folder for job subfolders (containing artifacts and logs).\n    \"\"\"\n\n    jobs_runner_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Runner configuration, matching one of `JobRunnerConfigLocal` or\n    `JobRunnerConfigSLURM` schemas.\n    \"\"\"\n\n    jobs_slurm_python_worker: str | None = None\n    \"\"\"\n    On SLURM deloyments, this is the Python interpreter that runs the\n    `fractal-server` worker from within the SLURM jobs.\n    \"\"\"\n\n    jobs_poll_interval: int\n    \"\"\"\n    On SLURM resources: the interval to wait before new `squeue` calls.\n    On local resources: ignored.\n    \"\"\"\n\n    # task_settings\n    tasks_local_dir: str\n    \"\"\"\n    Base local folder for task-package subfolders.\n    \"\"\"\n\n    tasks_python_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Python configuration for task collection. Example:\n    ```json\n    {\n      \"default_version\": \"3.10\",\n      \"versions:{\n        \"3.10\": \"/xxx/venv-3.10/bin/python\",\n        \"3.11\": \"/xxx/venv-3.11/bin/python\",\n        \"3.12\": \"/xxx/venv-3.12/bin/python\"\n       }\n    }\n    ```\n    \"\"\"\n\n    tasks_pixi_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Pixi configuration for task collection. Basic example:\n    ```json\n    {\n        \"default_version\": \"0.41.0\",\n        \"versions\": {\n            \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n            \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n        },\n    }\n    ```\n    \"\"\"\n\n    @property\n    def pip_cache_dir_arg(self: Self) -&gt; str:\n        \"\"\"\n        If `pip_cache_dir` is set (in `self.tasks_python_config`), then\n        return `--cache_dir /something`; else return `--no-cache-dir`.\n        \"\"\"\n        _pip_cache_dir = self.tasks_python_config.get(\"pip_cache_dir\", None)\n        if _pip_cache_dir is not None:\n            return f\"--cache-dir {_pip_cache_dir}\"\n        else:\n            return \"--no-cache-dir\"\n\n    # Check constraints\n    __table_args__ = (\n        # `type` column must be one of \"local\", \"slurm_sudo\" or \"slurm_ssh\"\n        CheckConstraint(\n            \"type IN ('local', 'slurm_sudo', 'slurm_ssh')\",\n            name=\"correct_type\",\n        ),\n        # If `type` is not \"local\", `jobs_slurm_python_worker` must be set\n        CheckConstraint(\n            \"(type = 'local') OR (jobs_slurm_python_worker IS NOT NULL)\",\n            name=\"jobs_slurm_python_worker_set\",\n        ),\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.host","title":"<code>host = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address for ssh connections, when <code>type=\"slurm_ssh\"</code>.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.jobs_local_dir","title":"<code>jobs_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for job subfolders (containing artifacts and logs).</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.jobs_poll_interval","title":"<code>jobs_poll_interval</code>  <code>instance-attribute</code>","text":"<p>On SLURM resources: the interval to wait before new <code>squeue</code> calls. On local resources: ignored.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.jobs_runner_config","title":"<code>jobs_runner_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Runner configuration, matching one of <code>JobRunnerConfigLocal</code> or <code>JobRunnerConfigSLURM</code> schemas.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.jobs_slurm_python_worker","title":"<code>jobs_slurm_python_worker = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>On SLURM deloyments, this is the Python interpreter that runs the <code>fractal-server</code> worker from within the SLURM jobs.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.name","title":"<code>name = Field(unique=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resource name.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.pip_cache_dir_arg","title":"<code>pip_cache_dir_arg</code>  <code>property</code>","text":"<p>If <code>pip_cache_dir</code> is set (in <code>self.tasks_python_config</code>), then return <code>--cache_dir /something</code>; else return <code>--no-cache-dir</code>.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.tasks_local_dir","title":"<code>tasks_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for task-package subfolders.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.tasks_pixi_config","title":"<code>tasks_pixi_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Pixi configuration for task collection. Basic example: <pre><code>{\n    \"default_version\": \"0.41.0\",\n    \"versions\": {\n        \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n        \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n    },\n}\n</code></pre></p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.tasks_python_config","title":"<code>tasks_python_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Python configuration for task collection. Example: <pre><code>{\n  \"default_version\": \"3.10\",\n  \"versions:{\n    \"3.10\": \"/xxx/venv-3.10/bin/python\",\n    \"3.11\": \"/xxx/venv-3.11/bin/python\",\n    \"3.12\": \"/xxx/venv-3.12/bin/python\"\n   }\n}\n</code></pre></p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.timestamp_created","title":"<code>timestamp_created = Field(default_factory=get_timestamp, sa_column=(Column(DateTime(timezone=True), nullable=False)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Creation timestamp (autogenerated).</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.Resource.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>One of <code>local</code>, <code>slurm_sudo</code> or <code>slurm_ssh</code> - matching with <code>settings.FRACTAL_RUNNER_BACKEND</code>.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.TaskGroupV2","title":"<code>TaskGroupV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/task_group.py</code> <pre><code>class TaskGroupV2(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    task_list: list[TaskV2] = Relationship(\n        sa_relationship_kwargs=dict(\n            lazy=\"selectin\", cascade=\"all, delete-orphan\"\n        ),\n    )\n\n    user_id: int = Field(foreign_key=\"user_oauth.id\")\n    user_group_id: int | None = Field(\n        foreign_key=\"usergroup.id\", default=None, ondelete=\"SET NULL\"\n    )\n    # TODO-2.17.1: make `resource_id` not nullable\n    resource_id: int | None = Field(\n        foreign_key=\"resource.id\", default=None, ondelete=\"RESTRICT\"\n    )\n\n    origin: str\n    pkg_name: str\n    version: str | None = None\n    python_version: str | None = None\n    pixi_version: str | None = None\n    path: str | None = None\n    archive_path: str | None = None\n    pip_extras: str | None = None\n    pinned_package_versions_pre: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    pinned_package_versions_post: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    env_info: str | None = None\n    venv_path: str | None = None\n    venv_size_in_kB: int | None = None\n    venv_file_number: int | None = None\n\n    active: bool = True\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    timestamp_last_used: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(\n            DateTime(timezone=True),\n            nullable=False,\n            server_default=(\n                datetime(2024, 11, 20, tzinfo=timezone.utc).isoformat()\n            ),\n        ),\n    )\n\n    @property\n    def pip_install_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        extras = f\"[{self.pip_extras}]\" if self.pip_extras is not None else \"\"\n\n        if self.archive_path is not None:\n            return f\"{self.archive_path}{extras}\"\n        else:\n            if self.version is None:\n                raise ValueError(\n                    \"Cannot run `pip_install_string` with \"\n                    f\"{self.pkg_name=}, {self.archive_path=}, {self.version=}.\"\n                )\n            return f\"{self.pkg_name}{extras}=={self.version}\"\n\n    @property\n    def pinned_package_versions_pre_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_pre is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_pre)\n        return output\n\n    @property\n    def pinned_package_versions_post_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_post is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_post)\n        return output\n</code></pre>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.TaskGroupV2.pinned_package_versions_post_string","title":"<code>pinned_package_versions_post_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.TaskGroupV2.pinned_package_versions_pre_string","title":"<code>pinned_package_versions_pre_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.TaskGroupV2.pip_install_string","title":"<code>pip_install_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.UserGroup","title":"<code>UserGroup</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for the <code>usergroup</code> database table.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int | None</code> <p>ID of the group</p> <code>name</code> <code>str</code> <p>Name of the group</p> <code>timestamp_created</code> <code>datetime</code> <p>Time of creation</p> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class UserGroup(SQLModel, table=True):\n    \"\"\"\n    ORM model for the `usergroup` database table.\n\n    Attributes:\n        id: ID of the group\n        name: Name of the group\n        timestamp_created: Time of creation\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str = Field(unique=True)\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    viewer_paths: list[str] = Field(\n        sa_column=Column(JSONB, server_default=\"[]\", nullable=False)\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.UserOAuth","title":"<code>UserOAuth</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for the <code>user_oauth</code> database table.</p> <p>This class is a modification of SQLModelBaseUserDB from from fastapi_users_db_sqlmodel. Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int | None</code> <code>email</code> <code>EmailStr</code> <code>hashed_password</code> <code>str</code> <code>is_active</code> <code>bool</code> <code>is_superuser</code> <code>bool</code> <code>is_verified</code> <code>bool</code> <code>oauth_accounts</code> <code>list[OAuthAccount]</code> <code>profile_id</code> <code>int | None</code> <code>project_dir</code> <code>str</code> <code>slurm_accounts</code> <code>list[str]</code> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class UserOAuth(SQLModel, table=True):\n    \"\"\"\n    ORM model for the `user_oauth` database table.\n\n    This class is a modification of SQLModelBaseUserDB from from\n    fastapi_users_db_sqlmodel. Original Copyright: 2022 Fran\u00e7ois Voron,\n    released under MIT licence.\n\n    Attributes:\n        id:\n        email:\n        hashed_password:\n        is_active:\n        is_superuser:\n        is_verified:\n        oauth_accounts:\n        profile_id:\n        project_dir:\n        slurm_accounts:\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n\n    __tablename__ = \"user_oauth\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    email: EmailStr = Field(\n        sa_column_kwargs={\"unique\": True, \"index\": True},\n        nullable=False,\n    )\n    hashed_password: str\n    is_active: bool = Field(default=True, nullable=False)\n    is_superuser: bool = Field(default=False, nullable=False)\n    is_verified: bool = Field(default=False, nullable=False)\n\n    oauth_accounts: list[\"OAuthAccount\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"lazy\": \"joined\", \"cascade\": \"all, delete\"},\n    )\n\n    profile_id: int | None = Field(\n        foreign_key=\"profile.id\",\n        default=None,\n        ondelete=\"RESTRICT\",\n    )\n\n    # TODO-2.17.1: update to `project_dir: str`\n    project_dir: str = Field(\n        sa_column=Column(\n            String,\n            server_default=\"/PLACEHOLDER\",\n            nullable=False,\n        )\n    )\n    slurm_accounts: list[str] = Field(\n        sa_column=Column(ARRAY(String), server_default=\"{}\"),\n    )\n\n    # TODO-2.17.1: remove\n    user_settings_id: int | None = Field(\n        foreign_key=\"user_settings.id\",\n        default=None,\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/models/#fractal_server.app.models.get_timestamp","title":"<code>get_timestamp()</code>","text":"<p>Get timezone aware timestamp.</p> Source code in <code>fractal_server/utils.py</code> <pre><code>def get_timestamp() -&gt; datetime:\n    \"\"\"\n    Get timezone aware timestamp.\n    \"\"\"\n    return datetime.now(tz=timezone.utc)\n</code></pre>"},{"location":"reference/fractal_server/app/models/linkusergroup/","title":"linkusergroup","text":""},{"location":"reference/fractal_server/app/models/linkusergroup/#fractal_server.app.models.linkusergroup.LinkUserGroup","title":"<code>LinkUserGroup</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Crossing table between User and UserGroup</p> Source code in <code>fractal_server/app/models/linkusergroup.py</code> <pre><code>class LinkUserGroup(SQLModel, table=True):\n    \"\"\"\n    Crossing table between User and UserGroup\n    \"\"\"\n\n    group_id: int = Field(\n        foreign_key=\"usergroup.id\", primary_key=True, ondelete=\"CASCADE\"\n    )\n    user_id: int = Field(\n        foreign_key=\"user_oauth.id\", primary_key=True, ondelete=\"CASCADE\"\n    )\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/models/linkuserproject/","title":"linkuserproject","text":""},{"location":"reference/fractal_server/app/models/linkuserproject/#fractal_server.app.models.linkuserproject.LinkUserProjectV2","title":"<code>LinkUserProjectV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Crossing table between User and ProjectV2</p> Source code in <code>fractal_server/app/models/linkuserproject.py</code> <pre><code>class LinkUserProjectV2(SQLModel, table=True):\n    \"\"\"\n    Crossing table between User and ProjectV2\n    \"\"\"\n\n    project_id: int = Field(foreign_key=\"projectv2.id\", primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", primary_key=True)\n</code></pre>"},{"location":"reference/fractal_server/app/models/security/","title":"security","text":""},{"location":"reference/fractal_server/app/models/security/#fractal_server.app.models.security.OAuthAccount","title":"<code>OAuthAccount</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for OAuth accounts (<code>oauthaccount</code> database table).</p> <p>This class is based on fastapi_users_db_sqlmodel::SQLModelBaseOAuthAccount. Original Copyright: 2021 Fran\u00e7ois Voron, released under MIT licence.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int | None</code> <code>user_id</code> <code>int</code> <code>user</code> <code>Optional[UserOAuth]</code> <code>oauth_name</code> <code>str</code> <code>access_token</code> <code>str</code> <code>expires_at</code> <code>int | None</code> <code>refresh_token</code> <code>str | None</code> <code>account_id</code> <code>str</code> <code>account_email</code> <code>str</code> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class OAuthAccount(SQLModel, table=True):\n    \"\"\"\n    ORM model for OAuth accounts (`oauthaccount` database table).\n\n    This class is based on fastapi_users_db_sqlmodel::SQLModelBaseOAuthAccount.\n    Original Copyright: 2021 Fran\u00e7ois Voron, released under MIT licence.\n\n    Attributes:\n        id:\n        user_id:\n        user:\n        oauth_name:\n        access_token:\n        expires_at:\n        refresh_token:\n        account_id:\n        account_email:\n    \"\"\"\n\n    __tablename__ = \"oauthaccount\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    user: Optional[\"UserOAuth\"] = Relationship(back_populates=\"oauth_accounts\")\n    oauth_name: str = Field(index=True, nullable=False)\n    access_token: str = Field(nullable=False)\n    expires_at: int | None = Field(nullable=True, default=None)\n    refresh_token: str | None = Field(nullable=True, default=None)\n    account_id: str = Field(index=True, nullable=False)\n    account_email: str = Field(nullable=False)\n    model_config = ConfigDict(from_attributes=True)\n</code></pre>"},{"location":"reference/fractal_server/app/models/security/#fractal_server.app.models.security.UserGroup","title":"<code>UserGroup</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for the <code>usergroup</code> database table.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int | None</code> <p>ID of the group</p> <code>name</code> <code>str</code> <p>Name of the group</p> <code>timestamp_created</code> <code>datetime</code> <p>Time of creation</p> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class UserGroup(SQLModel, table=True):\n    \"\"\"\n    ORM model for the `usergroup` database table.\n\n    Attributes:\n        id: ID of the group\n        name: Name of the group\n        timestamp_created: Time of creation\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str = Field(unique=True)\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    viewer_paths: list[str] = Field(\n        sa_column=Column(JSONB, server_default=\"[]\", nullable=False)\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/models/security/#fractal_server.app.models.security.UserOAuth","title":"<code>UserOAuth</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for the <code>user_oauth</code> database table.</p> <p>This class is a modification of SQLModelBaseUserDB from from fastapi_users_db_sqlmodel. Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int | None</code> <code>email</code> <code>EmailStr</code> <code>hashed_password</code> <code>str</code> <code>is_active</code> <code>bool</code> <code>is_superuser</code> <code>bool</code> <code>is_verified</code> <code>bool</code> <code>oauth_accounts</code> <code>list[OAuthAccount]</code> <code>profile_id</code> <code>int | None</code> <code>project_dir</code> <code>str</code> <code>slurm_accounts</code> <code>list[str]</code> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class UserOAuth(SQLModel, table=True):\n    \"\"\"\n    ORM model for the `user_oauth` database table.\n\n    This class is a modification of SQLModelBaseUserDB from from\n    fastapi_users_db_sqlmodel. Original Copyright: 2022 Fran\u00e7ois Voron,\n    released under MIT licence.\n\n    Attributes:\n        id:\n        email:\n        hashed_password:\n        is_active:\n        is_superuser:\n        is_verified:\n        oauth_accounts:\n        profile_id:\n        project_dir:\n        slurm_accounts:\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n\n    __tablename__ = \"user_oauth\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    email: EmailStr = Field(\n        sa_column_kwargs={\"unique\": True, \"index\": True},\n        nullable=False,\n    )\n    hashed_password: str\n    is_active: bool = Field(default=True, nullable=False)\n    is_superuser: bool = Field(default=False, nullable=False)\n    is_verified: bool = Field(default=False, nullable=False)\n\n    oauth_accounts: list[\"OAuthAccount\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"lazy\": \"joined\", \"cascade\": \"all, delete\"},\n    )\n\n    profile_id: int | None = Field(\n        foreign_key=\"profile.id\",\n        default=None,\n        ondelete=\"RESTRICT\",\n    )\n\n    # TODO-2.17.1: update to `project_dir: str`\n    project_dir: str = Field(\n        sa_column=Column(\n            String,\n            server_default=\"/PLACEHOLDER\",\n            nullable=False,\n        )\n    )\n    slurm_accounts: list[str] = Field(\n        sa_column=Column(ARRAY(String), server_default=\"{}\"),\n    )\n\n    # TODO-2.17.1: remove\n    user_settings_id: int | None = Field(\n        foreign_key=\"user_settings.id\",\n        default=None,\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/models/user_settings/","title":"user_settings","text":""},{"location":"reference/fractal_server/app/models/user_settings/#fractal_server.app.models.user_settings.UserSettings","title":"<code>UserSettings</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Comprehensive list of user settings.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int | None</code> <p>ID of database object</p> <code>slurm_accounts</code> <code>list[str]</code> <p>List of SLURM accounts, to be used upon Fractal job submission.</p> <code>ssh_host</code> <code>str | None</code> <p>SSH-reachable host where a SLURM client is available.</p> <code>ssh_username</code> <code>str | None</code> <p>User on <code>ssh_host</code>.</p> <code>ssh_private_key_path</code> <code>str | None</code> <p>Path of private SSH key for <code>ssh_username</code>.</p> <code>slurm_user</code> <code>str | None</code> <p>Local user, to be impersonated via <code>sudo -u</code></p> <code>project_dir</code> <code>str | None</code> <p>Folder where <code>slurm_user</code> can write.</p> Source code in <code>fractal_server/app/models/user_settings.py</code> <pre><code>class UserSettings(SQLModel, table=True):\n    \"\"\"\n    Comprehensive list of user settings.\n\n    Attributes:\n        id: ID of database object\n        slurm_accounts:\n            List of SLURM accounts, to be used upon Fractal job submission.\n        ssh_host: SSH-reachable host where a SLURM client is available.\n        ssh_username: User on `ssh_host`.\n        ssh_private_key_path: Path of private SSH key for `ssh_username`.\n        slurm_user: Local user, to be impersonated via `sudo -u`\n        project_dir: Folder where `slurm_user` can write.\n    \"\"\"\n\n    __tablename__ = \"user_settings\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    slurm_accounts: list[str] = Field(\n        sa_column=Column(JSONB, server_default=\"[]\", nullable=False)\n    )\n    ssh_host: str | None = None\n    ssh_username: str | None = None\n    ssh_private_key_path: str | None = None\n\n    slurm_user: str | None = None\n    project_dir: str | None = None\n\n    ssh_tasks_dir: str | None = None\n    ssh_jobs_dir: str | None = None\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/","title":"v2","text":"<p>v2 <code>models</code> module</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.LinkUserProjectV2","title":"<code>LinkUserProjectV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Crossing table between User and ProjectV2</p> Source code in <code>fractal_server/app/models/linkuserproject.py</code> <pre><code>class LinkUserProjectV2(SQLModel, table=True):\n    \"\"\"\n    Crossing table between User and ProjectV2\n    \"\"\"\n\n    project_id: int = Field(foreign_key=\"projectv2.id\", primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", primary_key=True)\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/resource.py</code> <pre><code>class Resource(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    type: str\n    \"\"\"\n    One of `local`, `slurm_sudo` or `slurm_ssh` - matching with\n    `settings.FRACTAL_RUNNER_BACKEND`.\n    \"\"\"\n\n    name: str = Field(unique=True)\n    \"\"\"\n    Resource name.\n    \"\"\"\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    \"\"\"\n    Creation timestamp (autogenerated).\n    \"\"\"\n\n    host: str | None = None\n    \"\"\"\n    Address for ssh connections, when `type=\"slurm_ssh\"`.\n    \"\"\"\n\n    jobs_local_dir: str\n    \"\"\"\n    Base local folder for job subfolders (containing artifacts and logs).\n    \"\"\"\n\n    jobs_runner_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Runner configuration, matching one of `JobRunnerConfigLocal` or\n    `JobRunnerConfigSLURM` schemas.\n    \"\"\"\n\n    jobs_slurm_python_worker: str | None = None\n    \"\"\"\n    On SLURM deloyments, this is the Python interpreter that runs the\n    `fractal-server` worker from within the SLURM jobs.\n    \"\"\"\n\n    jobs_poll_interval: int\n    \"\"\"\n    On SLURM resources: the interval to wait before new `squeue` calls.\n    On local resources: ignored.\n    \"\"\"\n\n    # task_settings\n    tasks_local_dir: str\n    \"\"\"\n    Base local folder for task-package subfolders.\n    \"\"\"\n\n    tasks_python_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Python configuration for task collection. Example:\n    ```json\n    {\n      \"default_version\": \"3.10\",\n      \"versions:{\n        \"3.10\": \"/xxx/venv-3.10/bin/python\",\n        \"3.11\": \"/xxx/venv-3.11/bin/python\",\n        \"3.12\": \"/xxx/venv-3.12/bin/python\"\n       }\n    }\n    ```\n    \"\"\"\n\n    tasks_pixi_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Pixi configuration for task collection. Basic example:\n    ```json\n    {\n        \"default_version\": \"0.41.0\",\n        \"versions\": {\n            \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n            \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n        },\n    }\n    ```\n    \"\"\"\n\n    @property\n    def pip_cache_dir_arg(self: Self) -&gt; str:\n        \"\"\"\n        If `pip_cache_dir` is set (in `self.tasks_python_config`), then\n        return `--cache_dir /something`; else return `--no-cache-dir`.\n        \"\"\"\n        _pip_cache_dir = self.tasks_python_config.get(\"pip_cache_dir\", None)\n        if _pip_cache_dir is not None:\n            return f\"--cache-dir {_pip_cache_dir}\"\n        else:\n            return \"--no-cache-dir\"\n\n    # Check constraints\n    __table_args__ = (\n        # `type` column must be one of \"local\", \"slurm_sudo\" or \"slurm_ssh\"\n        CheckConstraint(\n            \"type IN ('local', 'slurm_sudo', 'slurm_ssh')\",\n            name=\"correct_type\",\n        ),\n        # If `type` is not \"local\", `jobs_slurm_python_worker` must be set\n        CheckConstraint(\n            \"(type = 'local') OR (jobs_slurm_python_worker IS NOT NULL)\",\n            name=\"jobs_slurm_python_worker_set\",\n        ),\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.host","title":"<code>host = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address for ssh connections, when <code>type=\"slurm_ssh\"</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.jobs_local_dir","title":"<code>jobs_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for job subfolders (containing artifacts and logs).</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.jobs_poll_interval","title":"<code>jobs_poll_interval</code>  <code>instance-attribute</code>","text":"<p>On SLURM resources: the interval to wait before new <code>squeue</code> calls. On local resources: ignored.</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.jobs_runner_config","title":"<code>jobs_runner_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Runner configuration, matching one of <code>JobRunnerConfigLocal</code> or <code>JobRunnerConfigSLURM</code> schemas.</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.jobs_slurm_python_worker","title":"<code>jobs_slurm_python_worker = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>On SLURM deloyments, this is the Python interpreter that runs the <code>fractal-server</code> worker from within the SLURM jobs.</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.name","title":"<code>name = Field(unique=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resource name.</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.pip_cache_dir_arg","title":"<code>pip_cache_dir_arg</code>  <code>property</code>","text":"<p>If <code>pip_cache_dir</code> is set (in <code>self.tasks_python_config</code>), then return <code>--cache_dir /something</code>; else return <code>--no-cache-dir</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.tasks_local_dir","title":"<code>tasks_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for task-package subfolders.</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.tasks_pixi_config","title":"<code>tasks_pixi_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Pixi configuration for task collection. Basic example: <pre><code>{\n    \"default_version\": \"0.41.0\",\n    \"versions\": {\n        \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n        \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n    },\n}\n</code></pre></p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.tasks_python_config","title":"<code>tasks_python_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Python configuration for task collection. Example: <pre><code>{\n  \"default_version\": \"3.10\",\n  \"versions:{\n    \"3.10\": \"/xxx/venv-3.10/bin/python\",\n    \"3.11\": \"/xxx/venv-3.11/bin/python\",\n    \"3.12\": \"/xxx/venv-3.12/bin/python\"\n   }\n}\n</code></pre></p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.timestamp_created","title":"<code>timestamp_created = Field(default_factory=get_timestamp, sa_column=(Column(DateTime(timezone=True), nullable=False)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Creation timestamp (autogenerated).</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.Resource.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>One of <code>local</code>, <code>slurm_sudo</code> or <code>slurm_ssh</code> - matching with <code>settings.FRACTAL_RUNNER_BACKEND</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.TaskGroupV2","title":"<code>TaskGroupV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/task_group.py</code> <pre><code>class TaskGroupV2(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    task_list: list[TaskV2] = Relationship(\n        sa_relationship_kwargs=dict(\n            lazy=\"selectin\", cascade=\"all, delete-orphan\"\n        ),\n    )\n\n    user_id: int = Field(foreign_key=\"user_oauth.id\")\n    user_group_id: int | None = Field(\n        foreign_key=\"usergroup.id\", default=None, ondelete=\"SET NULL\"\n    )\n    # TODO-2.17.1: make `resource_id` not nullable\n    resource_id: int | None = Field(\n        foreign_key=\"resource.id\", default=None, ondelete=\"RESTRICT\"\n    )\n\n    origin: str\n    pkg_name: str\n    version: str | None = None\n    python_version: str | None = None\n    pixi_version: str | None = None\n    path: str | None = None\n    archive_path: str | None = None\n    pip_extras: str | None = None\n    pinned_package_versions_pre: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    pinned_package_versions_post: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    env_info: str | None = None\n    venv_path: str | None = None\n    venv_size_in_kB: int | None = None\n    venv_file_number: int | None = None\n\n    active: bool = True\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    timestamp_last_used: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(\n            DateTime(timezone=True),\n            nullable=False,\n            server_default=(\n                datetime(2024, 11, 20, tzinfo=timezone.utc).isoformat()\n            ),\n        ),\n    )\n\n    @property\n    def pip_install_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        extras = f\"[{self.pip_extras}]\" if self.pip_extras is not None else \"\"\n\n        if self.archive_path is not None:\n            return f\"{self.archive_path}{extras}\"\n        else:\n            if self.version is None:\n                raise ValueError(\n                    \"Cannot run `pip_install_string` with \"\n                    f\"{self.pkg_name=}, {self.archive_path=}, {self.version=}.\"\n                )\n            return f\"{self.pkg_name}{extras}=={self.version}\"\n\n    @property\n    def pinned_package_versions_pre_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_pre is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_pre)\n        return output\n\n    @property\n    def pinned_package_versions_post_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_post is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_post)\n        return output\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.TaskGroupV2.pinned_package_versions_post_string","title":"<code>pinned_package_versions_post_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.TaskGroupV2.pinned_package_versions_pre_string","title":"<code>pinned_package_versions_pre_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/#fractal_server.app.models.v2.TaskGroupV2.pip_install_string","title":"<code>pip_install_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/accounting/","title":"accounting","text":""},{"location":"reference/fractal_server/app/models/v2/dataset/","title":"dataset","text":""},{"location":"reference/fractal_server/app/models/v2/history/","title":"history","text":""},{"location":"reference/fractal_server/app/models/v2/job/","title":"job","text":""},{"location":"reference/fractal_server/app/models/v2/profile/","title":"profile","text":""},{"location":"reference/fractal_server/app/models/v2/project/","title":"project","text":""},{"location":"reference/fractal_server/app/models/v2/resource/","title":"resource","text":""},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/resource.py</code> <pre><code>class Resource(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    type: str\n    \"\"\"\n    One of `local`, `slurm_sudo` or `slurm_ssh` - matching with\n    `settings.FRACTAL_RUNNER_BACKEND`.\n    \"\"\"\n\n    name: str = Field(unique=True)\n    \"\"\"\n    Resource name.\n    \"\"\"\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    \"\"\"\n    Creation timestamp (autogenerated).\n    \"\"\"\n\n    host: str | None = None\n    \"\"\"\n    Address for ssh connections, when `type=\"slurm_ssh\"`.\n    \"\"\"\n\n    jobs_local_dir: str\n    \"\"\"\n    Base local folder for job subfolders (containing artifacts and logs).\n    \"\"\"\n\n    jobs_runner_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Runner configuration, matching one of `JobRunnerConfigLocal` or\n    `JobRunnerConfigSLURM` schemas.\n    \"\"\"\n\n    jobs_slurm_python_worker: str | None = None\n    \"\"\"\n    On SLURM deloyments, this is the Python interpreter that runs the\n    `fractal-server` worker from within the SLURM jobs.\n    \"\"\"\n\n    jobs_poll_interval: int\n    \"\"\"\n    On SLURM resources: the interval to wait before new `squeue` calls.\n    On local resources: ignored.\n    \"\"\"\n\n    # task_settings\n    tasks_local_dir: str\n    \"\"\"\n    Base local folder for task-package subfolders.\n    \"\"\"\n\n    tasks_python_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Python configuration for task collection. Example:\n    ```json\n    {\n      \"default_version\": \"3.10\",\n      \"versions:{\n        \"3.10\": \"/xxx/venv-3.10/bin/python\",\n        \"3.11\": \"/xxx/venv-3.11/bin/python\",\n        \"3.12\": \"/xxx/venv-3.12/bin/python\"\n       }\n    }\n    ```\n    \"\"\"\n\n    tasks_pixi_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Pixi configuration for task collection. Basic example:\n    ```json\n    {\n        \"default_version\": \"0.41.0\",\n        \"versions\": {\n            \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n            \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n        },\n    }\n    ```\n    \"\"\"\n\n    @property\n    def pip_cache_dir_arg(self: Self) -&gt; str:\n        \"\"\"\n        If `pip_cache_dir` is set (in `self.tasks_python_config`), then\n        return `--cache_dir /something`; else return `--no-cache-dir`.\n        \"\"\"\n        _pip_cache_dir = self.tasks_python_config.get(\"pip_cache_dir\", None)\n        if _pip_cache_dir is not None:\n            return f\"--cache-dir {_pip_cache_dir}\"\n        else:\n            return \"--no-cache-dir\"\n\n    # Check constraints\n    __table_args__ = (\n        # `type` column must be one of \"local\", \"slurm_sudo\" or \"slurm_ssh\"\n        CheckConstraint(\n            \"type IN ('local', 'slurm_sudo', 'slurm_ssh')\",\n            name=\"correct_type\",\n        ),\n        # If `type` is not \"local\", `jobs_slurm_python_worker` must be set\n        CheckConstraint(\n            \"(type = 'local') OR (jobs_slurm_python_worker IS NOT NULL)\",\n            name=\"jobs_slurm_python_worker_set\",\n        ),\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.host","title":"<code>host = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address for ssh connections, when <code>type=\"slurm_ssh\"</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.jobs_local_dir","title":"<code>jobs_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for job subfolders (containing artifacts and logs).</p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.jobs_poll_interval","title":"<code>jobs_poll_interval</code>  <code>instance-attribute</code>","text":"<p>On SLURM resources: the interval to wait before new <code>squeue</code> calls. On local resources: ignored.</p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.jobs_runner_config","title":"<code>jobs_runner_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Runner configuration, matching one of <code>JobRunnerConfigLocal</code> or <code>JobRunnerConfigSLURM</code> schemas.</p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.jobs_slurm_python_worker","title":"<code>jobs_slurm_python_worker = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>On SLURM deloyments, this is the Python interpreter that runs the <code>fractal-server</code> worker from within the SLURM jobs.</p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.name","title":"<code>name = Field(unique=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resource name.</p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.pip_cache_dir_arg","title":"<code>pip_cache_dir_arg</code>  <code>property</code>","text":"<p>If <code>pip_cache_dir</code> is set (in <code>self.tasks_python_config</code>), then return <code>--cache_dir /something</code>; else return <code>--no-cache-dir</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.tasks_local_dir","title":"<code>tasks_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for task-package subfolders.</p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.tasks_pixi_config","title":"<code>tasks_pixi_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Pixi configuration for task collection. Basic example: <pre><code>{\n    \"default_version\": \"0.41.0\",\n    \"versions\": {\n        \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n        \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n    },\n}\n</code></pre></p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.tasks_python_config","title":"<code>tasks_python_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Python configuration for task collection. Example: <pre><code>{\n  \"default_version\": \"3.10\",\n  \"versions:{\n    \"3.10\": \"/xxx/venv-3.10/bin/python\",\n    \"3.11\": \"/xxx/venv-3.11/bin/python\",\n    \"3.12\": \"/xxx/venv-3.12/bin/python\"\n   }\n}\n</code></pre></p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.timestamp_created","title":"<code>timestamp_created = Field(default_factory=get_timestamp, sa_column=(Column(DateTime(timezone=True), nullable=False)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Creation timestamp (autogenerated).</p>"},{"location":"reference/fractal_server/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>One of <code>local</code>, <code>slurm_sudo</code> or <code>slurm_ssh</code> - matching with <code>settings.FRACTAL_RUNNER_BACKEND</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/task/","title":"task","text":""},{"location":"reference/fractal_server/app/models/v2/task_group/","title":"task_group","text":""},{"location":"reference/fractal_server/app/models/v2/task_group/#fractal_server.app.models.v2.task_group.TaskGroupV2","title":"<code>TaskGroupV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/task_group.py</code> <pre><code>class TaskGroupV2(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    task_list: list[TaskV2] = Relationship(\n        sa_relationship_kwargs=dict(\n            lazy=\"selectin\", cascade=\"all, delete-orphan\"\n        ),\n    )\n\n    user_id: int = Field(foreign_key=\"user_oauth.id\")\n    user_group_id: int | None = Field(\n        foreign_key=\"usergroup.id\", default=None, ondelete=\"SET NULL\"\n    )\n    # TODO-2.17.1: make `resource_id` not nullable\n    resource_id: int | None = Field(\n        foreign_key=\"resource.id\", default=None, ondelete=\"RESTRICT\"\n    )\n\n    origin: str\n    pkg_name: str\n    version: str | None = None\n    python_version: str | None = None\n    pixi_version: str | None = None\n    path: str | None = None\n    archive_path: str | None = None\n    pip_extras: str | None = None\n    pinned_package_versions_pre: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    pinned_package_versions_post: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    env_info: str | None = None\n    venv_path: str | None = None\n    venv_size_in_kB: int | None = None\n    venv_file_number: int | None = None\n\n    active: bool = True\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    timestamp_last_used: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(\n            DateTime(timezone=True),\n            nullable=False,\n            server_default=(\n                datetime(2024, 11, 20, tzinfo=timezone.utc).isoformat()\n            ),\n        ),\n    )\n\n    @property\n    def pip_install_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        extras = f\"[{self.pip_extras}]\" if self.pip_extras is not None else \"\"\n\n        if self.archive_path is not None:\n            return f\"{self.archive_path}{extras}\"\n        else:\n            if self.version is None:\n                raise ValueError(\n                    \"Cannot run `pip_install_string` with \"\n                    f\"{self.pkg_name=}, {self.archive_path=}, {self.version=}.\"\n                )\n            return f\"{self.pkg_name}{extras}=={self.version}\"\n\n    @property\n    def pinned_package_versions_pre_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_pre is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_pre)\n        return output\n\n    @property\n    def pinned_package_versions_post_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_post is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_post)\n        return output\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/task_group/#fractal_server.app.models.v2.task_group.TaskGroupV2.pinned_package_versions_post_string","title":"<code>pinned_package_versions_post_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/task_group/#fractal_server.app.models.v2.task_group.TaskGroupV2.pinned_package_versions_pre_string","title":"<code>pinned_package_versions_pre_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/task_group/#fractal_server.app.models.v2.task_group.TaskGroupV2.pip_install_string","title":"<code>pip_install_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/fractal_server/app/models/v2/task_group/#fractal_server.app.models.v2.task_group._check_origin_not_pixi","title":"<code>_check_origin_not_pixi(origin)</code>","text":"<p>Raise <code>ValueError</code> if <code>origin==\"pixi\"</code></p> Source code in <code>fractal_server/app/models/v2/task_group.py</code> <pre><code>def _check_origin_not_pixi(origin: str):\n    \"\"\"\n    Raise `ValueError` if `origin==\"pixi\"`\n    \"\"\"\n    if origin == \"pixi\":\n        raise ValueError(f\"Cannot call 'pip_install_string' if {origin=}.\")\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/task_group/#fractal_server.app.models.v2.task_group._create_dependency_string","title":"<code>_create_dependency_string(pinned_versions)</code>","text":"<p>Expand e.g. <code>{\"a\": \"1.2\", \"b\": \"3\"}</code> into <code>\"a==1.2 b==3\"</code>.</p> Source code in <code>fractal_server/app/models/v2/task_group.py</code> <pre><code>def _create_dependency_string(pinned_versions: dict[str, str]) -&gt; str:\n    \"\"\"\n    Expand e.g. `{\"a\": \"1.2\", \"b\": \"3\"}` into `\"a==1.2 b==3\"`.\n    \"\"\"\n    output = \" \".join(\n        [f\"{key}=={value}\" for key, value in pinned_versions.items()]\n    )\n    return output\n</code></pre>"},{"location":"reference/fractal_server/app/models/v2/workflow/","title":"workflow","text":""},{"location":"reference/fractal_server/app/models/v2/workflowtask/","title":"workflowtask","text":""},{"location":"reference/fractal_server/app/routes/","title":"routes","text":""},{"location":"reference/fractal_server/app/routes/pagination/","title":"pagination","text":""},{"location":"reference/fractal_server/app/routes/admin/","title":"admin","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/","title":"v2","text":"<p><code>admin/v2</code> module</p>"},{"location":"reference/fractal_server/app/routes/admin/v2/_aux_functions/","title":"_aux_functions","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/accounting/","title":"accounting","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/impersonate/","title":"impersonate","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/job/","title":"job","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/job/#fractal_server.app.routes.admin.v2.job.download_job_logs","title":"<code>download_job_logs(job_id, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Download job folder</p> Source code in <code>fractal_server/app/routes/admin/v2/job.py</code> <pre><code>@router.get(\"/{job_id}/download/\", response_class=StreamingResponse)\nasync def download_job_logs(\n    job_id: int,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StreamingResponse:\n    \"\"\"\n    Download job folder\n    \"\"\"\n    # Get job from DB\n    job = await db.get(JobV2, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n    # Create and return byte stream for zipped log folder\n    PREFIX_ZIP = Path(job.working_dir).name\n    zip_filename = f\"{PREFIX_ZIP}_archive.zip\"\n    return StreamingResponse(\n        _zip_folder_to_byte_stream_iterator(folder=job.working_dir),\n        media_type=\"application/x-zip-compressed\",\n        headers={\"Content-Disposition\": f\"attachment;filename={zip_filename}\"},\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/job/#fractal_server.app.routes.admin.v2.job.stop_job","title":"<code>stop_job(job_id, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Stop execution of a workflow job.</p> Source code in <code>fractal_server/app/routes/admin/v2/job.py</code> <pre><code>@router.get(\"/{job_id}/stop/\", status_code=202)\nasync def stop_job(\n    job_id: int,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Stop execution of a workflow job.\n    \"\"\"\n\n    _check_shutdown_is_supported()\n\n    job = await db.get(JobV2, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n\n    _write_shutdown_file(job=job)\n\n    return Response(status_code=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/job/#fractal_server.app.routes.admin.v2.job.update_job","title":"<code>update_job(job_update, job_id, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Change the status of an existing job.</p> <p>This endpoint is only open to superusers, and it does not apply project-based access-control to jobs.</p> Source code in <code>fractal_server/app/routes/admin/v2/job.py</code> <pre><code>@router.patch(\"/{job_id}/\", response_model=JobReadV2)\nasync def update_job(\n    job_update: JobUpdateV2,\n    job_id: int,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; JobReadV2 | None:\n    \"\"\"\n    Change the status of an existing job.\n\n    This endpoint is only open to superusers, and it does not apply\n    project-based access-control to jobs.\n    \"\"\"\n    job = await db.get(JobV2, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n    if job.status != JobStatusTypeV2.SUBMITTED:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Job {job_id} has status {job.status=} != 'submitted'.\",\n        )\n\n    if job_update.status != JobStatusTypeV2.FAILED:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Cannot set job status to {job_update.status}\",\n        )\n\n    timestamp = get_timestamp()\n    setattr(job, \"status\", job_update.status)\n    setattr(job, \"end_timestamp\", timestamp)\n    setattr(\n        job,\n        \"log\",\n        f\"{job.log or ''}\\nThis job was manually marked as \"\n        f\"'{JobStatusTypeV2.FAILED}' by an admin ({timestamp.isoformat()}).\",\n    )\n\n    res = await db.execute(\n        select(HistoryRun)\n        .where(HistoryRun.job_id == job_id)\n        .order_by(HistoryRun.timestamp_started.desc())\n        .limit(1)\n    )\n    latest_run = res.scalar_one_or_none()\n    if latest_run is not None:\n        setattr(latest_run, \"status\", HistoryUnitStatus.FAILED)\n        res = await db.execute(\n            select(HistoryUnit).where(\n                HistoryUnit.history_run_id == latest_run.id\n            )\n        )\n        history_units = res.scalars().all()\n        for history_unit in history_units:\n            setattr(history_unit, \"status\", HistoryUnitStatus.FAILED)\n\n    await db.commit()\n    await db.refresh(job)\n    await db.close()\n    return job\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/job/#fractal_server.app.routes.admin.v2.job.view_job","title":"<code>view_job(id=None, user_id=None, project_id=None, dataset_id=None, workflow_id=None, status=None, start_timestamp_min=None, start_timestamp_max=None, end_timestamp_min=None, end_timestamp_max=None, log=True, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>ApplyWorkflow</code> table.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int | None</code> <p>If not <code>None</code>, select a given <code>applyworkflow.id</code>.</p> <code>None</code> <code>project_id</code> <code>int | None</code> <p>If not <code>None</code>, select a given <code>applyworkflow.project_id</code>.</p> <code>None</code> <code>dataset_id</code> <code>int | None</code> <p>If not <code>None</code>, select a given <code>applyworkflow.input_dataset_id</code>.</p> <code>None</code> <code>workflow_id</code> <code>int | None</code> <p>If not <code>None</code>, select a given <code>applyworkflow.workflow_id</code>.</p> <code>None</code> <code>status</code> <code>JobStatusTypeV2 | None</code> <p>If not <code>None</code>, select a given <code>applyworkflow.status</code>.</p> <code>None</code> <code>start_timestamp_min</code> <code>AwareDatetime | None</code> <p>If not <code>None</code>, select a rows with <code>start_timestamp</code> after <code>start_timestamp_min</code>.</p> <code>None</code> <code>start_timestamp_max</code> <code>AwareDatetime | None</code> <p>If not <code>None</code>, select a rows with <code>start_timestamp</code> before <code>start_timestamp_min</code>.</p> <code>None</code> <code>end_timestamp_min</code> <code>AwareDatetime | None</code> <p>If not <code>None</code>, select a rows with <code>end_timestamp</code> after <code>end_timestamp_min</code>.</p> <code>None</code> <code>end_timestamp_max</code> <code>AwareDatetime | None</code> <p>If not <code>None</code>, select a rows with <code>end_timestamp</code> before <code>end_timestamp_min</code>.</p> <code>None</code> <code>log</code> <code>bool</code> <p>If <code>True</code>, include <code>job.log</code>, if <code>False</code> <code>job.log</code> is set to <code>None</code>.</p> <code>True</code> Source code in <code>fractal_server/app/routes/admin/v2/job.py</code> <pre><code>@router.get(\"/\", response_model=list[JobReadV2])\nasync def view_job(\n    id: int | None = None,\n    user_id: int | None = None,\n    project_id: int | None = None,\n    dataset_id: int | None = None,\n    workflow_id: int | None = None,\n    status: JobStatusTypeV2 | None = None,\n    start_timestamp_min: AwareDatetime | None = None,\n    start_timestamp_max: AwareDatetime | None = None,\n    end_timestamp_min: AwareDatetime | None = None,\n    end_timestamp_max: AwareDatetime | None = None,\n    log: bool = True,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[JobReadV2]:\n    \"\"\"\n    Query `ApplyWorkflow` table.\n\n    Args:\n        id: If not `None`, select a given `applyworkflow.id`.\n        project_id: If not `None`, select a given `applyworkflow.project_id`.\n        dataset_id: If not `None`, select a given\n            `applyworkflow.input_dataset_id`.\n        workflow_id: If not `None`, select a given `applyworkflow.workflow_id`.\n        status: If not `None`, select a given `applyworkflow.status`.\n        start_timestamp_min: If not `None`, select a rows with\n            `start_timestamp` after `start_timestamp_min`.\n        start_timestamp_max: If not `None`, select a rows with\n            `start_timestamp` before `start_timestamp_min`.\n        end_timestamp_min: If not `None`, select a rows with `end_timestamp`\n            after `end_timestamp_min`.\n        end_timestamp_max: If not `None`, select a rows with `end_timestamp`\n            before `end_timestamp_min`.\n        log: If `True`, include `job.log`, if `False`\n            `job.log` is set to `None`.\n    \"\"\"\n\n    stm = select(JobV2)\n\n    if id is not None:\n        stm = stm.where(JobV2.id == id)\n    if user_id is not None:\n        stm = stm.join(ProjectV2).where(\n            ProjectV2.user_list.any(UserOAuth.id == user_id)\n        )\n    if project_id is not None:\n        stm = stm.where(JobV2.project_id == project_id)\n    if dataset_id is not None:\n        stm = stm.where(JobV2.dataset_id == dataset_id)\n    if workflow_id is not None:\n        stm = stm.where(JobV2.workflow_id == workflow_id)\n    if status is not None:\n        stm = stm.where(JobV2.status == status)\n    if start_timestamp_min is not None:\n        start_timestamp_min = start_timestamp_min\n        stm = stm.where(JobV2.start_timestamp &gt;= start_timestamp_min)\n    if start_timestamp_max is not None:\n        start_timestamp_max = start_timestamp_max\n        stm = stm.where(JobV2.start_timestamp &lt;= start_timestamp_max)\n    if end_timestamp_min is not None:\n        end_timestamp_min = end_timestamp_min\n        stm = stm.where(JobV2.end_timestamp &gt;= end_timestamp_min)\n    if end_timestamp_max is not None:\n        end_timestamp_max = end_timestamp_max\n        stm = stm.where(JobV2.end_timestamp &lt;= end_timestamp_max)\n\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    await db.close()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/profile/","title":"profile","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/profile/#fractal_server.app.routes.admin.v2.profile.delete_profile","title":"<code>delete_profile(profile_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete single <code>Profile</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/profile.py</code> <pre><code>@router.delete(\"/{profile_id}/\", status_code=204)\nasync def delete_profile(\n    profile_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Delete single `Profile`.\n    \"\"\"\n    profile = await _get_profile_or_404(profile_id=profile_id, db=db)\n\n    # Fail if at least one UserOAuth is associated with the Profile.\n    res = await db.execute(\n        select(func.count(UserOAuth.id)).where(\n            UserOAuth.profile_id == profile.id\n        )\n    )\n    associated_users_count = res.scalar()\n    if associated_users_count &gt; 0:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot delete Profile {profile_id} because it's associated\"\n                f\" with {associated_users_count} UserOAuths.\"\n            ),\n        )\n\n    # Delete\n    await db.delete(profile)\n    await db.commit()\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/profile/#fractal_server.app.routes.admin.v2.profile.get_profile_list","title":"<code>get_profile_list(superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query single <code>Profile</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/profile.py</code> <pre><code>@router.get(\"/\", response_model=list[ProfileRead], status_code=200)\nasync def get_profile_list(\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProfileRead:\n    \"\"\"\n    Query single `Profile`.\n    \"\"\"\n    res = await db.execute(select(Profile).order_by(Profile.id))\n    profiles = res.scalars().all()\n    return profiles\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/profile/#fractal_server.app.routes.admin.v2.profile.get_single_profile","title":"<code>get_single_profile(profile_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query single <code>Profile</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/profile.py</code> <pre><code>@router.get(\"/{profile_id}/\", response_model=ProfileRead, status_code=200)\nasync def get_single_profile(\n    profile_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProfileRead:\n    \"\"\"\n    Query single `Profile`.\n    \"\"\"\n    profile = await _get_profile_or_404(profile_id=profile_id, db=db)\n    return profile\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/profile/#fractal_server.app.routes.admin.v2.profile.put_profile","title":"<code>put_profile(profile_id, profile_update, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Override single <code>Profile</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/profile.py</code> <pre><code>@router.put(\"/{profile_id}/\", response_model=ProfileRead, status_code=200)\nasync def put_profile(\n    profile_id: int,\n    profile_update: ProfileCreate,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProfileRead:\n    \"\"\"\n    Override single `Profile`.\n    \"\"\"\n    profile = await _get_profile_or_404(profile_id=profile_id, db=db)\n\n    if profile_update.name and profile_update.name != profile.name:\n        await _check_profile_name(name=profile_update.name, db=db)\n\n    for key, value in profile_update.model_dump().items():\n        setattr(profile, key, value)\n    await db.commit()\n    await db.refresh(profile)\n    return profile\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/project/","title":"project","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/project/#fractal_server.app.routes.admin.v2.project.view_project","title":"<code>view_project(id=None, user_id=None, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>ProjectV2</code> table.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int | None</code> <p>If not <code>None</code>, select a given <code>project.id</code>.</p> <code>None</code> <code>user_id</code> <code>int | None</code> <p>If not <code>None</code>, select a given <code>project.user_id</code>.</p> <code>None</code> Source code in <code>fractal_server/app/routes/admin/v2/project.py</code> <pre><code>@router.get(\"/\", response_model=list[ProjectReadV2])\nasync def view_project(\n    id: int | None = None,\n    user_id: int | None = None,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ProjectReadV2]:\n    \"\"\"\n    Query `ProjectV2` table.\n\n    Args:\n        id: If not `None`, select a given `project.id`.\n        user_id: If not `None`, select a given `project.user_id`.\n    \"\"\"\n\n    stm = select(ProjectV2)\n\n    if id is not None:\n        stm = stm.where(ProjectV2.id == id)\n    if user_id is not None:\n        stm = stm.where(ProjectV2.user_list.any(UserOAuth.id == user_id))\n\n    res = await db.execute(stm)\n    project_list = res.scalars().all()\n    await db.close()\n\n    return project_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/resource/","title":"resource","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource._check_type_match_or_422","title":"<code>_check_type_match_or_422(new_resource)</code>","text":"<p>Handle case where <code>resource.type != FRACTAL_RUNNER_BACKEND</code></p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>def _check_type_match_or_422(new_resource: ResourceCreate) -&gt; None:\n    \"\"\"\n    Handle case where `resource.type != FRACTAL_RUNNER_BACKEND`\n    \"\"\"\n    settings = Inject(get_settings)\n    if settings.FRACTAL_RUNNER_BACKEND != new_resource.type:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"{settings.FRACTAL_RUNNER_BACKEND=} != \"\n                f\"{new_resource.type=}\"\n            ),\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.delete_resource","title":"<code>delete_resource(resource_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete single <code>Resource</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.delete(\"/{resource_id}/\", status_code=204)\nasync def delete_resource(\n    resource_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Delete single `Resource`.\n    \"\"\"\n    resource = await _get_resource_or_404(resource_id=resource_id, db=db)\n    try:\n        await db.delete(resource)\n        await db.commit()\n        return Response(status_code=status.HTTP_204_NO_CONTENT)\n    except IntegrityError as e:\n        await db.rollback()\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"IntegrityError for resource deletion. \"\n                f\"Original error:\\n{str(e)}\"\n            ),\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.get_resource","title":"<code>get_resource(resource_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query single <code>Resource</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.get(\"/{resource_id}/\", response_model=ResourceRead, status_code=200)\nasync def get_resource(\n    resource_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ResourceRead:\n    \"\"\"\n    Query single `Resource`.\n    \"\"\"\n    resource = await _get_resource_or_404(resource_id=resource_id, db=db)\n\n    return resource\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.get_resource_list","title":"<code>get_resource_list(superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>Resource</code> table.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.get(\"/\", response_model=list[ResourceRead], status_code=200)\nasync def get_resource_list(\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ResourceRead]:\n    \"\"\"\n    Query `Resource` table.\n    \"\"\"\n\n    stm = select(Resource).order_by(Resource.id)\n    res = await db.execute(stm)\n    resource_list = res.scalars().all()\n\n    return resource_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.get_resource_profiles","title":"<code>get_resource_profiles(resource_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>Profile</code>s for single <code>Resource</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.get(\n    \"/{resource_id}/profile/\",\n    response_model=list[ProfileRead],\n    status_code=200,\n)\nasync def get_resource_profiles(\n    resource_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ProfileRead]:\n    \"\"\"\n    Query `Profile`s for single `Resource`.\n    \"\"\"\n    await _get_resource_or_404(resource_id=resource_id, db=db)\n\n    res = await db.execute(\n        select(Profile).where(Profile.resource_id == resource_id)\n    )\n    profiles = res.scalars().all()\n\n    return profiles\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.post_profile","title":"<code>post_profile(resource_id, profile_create, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create new <code>Profile</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.post(\n    \"/{resource_id}/profile/\",\n    response_model=ProfileRead,\n    status_code=201,\n)\nasync def post_profile(\n    resource_id: int,\n    profile_create: ProfileCreate,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProfileRead:\n    \"\"\"\n    Create new `Profile`.\n    \"\"\"\n    resource = await _get_resource_or_404(resource_id=resource_id, db=db)\n\n    _check_resource_type_match_or_422(\n        resource=resource,\n        new_profile=profile_create,\n    )\n    await _check_profile_name(name=profile_create.name, db=db)\n\n    profile = Profile(\n        resource_id=resource_id,\n        **profile_create.model_dump(),\n    )\n\n    db.add(profile)\n    await db.commit()\n    await db.refresh(profile)\n    return profile\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.post_resource","title":"<code>post_resource(resource_create, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create new <code>Resource</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.post(\"/\", response_model=ResourceRead, status_code=201)\nasync def post_resource(\n    resource_create: ResourceCreate,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ResourceRead:\n    \"\"\"\n    Create new `Resource`.\n    \"\"\"\n\n    # Handle case where type!=FRACTAL_RUNNER_BACKEND\n    _check_type_match_or_422(resource_create)\n\n    await _check_resource_name(name=resource_create.name, db=db)\n\n    resource = Resource(**resource_create.model_dump())\n    db.add(resource)\n    await db.commit()\n    await db.refresh(resource)\n\n    return resource\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.put_resource","title":"<code>put_resource(resource_id, resource_update, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Overwrite a single <code>Resource</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.put(\n    \"/{resource_id}/\",\n    response_model=ResourceRead,\n    status_code=200,\n)\nasync def put_resource(\n    resource_id: int,\n    resource_update: ResourceCreate,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ResourceRead:\n    \"\"\"\n    Overwrite a single `Resource`.\n    \"\"\"\n\n    # Handle case where type!=FRACTAL_RUNNER_BACKEND\n    _check_type_match_or_422(resource_update)\n\n    resource = await _get_resource_or_404(resource_id=resource_id, db=db)\n\n    # Handle non-unique resource names\n    if resource_update.name and resource_update.name != resource.name:\n        await _check_resource_name(name=resource_update.name, db=db)\n\n    # Prepare new db object\n    for key, value in resource_update.model_dump().items():\n        setattr(resource, key, value)\n\n    await db.commit()\n    await db.refresh(resource)\n    return resource\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/task/","title":"task","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/task/#fractal_server.app.routes.admin.v2.task.query_tasks","title":"<code>query_tasks(id=None, source=None, version=None, name=None, max_number_of_results=25, category=None, modality=None, author=None, resource_id=None, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>TaskV2</code> table and get information about related items (WorkflowV2s and ProjectV2s)</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int | None</code> <p>If not <code>None</code>, query for matching <code>task.id</code>.</p> <code>None</code> <code>source</code> <code>str | None</code> <p>If not <code>None</code>, query for contained case insensitive <code>task.source</code>.</p> <code>None</code> <code>version</code> <code>str | None</code> <p>If not <code>None</code>, query for matching <code>task.version</code>.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>If not <code>None</code>, query for contained case insensitive <code>task.name</code>.</p> <code>None</code> <code>max_number_of_results</code> <code>int</code> <p>The maximum length of the response.</p> <code>25</code> <code>category</code> <code>str | None</code> <code>None</code> <code>modality</code> <code>str | None</code> <code>None</code> <code>author</code> <code>str | None</code> <code>None</code> <code>resource_id</code> <code>int | None</code> <code>None</code> Source code in <code>fractal_server/app/routes/admin/v2/task.py</code> <pre><code>@router.get(\"/\", response_model=list[TaskV2Info])\nasync def query_tasks(\n    id: int | None = None,\n    source: str | None = None,\n    version: str | None = None,\n    name: str | None = None,\n    max_number_of_results: int = 25,\n    category: str | None = None,\n    modality: str | None = None,\n    author: str | None = None,\n    resource_id: int | None = None,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[TaskV2Info]:\n    \"\"\"\n    Query `TaskV2` table and get information about related items\n    (WorkflowV2s and ProjectV2s)\n\n    Args:\n        id: If not `None`, query for matching `task.id`.\n        source: If not `None`, query for contained case insensitive\n            `task.source`.\n        version: If not `None`, query for matching `task.version`.\n        name: If not `None`, query for contained case insensitive `task.name`.\n        max_number_of_results: The maximum length of the response.\n        category:\n        modality:\n        author:\n        resource_id:\n    \"\"\"\n\n    stm = select(TaskV2)\n\n    if id is not None:\n        stm = stm.where(TaskV2.id == id)\n    if source is not None:\n        stm = stm.where(TaskV2.source.icontains(source))\n    if version is not None:\n        stm = stm.where(TaskV2.version == version)\n    if name is not None:\n        stm = stm.where(TaskV2.name.icontains(name))\n    if category is not None:\n        stm = stm.where(func.lower(TaskV2.category) == category.lower())\n    if modality is not None:\n        stm = stm.where(func.lower(TaskV2.modality) == modality.lower())\n    if author is not None:\n        stm = stm.where(TaskV2.authors.icontains(author))\n    if resource_id is not None:\n        stm = (\n            stm.join(TaskGroupV2)\n            .where(TaskGroupV2.id == TaskV2.taskgroupv2_id)\n            .where(TaskGroupV2.resource_id == resource_id)\n        )\n\n    stm = stm.order_by(TaskV2.id)\n    res = await db.execute(stm)\n    task_list = res.scalars().all()\n    if len(task_list) &gt; max_number_of_results:\n        await db.close()\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Too many Tasks ({len(task_list)} &gt; {max_number_of_results}).\"\n                \" Please add more query filters.\"\n            ),\n        )\n\n    task_info_list = []\n\n    for task in task_list:\n        stm = (\n            select(WorkflowV2)\n            .join(WorkflowTaskV2)\n            .where(WorkflowTaskV2.workflow_id == WorkflowV2.id)\n            .where(WorkflowTaskV2.task_id == task.id)\n        )\n        res = await db.execute(stm)\n        wf_list = res.scalars().all()\n\n        task_info_list.append(\n            dict(\n                task=task.model_dump(),\n                relationships=[\n                    dict(\n                        workflow_id=workflow.id,\n                        workflow_name=workflow.name,\n                        project_id=workflow.project.id,\n                        project_name=workflow.project.name,\n                        project_users=[\n                            dict(id=user.id, email=user.email)\n                            for user in workflow.project.user_list\n                        ],\n                    )\n                    for workflow in wf_list\n                ],\n            )\n        )\n\n    return task_info_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/task_group/","title":"task_group","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/task_group_lifecycle/","title":"task_group_lifecycle","text":""},{"location":"reference/fractal_server/app/routes/admin/v2/task_group_lifecycle/#fractal_server.app.routes.admin.v2.task_group_lifecycle.deactivate_task_group","title":"<code>deactivate_task_group(task_group_id, background_tasks, response, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Deactivate task-group venv</p> Source code in <code>fractal_server/app/routes/admin/v2/task_group_lifecycle.py</code> <pre><code>@router.post(\n    \"/{task_group_id}/deactivate/\",\n    response_model=TaskGroupActivityV2Read,\n)\nasync def deactivate_task_group(\n    task_group_id: int,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupActivityV2Read:\n    \"\"\"\n    Deactivate task-group venv\n    \"\"\"\n    task_group = await _get_task_group_or_404(\n        task_group_id=task_group_id, db=db\n    )\n\n    # Check that task-group is active\n    if not task_group.active:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot deactivate a task group with {task_group.active=}.\"\n            ),\n        )\n\n    # Check no other activity is ongoing\n    await check_no_ongoing_activity(task_group_id=task_group_id, db=db)\n\n    # Check no submitted jobs use tasks from this task group\n    await check_no_submitted_job(task_group_id=task_group.id, db=db)\n\n    # Shortcut for task-group with origin=\"other\"\n    if task_group.origin == TaskGroupV2OriginEnum.OTHER:\n        task_group.active = False\n        task_group_activity = TaskGroupActivityV2(\n            user_id=task_group.user_id,\n            taskgroupv2_id=task_group.id,\n            status=TaskGroupActivityStatusV2.OK,\n            action=TaskGroupActivityActionV2.DEACTIVATE,\n            pkg_name=task_group.pkg_name,\n            version=(task_group.version or \"N/A\"),\n            log=(\n                f\"Task group has {task_group.origin=}, set \"\n                \"task_group.active to False and exit.\"\n            ),\n            timestamp_started=get_timestamp(),\n            timestamp_ended=get_timestamp(),\n        )\n        db.add(task_group)\n        db.add(task_group_activity)\n        await db.commit()\n        response.status_code = status.HTTP_202_ACCEPTED\n        return task_group_activity\n\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatusV2.PENDING,\n        action=TaskGroupActivityActionV2.DEACTIVATE,\n        pkg_name=task_group.pkg_name,\n        version=task_group.version,\n        timestamp_started=get_timestamp(),\n    )\n    db.add(task_group_activity)\n    await db.commit()\n\n    user = await db.get(UserOAuth, task_group.user_id)\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(user=user, db=db)\n\n    # Submit background task\n    if resource.type == ResourceType.SLURM_SSH:\n        deactivate_function = deactivate_ssh\n    else:\n        deactivate_function = deactivate_local\n\n    background_tasks.add_task(\n        deactivate_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        resource=resource,\n        profile=profile,\n    )\n\n    logger.debug(\n        \"Admin task group deactivation endpoint: start deactivate \"\n        \"and return task_group_activity\"\n    )\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/fractal_server/app/routes/admin/v2/task_group_lifecycle/#fractal_server.app.routes.admin.v2.task_group_lifecycle.reactivate_task_group","title":"<code>reactivate_task_group(task_group_id, background_tasks, response, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Deactivate task-group venv</p> Source code in <code>fractal_server/app/routes/admin/v2/task_group_lifecycle.py</code> <pre><code>@router.post(\n    \"/{task_group_id}/reactivate/\",\n    response_model=TaskGroupActivityV2Read,\n)\nasync def reactivate_task_group(\n    task_group_id: int,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupActivityV2Read:\n    \"\"\"\n    Deactivate task-group venv\n    \"\"\"\n\n    task_group = await _get_task_group_or_404(\n        task_group_id=task_group_id, db=db\n    )\n\n    # Check that task-group is not active\n    if task_group.active:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot reactivate a task group with {task_group.active=}.\"\n            ),\n        )\n\n    # Check no other activity is ongoing\n    await check_no_ongoing_activity(task_group_id=task_group_id, db=db)\n\n    # Check no submitted jobs use tasks from this task group\n    await check_no_submitted_job(task_group_id=task_group.id, db=db)\n\n    # Shortcut for task-group with origin=\"other\"\n    if task_group.origin == TaskGroupV2OriginEnum.OTHER:\n        task_group.active = True\n        task_group_activity = TaskGroupActivityV2(\n            user_id=task_group.user_id,\n            taskgroupv2_id=task_group.id,\n            status=TaskGroupActivityStatusV2.OK,\n            action=TaskGroupActivityActionV2.REACTIVATE,\n            pkg_name=task_group.pkg_name,\n            version=(task_group.version or \"N/A\"),\n            log=(\n                f\"Task group has {task_group.origin=}, set \"\n                \"task_group.active to True and exit.\"\n            ),\n            timestamp_started=get_timestamp(),\n            timestamp_ended=get_timestamp(),\n        )\n        db.add(task_group)\n        db.add(task_group_activity)\n        await db.commit()\n        response.status_code = status.HTTP_202_ACCEPTED\n        return task_group_activity\n\n    if task_group.env_info is None:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot reactivate a task group with \"\n                f\"{task_group.env_info=}.\"\n            ),\n        )\n\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatusV2.PENDING,\n        action=TaskGroupActivityActionV2.REACTIVATE,\n        pkg_name=task_group.pkg_name,\n        version=task_group.version,\n        timestamp_started=get_timestamp(),\n    )\n    db.add(task_group_activity)\n    await db.commit()\n\n    # Get validated resource and profile\n    user = await db.get(UserOAuth, task_group.user_id)\n    resource, profile = await validate_user_profile(user=user, db=db)\n\n    # Submit background task\n    if resource.type == ResourceType.SLURM_SSH:\n        reactivate_function = reactivate_ssh\n    else:\n        reactivate_function = reactivate_local\n\n    background_tasks.add_task(\n        reactivate_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        resource=resource,\n        profile=profile,\n    )\n\n    logger.debug(\n        \"Admin task group reactivation endpoint: start reactivate \"\n        \"and return task_group_activity\"\n    )\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/","title":"api","text":"<p><code>api</code> module</p>"},{"location":"reference/fractal_server/app/routes/api/v2/","title":"v2","text":"<p><code>api/v2</code> module</p>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/","title":"_aux_functions","text":"<p>Auxiliary functions to get object from the database or perform simple checks</p>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._check_project_exists","title":"<code>_check_project_exists(*, project_name, user_id, db)</code>  <code>async</code>","text":"<p>Check that no other project with this name exists for this user.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Project name</p> required <code>user_id</code> <code>int</code> <p>User ID</p> required <code>db</code> <code>AsyncSession</code> required <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If such a project already exists</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _check_project_exists(\n    *,\n    project_name: str,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Check that no other project with this name exists for this user.\n\n    Args:\n        project_name: Project name\n        user_id: User ID\n        db:\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If such a project already exists\n    \"\"\"\n    stm = (\n        select(ProjectV2)\n        .join(LinkUserProjectV2)\n        .where(ProjectV2.name == project_name)\n        .where(LinkUserProjectV2.user_id == user_id)\n    )\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Project name ({project_name}) already in use\",\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._check_workflow_exists","title":"<code>_check_workflow_exists(*, name, project_id, db)</code>  <code>async</code>","text":"<p>Check that no other workflow of this project has the same name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Workflow name</p> required <code>project_id</code> <code>int</code> <p>Project ID</p> required <code>db</code> <code>AsyncSession</code> required <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If such a workflow already exists</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _check_workflow_exists(\n    *,\n    name: str,\n    project_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Check that no other workflow of this project has the same name.\n\n    Args:\n        name: Workflow name\n        project_id: Project ID\n        db:\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If such a workflow already exists\n    \"\"\"\n    stm = (\n        select(WorkflowV2)\n        .where(WorkflowV2.name == name)\n        .where(WorkflowV2.project_id == project_id)\n    )\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Workflow with {name=} and {project_id=} already exists.\",\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_dataset_check_owner","title":"<code>_get_dataset_check_owner(*, project_id, dataset_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a dataset and a project, after access control on the project</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>dataset_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>dict[Literal['dataset', 'project'], DatasetV2 | ProjectV2]</code> <p>Dictionary with the dataset and project objects (keys: <code>dataset</code>, <code>project</code>).</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the dataset is not associated to the project</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_dataset_check_owner(\n    *,\n    project_id: int,\n    dataset_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; dict[Literal[\"dataset\", \"project\"], DatasetV2 | ProjectV2]:\n    \"\"\"\n    Get a dataset and a project, after access control on the project\n\n    Args:\n        project_id:\n        dataset_id:\n        user_id:\n        db:\n\n    Returns:\n        Dictionary with the dataset and project objects (keys: `dataset`,\n            `project`).\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the dataset is not associated to the project\n    \"\"\"\n    # Access control for project\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user_id, db=db\n    )\n\n    # Get dataset\n    # (See issue 1087 for 'populate_existing=True')\n    dataset = await db.get(DatasetV2, dataset_id, populate_existing=True)\n\n    if not dataset:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Dataset not found\"\n        )\n    if dataset.project_id != project_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Invalid {project_id=} for {dataset_id=}\",\n        )\n\n    return dict(dataset=dataset, project=project)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_dataset_or_404","title":"<code>_get_dataset_or_404(*, dataset_id, db)</code>  <code>async</code>","text":"<p>Get a dataset or raise 404.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_dataset_or_404(\n    *,\n    dataset_id: int,\n    db: AsyncSession,\n) -&gt; DatasetV2:\n    \"\"\"\n    Get a dataset or raise 404.\n\n    Args:\n        dataset_id:\n        db:\n    \"\"\"\n    ds = await db.get(DatasetV2, dataset_id)\n    if ds is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Dataset {dataset_id} not found.\",\n        )\n    else:\n        return ds\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_job_check_owner","title":"<code>_get_job_check_owner(*, project_id, job_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a job and a project, after access control on the project</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>job_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>dict[Literal['job', 'project'], JobV2 | ProjectV2]</code> <p>Dictionary with the job and project objects (keys: <code>job</code>, <code>project</code>).</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the job is not associated to the project</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_job_check_owner(\n    *,\n    project_id: int,\n    job_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; dict[Literal[\"job\", \"project\"], JobV2 | ProjectV2]:\n    \"\"\"\n    Get a job and a project, after access control on the project\n\n    Args:\n        project_id:\n        job_id:\n        user_id:\n        db:\n\n    Returns:\n        Dictionary with the job and project objects (keys: `job`,\n            `project`).\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the job is not associated to the project\n    \"\"\"\n    # Access control for project\n    project = await _get_project_check_owner(\n        project_id=project_id,\n        user_id=user_id,\n        db=db,\n    )\n    # Get dataset\n    job = await db.get(JobV2, job_id)\n    if not job:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Job not found\"\n        )\n    if job.project_id != project_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Invalid {project_id=} for {job_id=}\",\n        )\n    return dict(job=job, project=project)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_project_check_owner","title":"<code>_get_project_check_owner(*, project_id, user_id, db)</code>  <code>async</code>","text":"<p>Check that user is a member of project and return the project.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>ProjectV2</code> <p>The project object</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=403_FORBIDDEN)</code> <p>If the user is not a member of the project</p> <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the project does not exist</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_project_check_owner(\n    *,\n    project_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; ProjectV2:\n    \"\"\"\n    Check that user is a member of project and return the project.\n\n    Args:\n        project_id:\n        user_id:\n        db:\n\n    Returns:\n        The project object\n\n    Raises:\n        HTTPException(status_code=403_FORBIDDEN):\n            If the user is not a member of the project\n        HTTPException(status_code=404_NOT_FOUND):\n            If the project does not exist\n    \"\"\"\n    project = await db.get(ProjectV2, project_id)\n\n    link_user_project = await db.get(LinkUserProjectV2, (project_id, user_id))\n    if not project:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Project not found\"\n        )\n    if not link_user_project:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=f\"Not allowed on project {project_id}\",\n        )\n\n    return project\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_submitted_job_or_none","title":"<code>_get_submitted_job_or_none(*, dataset_id, workflow_id, db)</code>  <code>async</code>","text":"<p>Get the submitted job for given dataset/workflow, if any.</p> <p>This function also handles the invalid branch where more than one job is found.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>int</code> required <code>workflow_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_submitted_job_or_none(\n    *,\n    dataset_id: int,\n    workflow_id: int,\n    db: AsyncSession,\n) -&gt; JobV2 | None:\n    \"\"\"\n    Get the submitted job for given dataset/workflow, if any.\n\n    This function also handles the invalid branch where more than one job\n    is found.\n\n    Args:\n        dataset_id:\n        workflow_id:\n        db:\n    \"\"\"\n    res = await db.execute(\n        _get_submitted_jobs_statement()\n        .where(JobV2.dataset_id == dataset_id)\n        .where(JobV2.workflow_id == workflow_id)\n    )\n    try:\n        return res.scalars().one_or_none()\n    except MultipleResultsFound as e:\n        error_msg = (\n            \"Multiple running jobs found for \"\n            f\"{dataset_id=} and {workflow_id=}.\"\n        )\n        logger.error(f\"{error_msg} Original error: {str(e)}.\")\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=error_msg,\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_submitted_jobs_statement","title":"<code>_get_submitted_jobs_statement()</code>","text":"<p>Returns:</p> Type Description <code>SelectOfScalar</code> <p>A sqlmodel statement that selects all <code>Job</code>s with</p> <code>SelectOfScalar</code> <p><code>Job.status</code> equal to <code>submitted</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>def _get_submitted_jobs_statement() -&gt; SelectOfScalar:\n    \"\"\"\n    Returns:\n        A sqlmodel statement that selects all `Job`s with\n        `Job.status` equal to `submitted`.\n    \"\"\"\n    stm = select(JobV2).where(JobV2.status == JobStatusTypeV2.SUBMITTED)\n    return stm\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_workflow_check_owner","title":"<code>_get_workflow_check_owner(*, workflow_id, project_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a workflow and a project, after access control on the project.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>int</code> required <code>project_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>WorkflowV2</code> <p>The workflow object.</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the workflow does not exist</p> <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the workflow is not associated to the project</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_workflow_check_owner(\n    *,\n    workflow_id: int,\n    project_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; WorkflowV2:\n    \"\"\"\n    Get a workflow and a project, after access control on the project.\n\n    Args:\n        workflow_id:\n        project_id:\n        user_id:\n        db:\n\n    Returns:\n        The workflow object.\n\n    Raises:\n        HTTPException(status_code=404_NOT_FOUND):\n            If the workflow does not exist\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the workflow is not associated to the project\n    \"\"\"\n\n    # Access control for project\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user_id, db=db\n    )\n\n    # Get workflow\n    # (See issue 1087 for 'populate_existing=True')\n    workflow = await db.get(WorkflowV2, workflow_id, populate_existing=True)\n\n    if not workflow:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Workflow not found\"\n        )\n    if workflow.project_id != project.id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(f\"Invalid {project_id=} for {workflow_id=}.\"),\n        )\n\n    return workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_workflow_or_404","title":"<code>_get_workflow_or_404(*, workflow_id, db)</code>  <code>async</code>","text":"<p>Get a workflow or raise 404.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_workflow_or_404(\n    *,\n    workflow_id: int,\n    db: AsyncSession,\n) -&gt; WorkflowV2:\n    \"\"\"\n    Get a workflow or raise 404.\n\n    Args:\n        workflow_id:\n        db:\n    \"\"\"\n    wf = await db.get(WorkflowV2, workflow_id)\n    if wf is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Workflow {workflow_id} not found.\",\n        )\n    else:\n        return wf\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_workflow_task_check_owner","title":"<code>_get_workflow_task_check_owner(*, project_id, workflow_id, workflow_task_id, user_id, db)</code>  <code>async</code>","text":"<p>Check that user has access to Workflow and WorkflowTask.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>workflow_id</code> <code>int</code> required <code>workflow_task_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>tuple[WorkflowTaskV2, WorkflowV2]</code> <p>Tuple of WorkflowTask and Workflow objects.</p> <p>Raises:</p> Type Description <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the WorkflowTask does not exist</p> <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If the WorkflowTask is not associated to the Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_workflow_task_check_owner(\n    *,\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; tuple[WorkflowTaskV2, WorkflowV2]:\n    \"\"\"\n    Check that user has access to Workflow and WorkflowTask.\n\n    Args:\n        project_id:\n        workflow_id:\n        workflow_task_id:\n        user_id:\n        db:\n\n    Returns:\n        Tuple of WorkflowTask and Workflow objects.\n\n    Raises:\n        HTTPException(status_code=404_NOT_FOUND):\n            If the WorkflowTask does not exist\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If the WorkflowTask is not associated to the Workflow\n    \"\"\"\n\n    # Access control for workflow\n    workflow = await _get_workflow_check_owner(\n        workflow_id=workflow_id,\n        project_id=project_id,\n        user_id=user_id,\n        db=db,\n    )\n\n    # If WorkflowTask is not in the db, exit\n    workflow_task = await db.get(WorkflowTaskV2, workflow_task_id)\n    if not workflow_task:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=\"WorkflowTask not found\",\n        )\n\n    # If WorkflowTask is not part of the expected Workflow, exit\n    if workflow_id != workflow_task.workflow_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Invalid {workflow_id=} for {workflow_task_id=}\",\n        )\n\n    return workflow_task, workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_workflowtask_or_404","title":"<code>_get_workflowtask_or_404(*, workflowtask_id, db)</code>  <code>async</code>","text":"<p>Get a workflow task or raise 404.</p> <p>Parameters:</p> Name Type Description Default <code>workflowtask_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_workflowtask_or_404(\n    *,\n    workflowtask_id: int,\n    db: AsyncSession,\n) -&gt; WorkflowTaskV2:\n    \"\"\"\n    Get a workflow task or raise 404.\n\n    Args:\n        workflowtask_id:\n        db:\n    \"\"\"\n    wftask = await db.get(WorkflowTaskV2, workflowtask_id)\n    if wftask is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"WorkflowTask {workflowtask_id} not found.\",\n        )\n    else:\n        return wftask\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._workflow_insert_task","title":"<code>_workflow_insert_task(*, workflow_id, task_id, meta_parallel=None, meta_non_parallel=None, args_non_parallel=None, args_parallel=None, type_filters=None, db)</code>  <code>async</code>","text":"<p>Insert a new WorkflowTask into Workflow.task_list</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>int</code> required <code>task_id</code> <code>int</code> required <code>meta_parallel</code> <code>dict[str, Any] | None</code> <code>None</code> <code>meta_non_parallel</code> <code>dict[str, Any] | None</code> <code>None</code> <code>args_non_parallel</code> <code>dict[str, Any] | None</code> <code>None</code> <code>args_parallel</code> <code>dict[str, Any] | None</code> <code>None</code> <code>type_filters</code> <code>dict[str, bool] | None</code> <code>None</code> <code>db</code> <code>AsyncSession</code> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _workflow_insert_task(\n    *,\n    workflow_id: int,\n    task_id: int,\n    meta_parallel: dict[str, Any] | None = None,\n    meta_non_parallel: dict[str, Any] | None = None,\n    args_non_parallel: dict[str, Any] | None = None,\n    args_parallel: dict[str, Any] | None = None,\n    type_filters: dict[str, bool] | None = None,\n    db: AsyncSession,\n) -&gt; WorkflowTaskV2:\n    \"\"\"\n    Insert a new WorkflowTask into Workflow.task_list\n\n    Args:\n        workflow_id:\n        task_id:\n\n        meta_parallel:\n        meta_non_parallel:\n        args_non_parallel:\n        args_parallel:\n        type_filters:\n        db:\n    \"\"\"\n    db_workflow = await db.get(WorkflowV2, workflow_id)\n    if db_workflow is None:\n        raise ValueError(f\"Workflow {workflow_id} does not exist\")\n\n    # Get task from db\n    db_task = await db.get(TaskV2, task_id)\n    if db_task is None:\n        raise ValueError(f\"TaskV2 {task_id} not found.\")\n    task_type = db_task.type\n\n    # Combine meta_parallel (higher priority)\n    # and db_task.meta_parallel (lower priority)\n    final_meta_parallel = (db_task.meta_parallel or {}).copy()\n    final_meta_parallel.update(meta_parallel or {})\n    if final_meta_parallel == {}:\n        final_meta_parallel = None\n    # Combine meta_non_parallel (higher priority)\n    # and db_task.meta_non_parallel (lower priority)\n    final_meta_non_parallel = (db_task.meta_non_parallel or {}).copy()\n    final_meta_non_parallel.update(meta_non_parallel or {})\n    if final_meta_non_parallel == {}:\n        final_meta_non_parallel = None\n\n    # Create DB entry\n    wf_task = WorkflowTaskV2(\n        task_type=task_type,\n        task_id=task_id,\n        args_non_parallel=args_non_parallel,\n        args_parallel=args_parallel,\n        meta_parallel=final_meta_parallel,\n        meta_non_parallel=final_meta_non_parallel,\n        type_filters=(type_filters or dict()),\n    )\n    db_workflow.task_list.append(wf_task)\n    flag_modified(db_workflow, \"task_list\")\n    await db.commit()\n\n    # See issue 1087 for 'populate_existing=True'\n    wf_task = await db.get(WorkflowTaskV2, wf_task.id, populate_existing=True)\n\n    return wf_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions.clean_app_job_list_v2","title":"<code>clean_app_job_list_v2(db, jobs_list)</code>  <code>async</code>","text":"<p>Remove from a job list all jobs with status different from submitted.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>AsyncSession</code> <p>Async database session</p> required <code>jobs_list</code> <code>list[int]</code> <p>List of job IDs currently associated to the app.</p> required Return <p>List of IDs for submitted jobs.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def clean_app_job_list_v2(\n    db: AsyncSession, jobs_list: list[int]\n) -&gt; list[int]:\n    \"\"\"\n    Remove from a job list all jobs with status different from submitted.\n\n    Args:\n        db: Async database session\n        jobs_list: List of job IDs currently associated to the app.\n\n    Return:\n        List of IDs for submitted jobs.\n    \"\"\"\n    stmt = select(JobV2).where(JobV2.id.in_(jobs_list))\n    result = await db.execute(stmt)\n    db_jobs_list = result.scalars().all()\n    submitted_job_ids = [\n        job.id\n        for job in db_jobs_list\n        if job.status == JobStatusTypeV2.SUBMITTED\n    ]\n    return submitted_job_ids\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_history/","title":"_aux_functions_history","text":""},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_history/#fractal_server.app.routes.api.v2._aux_functions_history._verify_workflow_and_dataset_access","title":"<code>_verify_workflow_and_dataset_access(*, project_id, workflow_id, dataset_id, user_id, db)</code>  <code>async</code>","text":"<p>Verify user access to a dataset/workflow pair.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>workflow_id</code> <code>int</code> required <code>dataset_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_history.py</code> <pre><code>async def _verify_workflow_and_dataset_access(\n    *,\n    project_id: int,\n    workflow_id: int,\n    dataset_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; dict[Literal[\"dataset\", \"workflow\"], DatasetV2 | WorkflowV2]:\n    \"\"\"\n    Verify user access to a dataset/workflow pair.\n\n    Args:\n        project_id:\n        workflow_id:\n        dataset_id:\n        user_id:\n        db:\n    \"\"\"\n    await _get_project_check_owner(\n        project_id=project_id,\n        user_id=user_id,\n        db=db,\n    )\n    workflow = await _get_workflow_or_404(\n        workflow_id=workflow_id,\n        db=db,\n    )\n    if workflow.project_id != project_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Workflow does not belong to expected project.\",\n        )\n    dataset = await _get_dataset_or_404(\n        dataset_id=dataset_id,\n        db=db,\n    )\n    if dataset.project_id != project_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Dataset does not belong to expected project.\",\n        )\n\n    return dict(dataset=dataset, workflow=workflow)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_history/#fractal_server.app.routes.api.v2._aux_functions_history.get_history_run_or_404","title":"<code>get_history_run_or_404(*, history_run_id, db)</code>  <code>async</code>","text":"<p>Get an existing HistoryRun  or raise a 404.</p> <p>Parameters:</p> Name Type Description Default <code>history_run_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_history.py</code> <pre><code>async def get_history_run_or_404(\n    *, history_run_id: int, db: AsyncSession\n) -&gt; HistoryRun:\n    \"\"\"\n    Get an existing HistoryRun  or raise a 404.\n\n    Args:\n        history_run_id:\n        db:\n    \"\"\"\n    history_run = await db.get(HistoryRun, history_run_id)\n    if history_run is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"HistoryRun {history_run_id} not found\",\n        )\n    return history_run\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_history/#fractal_server.app.routes.api.v2._aux_functions_history.get_history_unit_or_404","title":"<code>get_history_unit_or_404(*, history_unit_id, db)</code>  <code>async</code>","text":"<p>Get an existing HistoryUnit  or raise a 404.</p> <p>Parameters:</p> Name Type Description Default <code>history_unit_id</code> <code>int</code> <p>The <code>HistoryUnit</code> id</p> required <code>db</code> <code>AsyncSession</code> <p>An asynchronous db session</p> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_history.py</code> <pre><code>async def get_history_unit_or_404(\n    *, history_unit_id: int, db: AsyncSession\n) -&gt; HistoryUnit:\n    \"\"\"\n    Get an existing HistoryUnit  or raise a 404.\n\n    Args:\n        history_unit_id: The `HistoryUnit` id\n        db: An asynchronous db session\n    \"\"\"\n    history_unit = await db.get(HistoryUnit, history_unit_id)\n    if history_unit is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"HistoryUnit {history_unit_id} not found\",\n        )\n    return history_unit\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_history/#fractal_server.app.routes.api.v2._aux_functions_history.get_wftask_check_owner","title":"<code>get_wftask_check_owner(*, project_id, dataset_id, workflowtask_id, user_id, db)</code>  <code>async</code>","text":"<p>Verify user access for the history of this dataset and workflowtask.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>int</code> required <code>dataset_id</code> <code>int</code> required <code>workflowtask_id</code> <code>int</code> required <code>user_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_history.py</code> <pre><code>async def get_wftask_check_owner(\n    *,\n    project_id: int,\n    dataset_id: int,\n    workflowtask_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; WorkflowTaskV2:\n    \"\"\"\n    Verify user access for the history of this dataset and workflowtask.\n\n    Args:\n        project_id:\n        dataset_id:\n        workflowtask_id:\n        user_id:\n        db:\n    \"\"\"\n    wftask = await _get_workflowtask_or_404(\n        workflowtask_id=workflowtask_id,\n        db=db,\n    )\n    await _verify_workflow_and_dataset_access(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        workflow_id=wftask.workflow_id,\n        user_id=user_id,\n        db=db,\n    )\n    return wftask\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle/","title":"_aux_functions_task_lifecycle","text":""},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle/#fractal_server.app.routes.api.v2._aux_functions_task_lifecycle._find_latest_version_or_422","title":"<code>_find_latest_version_or_422(versions)</code>","text":"<p>For PEP 440 versions, this is easy enough for the client to do (using the <code>packaging</code> library [...]. For non-standard versions, there is no well-defined ordering, and clients will need to decide on what rule is appropriate for their needs. (https://peps.python.org/pep-0700/#why-not-provide-a-latest-version-value)</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle.py</code> <pre><code>def _find_latest_version_or_422(versions: list[str]) -&gt; str:\n    \"\"\"\n    &gt; For PEP 440 versions, this is easy enough for the client to do (using\n    &gt; the `packaging` library [...]. For non-standard versions, there is no\n    &gt; well-defined ordering, and clients will need to decide on what rule is\n    &gt; appropriate for their needs.\n    (https://peps.python.org/pep-0700/#why-not-provide-a-latest-version-value)\n    \"\"\"\n    try:\n        latest = max(versions, key=lambda v_str: Version(v_str))\n        return latest\n    except InvalidVersion as e:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Cannot find latest version (original error: {str(e)}).\",\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle/#fractal_server.app.routes.api.v2._aux_functions_task_lifecycle.check_no_ongoing_activity","title":"<code>check_no_ongoing_activity(*, task_group_id, db)</code>  <code>async</code>","text":"<p>Find ongoing activities for the same task group.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle.py</code> <pre><code>async def check_no_ongoing_activity(\n    *,\n    task_group_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Find ongoing activities for the same task group.\n\n    Args:\n        task_group_id:\n        db:\n    \"\"\"\n    # DB query\n    stm = (\n        select(TaskGroupActivityV2)\n        .where(TaskGroupActivityV2.taskgroupv2_id == task_group_id)\n        .where(TaskGroupActivityV2.status == TaskGroupActivityStatusV2.ONGOING)\n    )\n    res = await db.execute(stm)\n    ongoing_activities = res.scalars().all()\n\n    if ongoing_activities == []:\n        # All good, exit\n        return\n\n    msg = \"Found ongoing activities for the same task-group:\"\n    for ind, activity in enumerate(ongoing_activities):\n        msg = (\n            f\"{msg}\\n{ind + 1}) \"\n            f\"Action={activity.action}, \"\n            f\"status={activity.status}, \"\n            f\"timestamp_started={activity.timestamp_started}.\"\n        )\n    raise HTTPException(\n        status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n        detail=msg,\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle/#fractal_server.app.routes.api.v2._aux_functions_task_lifecycle.check_no_related_workflowtask","title":"<code>check_no_related_workflowtask(*, task_group, db)</code>  <code>async</code>","text":"<p>Raises an HTTPException if any of the tasks in the TaskGroup are referenced by an existing WorkflowTask.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle.py</code> <pre><code>async def check_no_related_workflowtask(\n    *,\n    task_group: TaskGroupV2,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Raises an HTTPException if any of the tasks in the TaskGroup are referenced\n    by an existing WorkflowTask.\n    \"\"\"\n    stm = select(WorkflowTaskV2).where(\n        WorkflowTaskV2.task_id.in_([task.id for task in task_group.task_list])\n    )\n    res = await db.execute(stm)\n    bad_wftask = res.scalars().first()\n    if bad_wftask is not None:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"TaskV2 {bad_wftask.task_id} is still in use\",\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle/#fractal_server.app.routes.api.v2._aux_functions_task_lifecycle.check_no_submitted_job","title":"<code>check_no_submitted_job(*, task_group_id, db)</code>  <code>async</code>","text":"<p>Find submitted jobs which include tasks from a given task group.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> <p>ID of the <code>TaskGroupV2</code> object.</p> required <code>db</code> <code>AsyncSession</code> <p>Asynchronous database session.</p> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle.py</code> <pre><code>async def check_no_submitted_job(\n    *,\n    task_group_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Find submitted jobs which include tasks from a given task group.\n\n    Args:\n        task_group_id: ID of the `TaskGroupV2` object.\n        db: Asynchronous database session.\n    \"\"\"\n    stm = (\n        select(func.count(JobV2.id))\n        .join(WorkflowV2, JobV2.workflow_id == WorkflowV2.id)\n        .join(WorkflowTaskV2, WorkflowTaskV2.workflow_id == WorkflowV2.id)\n        .join(TaskV2, WorkflowTaskV2.task_id == TaskV2.id)\n        .where(WorkflowTaskV2.order &gt;= JobV2.first_task_index)\n        .where(WorkflowTaskV2.order &lt;= JobV2.last_task_index)\n        .where(JobV2.status == JobStatusTypeV2.SUBMITTED)\n        .where(TaskV2.taskgroupv2_id == task_group_id)\n    )\n    res = await db.execute(stm)\n    num_submitted_jobs = res.scalar()\n    if num_submitted_jobs &gt; 0:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot act on task group because {num_submitted_jobs} \"\n                \"submitted jobs use its tasks.\"\n            ),\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle/#fractal_server.app.routes.api.v2._aux_functions_task_lifecycle.get_package_version_from_pypi","title":"<code>get_package_version_from_pypi(name, version=None)</code>  <code>async</code>","text":"<p>Make a GET call to PyPI JSON API and get latest compatible version.</p> <p>There are three cases:</p> <ol> <li><code>version</code> is set and it is found on PyPI as-is.</li> <li><code>version</code> is set but it is not found on PyPI as-is.</li> <li><code>version</code> is unset, and we query <code>PyPI</code> for latest.</li> </ol> <p>Ref https://warehouse.pypa.io/api-reference/json.html.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Package name.</p> required <code>version</code> <code>str | None</code> <p>Could be a correct version (<code>1.3.0</code>), an incomplete one (<code>1.3</code>) or <code>None</code>.</p> <code>None</code> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle.py</code> <pre><code>async def get_package_version_from_pypi(\n    name: str,\n    version: str | None = None,\n) -&gt; str:\n    \"\"\"\n    Make a GET call to PyPI JSON API and get latest *compatible* version.\n\n    There are three cases:\n\n    1. `version` is set and it is found on PyPI as-is.\n    2. `version` is set but it is not found on PyPI as-is.\n    3. `version` is unset, and we query `PyPI` for latest.\n\n    Ref https://warehouse.pypa.io/api-reference/json.html.\n\n    Args:\n        name: Package name.\n        version:\n            Could be a correct version (`1.3.0`), an incomplete one\n            (`1.3`) or `None`.\n    \"\"\"\n    normalized_name = normalize_package_name(name)\n    url = f\"https://pypi.org/simple/{normalized_name}/\"\n    hint = f\"Hint: specify the required version for '{name}'.\"\n\n    # Make request to PyPI\n    try:\n        async with AsyncClient(timeout=5.0) as client:\n            res = await client.get(url, headers=PYPI_JSON_HEADERS)\n    except TimeoutException as e:\n        error_msg = (\n            f\"A TimeoutException occurred while getting {url}.\\n\"\n            f\"Original error: {str(e)}.\"\n        )\n        logger.warning(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=error_msg,\n        )\n    except BaseException as e:\n        error_msg = (\n            f\"An unknown error occurred while getting {url}. \"\n            f\"Original error: {str(e)}.\"\n        )\n        logger.warning(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=error_msg,\n        )\n\n    # Parse response\n    if res.status_code != 200:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Could not get {url} (status_code {res.status_code}).\"\n                f\"\\n{hint}\"\n            ),\n        )\n    try:\n        response_data = res.json()\n        available_releases = response_data[\"versions\"]\n        latest_version = _find_latest_version_or_422(available_releases)\n    except KeyError as e:\n        logger.warning(\n            f\"A KeyError occurred while getting {url}. \"\n            f\"Original error: {str(e)}.\"\n        )\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"A KeyError error occurred while getting {url}.\\n{hint}\",\n        )\n\n    logger.info(\n        f\"Obtained data from {url}: \"\n        f\"{len(available_releases)} releases, \"\n        f\"latest={latest_version}.\"\n    )\n\n    if version is not None:\n        if version in available_releases:\n            logger.info(f\"Requested {version=} available on PyPI.\")\n            # Case 1: `version` is set and it is found on PyPI as-is\n            return version\n        else:\n            # Case 2: `version` is set but it is not found on PyPI as-is\n            # Filter using `version` as prefix, and sort\n            matching_versions = [\n                v for v in available_releases if v.startswith(version)\n            ]\n            logger.info(\n                f\"Requested {version=} not available on PyPI, \"\n                f\"found {len(matching_versions)} versions matching \"\n                f\"`{version}*`.\"\n            )\n            if len(matching_versions) == 0:\n                logger.info(f\"No version starting with {version} found.\")\n                raise HTTPException(\n                    status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                    detail=(\n                        f\"No version starting with {version} found.\\n\"\n                        f\"{hint}\"\n                    ),\n                )\n            else:\n                latest_matching_version = sorted(matching_versions)[-1]\n                return latest_matching_version\n    else:\n        # Case 3: `version` is unset and we use latest\n        logger.info(f\"No version requested, returning {latest_version=}.\")\n        return latest_version\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_task_version_update/","title":"_aux_functions_task_version_update","text":""},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_task_version_update/#fractal_server.app.routes.api.v2._aux_functions_task_version_update.get_new_workflow_task_meta","title":"<code>get_new_workflow_task_meta(*, old_workflow_task_meta, old_task_meta, new_task_meta)</code>","text":"<p>Prepare new meta field based on old/new tasks and old workflow task.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_version_update.py</code> <pre><code>def get_new_workflow_task_meta(\n    *,\n    old_workflow_task_meta: dict | None,\n    old_task_meta: dict | None,\n    new_task_meta: dict | None,\n) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Prepare new meta field based on old/new tasks and old workflow task.\n    \"\"\"\n\n    # When the whole `old_workflow_task_meta` is user-provided, use it\n    # as the outcome\n    if old_task_meta is None:\n        return old_workflow_task_meta\n\n    # When `old_workflow_task_meta` is unset, use the new-task meta as default.\n    if old_workflow_task_meta is None:\n        return new_task_meta\n\n    if new_task_meta is None:\n        new_task_meta = {}\n\n    # Find properties that were added to the old defaults\n    additions = {\n        k: v\n        for k, v in old_workflow_task_meta.items()\n        if v != old_task_meta.get(k)\n    }\n    # Find properties that were removed from the old defaults\n    removals = old_task_meta.keys() - old_workflow_task_meta.keys()\n\n    # Add `additions` and remove `removals`.\n    new_workflowtask_meta = {\n        k: v\n        for k, v in (new_task_meta | additions).items()\n        if k not in removals\n    }\n\n    return new_workflowtask_meta\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_tasks/","title":"_aux_functions_tasks","text":"<p>Auxiliary functions to get task and task-group object from the database or perform simple checks</p>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._check_type_filters_compatibility","title":"<code>_check_type_filters_compatibility(*, task_input_types, wftask_type_filters)</code>","text":"<p>Wrap <code>merge_type_filters</code> and raise <code>HTTPException</code> if needed.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>def _check_type_filters_compatibility(\n    *,\n    task_input_types: dict[str, bool],\n    wftask_type_filters: dict[str, bool],\n) -&gt; None:\n    \"\"\"\n    Wrap `merge_type_filters` and raise `HTTPException` if needed.\n    \"\"\"\n    try:\n        merge_type_filters(\n            task_input_types=task_input_types,\n            wftask_type_filters=wftask_type_filters,\n        )\n    except ValueError as e:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Incompatible type filters.\\nOriginal error: {str(e)}\",\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_full_access","title":"<code>_get_task_full_access(*, task_id, user_id, db)</code>  <code>async</code>","text":"<p>Get an existing task or raise a 404.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>int</code> <p>ID of the required task.</p> required <code>user_id</code> <code>int</code> <p>ID of the current user.</p> required <code>db</code> <code>AsyncSession</code> <p>An asynchronous db session.</p> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_full_access(\n    *,\n    task_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; TaskV2:\n    \"\"\"\n    Get an existing task or raise a 404.\n\n    Args:\n        task_id: ID of the required task.\n        user_id: ID of the current user.\n        db: An asynchronous db session.\n    \"\"\"\n    task = await _get_task_or_404(task_id=task_id, db=db)\n    task_group = await _get_task_group_full_access(\n        task_group_id=task.taskgroupv2_id, user_id=user_id, db=db\n    )\n\n    resource_id = await _get_user_resource_id(user_id=user_id, db=db)\n    if resource_id is None or resource_id != task_group.resource_id:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=f\"User {user_id} has no access to TaskGroup's Resource.\",\n        )\n\n    return task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_group_full_access","title":"<code>_get_task_group_full_access(*, task_group_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a task group or raise a 403 if user has no full access.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> <p>ID of the required task group.</p> required <code>user_id</code> <code>int</code> <p>ID of the current user.</p> required <code>db</code> <code>AsyncSession</code> <p>An asynchronous db session</p> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_group_full_access(\n    *,\n    task_group_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Get a task group or raise a 403 if user has no full access.\n\n    Args:\n        task_group_id: ID of the required task group.\n        user_id: ID of the current user.\n        db: An asynchronous db session\n    \"\"\"\n    task_group = await _get_task_group_or_404(\n        task_group_id=task_group_id, db=db\n    )\n\n    if task_group.user_id == user_id:\n        return task_group\n    else:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=(\n                \"Current user has no full access to \"\n                f\"TaskGroupV2 {task_group_id}.\",\n            ),\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_group_or_404","title":"<code>_get_task_group_or_404(*, task_group_id, db)</code>  <code>async</code>","text":"<p>Get an existing task group or raise a 404.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> <p>The TaskGroupV2 id</p> required <code>db</code> <code>AsyncSession</code> <p>An asynchronous db session</p> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_group_or_404(\n    *, task_group_id: int, db: AsyncSession\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Get an existing task group or raise a 404.\n\n    Args:\n        task_group_id: The TaskGroupV2 id\n        db: An asynchronous db session\n    \"\"\"\n    task_group = await db.get(TaskGroupV2, task_group_id)\n    if task_group is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"TaskGroupV2 {task_group_id} not found\",\n        )\n    return task_group\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_group_read_access","title":"<code>_get_task_group_read_access(*, task_group_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a task group or raise a 403 if user has no read access.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> <p>ID of the required task group.</p> required <code>user_id</code> <code>int</code> <p>ID of the current user.</p> required <code>db</code> <code>AsyncSession</code> <p>An asynchronous db session.</p> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_group_read_access(\n    *,\n    task_group_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Get a task group or raise a 403 if user has no read access.\n\n    Args:\n        task_group_id: ID of the required task group.\n        user_id: ID of the current user.\n        db: An asynchronous db session.\n    \"\"\"\n    task_group = await _get_task_group_or_404(\n        task_group_id=task_group_id, db=db\n    )\n\n    # Prepare exception to be used below\n    forbidden_exception = HTTPException(\n        status_code=status.HTTP_403_FORBIDDEN,\n        detail=(\n            \"Current user has no read access to TaskGroupV2 \"\n            f\"{task_group_id}.\"\n        ),\n    )\n\n    if task_group.user_id == user_id:\n        return task_group\n    elif task_group.user_group_id is None:\n        raise forbidden_exception\n    else:\n        stm = (\n            select(LinkUserGroup)\n            .join(UserOAuth)\n            .join(Profile)\n            .where(LinkUserGroup.group_id == task_group.user_group_id)\n            .where(LinkUserGroup.user_id == user_id)\n            .where(UserOAuth.id == user_id)\n            .where(Profile.id == UserOAuth.profile_id)\n            .where(task_group.resource_id == Profile.resource_id)\n        )\n        res = await db.execute(stm)\n        link = res.unique().scalars().one_or_none()\n        if link is None:\n            raise forbidden_exception\n        else:\n            return task_group\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_or_404","title":"<code>_get_task_or_404(*, task_id, db)</code>  <code>async</code>","text":"<p>Get an existing task or raise a 404.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>int</code> <p>ID of the required task.</p> required <code>db</code> <code>AsyncSession</code> <p>An asynchronous db session</p> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_or_404(*, task_id: int, db: AsyncSession) -&gt; TaskV2:\n    \"\"\"\n    Get an existing task or raise a 404.\n\n    Args:\n        task_id: ID of the required task.\n        db: An asynchronous db session\n    \"\"\"\n    task = await db.get(TaskV2, task_id)\n    if task is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"TaskV2 {task_id} not found\",\n        )\n    return task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_read_access","title":"<code>_get_task_read_access(*, task_id, user_id, db, require_active=False)</code>  <code>async</code>","text":"<p>Get an existing task or raise a 404.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>int</code> <p>ID of the required task.</p> required <code>user_id</code> <code>int</code> <p>ID of the current user.</p> required <code>db</code> <code>AsyncSession</code> <p>An asynchronous db session.</p> required <code>require_active</code> <code>bool</code> <p>If set, fail when the task group is not <code>active</code></p> <code>False</code> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_read_access(\n    *,\n    task_id: int,\n    user_id: int,\n    db: AsyncSession,\n    require_active: bool = False,\n) -&gt; TaskV2:\n    \"\"\"\n    Get an existing task or raise a 404.\n\n    Args:\n        task_id: ID of the required task.\n        user_id: ID of the current user.\n        db: An asynchronous db session.\n        require_active: If set, fail when the task group is not `active`\n    \"\"\"\n    task = await _get_task_or_404(task_id=task_id, db=db)\n    task_group = await _get_task_group_read_access(\n        task_group_id=task.taskgroupv2_id, user_id=user_id, db=db\n    )\n\n    resource_id = await _get_user_resource_id(user_id=user_id, db=db)\n    if resource_id is None or resource_id != task_group.resource_id:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=f\"User {user_id} has no access to TaskGroup's Resource.\",\n        )\n\n    if require_active and not task_group.active:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Error: task {task_id} ({task.name}) is not active.\",\n        )\n\n    return task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_valid_user_group_id","title":"<code>_get_valid_user_group_id(*, user_group_id=None, private, user_id, db)</code>  <code>async</code>","text":"<p>Validate query parameters for endpoints that create some task(s).</p> <p>Parameters:</p> Name Type Description Default <code>user_group_id</code> <code>int | None</code> <code>None</code> <code>private</code> <code>bool</code> required <code>user_id</code> <code>int</code> <p>ID of the current user</p> required <code>db</code> <code>AsyncSession</code> <p>An asynchronous db session.</p> required Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_valid_user_group_id(\n    *,\n    user_group_id: int | None = None,\n    private: bool,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; int | None:\n    \"\"\"\n    Validate query parameters for endpoints that create some task(s).\n\n    Args:\n        user_group_id:\n        private:\n        user_id: ID of the current user\n        db: An asynchronous db session.\n    \"\"\"\n    if (user_group_id is not None) and (private is True):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Cannot set both {user_group_id=} and {private=}\",\n        )\n    elif private is True:\n        user_group_id = None\n    elif user_group_id is None:\n        user_group_id = await _get_default_usergroup_id_or_none(db=db)\n    else:\n        await _verify_user_belongs_to_group(\n            user_id=user_id, user_group_id=user_group_id, db=db\n        )\n    return user_group_id\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._verify_non_duplication_group_path","title":"<code>_verify_non_duplication_group_path(*, path, resource_id, db)</code>  <code>async</code>","text":"<p>Verify uniqueness of non-<code>None</code> <code>TaskGroupV2.path</code></p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _verify_non_duplication_group_path(\n    *,\n    path: str | None,\n    resource_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Verify uniqueness of non-`None` `TaskGroupV2.path`\n    \"\"\"\n    if path is None:\n        return\n    stm = (\n        select(TaskGroupV2.id)\n        .where(TaskGroupV2.path == path)\n        .where(TaskGroupV2.resource_id == resource_id)\n    )\n    res = await db.execute(stm)\n    duplicate_ids = res.scalars().all()\n    if duplicate_ids:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Other TaskGroups already have {path=}: \"\n                f\"{sorted(duplicate_ids)}.\"\n            ),\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_task_group_disambiguation/","title":"_aux_task_group_disambiguation","text":""},{"location":"reference/fractal_server/app/routes/api/v2/_aux_task_group_disambiguation/#fractal_server.app.routes.api.v2._aux_task_group_disambiguation._disambiguate_task_groups","title":"<code>_disambiguate_task_groups(*, matching_task_groups, user_id, default_group_id, db)</code>  <code>async</code>","text":"<p>Find ownership-based top-priority task group, if any.</p> <p>Parameters:</p> Name Type Description Default <code>matching_task_groups</code> <code>list[TaskGroupV2]</code> required <code>user_id</code> <code>int</code> required <code>default_group_id</code> <code>int | None</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>TaskGroupV2 | None</code> <p>The task group or <code>None</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_task_group_disambiguation.py</code> <pre><code>async def _disambiguate_task_groups(\n    *,\n    matching_task_groups: list[TaskGroupV2],\n    user_id: int,\n    default_group_id: int | None,\n    db: AsyncSession,\n) -&gt; TaskGroupV2 | None:\n    \"\"\"\n    Find ownership-based top-priority task group, if any.\n\n    Args:\n        matching_task_groups:\n        user_id:\n        default_group_id:\n        db:\n\n    Returns:\n        The task group or `None`.\n    \"\"\"\n\n    # Highest priority: task groups created by user\n    list_user_ids = [tg.user_id for tg in matching_task_groups]\n    try:\n        ind_user_id = list_user_ids.index(user_id)\n        task_group = matching_task_groups[ind_user_id]\n        logger.debug(\n            \"[_disambiguate_task_groups] \"\n            f\"Found task group {task_group.id} with {user_id=}, return.\"\n        )\n        return task_group\n    except ValueError:\n        logger.debug(\n            \"[_disambiguate_task_groups] \"\n            f\"No task group with {user_id=}, continue.\"\n        )\n\n    # Medium priority: task groups owned by default user group\n    settings = Inject(get_settings)\n    if settings.FRACTAL_DEFAULT_GROUP_NAME is not None:\n        list_user_group_ids = [tg.user_group_id for tg in matching_task_groups]\n        try:\n            ind_user_group_id = list_user_group_ids.index(default_group_id)\n            task_group = matching_task_groups[ind_user_group_id]\n            logger.debug(\n                \"[_disambiguate_task_groups] \"\n                f\"Found task group {task_group.id} with {user_id=}, return.\"\n            )\n            return task_group\n        except ValueError:\n            logger.debug(\n                \"[_disambiguate_task_groups] \"\n                \"No task group with user_group_id=\"\n                f\"{default_group_id}, continue.\"\n            )\n\n    # Lowest priority: task groups owned by other groups, sorted\n    # according to age of the user/usergroup link\n    logger.debug(\n        \"[_disambiguate_task_groups] \"\n        \"Sort remaining task groups by oldest-user-link.\"\n    )\n    stm = (\n        select(LinkUserGroup.group_id)\n        .where(LinkUserGroup.user_id == user_id)\n        .where(LinkUserGroup.group_id.in_(list_user_group_ids))\n        .order_by(LinkUserGroup.timestamp_created.asc())\n    )\n    res = await db.execute(stm)\n    oldest_user_group_id = res.scalars().first()\n    logger.debug(\n        \"[_disambiguate_task_groups] \" f\"Result: {oldest_user_group_id=}.\"\n    )\n    task_group = next(\n        iter(\n            task_group\n            for task_group in matching_task_groups\n            if task_group.user_group_id == oldest_user_group_id\n        ),\n        None,\n    )\n    return task_group\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_task_group_disambiguation/#fractal_server.app.routes.api.v2._aux_task_group_disambiguation._disambiguate_task_groups_not_none","title":"<code>_disambiguate_task_groups_not_none(*, matching_task_groups, user_id, default_group_id, db)</code>  <code>async</code>","text":"<p>Find ownership-based top-priority task group, and fail otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>matching_task_groups</code> <code>list[TaskGroupV2]</code> required <code>user_id</code> <code>int</code> required <code>default_group_id</code> <code>int | None</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>TaskGroupV2</code> <p>The top-priority task group.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_task_group_disambiguation.py</code> <pre><code>async def _disambiguate_task_groups_not_none(\n    *,\n    matching_task_groups: list[TaskGroupV2],\n    user_id: int,\n    default_group_id: int | None,\n    db: AsyncSession,\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Find ownership-based top-priority task group, and fail otherwise.\n\n    Args:\n        matching_task_groups:\n        user_id:\n        default_group_id:\n        db:\n\n    Returns:\n        The top-priority task group.\n    \"\"\"\n    task_group = await _disambiguate_task_groups(\n        matching_task_groups=matching_task_groups,\n        user_id=user_id,\n        default_group_id=default_group_id,\n        db=db,\n    )\n    if task_group is None:\n        error_msg = (\n            \"[_disambiguate_task_groups_not_none] Could not find a task \"\n            f\"group ({user_id=}, {default_group_id=}).\"\n        )\n        logger.error(f\"UnreachableBranchError {error_msg}\")\n        raise UnreachableBranchError(error_msg)\n    else:\n        return task_group\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/_aux_task_group_disambiguation/#fractal_server.app.routes.api.v2._aux_task_group_disambiguation.remove_duplicate_task_groups","title":"<code>remove_duplicate_task_groups(*, task_groups, user_id, default_group_id, db)</code>  <code>async</code>","text":"<p>Extract an item for each <code>version</code> from a sorted task-group list.</p> <p>Parameters:</p> Name Type Description Default <code>task_groups</code> <code>list[TaskGroupV2]</code> <p>A list of task groups with identical <code>pkg_name</code></p> required <code>user_id</code> <code>int</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>list[TaskGroupV2]</code> <p>New list of task groups with no duplicate <code>(pkg_name,version)</code> entries</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_task_group_disambiguation.py</code> <pre><code>async def remove_duplicate_task_groups(\n    *,\n    task_groups: list[TaskGroupV2],\n    user_id: int,\n    default_group_id: int | None,\n    db: AsyncSession,\n) -&gt; list[TaskGroupV2]:\n    \"\"\"\n    Extract an item for each `version` from a *sorted* task-group list.\n\n    Args:\n        task_groups: A list of task groups with identical `pkg_name`\n        user_id: User ID\n\n    Returns:\n        New list of task groups with no duplicate `(pkg_name,version)` entries\n    \"\"\"\n\n    new_task_groups = [\n        (\n            await _disambiguate_task_groups_not_none(\n                matching_task_groups=list(groups),\n                user_id=user_id,\n                default_group_id=default_group_id,\n                db=db,\n            )\n        )\n        for version, groups in itertools.groupby(\n            task_groups, key=lambda tg: tg.version\n        )\n    ]\n    return new_task_groups\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/","title":"dataset","text":""},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.create_dataset","title":"<code>create_dataset(project_id, dataset, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Add new dataset to current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/dataset/\",\n    response_model=DatasetReadV2,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_dataset(\n    project_id: int,\n    dataset: DatasetCreateV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; DatasetReadV2 | None:\n    \"\"\"\n    Add new dataset to current project\n    \"\"\"\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n\n    if dataset.zarr_dir is None:\n        db_dataset = DatasetV2(\n            project_id=project_id,\n            zarr_dir=\"__PLACEHOLDER__\",\n            **dataset.model_dump(exclude={\"zarr_dir\"}),\n        )\n        db.add(db_dataset)\n        await db.commit()\n        await db.refresh(db_dataset)\n        path = (\n            f\"{user.project_dir}/fractal/\"\n            f\"{project_id}_{sanitize_string(project.name)}/\"\n            f\"{db_dataset.id}_{sanitize_string(db_dataset.name)}\"\n        )\n        normalized_path = normalize_url(path)\n        db_dataset.zarr_dir = normalized_path\n\n        db.add(db_dataset)\n        await db.commit()\n        await db.refresh(db_dataset)\n    else:\n        db_dataset = DatasetV2(project_id=project_id, **dataset.model_dump())\n        db.add(db_dataset)\n        await db.commit()\n        await db.refresh(db_dataset)\n\n    return db_dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.delete_dataset","title":"<code>delete_dataset(project_id, dataset_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    status_code=204,\n)\nasync def delete_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a dataset associated to the current project\n    \"\"\"\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current dataset.\n    stm = _get_submitted_jobs_statement().where(JobV2.dataset_id == dataset_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot delete dataset {dataset.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # Delete dataset\n    await db.delete(dataset)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.export_dataset","title":"<code>export_dataset(project_id, dataset_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Export an existing dataset</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/{dataset_id}/export/\",\n    response_model=DatasetExportV2,\n)\nasync def export_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; DatasetExportV2 | None:\n    \"\"\"\n    Export an existing dataset\n    \"\"\"\n    dict_dataset_project = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    await db.close()\n\n    dataset = dict_dataset_project[\"dataset\"]\n\n    return dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.get_user_datasets","title":"<code>get_user_datasets(user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the datasets of the current user</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\"/dataset/\", response_model=list[DatasetReadV2])\nasync def get_user_datasets(\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[DatasetReadV2]:\n    \"\"\"\n    Returns all the datasets of the current user\n    \"\"\"\n    stm = select(DatasetV2)\n    stm = stm.join(ProjectV2).where(\n        ProjectV2.user_list.any(UserOAuth.id == user.id)\n    )\n\n    res = await db.execute(stm)\n    dataset_list = res.scalars().all()\n    await db.close()\n    return dataset_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.import_dataset","title":"<code>import_dataset(project_id, dataset, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Import an existing dataset into a project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/dataset/import/\",\n    response_model=DatasetReadV2,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def import_dataset(\n    project_id: int,\n    dataset: DatasetImportV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; DatasetReadV2 | None:\n    \"\"\"\n    Import an existing dataset into a project\n    \"\"\"\n\n    # Preliminary checks\n    await _get_project_check_owner(\n        project_id=project_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    for image in dataset.images:\n        if not image.zarr_url.startswith(dataset.zarr_dir):\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=(\n                    f\"Cannot import dataset: zarr_url {image.zarr_url} is not \"\n                    f\"relative to zarr_dir={dataset.zarr_dir}.\"\n                ),\n            )\n\n    # Create new Dataset\n    db_dataset = DatasetV2(\n        project_id=project_id,\n        **dataset.model_dump(exclude_none=True),\n    )\n    db.add(db_dataset)\n    await db.commit()\n    await db.refresh(db_dataset)\n    await db.close()\n\n    return db_dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.read_dataset","title":"<code>read_dataset(project_id, dataset_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    response_model=DatasetReadV2,\n)\nasync def read_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; DatasetReadV2 | None:\n    \"\"\"\n    Get info on a dataset associated to the current project\n    \"\"\"\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n    await db.close()\n    return dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.read_dataset_list","title":"<code>read_dataset_list(project_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get dataset list for given project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/\",\n    response_model=list[DatasetReadV2],\n)\nasync def read_dataset_list(\n    project_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[DatasetReadV2] | None:\n    \"\"\"\n    Get dataset list for given project\n    \"\"\"\n    # Access control\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    # Find datasets of the current project. Note: this select/where approach\n    # has much better scaling than refreshing all elements of\n    # `project.dataset_list` - ref\n    # https://github.com/fractal-analytics-platform/fractal-server/pull/1082#issuecomment-1856676097.\n    stm = select(DatasetV2).where(DatasetV2.project_id == project.id)\n    res = await db.execute(stm)\n    dataset_list = res.scalars().all()\n    await db.close()\n    return dataset_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.update_dataset","title":"<code>update_dataset(project_id, dataset_id, dataset_update, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    response_model=DatasetReadV2,\n)\nasync def update_dataset(\n    project_id: int,\n    dataset_id: int,\n    dataset_update: DatasetUpdateV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; DatasetReadV2 | None:\n    \"\"\"\n    Edit a dataset associated to the current project\n    \"\"\"\n\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    db_dataset = output[\"dataset\"]\n\n    if (dataset_update.zarr_dir is not None) and (len(db_dataset.images) != 0):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot modify `zarr_dir` because the dataset has a non-empty \"\n                \"image list.\"\n            ),\n        )\n\n    for key, value in dataset_update.model_dump(exclude_unset=True).items():\n        setattr(db_dataset, key, value)\n\n    await db.commit()\n    await db.refresh(db_dataset)\n    await db.close()\n    return db_dataset\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/history/","title":"history","text":""},{"location":"reference/fractal_server/app/routes/api/v2/history/#fractal_server.app.routes.api.v2.history.get_dataset_history","title":"<code>get_dataset_history(project_id, dataset_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns a list of all HistoryRuns associated to a given dataset, sorted by timestamp.</p> Source code in <code>fractal_server/app/routes/api/v2/history.py</code> <pre><code>@router.get(\"/project/{project_id}/dataset/{dataset_id}/history/\")\nasync def get_dataset_history(\n    project_id: int,\n    dataset_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[HistoryRunRead]:\n    \"\"\"\n    Returns a list of all HistoryRuns associated to a given dataset, sorted by\n    timestamp.\n    \"\"\"\n    # Access control\n    await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    res = await db.execute(\n        select(HistoryRun)\n        .where(HistoryRun.dataset_id == dataset_id)\n        .order_by(HistoryRun.timestamp_started)\n    )\n    history_run_list = res.scalars().all()\n\n    return history_run_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/images/","title":"images","text":""},{"location":"reference/fractal_server/app/routes/api/v2/images/#fractal_server.app.routes.api.v2.images.ImageQuery","title":"<code>ImageQuery</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Query for a list of images.</p> <p>Attributes:</p> Name Type Description <code>type_filters</code> <code>TypeFilters</code> <code>attribute_filters</code> <code>AttributeFilters</code> Source code in <code>fractal_server/app/routes/api/v2/images.py</code> <pre><code>class ImageQuery(BaseModel):\n    \"\"\"\n    Query for a list of images.\n\n    Attributes:\n        type_filters:\n        attribute_filters:\n    \"\"\"\n\n    type_filters: TypeFilters = Field(default_factory=dict)\n    attribute_filters: AttributeFilters = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/","title":"job","text":""},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.download_job_logs","title":"<code>download_job_logs(project_id, job_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Download zipped job folder</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/download/\",\n    response_class=StreamingResponse,\n)\nasync def download_job_logs(\n    project_id: int,\n    job_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StreamingResponse:\n    \"\"\"\n    Download zipped job folder\n    \"\"\"\n    output = await _get_job_check_owner(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        db=db,\n    )\n    job = output[\"job\"]\n    zip_name = f\"{Path(job.working_dir).name}_archive.zip\"\n\n    zip_bytes_iterator = await zip_folder_threaded(job.working_dir)\n\n    return StreamingResponse(\n        zip_bytes_iterator,\n        media_type=\"application/x-zip-compressed\",\n        headers={\"Content-Disposition\": f\"attachment;filename={zip_name}\"},\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.get_job_list","title":"<code>get_job_list(project_id, user=Depends(current_user_act_ver_prof), log=True, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get job list for given project</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/\",\n    response_model=list[JobReadV2],\n)\nasync def get_job_list(\n    project_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    log: bool = True,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[JobReadV2] | None:\n    \"\"\"\n    Get job list for given project\n    \"\"\"\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n\n    stm = select(JobV2).where(JobV2.project_id == project.id)\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    await db.close()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.get_user_jobs","title":"<code>get_user_jobs(user=Depends(current_user_act_ver_prof), log=True, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the jobs of the current user</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\"/job/\", response_model=list[JobReadV2])\nasync def get_user_jobs(\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    log: bool = True,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[JobReadV2]:\n    \"\"\"\n    Returns all the jobs of the current user\n    \"\"\"\n    stm = (\n        select(JobV2)\n        .join(ProjectV2)\n        .where(ProjectV2.user_list.any(UserOAuth.id == user.id))\n    )\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    await db.close()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.get_workflow_jobs","title":"<code>get_workflow_jobs(project_id, workflow_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the jobs related to a specific workflow</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/job/\",\n    response_model=list[JobReadV2],\n)\nasync def get_workflow_jobs(\n    project_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[JobReadV2] | None:\n    \"\"\"\n    Returns all the jobs related to a specific workflow\n    \"\"\"\n    await _get_workflow_check_owner(\n        project_id=project_id, workflow_id=workflow_id, user_id=user.id, db=db\n    )\n    stm = select(JobV2).where(JobV2.workflow_id == workflow_id)\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    return job_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.read_job","title":"<code>read_job(project_id, job_id, show_tmp_logs=False, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return info on an existing job</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/\",\n    response_model=JobReadV2,\n)\nasync def read_job(\n    project_id: int,\n    job_id: int,\n    show_tmp_logs: bool = False,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; JobReadV2 | None:\n    \"\"\"\n    Return info on an existing job\n    \"\"\"\n\n    output = await _get_job_check_owner(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        db=db,\n    )\n    job = output[\"job\"]\n    await db.close()\n\n    if show_tmp_logs and (job.status == JobStatusTypeV2.SUBMITTED):\n        try:\n            with open(f\"{job.working_dir}/{WORKFLOW_LOG_FILENAME}\") as f:\n                job.log = f.read()\n        except FileNotFoundError:\n            pass\n\n    return job\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.stop_job","title":"<code>stop_job(project_id, job_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Stop execution of a workflow job.</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/stop/\",\n    status_code=202,\n)\nasync def stop_job(\n    project_id: int,\n    job_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Stop execution of a workflow job.\n    \"\"\"\n\n    _check_shutdown_is_supported()\n\n    # Get job from DB\n    output = await _get_job_check_owner(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        db=db,\n    )\n    job = output[\"job\"]\n\n    _write_shutdown_file(job=job)\n\n    return Response(status_code=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/pre_submission_checks/","title":"pre_submission_checks","text":""},{"location":"reference/fractal_server/app/routes/api/v2/project/","title":"project","text":""},{"location":"reference/fractal_server/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.create_project","title":"<code>create_project(project, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create new project</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.post(\"/project/\", response_model=ProjectReadV2, status_code=201)\nasync def create_project(\n    project: ProjectCreateV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProjectReadV2 | None:\n    \"\"\"\n    Create new project\n    \"\"\"\n\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(\n        user=user,\n        db=db,\n    )\n    resource_id = resource.id\n\n    # Check that there is no project with the same user and name\n    await _check_project_exists(\n        project_name=project.name, user_id=user.id, db=db\n    )\n\n    db_project = ProjectV2(**project.model_dump(), resource_id=resource_id)\n    db_project.user_list.append(user)\n\n    db.add(db_project)\n    await db.commit()\n    await db.refresh(db_project)\n    await db.close()\n\n    return db_project\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.delete_project","title":"<code>delete_project(project_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete project</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.delete(\"/project/{project_id}/\", status_code=204)\nasync def delete_project(\n    project_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete project\n    \"\"\"\n\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    logger = set_logger(__name__)\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current project.\n    stm = _get_submitted_jobs_statement().where(JobV2.project_id == project_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot delete project {project.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    logger.info(f\"Adding Project[{project.id}] to deletion.\")\n    await db.delete(project)\n\n    logger.info(\"Committing changes to db...\")\n    await db.commit()\n\n    logger.info(\"Everything  has been deleted correctly.\")\n    reset_logger_handlers(logger)\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.get_list_project","title":"<code>get_list_project(user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return list of projects user is member of</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.get(\"/project/\", response_model=list[ProjectReadV2])\nasync def get_list_project(\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ProjectV2]:\n    \"\"\"\n    Return list of projects user is member of\n    \"\"\"\n    stm = (\n        select(ProjectV2)\n        .join(LinkUserProjectV2)\n        .where(LinkUserProjectV2.user_id == user.id)\n    )\n    res = await db.execute(stm)\n    project_list = res.scalars().all()\n    await db.close()\n    return project_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.read_project","title":"<code>read_project(project_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return info on an existing project</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.get(\"/project/{project_id}/\", response_model=ProjectReadV2)\nasync def read_project(\n    project_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProjectReadV2 | None:\n    \"\"\"\n    Return info on an existing project\n    \"\"\"\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    await db.close()\n    return project\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/status_legacy/","title":"status_legacy","text":""},{"location":"reference/fractal_server/app/routes/api/v2/status_legacy/#fractal_server.app.routes.api.v2.status_legacy.get_workflowtask_status","title":"<code>get_workflowtask_status(project_id, dataset_id, workflow_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Extract the status of all <code>WorkflowTaskV2</code> of a given <code>WorkflowV2</code> that ran on a given <code>DatasetV2</code>.</p> <p>NOTE: the current endpoint is not guaranteed to provide consistent results if the workflow task list is modified in a non-trivial way (that is, by adding intermediate tasks, removing tasks, or changing their order). See fractal-server GitHub issues: 793, 1083.</p> Source code in <code>fractal_server/app/routes/api/v2/status_legacy.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/status-legacy/\",\n    response_model=LegacyStatusReadV2,\n)\nasync def get_workflowtask_status(\n    project_id: int,\n    dataset_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; LegacyStatusReadV2 | None:\n    \"\"\"\n    Extract the status of all `WorkflowTaskV2` of a given `WorkflowV2` that ran\n    on a given `DatasetV2`.\n\n    *NOTE*: the current endpoint is not guaranteed to provide consistent\n    results if the workflow task list is modified in a non-trivial way\n    (that is, by adding intermediate tasks, removing tasks, or changing their\n    order). See fractal-server GitHub issues: 793, 1083.\n    \"\"\"\n    # Get the dataset DB entry\n    output = await _get_dataset_check_owner(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n\n    # Get the workflow DB entry\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Check whether there exists a submitted job associated to this\n    # workflow/dataset pair. If it does exist, it will be used later.\n    # If there are multiple jobs, raise an error.\n    stm = _get_submitted_jobs_statement()\n    stm = stm.where(JobV2.dataset_id == dataset_id)\n    stm = stm.where(JobV2.workflow_id == workflow_id)\n    res = await db.execute(stm)\n    running_jobs = res.scalars().all()\n    if len(running_jobs) == 0:\n        running_job = None\n    elif len(running_jobs) == 1:\n        running_job = running_jobs[0]\n    else:\n        string_ids = str([job.id for job in running_jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot get WorkflowTaskV2 statuses as DatasetV2 {dataset.id}\"\n                f\" is linked to multiple active jobs: {string_ids}.\"\n            ),\n        )\n\n    # Initialize empty dictionary for WorkflowTaskV2 status\n    workflow_tasks_status_dict: dict = {}\n\n    # Lowest priority: read status from DB, which corresponds to jobs that are\n    # not running\n    history = dataset.history\n    for history_item in history:\n        wftask_id = history_item[\"workflowtask\"][\"id\"]\n        wftask_status = history_item[\"status\"]\n        workflow_tasks_status_dict[wftask_id] = wftask_status\n\n    if running_job is None:\n        # If no job is running, the chronological-last history item is also the\n        # positional-last workflow task to be included in the response.\n        if len(history) &gt; 0:\n            last_valid_wftask_id = history[-1][\"workflowtask\"][\"id\"]\n        else:\n            last_valid_wftask_id = None\n    else:\n        # If a job is running, then gather more up-to-date information\n\n        # Mid priority: Set all WorkflowTask's that are part of the running job\n        # as \"submitted\"\n        start = running_job.first_task_index\n        end = running_job.last_task_index + 1\n\n        running_job_wftasks = workflow.task_list[start:end]\n        running_job_statuses = [\n            workflow_tasks_status_dict.get(wft.id, None)\n            for wft in running_job_wftasks\n        ]\n        try:\n            first_submitted_index = running_job_statuses.index(\n                WorkflowTaskStatusTypeV2.SUBMITTED\n            )\n        except ValueError:\n            logger.warning(\n                f\"Job {running_job.id} is submitted but its task list does not\"\n                f\" contain a {WorkflowTaskStatusTypeV2.SUBMITTED} task.\"\n            )\n            first_submitted_index = 0\n\n        for wftask in running_job_wftasks[first_submitted_index:]:\n            workflow_tasks_status_dict[\n                wftask.id\n            ] = WorkflowTaskStatusTypeV2.SUBMITTED\n\n        # The last workflow task that is included in the submitted job is also\n        # the positional-last workflow task to be included in the response.\n        try:\n            last_valid_wftask_id = workflow.task_list[end - 1].id\n        except IndexError as e:\n            logger.warning(\n                f\"Handled IndexError in `get_workflowtask_status` ({str(e)}).\"\n            )\n            logger.warning(\n                \"Additional information: \"\n                f\"{running_job.first_task_index=}; \"\n                f\"{running_job.last_task_index=}; \"\n                f\"{len(workflow.task_list)=}; \"\n                f\"{dataset_id=}; \"\n                f\"{workflow_id=}.\"\n            )\n            last_valid_wftask_id = None\n            logger.warning(f\"Now setting {last_valid_wftask_id=}.\")\n\n    # Based on previously-gathered information, clean up the response body\n    clean_workflow_tasks_status_dict = {}\n    for wf_task in workflow.task_list:\n        wf_task_status = workflow_tasks_status_dict.get(wf_task.id)\n        if wf_task_status is None:\n            # If a wftask ID was not found, ignore it and continue\n            continue\n        clean_workflow_tasks_status_dict[str(wf_task.id)] = wf_task_status\n        if wf_task_status == WorkflowTaskStatusTypeV2.FAILED:\n            # Starting from the beginning of `workflow.task_list`, stop the\n            # first time that you hit a failed job\n            break\n        if wf_task.id == last_valid_wftask_id:\n            # Starting from the beginning of `workflow.task_list`, stop the\n            # first time that you hit `last_valid_wftask_id``\n            break\n\n    response_body = LegacyStatusReadV2(status=clean_workflow_tasks_status_dict)\n    return response_body\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/submit/","title":"submit","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task/","title":"task","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.create_task","title":"<code>create_task(task, user_group_id=None, private=False, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create a new task</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.post(\n    \"/\", response_model=TaskReadV2, status_code=status.HTTP_201_CREATED\n)\nasync def create_task(\n    task: TaskCreateV2,\n    user_group_id: int | None = None,\n    private: bool = False,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskReadV2 | None:\n    \"\"\"\n    Create a new task\n    \"\"\"\n\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(\n        user=user,\n        db=db,\n    )\n    resource_id = resource.id\n\n    # Validate query parameters related to user-group ownership\n    user_group_id = await _get_valid_user_group_id(\n        user_group_id=user_group_id,\n        private=private,\n        user_id=user.id,\n        db=db,\n    )\n\n    if task.type == TaskType.PARALLEL and (\n        task.args_schema_non_parallel is not None\n        or task.meta_non_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot set `TaskV2.args_schema_non_parallel` or \"\n                \"`TaskV2.args_schema_non_parallel` if TaskV2 is parallel\"\n            ),\n        )\n    elif task.type == TaskType.NON_PARALLEL and (\n        task.args_schema_parallel is not None or task.meta_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot set `TaskV2.args_schema_parallel` or \"\n                \"`TaskV2.args_schema_parallel` if TaskV2 is non_parallel\"\n            ),\n        )\n\n    # Add task\n\n    db_task = TaskV2(**task.model_dump(exclude_unset=True))\n    pkg_name = db_task.name\n    await _verify_non_duplication_user_constraint(\n        db=db,\n        pkg_name=pkg_name,\n        user_id=user.id,\n        version=db_task.version,\n        user_resource_id=resource_id,\n    )\n    await _verify_non_duplication_group_constraint(\n        db=db,\n        pkg_name=pkg_name,\n        user_group_id=user_group_id,\n        version=db_task.version,\n    )\n    db_task_group = TaskGroupV2(\n        user_id=user.id,\n        user_group_id=user_group_id,\n        resource_id=resource_id,\n        active=True,\n        task_list=[db_task],\n        origin=TaskGroupV2OriginEnum.OTHER,\n        version=db_task.version,\n        pkg_name=pkg_name,\n    )\n    db.add(db_task_group)\n    await db.commit()\n    await db.refresh(db_task)\n    await db.close()\n\n    return db_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.delete_task","title":"<code>delete_task(task_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a task</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.delete(\"/{task_id}/\", status_code=204)\nasync def delete_task(\n    task_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a task\n    \"\"\"\n    raise HTTPException(\n        status_code=status.HTTP_405_METHOD_NOT_ALLOWED,\n        detail=(\n            \"Cannot delete single tasks, \"\n            \"please operate directly on task groups.\"\n        ),\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.get_list_task","title":"<code>get_list_task(args_schema=True, category=None, modality=None, author=None, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get list of available tasks</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.get(\"/\", response_model=list[TaskReadV2])\nasync def get_list_task(\n    args_schema: bool = True,\n    category: str | None = None,\n    modality: str | None = None,\n    author: str | None = None,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[TaskReadV2]:\n    \"\"\"\n    Get list of available tasks\n    \"\"\"\n\n    user_resource_id = await _get_user_resource_id(user_id=user.id, db=db)\n\n    stm = (\n        select(TaskV2)\n        .join(TaskGroupV2)\n        .where(TaskGroupV2.id == TaskV2.taskgroupv2_id)\n        .where(TaskGroupV2.resource_id == user_resource_id)\n        .where(\n            or_(\n                TaskGroupV2.user_id == user.id,\n                TaskGroupV2.user_group_id.in_(\n                    select(LinkUserGroup.group_id).where(\n                        LinkUserGroup.user_id == user.id\n                    )\n                ),\n            )\n        )\n    )\n    if category is not None:\n        stm = stm.where(func.lower(TaskV2.category) == category.lower())\n    if modality is not None:\n        stm = stm.where(func.lower(TaskV2.modality) == modality.lower())\n    if author is not None:\n        stm = stm.where(TaskV2.authors.icontains(author))\n\n    stm = stm.order_by(TaskV2.id)\n    res = await db.execute(stm)\n    task_list = list(res.scalars().all())\n    await db.close()\n    if args_schema is False:\n        for task in task_list:\n            setattr(task, \"args_schema_parallel\", None)\n            setattr(task, \"args_schema_non_parallel\", None)\n\n    return task_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.get_task","title":"<code>get_task(task_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on a specific task</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.get(\"/{task_id}/\", response_model=TaskReadV2)\nasync def get_task(\n    task_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskReadV2:\n    \"\"\"\n    Get info on a specific task\n    \"\"\"\n    task = await _get_task_read_access(task_id=task_id, user_id=user.id, db=db)\n    return task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.patch_task","title":"<code>patch_task(task_id, task_update, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a specific task (restricted to task owner)</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.patch(\"/{task_id}/\", response_model=TaskReadV2)\nasync def patch_task(\n    task_id: int,\n    task_update: TaskUpdateV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskReadV2 | None:\n    \"\"\"\n    Edit a specific task (restricted to task owner)\n    \"\"\"\n\n    # Retrieve task from database\n    db_task = await _get_task_full_access(\n        task_id=task_id, user_id=user.id, db=db\n    )\n    update = task_update.model_dump(exclude_unset=True)\n\n    # Forbid changes that set a previously unset command\n    if db_task.type == TaskType.NON_PARALLEL and \"command_parallel\" in update:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Cannot set an unset `command_parallel`.\",\n        )\n    if db_task.type == TaskType.PARALLEL and \"command_non_parallel\" in update:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Cannot set an unset `command_non_parallel`.\",\n        )\n\n    for key, value in update.items():\n        setattr(db_task, key, value)\n\n    await db.commit()\n    await db.refresh(db_task)\n    await db.close()\n    return db_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_collection/","title":"task_collection","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task_collection/#fractal_server.app.routes.api.v2.task_collection.CollectionRequestData","title":"<code>CollectionRequestData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Validate form data and wheel file.</p> Source code in <code>fractal_server/app/routes/api/v2/task_collection.py</code> <pre><code>class CollectionRequestData(BaseModel):\n    \"\"\"\n    Validate form data _and_ wheel file.\n    \"\"\"\n\n    task_collect: TaskCollectPipV2\n    file: UploadFile | None = None\n    origin: TaskGroupV2OriginEnum\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_data(cls, values):\n        file = values.get(\"file\")\n        package = values.get(\"task_collect\").package\n        package_version = values.get(\"task_collect\").package_version\n\n        if file is None:\n            if package is None:\n                raise ValueError(\n                    \"When no `file` is provided, `package` is required.\"\n                )\n            values[\"origin\"] = TaskGroupV2OriginEnum.PYPI\n        else:\n            if package is not None:\n                raise ValueError(\n                    \"Cannot set `package` when `file` is provided \"\n                    f\"(given package='{package}').\"\n                )\n            if package_version is not None:\n                raise ValueError(\n                    \"Cannot set `package_version` when `file` is \"\n                    f\"provided (given package_version='{package_version}').\"\n                )\n            values[\"origin\"] = TaskGroupV2OriginEnum.WHEELFILE\n\n            for forbidden_char in FORBIDDEN_CHAR_WHEEL:\n                if forbidden_char in file.filename:\n                    raise ValueError(\n                        \"Wheel filename has forbidden characters, \"\n                        f\"{FORBIDDEN_CHAR_WHEEL}\"\n                    )\n\n        return values\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_collection/#fractal_server.app.routes.api.v2.task_collection.collect_tasks_pip","title":"<code>collect_tasks_pip(response, background_tasks, request_data=Depends(parse_request_data), private=False, user_group_id=None, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Task-collection endpoint</p> Source code in <code>fractal_server/app/routes/api/v2/task_collection.py</code> <pre><code>@router.post(\n    \"/collect/pip/\",\n    response_model=TaskGroupActivityV2Read,\n)\nasync def collect_tasks_pip(\n    response: Response,\n    background_tasks: BackgroundTasks,\n    request_data: CollectionRequestData = Depends(parse_request_data),\n    private: bool = False,\n    user_group_id: int | None = None,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupActivityV2Read:\n    \"\"\"\n    Task-collection endpoint\n    \"\"\"\n\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(\n        user=user,\n        db=db,\n    )\n    resource_id = resource.id\n\n    # Get some validated request data\n    task_collect = request_data.task_collect\n\n    # Initialize task-group attributes\n    task_group_attrs = dict(\n        user_id=user.id,\n        resource_id=resource_id,\n        origin=request_data.origin,\n    )\n\n    # Set/check python version\n    if task_collect.python_version is None:\n        task_group_attrs[\"python_version\"] = resource.tasks_python_config[\n            \"default_version\"\n        ]\n    else:\n        task_group_attrs[\"python_version\"] = task_collect.python_version\n    try:\n        get_python_interpreter(\n            python_version=task_group_attrs[\"python_version\"],\n            resource=resource,\n        )\n    except ValueError:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Python version {task_group_attrs['python_version']} is \"\n                \"not available for Fractal task collection.\"\n            ),\n        )\n\n    # Set pip_extras\n    if task_collect.package_extras is not None:\n        task_group_attrs[\"pip_extras\"] = task_collect.package_extras\n\n    # Set pinned_package_versions\n    if task_collect.pinned_package_versions_pre is not None:\n        task_group_attrs[\n            \"pinned_package_versions_pre\"\n        ] = task_collect.pinned_package_versions_pre\n    if task_collect.pinned_package_versions_post is not None:\n        task_group_attrs[\n            \"pinned_package_versions_post\"\n        ] = task_collect.pinned_package_versions_post\n\n    # Initialize wheel_file_content as None\n    wheel_file = None\n\n    # Set pkg_name, version, origin and archive_path\n    if request_data.origin == TaskGroupV2OriginEnum.WHEELFILE:\n        try:\n            wheel_filename = request_data.file.filename\n            wheel_info = _parse_wheel_filename(wheel_filename)\n            wheel_file_content = await request_data.file.read()\n            wheel_file = FractalUploadedFile(\n                filename=wheel_filename,\n                contents=wheel_file_content,\n            )\n        except ValueError as e:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=(\n                    f\"Invalid wheel-file name {wheel_filename}. \"\n                    f\"Original error: {str(e)}\",\n                ),\n            )\n        task_group_attrs[\"pkg_name\"] = normalize_package_name(\n            wheel_info[\"distribution\"]\n        )\n        task_group_attrs[\"version\"] = wheel_info[\"version\"]\n    elif request_data.origin == TaskGroupV2OriginEnum.PYPI:\n        pkg_name = task_collect.package\n        task_group_attrs[\"pkg_name\"] = normalize_package_name(pkg_name)\n        latest_version = await get_package_version_from_pypi(\n            task_collect.package,\n            task_collect.package_version,\n        )\n        task_group_attrs[\"version\"] = latest_version\n\n    # Validate query parameters related to user-group ownership\n    user_group_id = await _get_valid_user_group_id(\n        user_group_id=user_group_id,\n        private=private,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Set user_group_id\n    task_group_attrs[\"user_group_id\"] = user_group_id\n\n    # Set path and venv_path\n    if resource.type == ResourceType.SLURM_SSH:\n        base_tasks_path = profile.tasks_remote_dir\n    else:\n        base_tasks_path = resource.tasks_local_dir\n    task_group_path = (\n        Path(base_tasks_path)\n        / str(user.id)\n        / task_group_attrs[\"pkg_name\"]\n        / task_group_attrs[\"version\"]\n    ).as_posix()\n    task_group_attrs[\"path\"] = task_group_path\n    task_group_attrs[\"venv_path\"] = Path(task_group_path, \"venv\").as_posix()\n\n    # Validate TaskGroupV2 attributes\n    try:\n        TaskGroupCreateV2Strict(**task_group_attrs)\n    except ValidationError as e:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Invalid task-group object. Original error: {e}\",\n        )\n\n    # Database checks\n\n    # Verify non-duplication constraints\n    await _verify_non_duplication_user_constraint(\n        user_id=user.id,\n        pkg_name=task_group_attrs[\"pkg_name\"],\n        version=task_group_attrs[\"version\"],\n        user_resource_id=resource_id,\n        db=db,\n    )\n    await _verify_non_duplication_group_constraint(\n        user_group_id=task_group_attrs[\"user_group_id\"],\n        pkg_name=task_group_attrs[\"pkg_name\"],\n        version=task_group_attrs[\"version\"],\n        db=db,\n    )\n    await _verify_non_duplication_group_path(\n        path=task_group_attrs[\"path\"],\n        resource_id=resource_id,\n        db=db,\n    )\n\n    # On-disk checks\n\n    if resource.type != ResourceType.SLURM_SSH:\n        # Verify that folder does not exist (for local collection)\n        if Path(task_group_path).exists():\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=f\"{task_group_path} already exists.\",\n            )\n\n    # Create TaskGroupV2 object\n    task_group = TaskGroupV2(**task_group_attrs)\n    db.add(task_group)\n    await db.commit()\n    await db.refresh(task_group)\n    db.expunge(task_group)\n\n    # All checks are OK, proceed with task collection\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatusV2.PENDING,\n        action=TaskGroupActivityActionV2.COLLECT,\n        pkg_name=task_group.pkg_name,\n        version=task_group.version,\n    )\n    db.add(task_group_activity)\n    await db.commit()\n    await db.refresh(task_group_activity)\n    logger = set_logger(logger_name=\"collect_tasks_pip\")\n\n    # END of SSH/non-SSH common part\n\n    if resource.type == ResourceType.SLURM_SSH:\n        collect_function = collect_ssh\n    else:\n        collect_function = collect_local\n\n    background_tasks.add_task(\n        collect_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        wheel_file=wheel_file,\n        resource=resource,\n        profile=profile,\n    )\n\n    logger.debug(\n        \"Task-collection endpoint: start background collection \"\n        \"and return task_group_activity\"\n    )\n    reset_logger_handlers(logger)\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_collection/#fractal_server.app.routes.api.v2.task_collection.parse_request_data","title":"<code>parse_request_data(package=Form(None), package_version=Form(None), package_extras=Form(None), python_version=Form(None), pinned_package_versions_pre=Form(None), pinned_package_versions_post=Form(None), file=File(None))</code>","text":"<p>Expand the parsing/validation of <code>parse_form_data</code>, based on <code>file</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/task_collection.py</code> <pre><code>def parse_request_data(\n    package: str | None = Form(None),\n    package_version: str | None = Form(None),\n    package_extras: str | None = Form(None),\n    python_version: str | None = Form(None),\n    pinned_package_versions_pre: str | None = Form(None),\n    pinned_package_versions_post: str | None = Form(None),\n    file: UploadFile | None = File(None),\n) -&gt; CollectionRequestData:\n    \"\"\"\n    Expand the parsing/validation of `parse_form_data`, based on `file`.\n    \"\"\"\n\n    try:\n        # Convert dict_pinned_pkg from a JSON string into a Python dictionary\n        dict_pinned_pkg_pre = (\n            json.loads(pinned_package_versions_pre)\n            if pinned_package_versions_pre\n            else None\n        )\n        dict_pinned_pkg_post = (\n            json.loads(pinned_package_versions_post)\n            if pinned_package_versions_post\n            else None\n        )\n        # Validate and coerce form data\n        task_collect_pip = TaskCollectPipV2(\n            package=package,\n            package_version=package_version,\n            package_extras=package_extras,\n            python_version=python_version,\n            pinned_package_versions_pre=dict_pinned_pkg_pre,\n            pinned_package_versions_post=dict_pinned_pkg_post,\n        )\n\n        data = CollectionRequestData(\n            task_collect=task_collect_pip,\n            file=file,\n        )\n\n    except (ValidationError, json.JSONDecodeError) as e:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Invalid request-body\\n{str(e)}\",\n        )\n\n    return data\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_collection_custom/","title":"task_collection_custom","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task_collection_pixi/","title":"task_collection_pixi","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task_group/","title":"task_group","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task_group/#fractal_server.app.routes.api.v2.task_group._version_sort_key","title":"<code>_version_sort_key(task_group)</code>","text":"<p>Returns a tuple used as (reverse) ordering key for TaskGroups in <code>get_task_group_list</code>. The TaskGroups with a parsable versions are the first in order, sorted according to the sorting rules of packaging.version.Version. Next in order we have the TaskGroups with non-null non-parsable versions, sorted alphabetically. Last we have the TaskGroups with null version.</p> Source code in <code>fractal_server/app/routes/api/v2/task_group.py</code> <pre><code>def _version_sort_key(\n    task_group: TaskGroupV2,\n) -&gt; tuple[int, Version | str | None]:\n    \"\"\"\n    Returns a tuple used as (reverse) ordering key for TaskGroups in\n    `get_task_group_list`.\n    The TaskGroups with a parsable versions are the first in order,\n    sorted according to the sorting rules of packaging.version.Version.\n    Next in order we have the TaskGroups with non-null non-parsable versions,\n    sorted alphabetically.\n    Last we have the TaskGroups with null version.\n    \"\"\"\n    if task_group.version is None:\n        return (0, task_group.version)\n    try:\n        return (2, parse(task_group.version))\n    except InvalidVersion:\n        return (1, task_group.version)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_group/#fractal_server.app.routes.api.v2.task_group.get_task_group","title":"<code>get_task_group(task_group_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get single TaskGroup</p> Source code in <code>fractal_server/app/routes/api/v2/task_group.py</code> <pre><code>@router.get(\"/{task_group_id}/\", response_model=TaskGroupReadV2)\nasync def get_task_group(\n    task_group_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupReadV2:\n    \"\"\"\n    Get single TaskGroup\n    \"\"\"\n    task_group = await _get_task_group_read_access(\n        task_group_id=task_group_id,\n        user_id=user.id,\n        db=db,\n    )\n    return task_group\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_group/#fractal_server.app.routes.api.v2.task_group.get_task_group_list","title":"<code>get_task_group_list(user=Depends(current_user_act_ver_prof), db=Depends(get_async_db), only_active=False, only_owner=False, args_schema=True)</code>  <code>async</code>","text":"<p>Get all accessible TaskGroups</p> Source code in <code>fractal_server/app/routes/api/v2/task_group.py</code> <pre><code>@router.get(\"/\", response_model=list[tuple[str, list[TaskGroupReadV2]]])\nasync def get_task_group_list(\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n    only_active: bool = False,\n    only_owner: bool = False,\n    args_schema: bool = True,\n) -&gt; list[tuple[str, list[TaskGroupReadV2]]]:\n    \"\"\"\n    Get all accessible TaskGroups\n    \"\"\"\n    if only_owner:\n        condition = TaskGroupV2.user_id == user.id\n    else:\n        condition = or_(\n            TaskGroupV2.user_id == user.id,\n            TaskGroupV2.user_group_id.in_(\n                select(LinkUserGroup.group_id).where(\n                    LinkUserGroup.user_id == user.id\n                )\n            ),\n        )\n\n    user_resource_id = await _get_user_resource_id(user_id=user.id, db=db)\n    stm = (\n        select(TaskGroupV2)\n        .where(TaskGroupV2.resource_id == user_resource_id)\n        .where(condition)\n        .order_by(TaskGroupV2.pkg_name)\n    )\n    if only_active:\n        stm = stm.where(TaskGroupV2.active)\n\n    res = await db.execute(stm)\n    task_groups = res.scalars().all()\n\n    if args_schema is False:\n        for taskgroup in task_groups:\n            for task in taskgroup.task_list:\n                setattr(task, \"args_schema_non_parallel\", None)\n                setattr(task, \"args_schema_parallel\", None)\n\n    default_group_id = await _get_default_usergroup_id_or_none(db)\n    grouped_result = [\n        (\n            pkg_name,\n            (\n                await remove_duplicate_task_groups(\n                    task_groups=sorted(\n                        list(groups),\n                        key=_version_sort_key,\n                        reverse=True,\n                    ),\n                    user_id=user.id,\n                    default_group_id=default_group_id,\n                    db=db,\n                )\n            ),\n        )\n        for pkg_name, groups in itertools.groupby(\n            task_groups, key=lambda tg: tg.pkg_name\n        )\n    ]\n    return grouped_result\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_group/#fractal_server.app.routes.api.v2.task_group.patch_task_group","title":"<code>patch_task_group(task_group_id, task_group_update, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Patch single TaskGroup</p> Source code in <code>fractal_server/app/routes/api/v2/task_group.py</code> <pre><code>@router.patch(\"/{task_group_id}/\", response_model=TaskGroupReadV2)\nasync def patch_task_group(\n    task_group_id: int,\n    task_group_update: TaskGroupUpdateV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupReadV2:\n    \"\"\"\n    Patch single TaskGroup\n    \"\"\"\n    task_group = await _get_task_group_full_access(\n        task_group_id=task_group_id,\n        user_id=user.id,\n        db=db,\n    )\n    if (\n        \"user_group_id\" in task_group_update.model_dump(exclude_unset=True)\n        and task_group_update.user_group_id != task_group.user_group_id\n    ):\n        await _verify_non_duplication_group_constraint(\n            db=db,\n            pkg_name=task_group.pkg_name,\n            version=task_group.version,\n            user_group_id=task_group_update.user_group_id,\n        )\n    for key, value in task_group_update.model_dump(exclude_unset=True).items():\n        if (key == \"user_group_id\") and (value is not None):\n            await _verify_user_belongs_to_group(\n                user_id=user.id, user_group_id=value, db=db\n            )\n        setattr(task_group, key, value)\n\n    db.add(task_group)\n    await db.commit()\n    await db.refresh(task_group)\n    return task_group\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_group_lifecycle/","title":"task_group_lifecycle","text":""},{"location":"reference/fractal_server/app/routes/api/v2/task_group_lifecycle/#fractal_server.app.routes.api.v2.task_group_lifecycle.deactivate_task_group","title":"<code>deactivate_task_group(task_group_id, background_tasks, response, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Deactivate task-group venv</p> Source code in <code>fractal_server/app/routes/api/v2/task_group_lifecycle.py</code> <pre><code>@router.post(\n    \"/{task_group_id}/deactivate/\",\n    response_model=TaskGroupActivityV2Read,\n)\nasync def deactivate_task_group(\n    task_group_id: int,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupActivityV2Read:\n    \"\"\"\n    Deactivate task-group venv\n    \"\"\"\n\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(user=user, db=db)\n\n    # Check access\n    task_group = await _get_task_group_full_access(\n        task_group_id=task_group_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Check no other activity is ongoing\n    await check_no_ongoing_activity(task_group_id=task_group_id, db=db)\n\n    # Check no submitted jobs use tasks from this task group\n    await check_no_submitted_job(task_group_id=task_group.id, db=db)\n\n    # Check that task-group is active\n    if not task_group.active:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot deactivate a task group with {task_group.active=}.\"\n            ),\n        )\n\n    # Shortcut for task-group with origin=\"other\"\n    if task_group.origin == TaskGroupV2OriginEnum.OTHER:\n        task_group.active = False\n        task_group_activity = TaskGroupActivityV2(\n            user_id=task_group.user_id,\n            taskgroupv2_id=task_group.id,\n            status=TaskGroupActivityStatusV2.OK,\n            action=TaskGroupActivityActionV2.DEACTIVATE,\n            pkg_name=task_group.pkg_name,\n            version=(task_group.version or \"N/A\"),\n            log=(\n                f\"Task group has {task_group.origin=}, set \"\n                \"task_group.active to False and exit.\"\n            ),\n            timestamp_started=get_timestamp(),\n            timestamp_ended=get_timestamp(),\n        )\n        db.add(task_group)\n        db.add(task_group_activity)\n        await db.commit()\n        response.status_code = status.HTTP_202_ACCEPTED\n        return task_group_activity\n\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatusV2.PENDING,\n        action=TaskGroupActivityActionV2.DEACTIVATE,\n        pkg_name=task_group.pkg_name,\n        version=task_group.version,\n        timestamp_started=get_timestamp(),\n    )\n    task_group.active = False\n    db.add(task_group)\n    db.add(task_group_activity)\n    await db.commit()\n\n    # Submit background task\n    if resource.type == ResourceType.SLURM_SSH:\n        if task_group.origin == TaskGroupV2OriginEnum.PIXI:\n            deactivate_function = deactivate_ssh_pixi\n        else:\n            deactivate_function = deactivate_ssh\n    else:\n        if task_group.origin == TaskGroupV2OriginEnum.PIXI:\n            deactivate_function = deactivate_local_pixi\n        else:\n            deactivate_function = deactivate_local\n    background_tasks.add_task(\n        deactivate_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        resource=resource,\n        profile=profile,\n    )\n\n    logger.debug(\n        \"Task group deactivation endpoint: start deactivate \"\n        \"and return task_group_activity\"\n    )\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_group_lifecycle/#fractal_server.app.routes.api.v2.task_group_lifecycle.delete_task_group","title":"<code>delete_task_group(task_group_id, background_tasks, response, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Deletion of task-group from db and file system</p> Source code in <code>fractal_server/app/routes/api/v2/task_group_lifecycle.py</code> <pre><code>@router.post(\n    \"/{task_group_id}/delete/\",\n    status_code=202,\n)\nasync def delete_task_group(\n    task_group_id: int,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupActivityV2Read:\n    \"\"\"\n    Deletion of task-group from db and file system\n    \"\"\"\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(user=user, db=db)\n\n    task_group = await _get_task_group_full_access(\n        task_group_id=task_group_id,\n        user_id=user.id,\n        db=db,\n    )\n    await check_no_ongoing_activity(task_group_id=task_group_id, db=db)\n    await check_no_related_workflowtask(task_group=task_group, db=db)\n\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatusV2.PENDING,\n        action=TaskGroupActivityActionV2.DELETE,\n        pkg_name=task_group.pkg_name,\n        version=(task_group.version or \"N/A\"),\n        timestamp_started=get_timestamp(),\n    )\n    db.add(task_group_activity)\n    await db.commit()\n\n    if resource.type == ResourceType.SLURM_SSH:\n        delete_function = delete_ssh\n    else:\n        delete_function = delete_local\n\n    background_tasks.add_task(\n        delete_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        resource=resource,\n        profile=profile,\n    )\n\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_group_lifecycle/#fractal_server.app.routes.api.v2.task_group_lifecycle.reactivate_task_group","title":"<code>reactivate_task_group(task_group_id, background_tasks, response, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Deactivate task-group venv</p> Source code in <code>fractal_server/app/routes/api/v2/task_group_lifecycle.py</code> <pre><code>@router.post(\n    \"/{task_group_id}/reactivate/\",\n    response_model=TaskGroupActivityV2Read,\n)\nasync def reactivate_task_group(\n    task_group_id: int,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupReadV2:\n    \"\"\"\n    Deactivate task-group venv\n    \"\"\"\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(user=user, db=db)\n\n    # Check access\n    task_group = await _get_task_group_full_access(\n        task_group_id=task_group_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Check that task-group is not active\n    if task_group.active:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot reactivate a task group with {task_group.active=}.\"\n            ),\n        )\n\n    # Check no other activity is ongoing\n    await check_no_ongoing_activity(task_group_id=task_group_id, db=db)\n\n    # Check no submitted jobs use tasks from this task group\n    await check_no_submitted_job(task_group_id=task_group.id, db=db)\n\n    # Shortcut for task-group with origin=\"other\"\n    if task_group.origin == TaskGroupV2OriginEnum.OTHER:\n        task_group.active = True\n        task_group_activity = TaskGroupActivityV2(\n            user_id=task_group.user_id,\n            taskgroupv2_id=task_group.id,\n            status=TaskGroupActivityStatusV2.OK,\n            action=TaskGroupActivityActionV2.REACTIVATE,\n            pkg_name=task_group.pkg_name,\n            version=(task_group.version or \"N/A\"),\n            log=(\n                f\"Task group has {task_group.origin=}, set \"\n                \"task_group.active to True and exit.\"\n            ),\n            timestamp_started=get_timestamp(),\n            timestamp_ended=get_timestamp(),\n        )\n        db.add(task_group)\n        db.add(task_group_activity)\n        await db.commit()\n        response.status_code = status.HTTP_202_ACCEPTED\n        return task_group_activity\n\n    if task_group.env_info is None:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot reactivate a task group with \"\n                f\"{task_group.env_info=}.\"\n            ),\n        )\n\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatusV2.PENDING,\n        action=TaskGroupActivityActionV2.REACTIVATE,\n        pkg_name=task_group.pkg_name,\n        version=task_group.version,\n        timestamp_started=get_timestamp(),\n    )\n    db.add(task_group_activity)\n    await db.commit()\n\n    # Submit background task\n    if resource.type == ResourceType.SLURM_SSH:\n        if task_group.origin == TaskGroupV2OriginEnum.PIXI:\n            reactivate_function = reactivate_ssh_pixi\n        else:\n            reactivate_function = reactivate_ssh\n    else:\n        if task_group.origin == TaskGroupV2OriginEnum.PIXI:\n            reactivate_function = reactivate_local_pixi\n        else:\n            reactivate_function = reactivate_local\n    background_tasks.add_task(\n        reactivate_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        resource=resource,\n        profile=profile,\n    )\n    logger.debug(\n        \"Task group reactivation endpoint: start reactivate \"\n        \"and return task_group_activity\"\n    )\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/task_version_update/","title":"task_version_update","text":""},{"location":"reference/fractal_server/app/routes/api/v2/workflow/","title":"workflow","text":""},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.create_workflow","title":"<code>create_workflow(project_id, workflow, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create a workflow, associate to a project</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/\",\n    response_model=WorkflowReadV2,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_workflow(\n    project_id: int,\n    workflow: WorkflowCreateV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowReadV2 | None:\n    \"\"\"\n    Create a workflow, associate to a project\n    \"\"\"\n    await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    await _check_workflow_exists(\n        name=workflow.name, project_id=project_id, db=db\n    )\n\n    db_workflow = WorkflowV2(project_id=project_id, **workflow.model_dump())\n    db.add(db_workflow)\n    await db.commit()\n    await db.refresh(db_workflow)\n    await db.close()\n    return db_workflow\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.delete_workflow","title":"<code>delete_workflow(project_id, workflow_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    status_code=status.HTTP_204_NO_CONTENT,\n)\nasync def delete_workflow(\n    project_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current workflow.\n    stm = _get_submitted_jobs_statement().where(\n        JobV2.workflow_id == workflow.id\n    )\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot delete workflow {workflow.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # Delete workflow\n    await db.delete(workflow)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.export_workflow","title":"<code>export_workflow(project_id, workflow_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Export an existing workflow, after stripping all IDs</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/export/\",\n    response_model=WorkflowExportV2,\n)\nasync def export_workflow(\n    project_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowExportV2 | None:\n    \"\"\"\n    Export an existing workflow, after stripping all IDs\n    \"\"\"\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n    wf_task_list = []\n    for wftask in workflow.task_list:\n        task_group = await db.get(TaskGroupV2, wftask.task.taskgroupv2_id)\n        wf_task_list.append(wftask.model_dump())\n        wf_task_list[-1][\"task\"] = dict(\n            pkg_name=task_group.pkg_name,\n            version=task_group.version,\n            name=wftask.task.name,\n        )\n\n    wf = WorkflowExportV2(\n        **workflow.model_dump(),\n        task_list=wf_task_list,\n    )\n    return wf\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.get_user_workflows","title":"<code>get_user_workflows(user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the workflows of the current user</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\"/workflow/\", response_model=list[WorkflowReadV2])\nasync def get_user_workflows(\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[WorkflowReadV2]:\n    \"\"\"\n    Returns all the workflows of the current user\n    \"\"\"\n    stm = select(WorkflowV2)\n    stm = stm.join(ProjectV2).where(\n        ProjectV2.user_list.any(UserOAuth.id == user.id)\n    )\n    res = await db.execute(stm)\n    workflow_list = res.scalars().all()\n    return workflow_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.get_workflow_list","title":"<code>get_workflow_list(project_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get workflow list for given project</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/\",\n    response_model=list[WorkflowReadV2],\n)\nasync def get_workflow_list(\n    project_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[WorkflowReadV2] | None:\n    \"\"\"\n    Get workflow list for given project\n    \"\"\"\n    # Access control\n    project = await _get_project_check_owner(\n        project_id=project_id, user_id=user.id, db=db\n    )\n    # Find workflows of the current project. Note: this select/where approach\n    # has much better scaling than refreshing all elements of\n    # `project.workflow_list` - ref\n    # https://github.com/fractal-analytics-platform/fractal-server/pull/1082#issuecomment-1856676097.\n    stm = select(WorkflowV2).where(WorkflowV2.project_id == project.id)\n    workflow_list = (await db.execute(stm)).scalars().all()\n    return workflow_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.get_workflow_type_filters","title":"<code>get_workflow_type_filters(project_id, workflow_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on type/type-filters flow for a workflow.</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\"/project/{project_id}/workflow/{workflow_id}/type-filters-flow/\")\nasync def get_workflow_type_filters(\n    project_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[WorkflowTaskTypeFiltersInfo]:\n    \"\"\"\n    Get info on type/type-filters flow for a workflow.\n    \"\"\"\n\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    num_tasks = len(workflow.task_list)\n    if num_tasks == 0:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Workflow has no tasks.\",\n        )\n\n    current_type_filters = {}\n\n    response_items = []\n    for wftask in workflow.task_list:\n        # Compute input_type_filters, based on wftask and task manifest\n        input_type_filters = merge_type_filters(\n            wftask_type_filters=wftask.type_filters,\n            task_input_types=wftask.task.input_types,\n        )\n\n        # Append current item to response list\n        response_items.append(\n            dict(\n                workflowtask_id=wftask.id,\n                current_type_filters=copy(current_type_filters),\n                input_type_filters=copy(input_type_filters),\n                output_type_filters=copy(wftask.task.output_types),\n            )\n        )\n\n        # Update `current_type_filters`\n        current_type_filters.update(wftask.task.output_types)\n\n    return response_items\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.read_workflow","title":"<code>read_workflow(project_id, workflow_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on an existing workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    response_model=WorkflowReadV2WithWarnings,\n)\nasync def read_workflow(\n    project_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowReadV2WithWarnings | None:\n    \"\"\"\n    Get info on an existing workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    wftask_list_with_warnings = await _add_warnings_to_workflow_tasks(\n        wftask_list=workflow.task_list, user_id=user.id, db=db\n    )\n    workflow_data = dict(\n        **workflow.model_dump(),\n        project=workflow.project,\n        task_list=wftask_list_with_warnings,\n    )\n\n    return workflow_data\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.update_workflow","title":"<code>update_workflow(project_id, workflow_id, patch, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    response_model=WorkflowReadV2WithWarnings,\n)\nasync def update_workflow(\n    project_id: int,\n    workflow_id: int,\n    patch: WorkflowUpdateV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowReadV2WithWarnings | None:\n    \"\"\"\n    Edit a workflow\n    \"\"\"\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    if patch.name:\n        await _check_workflow_exists(\n            name=patch.name, project_id=project_id, db=db\n        )\n\n    for key, value in patch.model_dump(exclude_unset=True).items():\n        if key == \"reordered_workflowtask_ids\":\n            if await _workflow_has_submitted_job(\n                workflow_id=workflow_id, db=db\n            ):\n                raise HTTPException(\n                    status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                    detail=(\n                        \"Cannot re-order WorkflowTasks while a Job is running \"\n                        \"for this Workflow.\"\n                    ),\n                )\n\n            current_workflowtask_ids = [\n                wftask.id for wftask in workflow.task_list\n            ]\n            num_tasks = len(workflow.task_list)\n            if len(value) != num_tasks or set(value) != set(\n                current_workflowtask_ids\n            ):\n                raise HTTPException(\n                    status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                    detail=(\n                        \"`reordered_workflowtask_ids` must be a permutation of\"\n                        f\" {current_workflowtask_ids} (given {value})\"\n                    ),\n                )\n            for ind_wftask in range(num_tasks):\n                new_order = value.index(workflow.task_list[ind_wftask].id)\n                workflow.task_list[ind_wftask].order = new_order\n        else:\n            setattr(workflow, key, value)\n\n    await db.commit()\n    await db.refresh(workflow)\n    await db.close()\n\n    wftask_list_with_warnings = await _add_warnings_to_workflow_tasks(\n        wftask_list=workflow.task_list, user_id=user.id, db=db\n    )\n    workflow_data = dict(\n        **workflow.model_dump(),\n        project=workflow.project,\n        task_list=wftask_list_with_warnings,\n    )\n\n    return workflow_data\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow_import/","title":"workflow_import","text":""},{"location":"reference/fractal_server/app/routes/api/v2/workflow_import/#fractal_server.app.routes.api.v2.workflow_import._get_task_by_source","title":"<code>_get_task_by_source(source, task_groups_list)</code>  <code>async</code>","text":"<p>Find task with a given source.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p><code>source</code> of the task to be imported.</p> required <code>task_groups_list</code> <code>list[TaskGroupV2]</code> <p>Current list of valid task groups.</p> required Return <p><code>id</code> of the matching task, or <code>None</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/workflow_import.py</code> <pre><code>async def _get_task_by_source(\n    source: str,\n    task_groups_list: list[TaskGroupV2],\n) -&gt; int | None:\n    \"\"\"\n    Find task with a given source.\n\n    Args:\n        source: `source` of the task to be imported.\n        task_groups_list: Current list of valid task groups.\n\n    Return:\n        `id` of the matching task, or `None`.\n    \"\"\"\n    task_id = next(\n        iter(\n            task.id\n            for task_group in task_groups_list\n            for task in task_group.task_list\n            if task.source == source\n        ),\n        None,\n    )\n    return task_id\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow_import/#fractal_server.app.routes.api.v2.workflow_import._get_task_by_taskimport","title":"<code>_get_task_by_taskimport(*, task_import, task_groups_list, user_id, default_group_id, db)</code>  <code>async</code>","text":"<p>Find a task based on <code>task_import</code>.</p> <p>Parameters:</p> Name Type Description Default <code>task_import</code> <code>TaskImportV2</code> <p>Info on task to be imported.</p> required <code>task_groups_list</code> <code>list[TaskGroupV2]</code> <p>Current list of valid task groups.</p> required <code>user_id</code> <code>int</code> <p>ID of current user.</p> required <code>default_group_id</code> <code>int | None</code> <p>ID of default user group.</p> required <code>db</code> <code>AsyncSession</code> <p>Asynchronous database session.</p> required Return <p><code>id</code> of the matching task, or <code>None</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/workflow_import.py</code> <pre><code>async def _get_task_by_taskimport(\n    *,\n    task_import: TaskImportV2,\n    task_groups_list: list[TaskGroupV2],\n    user_id: int,\n    default_group_id: int | None,\n    db: AsyncSession,\n) -&gt; int | None:\n    \"\"\"\n    Find a task based on `task_import`.\n\n    Args:\n        task_import: Info on task to be imported.\n        task_groups_list: Current list of valid task groups.\n        user_id: ID of current user.\n        default_group_id: ID of default user group.\n        db: Asynchronous database session.\n\n    Return:\n        `id` of the matching task, or `None`.\n    \"\"\"\n\n    logger.info(f\"[_get_task_by_taskimport] START, {task_import=}\")\n\n    # Filter by `pkg_name` and by presence of a task with given `name`.\n    matching_task_groups = [\n        task_group\n        for task_group in task_groups_list\n        if (\n            task_group.pkg_name == task_import.pkg_name\n            and task_import.name\n            in [task.name for task in task_group.task_list]\n        )\n    ]\n    if len(matching_task_groups) &lt; 1:\n        logger.info(\n            \"[_get_task_by_taskimport] \"\n            f\"No task group with {task_import.pkg_name=} \"\n            f\"and a task with {task_import.name=}.\"\n        )\n        return None\n\n    # Determine target `version`\n    # Note that task_import.version cannot be \"\", due to a validator\n    if task_import.version is None:\n        logger.info(\n            \"[_get_task_by_taskimport] \"\n            \"No version requested, looking for latest.\"\n        )\n        latest_task = max(\n            matching_task_groups, key=lambda tg: tg.version or \"\"\n        )\n        version = latest_task.version\n        logger.info(\n            f\"[_get_task_by_taskimport] Latest version set to {version}.\"\n        )\n    else:\n        version = task_import.version\n\n    # Filter task groups by version\n    final_matching_task_groups = list(\n        filter(lambda tg: tg.version == version, matching_task_groups)\n    )\n\n    if len(final_matching_task_groups) &lt; 1:\n        logger.info(\n            \"[_get_task_by_taskimport] \"\n            \"No task group left after filtering by version.\"\n        )\n        return None\n    elif len(final_matching_task_groups) == 1:\n        final_task_group = final_matching_task_groups[0]\n        logger.info(\n            \"[_get_task_by_taskimport] \"\n            \"Found a single task group, after filtering by version.\"\n        )\n    else:\n        logger.info(\n            \"[_get_task_by_taskimport] \"\n            f\"Found {len(final_matching_task_groups)} task groups, \"\n            \"after filtering by version.\"\n        )\n        final_task_group = await _disambiguate_task_groups(\n            matching_task_groups=final_matching_task_groups,\n            user_id=user_id,\n            db=db,\n            default_group_id=default_group_id,\n        )\n        if final_task_group is None:\n            logger.info(\n                \"[_get_task_by_taskimport] Disambiguation returned None.\"\n            )\n            return None\n\n    # Find task with given name\n    task_id = next(\n        iter(\n            task.id\n            for task in final_task_group.task_list\n            if task.name == task_import.name\n        ),\n        None,\n    )\n\n    logger.info(f\"[_get_task_by_taskimport] END, {task_import=}, {task_id=}.\")\n\n    return task_id\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow_import/#fractal_server.app.routes.api.v2.workflow_import._get_user_accessible_taskgroups","title":"<code>_get_user_accessible_taskgroups(*, user_id, user_resource_id, db)</code>  <code>async</code>","text":"<p>Retrieve list of task groups that the user has access to.</p> Source code in <code>fractal_server/app/routes/api/v2/workflow_import.py</code> <pre><code>async def _get_user_accessible_taskgroups(\n    *,\n    user_id: int,\n    user_resource_id: int,\n    db: AsyncSession,\n) -&gt; list[TaskGroupV2]:\n    \"\"\"\n    Retrieve list of task groups that the user has access to.\n    \"\"\"\n\n    stm = (\n        select(TaskGroupV2)\n        .where(\n            or_(\n                TaskGroupV2.user_id == user_id,\n                TaskGroupV2.user_group_id.in_(\n                    select(LinkUserGroup.group_id).where(\n                        LinkUserGroup.user_id == user_id\n                    )\n                ),\n            )\n        )\n        .where(TaskGroupV2.resource_id == user_resource_id)\n    )\n    res = await db.execute(stm)\n    accessible_task_groups = res.scalars().all()\n    logger.info(\n        f\"Found {len(accessible_task_groups)} accessible \"\n        f\"task groups for {user_id=}.\"\n    )\n    return accessible_task_groups\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflow_import/#fractal_server.app.routes.api.v2.workflow_import.import_workflow","title":"<code>import_workflow(project_id, workflow_import, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Import an existing workflow into a project and create required objects.</p> Source code in <code>fractal_server/app/routes/api/v2/workflow_import.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/import/\",\n    response_model=WorkflowReadV2WithWarnings,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def import_workflow(\n    project_id: int,\n    workflow_import: WorkflowImportV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowReadV2WithWarnings:\n    \"\"\"\n    Import an existing workflow into a project and create required objects.\n    \"\"\"\n\n    user_resource_id = await _get_user_resource_id(user_id=user.id, db=db)\n\n    # Preliminary checks\n    await _get_project_check_owner(\n        project_id=project_id,\n        user_id=user.id,\n        db=db,\n    )\n    await _check_workflow_exists(\n        name=workflow_import.name,\n        project_id=project_id,\n        db=db,\n    )\n\n    task_group_list = await _get_user_accessible_taskgroups(\n        user_id=user.id,\n        db=db,\n        user_resource_id=user_resource_id,\n    )\n    default_group_id = await _get_default_usergroup_id_or_none(db)\n\n    list_wf_tasks = []\n    list_task_ids = []\n    for wf_task in workflow_import.task_list:\n        task_import = wf_task.task\n        if isinstance(task_import, TaskImportV2Legacy):\n            task_id = await _get_task_by_source(\n                source=task_import.source,\n                task_groups_list=task_group_list,\n            )\n        else:\n            task_id = await _get_task_by_taskimport(\n                task_import=task_import,\n                user_id=user.id,\n                default_group_id=default_group_id,\n                task_groups_list=task_group_list,\n                db=db,\n            )\n        if task_id is None:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=f\"Could not find a task matching with {wf_task.task}.\",\n            )\n        new_wf_task = WorkflowTaskCreateV2(\n            **wf_task.model_dump(exclude_none=True, exclude={\"task\"})\n        )\n        list_wf_tasks.append(new_wf_task)\n        list_task_ids.append(task_id)\n\n    for wftask, task_id in zip(list_wf_tasks, list_task_ids):\n        task = await db.get(TaskV2, task_id)\n        _check_type_filters_compatibility(\n            task_input_types=task.input_types,\n            wftask_type_filters=wftask.type_filters,\n        )\n\n    # Create new Workflow\n    db_workflow = WorkflowV2(\n        project_id=project_id,\n        **workflow_import.model_dump(exclude_none=True, exclude={\"task_list\"}),\n    )\n    db.add(db_workflow)\n    await db.commit()\n    await db.refresh(db_workflow)\n\n    # Insert task into the workflow\n    for ind, new_wf_task in enumerate(list_wf_tasks):\n        await _workflow_insert_task(\n            **new_wf_task.model_dump(),\n            workflow_id=db_workflow.id,\n            task_id=list_task_ids[ind],\n            db=db,\n        )\n\n    # Add warnings for non-active tasks (or non-accessible tasks,\n    # although that should never happen)\n    wftask_list_with_warnings = await _add_warnings_to_workflow_tasks(\n        wftask_list=db_workflow.task_list, user_id=user.id, db=db\n    )\n    workflow_data = dict(\n        **db_workflow.model_dump(),\n        project=db_workflow.project,\n        task_list=wftask_list_with_warnings,\n    )\n\n    return workflow_data\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflowtask/","title":"workflowtask","text":""},{"location":"reference/fractal_server/app/routes/api/v2/workflowtask/#fractal_server.app.routes.api.v2.workflowtask.create_workflowtask","title":"<code>create_workflowtask(project_id, workflow_id, task_id, wftask, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Add a WorkflowTask to a Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflowtask.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/\",\n    response_model=WorkflowTaskReadV2,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    task_id: int,\n    wftask: WorkflowTaskCreateV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowTaskReadV2 | None:\n    \"\"\"\n    Add a WorkflowTask to a Workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_owner(\n        project_id=project_id, workflow_id=workflow_id, user_id=user.id, db=db\n    )\n\n    task = await _get_task_read_access(\n        task_id=task_id, user_id=user.id, db=db, require_active=True\n    )\n\n    if task.type == TaskType.PARALLEL:\n        if (\n            wftask.meta_non_parallel is not None\n            or wftask.args_non_parallel is not None\n        ):\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=(\n                    \"Cannot set `WorkflowTaskV2.meta_non_parallel` or \"\n                    \"`WorkflowTask.args_non_parallel` if the associated Task \"\n                    \"is `parallel`.\"\n                ),\n            )\n    elif task.type == TaskType.NON_PARALLEL:\n        if (\n            wftask.meta_parallel is not None\n            or wftask.args_parallel is not None\n        ):\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=(\n                    \"Cannot set `WorkflowTaskV2.meta_parallel` or \"\n                    \"`WorkflowTask.args_parallel` if the associated Task \"\n                    \"is `non_parallel`.\"\n                ),\n            )\n\n    _check_type_filters_compatibility(\n        task_input_types=task.input_types,\n        wftask_type_filters=wftask.type_filters,\n    )\n\n    wftask_db = await _workflow_insert_task(\n        workflow_id=workflow.id,\n        task_id=task_id,\n        meta_non_parallel=wftask.meta_non_parallel,\n        meta_parallel=wftask.meta_parallel,\n        args_non_parallel=wftask.args_non_parallel,\n        args_parallel=wftask.args_parallel,\n        type_filters=wftask.type_filters,\n        db=db,\n    )\n\n    return wftask_db\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflowtask/#fractal_server.app.routes.api.v2.workflowtask.delete_workflowtask","title":"<code>delete_workflowtask(project_id, workflow_id, workflow_task_id, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a WorkflowTask of a Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflowtask.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}/\",\n    status_code=status.HTTP_204_NO_CONTENT,\n)\nasync def delete_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a WorkflowTask of a Workflow\n    \"\"\"\n\n    db_workflow_task, db_workflow = await _get_workflow_task_check_owner(\n        project_id=project_id,\n        workflow_task_id=workflow_task_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    if await _workflow_has_submitted_job(workflow_id=workflow_id, db=db):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot delete a WorkflowTask while a Job is running for this \"\n                \"Workflow.\"\n            ),\n        )\n\n    # Delete WorkflowTask\n    await db.delete(db_workflow_task)\n    await db.commit()\n\n    await db.refresh(db_workflow)\n    db_workflow.task_list.reorder()\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/api/v2/workflowtask/#fractal_server.app.routes.api.v2.workflowtask.update_workflowtask","title":"<code>update_workflowtask(project_id, workflow_id, workflow_task_id, workflow_task_update, user=Depends(current_user_act_ver_prof), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a WorkflowTask of a Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflowtask.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}/\",\n    response_model=WorkflowTaskReadV2,\n)\nasync def update_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    workflow_task_update: WorkflowTaskUpdateV2,\n    user: UserOAuth = Depends(current_user_act_ver_prof),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowTaskReadV2 | None:\n    \"\"\"\n    Edit a WorkflowTask of a Workflow\n    \"\"\"\n\n    db_wf_task, db_workflow = await _get_workflow_task_check_owner(\n        project_id=project_id,\n        workflow_task_id=workflow_task_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        db=db,\n    )\n    if workflow_task_update.type_filters is not None:\n        _check_type_filters_compatibility(\n            task_input_types=db_wf_task.task.input_types,\n            wftask_type_filters=workflow_task_update.type_filters,\n        )\n\n    if db_wf_task.task_type == TaskType.PARALLEL and (\n        workflow_task_update.args_non_parallel is not None\n        or workflow_task_update.meta_non_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot patch `WorkflowTaskV2.args_non_parallel` or \"\n                \"`WorkflowTask.meta_non_parallel` if the associated Task is \"\n                \"parallel.\"\n            ),\n        )\n    elif db_wf_task.task_type in [\n        TaskType.NON_PARALLEL,\n        TaskType.CONVERTER_NON_PARALLEL,\n    ] and (\n        workflow_task_update.args_parallel is not None\n        or workflow_task_update.meta_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot patch `WorkflowTaskV2.args_parallel` or \"\n                \"`WorkflowTask.meta_parallel` if the associated Task is \"\n                \"non parallel.\"\n            ),\n        )\n\n    for key, value in workflow_task_update.model_dump(\n        exclude_unset=True\n    ).items():\n        if key == \"args_parallel\":\n            # Get default arguments via a Task property method\n            actual_args = deepcopy(value)\n            if not actual_args:\n                actual_args = None\n            setattr(db_wf_task, key, actual_args)\n        elif key == \"args_non_parallel\":\n            actual_args = deepcopy(value)\n            if not actual_args:\n                actual_args = None\n            setattr(db_wf_task, key, actual_args)\n        elif key in [\"meta_parallel\", \"meta_non_parallel\", \"type_filters\"]:\n            setattr(db_wf_task, key, value)\n        else:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=f\"patch_workflow_task endpoint cannot set {key=}\",\n            )\n\n    await db.commit()\n    await db.refresh(db_wf_task)\n    await db.close()\n\n    return db_wf_task\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/","title":"auth","text":""},{"location":"reference/fractal_server/app/routes/auth/#fractal_server.app.routes.auth.current_user_act_ver_prof","title":"<code>current_user_act_ver_prof(user=Depends(current_user_act_ver))</code>  <code>async</code>","text":"<p>Require a active&amp;verified user, with a non-null <code>profile_id</code>.</p> <p>Raises 401 if user does not exist or is not active. Raises 403 if user is not verified or has null <code>profile_id</code>.</p> Source code in <code>fractal_server/app/routes/auth/__init__.py</code> <pre><code>async def current_user_act_ver_prof(\n    user: UserOAuth = Depends(current_user_act_ver),\n) -&gt; UserOAuth:\n    \"\"\"\n    Require a active&amp;verified user, with a non-null `profile_id`.\n\n    Raises 401 if user does not exist or is not active.\n    Raises 403 if user is not verified or has null `profile_id`.\n    \"\"\"\n    if user.profile_id is None:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=(\n                f\"Forbidden access \"\n                f\"({user.is_verified=} {user.profile_id=}).\"\n            ),\n        )\n    return user\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/_aux_auth/","title":"_aux_auth","text":""},{"location":"reference/fractal_server/app/routes/auth/_aux_auth/#fractal_server.app.routes.auth._aux_auth._get_default_usergroup_id_or_none","title":"<code>_get_default_usergroup_id_or_none(db)</code>  <code>async</code>","text":"<p>Return the ID of the group named <code>\"All\"</code>, if <code>FRACTAL_DEFAULT_GROUP_NAME</code> is set and such group exists. Return <code>None</code>, if <code>FRACTAL_DEFAULT_GROUP_NAME=None</code> or if the <code>\"All\"</code> group does not exist.</p> Source code in <code>fractal_server/app/routes/auth/_aux_auth.py</code> <pre><code>async def _get_default_usergroup_id_or_none(db: AsyncSession) -&gt; int | None:\n    \"\"\"\n    Return the ID of the group named `\"All\"`, if `FRACTAL_DEFAULT_GROUP_NAME`\n    is set and such group exists. Return `None`, if\n    `FRACTAL_DEFAULT_GROUP_NAME=None` or if the `\"All\"` group does not exist.\n    \"\"\"\n    settings = Inject(get_settings)\n    stm = select(UserGroup.id).where(\n        UserGroup.name == settings.FRACTAL_DEFAULT_GROUP_NAME\n    )\n    res = await db.execute(stm)\n    user_group_id = res.scalars().one_or_none()\n\n    if (\n        settings.FRACTAL_DEFAULT_GROUP_NAME is not None\n        and user_group_id is None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=(\n                f\"User group '{settings.FRACTAL_DEFAULT_GROUP_NAME}'\"\n                \" not found.\",\n            ),\n        )\n\n    return user_group_id\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/_aux_auth/#fractal_server.app.routes.auth._aux_auth._get_single_user_with_groups","title":"<code>_get_single_user_with_groups(user, db)</code>  <code>async</code>","text":"<p>Enrich a user object by filling its <code>group_ids_names</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>UserOAuth</code> <p>The current <code>UserOAuth</code> object</p> required <code>db</code> <code>AsyncSession</code> <p>Async db session</p> required <p>Returns:</p> Type Description <code>UserRead</code> <p>A <code>UserRead</code> object with <code>group_ids_names</code> dict</p> Source code in <code>fractal_server/app/routes/auth/_aux_auth.py</code> <pre><code>async def _get_single_user_with_groups(\n    user: UserOAuth,\n    db: AsyncSession,\n) -&gt; UserRead:\n    \"\"\"\n    Enrich a user object by filling its `group_ids_names` attribute.\n\n    Args:\n        user: The current `UserOAuth` object\n        db: Async db session\n\n    Returns:\n        A `UserRead` object with `group_ids_names` dict\n    \"\"\"\n\n    settings = Inject(get_settings)\n\n    stm_groups = (\n        select(UserGroup)\n        .join(LinkUserGroup)\n        .where(LinkUserGroup.user_id == user.id)\n        .order_by(asc(LinkUserGroup.timestamp_created))\n    )\n    res = await db.execute(stm_groups)\n    groups = res.scalars().unique().all()\n    group_ids_names = [(group.id, group.name) for group in groups]\n\n    # Identify the default-group position in the list of groups\n    index = next(\n        (\n            ind\n            for ind, group_tuple in enumerate(group_ids_names)\n            if group_tuple[1] == settings.FRACTAL_DEFAULT_GROUP_NAME\n        ),\n        None,\n    )\n    if (index is None) or (index == 0):\n        # Either the default group does not exist, or it is already the first\n        # one. No action needed.\n        pass\n    else:\n        # Move the default group to the first position\n        default_group = group_ids_names.pop(index)\n        group_ids_names.insert(0, default_group)\n\n    # Create dump of `user.oauth_accounts` relationship\n    oauth_accounts = [\n        oauth_account.model_dump() for oauth_account in user.oauth_accounts\n    ]\n\n    return UserRead(\n        **user.model_dump(),\n        group_ids_names=group_ids_names,\n        oauth_accounts=oauth_accounts,\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/_aux_auth/#fractal_server.app.routes.auth._aux_auth._get_single_usergroup_with_user_ids","title":"<code>_get_single_usergroup_with_user_ids(group_id, db)</code>  <code>async</code>","text":"<p>Get a group, and construct its <code>user_ids</code> list.</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>int</code> required <code>db</code> <code>AsyncSession</code> required <p>Returns:</p> Type Description <code>UserGroupRead</code> <p><code>UserGroupRead</code> object, with <code>user_ids</code> attribute populated</p> <code>UserGroupRead</code> <p>from database.</p> Source code in <code>fractal_server/app/routes/auth/_aux_auth.py</code> <pre><code>async def _get_single_usergroup_with_user_ids(\n    group_id: int, db: AsyncSession\n) -&gt; UserGroupRead:\n    \"\"\"\n    Get a group, and construct its `user_ids` list.\n\n    Args:\n        group_id:\n        db:\n\n    Returns:\n        `UserGroupRead` object, with `user_ids` attribute populated\n        from database.\n    \"\"\"\n    group = await _usergroup_or_404(group_id, db)\n\n    # Get all user/group links\n    stm_links = select(LinkUserGroup).where(LinkUserGroup.group_id == group_id)\n    res = await db.execute(stm_links)\n    links = res.scalars().all()\n    user_ids = [link.user_id for link in links]\n\n    return UserGroupRead(**group.model_dump(), user_ids=user_ids)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/_aux_auth/#fractal_server.app.routes.auth._aux_auth._user_or_404","title":"<code>_user_or_404(user_id, db)</code>  <code>async</code>","text":"<p>Get a user from db, or raise a 404 HTTP exception if missing.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>int</code> <p>ID of the user</p> required <code>db</code> <code>AsyncSession</code> <p>Async db session</p> required Source code in <code>fractal_server/app/routes/auth/_aux_auth.py</code> <pre><code>async def _user_or_404(user_id: int, db: AsyncSession) -&gt; UserOAuth:\n    \"\"\"\n    Get a user from db, or raise a 404 HTTP exception if missing.\n\n    Args:\n        user_id: ID of the user\n        db: Async db session\n    \"\"\"\n    user = await db.get(UserOAuth, user_id, populate_existing=True)\n    if user is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"User {user_id} not found.\",\n        )\n    return user\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/current_user/","title":"current_user","text":"<p>Definition of <code>/auth/current-user/</code> endpoints</p>"},{"location":"reference/fractal_server/app/routes/auth/current_user/#fractal_server.app.routes.auth.current_user.get_current_user","title":"<code>get_current_user(group_ids_names=False, user=Depends(current_user_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return current user</p> Source code in <code>fractal_server/app/routes/auth/current_user.py</code> <pre><code>@router_current_user.get(\"/current-user/\", response_model=UserRead)\nasync def get_current_user(\n    group_ids_names: bool = False,\n    user: UserOAuth = Depends(current_user_act),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Return current user\n    \"\"\"\n    if group_ids_names is True:\n        user_with_groups = await _get_single_user_with_groups(user, db)\n        return user_with_groups\n    else:\n        return user\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/current_user/#fractal_server.app.routes.auth.current_user.get_current_user_allowed_viewer_paths","title":"<code>get_current_user_allowed_viewer_paths(current_user=Depends(current_user_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns the allowed viewer paths for current user, according to the selected FRACTAL_DATA_AUTH_SCHEME</p> Source code in <code>fractal_server/app/routes/auth/current_user.py</code> <pre><code>@router_current_user.get(\n    \"/current-user/allowed-viewer-paths/\", response_model=list[str]\n)\nasync def get_current_user_allowed_viewer_paths(\n    current_user: UserOAuth = Depends(current_user_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[str]:\n    \"\"\"\n    Returns the allowed viewer paths for current user, according to the\n    selected FRACTAL_DATA_AUTH_SCHEME\n    \"\"\"\n\n    data_settings = Inject(get_data_settings)\n\n    authorized_paths = []\n\n    if data_settings.FRACTAL_DATA_AUTH_SCHEME == DataAuthScheme.NONE:\n        return authorized_paths\n\n    # Append `project_dir` to the list of authorized paths\n    authorized_paths.append(current_user.project_dir)\n\n    # If auth scheme is \"users-folders\" and `slurm_user` is set,\n    # build and append the user folder\n    if (\n        data_settings.FRACTAL_DATA_AUTH_SCHEME == DataAuthScheme.USERS_FOLDERS\n        and current_user.profile_id is not None\n    ):\n        profile = await db.get(Profile, current_user.profile_id)\n        if profile is not None and profile.username is not None:\n            base_folder = data_settings.FRACTAL_DATA_BASE_FOLDER\n            user_folder = os.path.join(base_folder, profile.username)\n            authorized_paths.append(user_folder)\n\n    if data_settings.FRACTAL_DATA_AUTH_SCHEME == DataAuthScheme.VIEWER_PATHS:\n        # Returns the union of `viewer_paths` for all user's groups\n        cmd = (\n            select(UserGroup.viewer_paths)\n            .join(LinkUserGroup)\n            .where(LinkUserGroup.group_id == UserGroup.id)\n            .where(LinkUserGroup.user_id == current_user.id)\n        )\n        res = await db.execute(cmd)\n        viewer_paths_nested = res.scalars().all()\n\n        # Flatten a nested object and make its elements unique\n        all_viewer_paths_set = {\n            path\n            for _viewer_paths in viewer_paths_nested\n            for path in _viewer_paths\n        }\n        authorized_paths.extend(all_viewer_paths_set)\n\n    return authorized_paths\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/current_user/#fractal_server.app.routes.auth.current_user.patch_current_user","title":"<code>patch_current_user(user_update, current_user=Depends(current_user_act), user_manager=Depends(get_user_manager), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Note: a user cannot patch their own password (as enforced within the <code>UserUpdateStrict</code> schema).</p> Source code in <code>fractal_server/app/routes/auth/current_user.py</code> <pre><code>@router_current_user.patch(\"/current-user/\", response_model=UserRead)\nasync def patch_current_user(\n    user_update: UserUpdateStrict,\n    current_user: UserOAuth = Depends(current_user_act),\n    user_manager: UserManager = Depends(get_user_manager),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Note: a user cannot patch their own password (as enforced within the\n    `UserUpdateStrict` schema).\n    \"\"\"\n    update = UserUpdate(**user_update.model_dump(exclude_unset=True))\n\n    # NOTE: here it would be relevant to catch an `InvalidPasswordException`\n    # (from `fastapi_users.exceptions`), if we were to allow users change\n    # their own password\n\n    user = await user_manager.update(update, current_user, safe=True)\n    validated_user = UserOAuth.model_validate(user.model_dump())\n\n    patched_user = await db.get(\n        UserOAuth, validated_user.id, populate_existing=True\n    )\n    patched_user_with_groups = await _get_single_user_with_groups(\n        patched_user, db\n    )\n    return patched_user_with_groups\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/group/","title":"group","text":"<p>Definition of <code>/auth/group/</code> routes</p>"},{"location":"reference/fractal_server/app/routes/auth/group/#fractal_server.app.routes.auth.group.delete_single_group","title":"<code>delete_single_group(group_id, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a user group.</p> <p>If <code>FRACTAL_DEFAULT_GROUP_NAME=\"All\"</code>, a group named <code>\"All\"</code> cannot be deleted. If <code>FRACTAL_DEFAULT_GROUP_NAME=None</code>, any group can be deleted.</p> Source code in <code>fractal_server/app/routes/auth/group.py</code> <pre><code>@router_group.delete(\"/group/{group_id}/\", status_code=204)\nasync def delete_single_group(\n    group_id: int,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a user group.\n\n    If `FRACTAL_DEFAULT_GROUP_NAME=\"All\"`, a group named `\"All\"` cannot be\n    deleted. If `FRACTAL_DEFAULT_GROUP_NAME=None`, any group can be deleted.\n    \"\"\"\n    settings = Inject(get_settings)\n    group = await _usergroup_or_404(group_id, db)\n\n    if group.name == settings.FRACTAL_DEFAULT_GROUP_NAME:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot delete default UserGroup \"\n                f\"'{settings.FRACTAL_DEFAULT_GROUP_NAME}'.\"\n            ),\n        )\n\n    await db.delete(group)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/login/","title":"login","text":"<p>Definition of <code>/auth/{login,logout}/</code>, <code>/auth/token/{login/logout}</code> routes.</p>"},{"location":"reference/fractal_server/app/routes/auth/oauth/","title":"oauth","text":""},{"location":"reference/fractal_server/app/routes/auth/oauth/#fractal_server.app.routes.auth.oauth.get_oauth_router","title":"<code>get_oauth_router()</code>","text":"<p>Get the <code>APIRouter</code> object for OAuth endpoints.</p> Source code in <code>fractal_server/app/routes/auth/oauth.py</code> <pre><code>def get_oauth_router() -&gt; APIRouter | None:\n    \"\"\"\n    Get the `APIRouter` object for OAuth endpoints.\n    \"\"\"\n    router_oauth = APIRouter()\n    settings = Inject(get_settings)\n    oauth_settings = Inject(get_oauth_settings)\n    if not oauth_settings.is_set:\n        return None\n\n    client_name = oauth_settings.OAUTH_CLIENT_NAME\n    if client_name == \"google\":\n        client = _create_client_google(oauth_settings)\n    elif client_name == \"github\":\n        client = _create_client_github(oauth_settings)\n    else:\n        client = _create_client_oidc(oauth_settings)\n\n    router_oauth.include_router(\n        fastapi_users.get_oauth_router(\n            client,\n            cookie_backend,\n            settings.JWT_SECRET_KEY,\n            is_verified_by_default=False,\n            associate_by_email=True,\n            redirect_url=oauth_settings.OAUTH_REDIRECT_URL,\n        ),\n        prefix=f\"/{client_name}\",\n    )\n\n    # Add trailing slash to all routes' paths\n    for route in router_oauth.routes:\n        if not route.path.endswith(\"/\"):\n            route.path = f\"{route.path}/\"\n\n    return router_oauth\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/register/","title":"register","text":"<p>Definition of <code>/auth/register/</code> routes.</p>"},{"location":"reference/fractal_server/app/routes/auth/router/","title":"router","text":""},{"location":"reference/fractal_server/app/routes/auth/users/","title":"users","text":"<p>Definition of <code>/auth/users/</code> routes</p>"},{"location":"reference/fractal_server/app/routes/auth/users/#fractal_server.app.routes.auth.users.list_users","title":"<code>list_users(profile_id=None, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return list of all users</p> Source code in <code>fractal_server/app/routes/auth/users.py</code> <pre><code>@router_users.get(\"/users/\", response_model=list[UserRead])\nasync def list_users(\n    profile_id: int | None = None,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Return list of all users\n    \"\"\"\n    stm = select(UserOAuth)\n    if profile_id is not None:\n        stm = stm.where(UserOAuth.profile_id == profile_id)\n    res = await db.execute(stm)\n    user_list = res.scalars().unique().all()\n\n    # Get all user/group links\n    stm_all_links = select(LinkUserGroup)\n    res = await db.execute(stm_all_links)\n    links = res.scalars().all()\n\n    # TODO: possible optimizations for this construction are listed in\n    # https://github.com/fractal-analytics-platform/fractal-server/issues/1742\n    for ind, user in enumerate(user_list):\n        user_list[ind] = dict(\n            **user.model_dump(),\n            oauth_accounts=user.oauth_accounts,\n            group_ids=[\n                link.group_id for link in links if link.user_id == user.id\n            ],\n        )\n\n    return user_list\n</code></pre>"},{"location":"reference/fractal_server/app/routes/auth/users/#fractal_server.app.routes.auth.users.patch_user","title":"<code>patch_user(user_id, user_update, current_superuser=Depends(current_superuser_act), user_manager=Depends(get_user_manager), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Custom version of the PATCH-user route from <code>fastapi-users</code>.</p> Source code in <code>fractal_server/app/routes/auth/users.py</code> <pre><code>@router_users.patch(\"/users/{user_id}/\", response_model=UserRead)\nasync def patch_user(\n    user_id: int,\n    user_update: UserUpdate,\n    current_superuser: UserOAuth = Depends(current_superuser_act),\n    user_manager: UserManager = Depends(get_user_manager),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Custom version of the PATCH-user route from `fastapi-users`.\n    \"\"\"\n\n    # Check that user exists\n    user_to_patch = await _user_or_404(user_id, db)\n\n    if user_update.profile_id is not None:\n        profile = await db.get(Profile, user_update.profile_id)\n        if profile is None:\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND,\n                detail=f\"Profile {user_update.profile_id} not found.\",\n            )\n\n    # Modify user attributes\n    try:\n        user = await user_manager.update(\n            user_update,\n            user_to_patch,\n            safe=False,\n            request=None,\n        )\n        validated_user = UserOAuth.model_validate(user.model_dump())\n        patched_user = await db.get(\n            UserOAuth, validated_user.id, populate_existing=True\n        )\n    except exceptions.InvalidPasswordException as e:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail={\n                \"code\": ErrorCode.UPDATE_USER_INVALID_PASSWORD,\n                \"reason\": e.reason,\n            },\n        )\n    except exceptions.UserAlreadyExists:\n        raise HTTPException(\n            status.HTTP_400_BAD_REQUEST,\n            detail=ErrorCode.UPDATE_USER_EMAIL_ALREADY_EXISTS,\n        )\n\n    # Enrich user object with `group_ids_names` attribute\n    patched_user_with_groups = await _get_single_user_with_groups(\n        patched_user, db\n    )\n\n    return patched_user_with_groups\n</code></pre>"},{"location":"reference/fractal_server/app/routes/aux/","title":"aux","text":""},{"location":"reference/fractal_server/app/routes/aux/_job/","title":"_job","text":""},{"location":"reference/fractal_server/app/routes/aux/_job/#fractal_server.app.routes.aux._job._write_shutdown_file","title":"<code>_write_shutdown_file(*, job)</code>","text":"<p>Write job's shutdown file.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>JobV2</code> required <p>Note: we are not marking the job as failed (by setting its <code>status</code> attribute) here, since this will be done by the runner backend as soon as it detects the shutdown-trigerring file and performs the actual shutdown.</p> Source code in <code>fractal_server/app/routes/aux/_job.py</code> <pre><code>def _write_shutdown_file(*, job: JobV2):\n    \"\"\"\n    Write job's shutdown file.\n\n    Args:\n        job:\n\n    Note: we are **not** marking the job as failed (by setting its `status`\n    attribute) here, since this will be done by the runner backend as soon as\n    it detects the shutdown-trigerring file and performs the actual shutdown.\n    \"\"\"\n    shutdown_file = Path(job.working_dir) / SHUTDOWN_FILENAME\n    with shutdown_file.open(\"w\") as f:\n        f.write(f\"Trigger executor shutdown for {job.id=}.\")\n</code></pre>"},{"location":"reference/fractal_server/app/routes/aux/_runner/","title":"_runner","text":""},{"location":"reference/fractal_server/app/routes/aux/_runner/#fractal_server.app.routes.aux._runner._check_shutdown_is_supported","title":"<code>_check_shutdown_is_supported()</code>","text":"<p>Raises:</p> Type Description <code>HTTPException(status_code=HTTP_422_UNPROCESSABLE_CONTENT)</code> <p>If FRACTAL_RUNNER_BACKEND is the thread-based 'local' backend.</p> Source code in <code>fractal_server/app/routes/aux/_runner.py</code> <pre><code>def _check_shutdown_is_supported():\n    \"\"\"\n    Raises:\n        HTTPException(status_code=HTTP_422_UNPROCESSABLE_CONTENT):\n            If FRACTAL_RUNNER_BACKEND is the thread-based 'local' backend.\n    \"\"\"\n    settings = Inject(get_settings)\n    backend = settings.FRACTAL_RUNNER_BACKEND\n\n    if not _backend_supports_shutdown(backend):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Stopping a job execution is not implemented for \"\n                f\"FRACTAL_RUNNER_BACKEND={backend}.\"\n            ),\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/routes/aux/validate_user_profile/","title":"validate_user_profile","text":""},{"location":"reference/fractal_server/app/routes/aux/validate_user_profile/#fractal_server.app.routes.aux.validate_user_profile.validate_user_profile","title":"<code>validate_user_profile(*, user, db)</code>  <code>async</code>","text":"<p>Validate profile and resource associated to a given user.</p> <p>Note: this only returns non-db-bound objects.</p> Source code in <code>fractal_server/app/routes/aux/validate_user_profile.py</code> <pre><code>async def validate_user_profile(\n    *,\n    user: UserOAuth,\n    db: AsyncSession,\n) -&gt; tuple[Resource, Profile]:\n    \"\"\"\n    Validate profile and resource associated to a given user.\n\n    Note: this only returns non-db-bound objects.\n    \"\"\"\n    await user_has_profile_or_422(user=user)\n    profile = await db.get(Profile, user.profile_id)\n    resource = await db.get(Resource, profile.resource_id)\n    try:\n        cast_serialize_resource(\n            resource.model_dump(exclude={\"id\", \"timestamp_created\"}),\n        )\n        cast_serialize_profile(\n            profile.model_dump(exclude={\"resource_id\", \"id\"}),\n        )\n        db.expunge(resource)\n        db.expunge(profile)\n\n        return resource, profile\n\n    except ValidationError as e:\n        error_msg = (\n            \"User resource/profile are not valid for \"\n            f\"resource type '{resource.type}'. \"\n            f\"Original error: {str(e)}\"\n        )\n        logger.warning(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=error_msg,\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/","title":"schemas","text":""},{"location":"reference/fractal_server/app/schemas/#fractal_server.app.schemas.OAuthAccountRead","title":"<code>OAuthAccountRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for storing essential <code>OAuthAccount</code> information within <code>UserRead.oauth_accounts</code>.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>ID of the row in fractal-owned <code>oauthaccount</code> table.</p> <code>account_email</code> <code>str</code> <p>Email associated to OAuth account</p> <code>oauth_name</code> <code>str</code> <p>Name of the OAuth provider (e.g. <code>github</code>)</p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class OAuthAccountRead(BaseModel):\n    \"\"\"\n    Schema for storing essential `OAuthAccount` information within\n    `UserRead.oauth_accounts`.\n\n    Attributes:\n        id: ID of the row in fractal-owned `oauthaccount` table.\n        account_email: Email associated to OAuth account\n        oauth_name: Name of the OAuth provider (e.g. `github`)\n    \"\"\"\n\n    id: int\n    account_email: str\n    oauth_name: str\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/#fractal_server.app.schemas.UserCreate","title":"<code>UserCreate</code>","text":"<p>               Bases: <code>BaseUserCreate</code></p> <p>Schema for <code>User</code> creation.</p> <p>Attributes:</p> Name Type Description <code>profile_id</code> <code>int | None</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserCreate(schemas.BaseUserCreate):\n    \"\"\"\n    Schema for `User` creation.\n\n    Attributes:\n        profile_id:\n    \"\"\"\n\n    profile_id: int | None = None\n    project_dir: Annotated[AbsolutePathStr, AfterValidator(_validate_cmd)]\n    slurm_accounts: list[str] = Field(default_factory=list)\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/#fractal_server.app.schemas.UserGroupCreate","title":"<code>UserGroupCreate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>UserGroup</code> creation</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Group name</p> Source code in <code>fractal_server/app/schemas/user_group.py</code> <pre><code>class UserGroupCreate(BaseModel):\n    \"\"\"\n    Schema for `UserGroup` creation\n\n    Attributes:\n        name: Group name\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    name: str\n    viewer_paths: ListUniqueAbsolutePathStr = Field(default_factory=list)\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/#fractal_server.app.schemas.UserGroupRead","title":"<code>UserGroupRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>UserGroup</code> read</p> <p>NOTE: <code>user_ids</code> does not correspond to a column of the <code>UserGroup</code> table, but it is rather computed dynamically in relevant endpoints.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Group ID</p> <code>name</code> <code>str</code> <p>Group name</p> <code>timestamp_created</code> <code>AwareDatetime</code> <p>Creation timestamp</p> <code>user_ids</code> <code>list[int] | None</code> <p>IDs of users of this group</p> Source code in <code>fractal_server/app/schemas/user_group.py</code> <pre><code>class UserGroupRead(BaseModel):\n    \"\"\"\n    Schema for `UserGroup` read\n\n    NOTE: `user_ids` does not correspond to a column of the `UserGroup` table,\n    but it is rather computed dynamically in relevant endpoints.\n\n    Attributes:\n        id: Group ID\n        name: Group name\n        timestamp_created: Creation timestamp\n        user_ids: IDs of users of this group\n    \"\"\"\n\n    id: int\n    name: str\n    timestamp_created: AwareDatetime\n    user_ids: list[int] | None = None\n    viewer_paths: list[str]\n\n    @field_serializer(\"timestamp_created\")\n    def serialize_datetime(v: datetime) -&gt; str:\n        return v.isoformat()\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/#fractal_server.app.schemas.UserGroupUpdate","title":"<code>UserGroupUpdate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>UserGroup</code> update</p> Source code in <code>fractal_server/app/schemas/user_group.py</code> <pre><code>class UserGroupUpdate(BaseModel):\n    \"\"\"\n    Schema for `UserGroup` update\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    viewer_paths: ListUniqueAbsolutePathStr = None\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/#fractal_server.app.schemas.UserRead","title":"<code>UserRead</code>","text":"<p>               Bases: <code>BaseUser[int]</code></p> <p>Schema for <code>User</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>group_ids_names</code> <code>list[tuple[int, str]] | None</code> <code>oauth_accounts</code> <code>list[OAuthAccountRead]</code> <code>profile_id</code> <code>int | None</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserRead(schemas.BaseUser[int]):\n    \"\"\"\n    Schema for `User` read from database.\n\n    Attributes:\n        group_ids_names:\n        oauth_accounts:\n        profile_id:\n    \"\"\"\n\n    group_ids_names: list[tuple[int, str]] | None = None\n    oauth_accounts: list[OAuthAccountRead]\n    profile_id: int | None = None\n    project_dir: str\n    slurm_accounts: list[str]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/#fractal_server.app.schemas.UserUpdate","title":"<code>UserUpdate</code>","text":"<p>               Bases: <code>BaseUserUpdate</code></p> <p>Schema for <code>User</code> update.</p> <p>Attributes:</p> Name Type Description <code>password</code> <code>NonEmptyStr</code> <code>email</code> <code>EmailStr</code> <code>is_active</code> <code>bool</code> <code>is_superuser</code> <code>bool</code> <code>is_verified</code> <code>bool</code> <code>profile_id</code> <code>int | None</code> <code>project_dir</code> <code>Annotated[AbsolutePathStr, AfterValidator(_validate_cmd)]</code> <code>slurm_accounts</code> <code>ListUniqueNonEmptyString</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdate(schemas.BaseUserUpdate):\n    \"\"\"\n    Schema for `User` update.\n\n    Attributes:\n        password:\n        email:\n        is_active:\n        is_superuser:\n        is_verified:\n        profile_id:\n        project_dir:\n        slurm_accounts:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    password: NonEmptyStr = None\n    email: EmailStr = None\n    is_active: bool = None\n    is_superuser: bool = None\n    is_verified: bool = None\n    profile_id: int | None = None\n    project_dir: Annotated[\n        AbsolutePathStr, AfterValidator(_validate_cmd)\n    ] = None\n    slurm_accounts: ListUniqueNonEmptyString = None\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/#fractal_server.app.schemas.UserUpdateGroups","title":"<code>UserUpdateGroups</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>POST /auth/users/{user_id}/set-groups/</code></p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdateGroups(BaseModel):\n    \"\"\"\n    Schema for `POST /auth/users/{user_id}/set-groups/`\n\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    group_ids: ListUniqueNonNegativeInt = Field(min_length=1)\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/#fractal_server.app.schemas.UserUpdateStrict","title":"<code>UserUpdateStrict</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>User</code> self-editing.</p> <p>Attributes:</p> Name Type Description <code>slurm_accounts</code> <code>ListUniqueNonEmptyString</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdateStrict(BaseModel):\n    \"\"\"\n    Schema for `User` self-editing.\n\n    Attributes:\n        slurm_accounts:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    slurm_accounts: ListUniqueNonEmptyString = None\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/#fractal_server.app.schemas.validate_cmd","title":"<code>validate_cmd(command, *, allow_char=None, attribute_name='Command')</code>","text":"<p>Assert that the provided <code>command</code> does not contain any of the forbidden characters for commands (fractal_server.string_tools.NOT_ALLOWED_FOR_COMMANDS)</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>command to validate.</p> required <code>allow_char</code> <code>str | None</code> <p>chars to accept among the forbidden ones</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute, to be used in error message.</p> <code>'Command'</code> Source code in <code>fractal_server/string_tools.py</code> <pre><code>def validate_cmd(\n    command: str,\n    *,\n    allow_char: str | None = None,\n    attribute_name: str = \"Command\",\n):\n    \"\"\"\n    Assert that the provided `command` does not contain any of the forbidden\n    characters for commands\n    (fractal_server.string_tools.__NOT_ALLOWED_FOR_COMMANDS__)\n\n    Args:\n        command: command to validate.\n        allow_char: chars to accept among the forbidden ones\n        attribute_name: Name of the attribute, to be used in error message.\n    \"\"\"\n    if not isinstance(command, str):\n        raise ValueError(f\"{command=} is not a string.\")\n    forbidden = set(__NOT_ALLOWED_FOR_COMMANDS__)\n    if allow_char is not None:\n        forbidden = forbidden - set(allow_char)\n    if not forbidden.isdisjoint(set(command)):\n        raise ValueError(\n            f\"{attribute_name} must not contain any of this characters: \"\n            f\"'{forbidden}'\\n\"\n            f\"Provided {attribute_name.lower()}: '{command}'.\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user/","title":"user","text":""},{"location":"reference/fractal_server/app/schemas/user/#fractal_server.app.schemas.user.OAuthAccountRead","title":"<code>OAuthAccountRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for storing essential <code>OAuthAccount</code> information within <code>UserRead.oauth_accounts</code>.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>ID of the row in fractal-owned <code>oauthaccount</code> table.</p> <code>account_email</code> <code>str</code> <p>Email associated to OAuth account</p> <code>oauth_name</code> <code>str</code> <p>Name of the OAuth provider (e.g. <code>github</code>)</p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class OAuthAccountRead(BaseModel):\n    \"\"\"\n    Schema for storing essential `OAuthAccount` information within\n    `UserRead.oauth_accounts`.\n\n    Attributes:\n        id: ID of the row in fractal-owned `oauthaccount` table.\n        account_email: Email associated to OAuth account\n        oauth_name: Name of the OAuth provider (e.g. `github`)\n    \"\"\"\n\n    id: int\n    account_email: str\n    oauth_name: str\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user/#fractal_server.app.schemas.user.UserCreate","title":"<code>UserCreate</code>","text":"<p>               Bases: <code>BaseUserCreate</code></p> <p>Schema for <code>User</code> creation.</p> <p>Attributes:</p> Name Type Description <code>profile_id</code> <code>int | None</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserCreate(schemas.BaseUserCreate):\n    \"\"\"\n    Schema for `User` creation.\n\n    Attributes:\n        profile_id:\n    \"\"\"\n\n    profile_id: int | None = None\n    project_dir: Annotated[AbsolutePathStr, AfterValidator(_validate_cmd)]\n    slurm_accounts: list[str] = Field(default_factory=list)\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user/#fractal_server.app.schemas.user.UserRead","title":"<code>UserRead</code>","text":"<p>               Bases: <code>BaseUser[int]</code></p> <p>Schema for <code>User</code> read from database.</p> <p>Attributes:</p> Name Type Description <code>group_ids_names</code> <code>list[tuple[int, str]] | None</code> <code>oauth_accounts</code> <code>list[OAuthAccountRead]</code> <code>profile_id</code> <code>int | None</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserRead(schemas.BaseUser[int]):\n    \"\"\"\n    Schema for `User` read from database.\n\n    Attributes:\n        group_ids_names:\n        oauth_accounts:\n        profile_id:\n    \"\"\"\n\n    group_ids_names: list[tuple[int, str]] | None = None\n    oauth_accounts: list[OAuthAccountRead]\n    profile_id: int | None = None\n    project_dir: str\n    slurm_accounts: list[str]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user/#fractal_server.app.schemas.user.UserUpdate","title":"<code>UserUpdate</code>","text":"<p>               Bases: <code>BaseUserUpdate</code></p> <p>Schema for <code>User</code> update.</p> <p>Attributes:</p> Name Type Description <code>password</code> <code>NonEmptyStr</code> <code>email</code> <code>EmailStr</code> <code>is_active</code> <code>bool</code> <code>is_superuser</code> <code>bool</code> <code>is_verified</code> <code>bool</code> <code>profile_id</code> <code>int | None</code> <code>project_dir</code> <code>Annotated[AbsolutePathStr, AfterValidator(_validate_cmd)]</code> <code>slurm_accounts</code> <code>ListUniqueNonEmptyString</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdate(schemas.BaseUserUpdate):\n    \"\"\"\n    Schema for `User` update.\n\n    Attributes:\n        password:\n        email:\n        is_active:\n        is_superuser:\n        is_verified:\n        profile_id:\n        project_dir:\n        slurm_accounts:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    password: NonEmptyStr = None\n    email: EmailStr = None\n    is_active: bool = None\n    is_superuser: bool = None\n    is_verified: bool = None\n    profile_id: int | None = None\n    project_dir: Annotated[\n        AbsolutePathStr, AfterValidator(_validate_cmd)\n    ] = None\n    slurm_accounts: ListUniqueNonEmptyString = None\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user/#fractal_server.app.schemas.user.UserUpdateGroups","title":"<code>UserUpdateGroups</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>POST /auth/users/{user_id}/set-groups/</code></p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdateGroups(BaseModel):\n    \"\"\"\n    Schema for `POST /auth/users/{user_id}/set-groups/`\n\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    group_ids: ListUniqueNonNegativeInt = Field(min_length=1)\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user/#fractal_server.app.schemas.user.UserUpdateStrict","title":"<code>UserUpdateStrict</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>User</code> self-editing.</p> <p>Attributes:</p> Name Type Description <code>slurm_accounts</code> <code>ListUniqueNonEmptyString</code> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdateStrict(BaseModel):\n    \"\"\"\n    Schema for `User` self-editing.\n\n    Attributes:\n        slurm_accounts:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    slurm_accounts: ListUniqueNonEmptyString = None\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user_group/","title":"user_group","text":""},{"location":"reference/fractal_server/app/schemas/user_group/#fractal_server.app.schemas.user_group.UserGroupCreate","title":"<code>UserGroupCreate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>UserGroup</code> creation</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Group name</p> Source code in <code>fractal_server/app/schemas/user_group.py</code> <pre><code>class UserGroupCreate(BaseModel):\n    \"\"\"\n    Schema for `UserGroup` creation\n\n    Attributes:\n        name: Group name\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    name: str\n    viewer_paths: ListUniqueAbsolutePathStr = Field(default_factory=list)\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user_group/#fractal_server.app.schemas.user_group.UserGroupRead","title":"<code>UserGroupRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>UserGroup</code> read</p> <p>NOTE: <code>user_ids</code> does not correspond to a column of the <code>UserGroup</code> table, but it is rather computed dynamically in relevant endpoints.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Group ID</p> <code>name</code> <code>str</code> <p>Group name</p> <code>timestamp_created</code> <code>AwareDatetime</code> <p>Creation timestamp</p> <code>user_ids</code> <code>list[int] | None</code> <p>IDs of users of this group</p> Source code in <code>fractal_server/app/schemas/user_group.py</code> <pre><code>class UserGroupRead(BaseModel):\n    \"\"\"\n    Schema for `UserGroup` read\n\n    NOTE: `user_ids` does not correspond to a column of the `UserGroup` table,\n    but it is rather computed dynamically in relevant endpoints.\n\n    Attributes:\n        id: Group ID\n        name: Group name\n        timestamp_created: Creation timestamp\n        user_ids: IDs of users of this group\n    \"\"\"\n\n    id: int\n    name: str\n    timestamp_created: AwareDatetime\n    user_ids: list[int] | None = None\n    viewer_paths: list[str]\n\n    @field_serializer(\"timestamp_created\")\n    def serialize_datetime(v: datetime) -&gt; str:\n        return v.isoformat()\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/user_group/#fractal_server.app.schemas.user_group.UserGroupUpdate","title":"<code>UserGroupUpdate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>UserGroup</code> update</p> Source code in <code>fractal_server/app/schemas/user_group.py</code> <pre><code>class UserGroupUpdate(BaseModel):\n    \"\"\"\n    Schema for `UserGroup` update\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    viewer_paths: ListUniqueAbsolutePathStr = None\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/","title":"v2","text":""},{"location":"reference/fractal_server/app/schemas/v2/accounting/","title":"accounting","text":""},{"location":"reference/fractal_server/app/schemas/v2/dataset/","title":"dataset","text":""},{"location":"reference/fractal_server/app/schemas/v2/dataset/#fractal_server.app.schemas.v2.dataset.DatasetExportV2","title":"<code>DatasetExportV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for <code>Dataset</code> export.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <code>zarr_dir</code> <code>str</code> <code>images</code> <code>list[SingleImage]</code> Source code in <code>fractal_server/app/schemas/v2/dataset.py</code> <pre><code>class DatasetExportV2(BaseModel):\n    \"\"\"\n    Class for `Dataset` export.\n\n    Attributes:\n        name:\n        zarr_dir:\n        images:\n    \"\"\"\n\n    name: str\n    zarr_dir: str\n    images: list[SingleImage]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/dataset/#fractal_server.app.schemas.v2.dataset.DatasetImportV2","title":"<code>DatasetImportV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for <code>Dataset</code> import.</p> <p>We are dropping <code>model_config = ConfigDict(extra=\"forbid\")</code> so that any kind of legacy filters can be included in the payload, and ignored in the API.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <code>zarr_dir</code> <code>ZarrDirStr</code> <code>images</code> <code>list[SingleImage]</code> Source code in <code>fractal_server/app/schemas/v2/dataset.py</code> <pre><code>class DatasetImportV2(BaseModel):\n    \"\"\"\n    Class for `Dataset` import.\n\n    We are dropping `model_config = ConfigDict(extra=\"forbid\")` so that any\n    kind of legacy filters can be included in the payload, and ignored in the\n    API.\n\n    Attributes:\n        name:\n        zarr_dir:\n        images:\n    \"\"\"\n\n    name: str\n    zarr_dir: ZarrDirStr\n    images: list[SingleImage] = Field(default_factory=list)\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/dumps/","title":"dumps","text":"<p>Dump models differ from their Read counterpart in that: * They are directly JSON-able, without any additional encoder. * They may include only a subset of the available fields.</p> <p>These models are used in at least two situations: 1. In the \"*_dump\" attributes of Job models; 2. In the history items, to trim their size.</p>"},{"location":"reference/fractal_server/app/schemas/v2/dumps/#fractal_server.app.schemas.v2.dumps.DatasetDumpV2","title":"<code>DatasetDumpV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>We do not include 'model_config = ConfigDict(extra=\"forbid\")' because legacy data may include 'type_filters' or 'attribute_filters' and we want to avoid response-validation errors.</p> Source code in <code>fractal_server/app/schemas/v2/dumps.py</code> <pre><code>class DatasetDumpV2(BaseModel):\n    \"\"\"\n    We do not include 'model_config = ConfigDict(extra=\"forbid\")' because\n    legacy data may include 'type_filters' or 'attribute_filters' and we\n    want to avoid response-validation errors.\n    \"\"\"\n\n    id: int\n    name: str\n    project_id: int\n    timestamp_created: str\n    zarr_dir: str\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/dumps/#fractal_server.app.schemas.v2.dumps.WorkflowTaskDumpV2","title":"<code>WorkflowTaskDumpV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>We do not include 'model_config = ConfigDict(extra=\"forbid\")' because legacy data may include 'input_filters' field and we want to avoid response-validation errors for the endpoints that GET datasets.</p> Source code in <code>fractal_server/app/schemas/v2/dumps.py</code> <pre><code>class WorkflowTaskDumpV2(BaseModel):\n    \"\"\"\n    We do not include 'model_config = ConfigDict(extra=\"forbid\")'\n    because legacy data may include 'input_filters' field and we want to avoid\n    response-validation errors for the endpoints that GET datasets.\n    \"\"\"\n\n    id: int\n    workflow_id: int\n    order: int | None = None\n\n    type_filters: dict[str, bool]\n\n    task_id: int | None = None\n    task: TaskDumpV2 | None = None\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/history/","title":"history","text":""},{"location":"reference/fractal_server/app/schemas/v2/history/#fractal_server.app.schemas.v2.history.HistoryUnitStatus","title":"<code>HistoryUnitStatus</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available status for images</p> <p>Attributes:</p> Name Type Description <code>SUBMITTED</code> <code>DONE</code> <code>FAILED</code> Source code in <code>fractal_server/app/schemas/v2/history.py</code> <pre><code>class HistoryUnitStatus(StrEnum):\n    \"\"\"\n    Available status for images\n\n    Attributes:\n        SUBMITTED:\n        DONE:\n        FAILED:\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/history/#fractal_server.app.schemas.v2.history.HistoryUnitStatusWithUnset","title":"<code>HistoryUnitStatusWithUnset</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available status for history queries</p> <p>Attributes:</p> Name Type Description <code>SUBMITTED</code> <code>DONE</code> <code>FAILED</code> Source code in <code>fractal_server/app/schemas/v2/history.py</code> <pre><code>class HistoryUnitStatusWithUnset(StrEnum):\n    \"\"\"\n    Available status for history queries\n\n    Attributes:\n        SUBMITTED:\n        DONE:\n        FAILED:\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n    UNSET = \"unset\"\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/job/","title":"job","text":""},{"location":"reference/fractal_server/app/schemas/v2/job/#fractal_server.app.schemas.v2.job.JobStatusTypeV2","title":"<code>JobStatusTypeV2</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the available job statuses</p> <p>Attributes:</p> Name Type Description <code>SUBMITTED</code> <p>The job was created. This does not guarantee that it was also submitted to an executor (e.g. other errors could have prevented this), nor that it is actually running (e.g. SLURM jobs could be still in the queue).</p> <code>DONE</code> <p>The job successfully reached its end.</p> <code>FAILED</code> <p>The workflow terminated with an error.</p> Source code in <code>fractal_server/app/schemas/v2/job.py</code> <pre><code>class JobStatusTypeV2(StrEnum):\n    \"\"\"\n    Define the available job statuses\n\n    Attributes:\n        SUBMITTED:\n            The job was created. This does not guarantee that it was also\n            submitted to an executor (e.g. other errors could have prevented\n            this), nor that it is actually running (e.g. SLURM jobs could be\n            still in the queue).\n        DONE:\n            The job successfully reached its end.\n        FAILED:\n            The workflow terminated with an error.\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/manifest/","title":"manifest","text":""},{"location":"reference/fractal_server/app/schemas/v2/manifest/#fractal_server.app.schemas.v2.manifest.ManifestV2","title":"<code>ManifestV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Packages containing tasks are required to include a special file <code>__FRACTAL_MANIFEST__.json</code> in order to be discovered and used by Fractal.</p> <p>This model class and the model classes it depends on provide the base schema to read, write and validate manifests.</p> <p>Attributes:</p> Name Type Description <code>manifest_version</code> <code>Literal['2']</code> <p>A version string that provides indication for compatibility between manifests as the schema evolves. This is for instance used by Fractal to determine which subclass of the present base class needs be used to read and validate the input.</p> <code>task_list</code> <p>list[TaskManifestType] The list of tasks, represented as specified by subclasses of the _TaskManifestBase (a.k.a. TaskManifestType)</p> <code>has_args_schemas</code> <code>bool</code> <p><code>True</code> if the manifest includes JSON Schemas for the arguments of each task.</p> <code>args_schema_version</code> <code>str | None</code> <p>Label of how <code>args_schema</code>s were generated (e.g. <code>pydantic_v1</code>).</p> Source code in <code>fractal_server/app/schemas/v2/manifest.py</code> <pre><code>class ManifestV2(BaseModel):\n    \"\"\"\n    Packages containing tasks are required to include a special file\n    `__FRACTAL_MANIFEST__.json` in order to be discovered and used by Fractal.\n\n    This model class and the model classes it depends on provide the base\n    schema to read, write and validate manifests.\n\n    Attributes:\n        manifest_version:\n            A version string that provides indication for compatibility between\n            manifests as the schema evolves. This is for instance used by\n            Fractal to determine which subclass of the present base class needs\n            be used to read and validate the input.\n        task_list : list[TaskManifestType]\n            The list of tasks, represented as specified by subclasses of the\n            _TaskManifestBase (a.k.a. TaskManifestType)\n        has_args_schemas:\n            `True` if the manifest includes JSON Schemas for the arguments of\n            each task.\n        args_schema_version:\n            Label of how `args_schema`s were generated (e.g. `pydantic_v1`).\n    \"\"\"\n\n    manifest_version: Literal[\"2\"]\n    task_list: list[TaskManifestV2]\n    has_args_schemas: bool = False\n    args_schema_version: str | None = None\n    authors: NonEmptyStr | None = None\n\n    @model_validator(mode=\"after\")\n    def _check_args_schemas_are_present(self):\n        has_args_schemas = self.has_args_schemas\n        task_list = self.task_list\n        if has_args_schemas is True:\n            for task in task_list:\n                if task.executable_parallel is not None:\n                    if task.args_schema_parallel is None:\n                        raise ValueError(\n                            f\"Manifest has {has_args_schemas=}, but \"\n                            f\"task '{task.name}' has \"\n                            f\"{task.args_schema_parallel=}.\"\n                        )\n                if task.executable_non_parallel is not None:\n                    if task.args_schema_non_parallel is None:\n                        raise ValueError(\n                            f\"Manifest has {has_args_schemas=}, but \"\n                            f\"task '{task.name}' has \"\n                            f\"{task.args_schema_non_parallel=}.\"\n                        )\n        return self\n\n    @model_validator(mode=\"after\")\n    def _unique_task_names(self):\n        task_list = self.task_list\n        task_list_names = [t.name for t in task_list]\n        if len(set(task_list_names)) != len(task_list_names):\n            raise ValueError(\n                (\n                    \"Task names in manifest must be unique.\\n\",\n                    f\"Given: {task_list_names}.\",\n                )\n            )\n        return self\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/manifest/#fractal_server.app.schemas.v2.manifest.TaskManifestV2","title":"<code>TaskManifestV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a task within a V2 manifest.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The task name</p> <code>executable</code> <code>str</code> <p>Path to the executable relative to the package root</p> <p>Note: by package root we mean \"as it will be installed\". If a package <code>Pkg</code> installs in the folder <code>pkg</code> the executable <code>pkg/executable.py</code>, this attribute must contain only <code>executable.py</code>.</p> <code>input_type</code> <code>str</code> <p>The input type accepted by the task</p> <code>output_type</code> <code>str</code> <p>The output type returned by the task</p> <code>meta</code> <code>str</code> <p>Additional information about the package, such as hash of the executable, specific runtime requirements (e.g., need_gpu=True), etc.</p> <code>args_schema</code> <code>str</code> <p>JSON Schema for task arguments</p> <code>docs_info</code> <code>str | None</code> <p>Additional information about the Task, coming from the docstring.</p> <code>docs_link</code> <code>HttpUrlStr | None</code> <p>Link to Task docs.</p> Source code in <code>fractal_server/app/schemas/v2/manifest.py</code> <pre><code>class TaskManifestV2(BaseModel):\n    \"\"\"\n    Represents a task within a V2 manifest.\n\n    Attributes:\n        name:\n            The task name\n        executable:\n            Path to the executable relative to the package root\n\n            Note: by package root we mean \"as it will be installed\". If a\n            package `Pkg` installs in the folder `pkg` the executable\n            `pkg/executable.py`, this attribute must contain only\n            `executable.py`.\n        input_type:\n            The input type accepted by the task\n        output_type:\n            The output type returned by the task\n        meta:\n            Additional information about the package, such as hash of the\n            executable, specific runtime requirements (e.g., need_gpu=True),\n            etc.\n        args_schema:\n            JSON Schema for task arguments\n        docs_info:\n            Additional information about the Task, coming from the docstring.\n        docs_link:\n            Link to Task docs.\n    \"\"\"\n\n    name: str\n    executable_non_parallel: str | None = None\n    executable_parallel: str | None = None\n    input_types: dict[str, bool] = Field(default_factory=dict)\n    output_types: dict[str, bool] = Field(default_factory=dict)\n    meta_non_parallel: DictStrAny = Field(default_factory=dict)\n    meta_parallel: DictStrAny = Field(default_factory=dict)\n    args_schema_non_parallel: DictStrAny | None = None\n    args_schema_parallel: DictStrAny | None = None\n    docs_info: str | None = None\n    docs_link: HttpUrlStr | None = None\n\n    category: str | None = None\n    modality: str | None = None\n    tags: list[str] = Field(default_factory=list)\n\n    type: None | TaskType = None\n\n    @model_validator(mode=\"after\")\n    def validate_executable_args_meta(self):\n        executable_non_parallel = self.executable_non_parallel\n        executable_parallel = self.executable_parallel\n        if (executable_non_parallel is None) and (executable_parallel is None):\n            raise ValueError(\n                \"`TaskManifestV2.executable_non_parallel` and \"\n                \"`TaskManifestV2.executable_parallel` cannot be both None.\"\n            )\n\n        elif executable_non_parallel is None:\n            meta_non_parallel = self.meta_non_parallel\n            if meta_non_parallel != {}:\n                raise ValueError(\n                    \"`TaskManifestV2.meta_non_parallel` must be an empty dict \"\n                    \"if `TaskManifestV2.executable_non_parallel` is None. \"\n                    f\"Given: {meta_non_parallel}.\"\n                )\n\n            args_schema_non_parallel = self.args_schema_non_parallel\n            if args_schema_non_parallel is not None:\n                raise ValueError(\n                    \"`TaskManifestV2.args_schema_non_parallel` must be None \"\n                    \"if `TaskManifestV2.executable_non_parallel` is None. \"\n                    f\"Given: {args_schema_non_parallel}.\"\n                )\n\n        elif executable_parallel is None:\n            meta_parallel = self.meta_parallel\n            if meta_parallel != {}:\n                raise ValueError(\n                    \"`TaskManifestV2.meta_parallel` must be an empty dict if \"\n                    \"`TaskManifestV2.executable_parallel` is None. \"\n                    f\"Given: {meta_parallel}.\"\n                )\n\n            args_schema_parallel = self.args_schema_parallel\n            if args_schema_parallel is not None:\n                raise ValueError(\n                    \"`TaskManifestV2.args_schema_parallel` must be None if \"\n                    \"`TaskManifestV2.executable_parallel` is None. \"\n                    f\"Given: {args_schema_parallel}.\"\n                )\n\n        return self\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/profile/","title":"profile","text":""},{"location":"reference/fractal_server/app/schemas/v2/profile/#fractal_server.app.schemas.v2.profile.cast_serialize_profile","title":"<code>cast_serialize_profile(_data)</code>","text":"<p>Cast/serialize round-trip for <code>Profile</code> data.</p> <p>We use <code>@validate_call</code> because <code>ProfileeCreate</code> is a <code>Union</code> type and it cannot be instantiated directly.</p> Return <p>Serialized version of a valid profile object.</p> Source code in <code>fractal_server/app/schemas/v2/profile.py</code> <pre><code>@validate_call\ndef cast_serialize_profile(_data: ProfileCreate) -&gt; dict[str, Any]:\n    \"\"\"\n    Cast/serialize round-trip for `Profile` data.\n\n    We use `@validate_call` because `ProfileeCreate` is a `Union` type and it\n    cannot be instantiated directly.\n\n    Return:\n        Serialized version of a valid profile object.\n    \"\"\"\n    return _data.model_dump()\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/project/","title":"project","text":""},{"location":"reference/fractal_server/app/schemas/v2/resource/","title":"resource","text":""},{"location":"reference/fractal_server/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.cast_serialize_pixi_settings","title":"<code>cast_serialize_pixi_settings(v)</code>","text":"<p>Validate current value, and enrich it with default values.</p> Source code in <code>fractal_server/app/schemas/v2/resource.py</code> <pre><code>def cast_serialize_pixi_settings(\n    v: dict[NonEmptyStr, Any],\n) -&gt; dict[NonEmptyStr, Any]:\n    \"\"\"\n    Validate current value, and enrich it with default values.\n    \"\"\"\n    if v != {}:\n        v = TasksPixiSettings(**v).model_dump()\n    return v\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.cast_serialize_resource","title":"<code>cast_serialize_resource(_data)</code>","text":"<p>Cast/serialize round-trip for <code>Resource</code> data.</p> <p>We use <code>@validate_call</code> because <code>ResourceCreate</code> is a <code>Union</code> type and it cannot be instantiated directly.</p> Return <p>Serialized version of a valid resource object.</p> Source code in <code>fractal_server/app/schemas/v2/resource.py</code> <pre><code>@validate_call\ndef cast_serialize_resource(_data: ResourceCreate) -&gt; dict[str, Any]:\n    \"\"\"\n    Cast/serialize round-trip for `Resource` data.\n\n    We use `@validate_call` because `ResourceCreate` is a `Union` type and it\n    cannot be instantiated directly.\n\n    Return:\n        Serialized version of a valid resource object.\n    \"\"\"\n    return _data.model_dump()\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/status_legacy/","title":"status_legacy","text":""},{"location":"reference/fractal_server/app/schemas/v2/status_legacy/#fractal_server.app.schemas.v2.status_legacy.LegacyStatusReadV2","title":"<code>LegacyStatusReadV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response type for the <code>/project/{project_id}/status/</code> endpoint</p> Source code in <code>fractal_server/app/schemas/v2/status_legacy.py</code> <pre><code>class LegacyStatusReadV2(BaseModel):\n    \"\"\"\n    Response type for the\n    `/project/{project_id}/status/` endpoint\n    \"\"\"\n\n    status: dict[\n        str,\n        WorkflowTaskStatusTypeV2,\n    ] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/status_legacy/#fractal_server.app.schemas.v2.status_legacy.WorkflowTaskStatusTypeV2","title":"<code>WorkflowTaskStatusTypeV2</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the available values for the status of a <code>WorkflowTask</code>.</p> <p>This model is used within the <code>Dataset.history</code> attribute, which is constructed in the runner and then used in the API (e.g. in the <code>api/v2/project/{project_id}/dataset/{dataset_id}/status</code> endpoint).</p> <p>Attributes:</p> Name Type Description <code>SUBMITTED</code> <p>The <code>WorkflowTask</code> is part of a running job.</p> <code>DONE</code> <p>The most-recent execution of this <code>WorkflowTask</code> was successful.</p> <code>FAILED</code> <p>The most-recent execution of this <code>WorkflowTask</code> failed.</p> Source code in <code>fractal_server/app/schemas/v2/status_legacy.py</code> <pre><code>class WorkflowTaskStatusTypeV2(StrEnum):\n    \"\"\"\n    Define the available values for the status of a `WorkflowTask`.\n\n    This model is used within the `Dataset.history` attribute, which is\n    constructed in the runner and then used in the API (e.g. in the\n    `api/v2/project/{project_id}/dataset/{dataset_id}/status` endpoint).\n\n    Attributes:\n        SUBMITTED: The `WorkflowTask` is part of a running job.\n        DONE: The most-recent execution of this `WorkflowTask` was successful.\n        FAILED: The most-recent execution of this `WorkflowTask` failed.\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/task/","title":"task","text":""},{"location":"reference/fractal_server/app/schemas/v2/task/#fractal_server.app.schemas.v2.task.TaskType","title":"<code>TaskType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the available task types.</p> Source code in <code>fractal_server/app/schemas/v2/task.py</code> <pre><code>class TaskType(StrEnum):\n    \"\"\"\n    Define the available task types.\n    \"\"\"\n\n    COMPOUND = \"compound\"\n    CONVERTER_COMPOUND = \"converter_compound\"\n    NON_PARALLEL = \"non_parallel\"\n    CONVERTER_NON_PARALLEL = \"converter_non_parallel\"\n    PARALLEL = \"parallel\"\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/task_collection/","title":"task_collection","text":""},{"location":"reference/fractal_server/app/schemas/v2/task_collection/#fractal_server.app.schemas.v2.task_collection.FractalUploadedFile","title":"<code>FractalUploadedFile</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for data sent from the endpoint to the background task.</p> Source code in <code>fractal_server/app/schemas/v2/task_collection.py</code> <pre><code>class FractalUploadedFile(BaseModel):\n    \"\"\"\n    Model for data sent from the endpoint to the background task.\n    \"\"\"\n\n    filename: str\n    contents: bytes\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/task_collection/#fractal_server.app.schemas.v2.task_collection.TaskCollectCustomV2","title":"<code>TaskCollectCustomV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>manifest</code> <code>ManifestV2</code> <p>Manifest of a Fractal task package (this is typically the content of <code>__FRACTAL_MANIFEST__.json</code>).</p> <code>python_interpreter</code> <code>AbsolutePathStr</code> <p>Absolute path to the Python interpreter to be used for running tasks.</p> <code>name</code> <code>AbsolutePathStr</code> <p>A name identifying this package, that will fill the <code>TaskGroupV2.pkg_name</code> column.</p> <code>package_root</code> <code>AbsolutePathStr | None</code> <p>The folder where the package is installed. If not provided, it will be extracted via <code>pip show</code> (requires <code>package_name</code> to be set).</p> <code>package_name</code> <code>NonEmptyStr | None</code> <p>Name of the package, as used for <code>import &lt;package_name&gt;</code>; this is then used to extract the package directory (<code>package_root</code>) via <code>pip show &lt;package_name&gt;</code>.</p> <code>version</code> <code>NonEmptyStr | None</code> <p>Optional version of tasks to be collected.</p> Source code in <code>fractal_server/app/schemas/v2/task_collection.py</code> <pre><code>class TaskCollectCustomV2(BaseModel):\n    \"\"\"\n    Attributes:\n        manifest: Manifest of a Fractal task package (this is typically the\n            content of `__FRACTAL_MANIFEST__.json`).\n        python_interpreter: Absolute path to the Python interpreter to be used\n            for running tasks.\n        name: A name identifying this package, that will fill the\n            `TaskGroupV2.pkg_name` column.\n        package_root: The folder where the package is installed.\n            If not provided, it will be extracted via `pip show`\n            (requires `package_name` to be set).\n        package_name: Name of the package, as used for `import &lt;package_name&gt;`;\n            this is then used to extract the package directory (`package_root`)\n            via `pip show &lt;package_name&gt;`.\n        version: Optional version of tasks to be collected.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    manifest: ManifestV2\n    python_interpreter: AbsolutePathStr\n    label: NonEmptyStr\n    package_root: AbsolutePathStr | None = None\n    package_name: NonEmptyStr | None = None\n    version: NonEmptyStr | None = None\n\n    @field_validator(\"package_name\", mode=\"after\")\n    @classmethod\n    def validate_package_name(cls, value):\n        if value is not None:\n            validate_cmd(value)\n        return value\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def one_of_package_root_or_name(cls, values):\n        package_root = values.get(\"package_root\")\n        package_name = values.get(\"package_name\")\n        if (package_root is None and package_name is None) or (\n            package_root is not None and package_name is not None\n        ):\n            raise ValueError(\n                \"One and only one must be set between \"\n                \"'package_root' and 'package_name'\"\n            )\n        return values\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/task_collection/#fractal_server.app.schemas.v2.task_collection.TaskCollectPipV2","title":"<code>TaskCollectPipV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>TaskCollectPipV2 class</p> <p>This class only encodes the attributes required to trigger a task-collection operation. Other attributes (that are assigned during task collection) are defined as part of fractal-server.</p> <p>Two cases are supported:</p> <pre><code>1. `package` is the name of a package that can be installed via `pip`.\n1. `package=None`, and a wheel file is uploaded within the API request.\n</code></pre> <p>Attributes:</p> Name Type Description <code>package</code> <code>NonEmptyStr | None</code> <p>The name of a <code>pip</code>-installable package, or <code>None</code>.</p> <code>package_version</code> <code>NonEmptyStr | None</code> <p>Version of the package</p> <code>package_extras</code> <code>NonEmptyStr | None</code> <p>Package extras to include in the <code>pip install</code> command</p> <code>python_version</code> <code>Literal['3.9', '3.10', '3.11', '3.12', '3.13'] | None</code> <p>Python version to install and run the package tasks</p> <code>pinned_package_versions_pre</code> <code>DictStrStr | None</code> <p>dictionary 'package':'version' used to pre-pin versions for specific packages.</p> <code>pinned_package_versions_post</code> <code>DictStrStr | None</code> <p>dictionary 'package':'version' used to post-pin versions for specific packages.</p> Source code in <code>fractal_server/app/schemas/v2/task_collection.py</code> <pre><code>class TaskCollectPipV2(BaseModel):\n    \"\"\"\n    TaskCollectPipV2 class\n\n    This class only encodes the attributes required to trigger a\n    task-collection operation. Other attributes (that are assigned *during*\n    task collection) are defined as part of fractal-server.\n\n    Two cases are supported:\n\n        1. `package` is the name of a package that can be installed via `pip`.\n        1. `package=None`, and a wheel file is uploaded within the API request.\n\n    Attributes:\n        package: The name of a `pip`-installable package, or `None`.\n        package_version: Version of the package\n        package_extras: Package extras to include in the `pip install` command\n        python_version: Python version to install and run the package tasks\n        pinned_package_versions_pre:\n            dictionary 'package':'version' used to pre-pin versions for\n            specific packages.\n        pinned_package_versions_post:\n            dictionary 'package':'version' used to post-pin versions for\n            specific packages.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    package: NonEmptyStr | None = None\n    package_version: NonEmptyStr | None = None\n    package_extras: NonEmptyStr | None = None\n    python_version: Literal[\n        \"3.9\", \"3.10\", \"3.11\", \"3.12\", \"3.13\"\n    ] | None = None\n    pinned_package_versions_pre: DictStrStr | None = None\n    pinned_package_versions_post: DictStrStr | None = None\n\n    @field_validator(\n        \"package\",\n        \"package_version\",\n        \"package_extras\",\n        mode=\"after\",\n    )\n    @classmethod\n    def validate_commands(cls, value):\n        if value is not None:\n            validate_cmd(value)\n        return value\n\n    @field_validator(\n        \"pinned_package_versions_pre\",\n        \"pinned_package_versions_post\",\n        mode=\"after\",\n    )\n    @classmethod\n    def validate_pinned_package_versions(cls, value):\n        if value is not None:\n            for pkg, version in value.items():\n                validate_cmd(pkg)\n                validate_cmd(version)\n        return value\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/task_group/","title":"task_group","text":""},{"location":"reference/fractal_server/app/schemas/v2/task_group/#fractal_server.app.schemas.v2.task_group.TaskGroupCreateV2Strict","title":"<code>TaskGroupCreateV2Strict</code>","text":"<p>               Bases: <code>TaskGroupCreateV2</code></p> <p>A strict version of TaskGroupCreateV2, to be used for task collection.</p> Source code in <code>fractal_server/app/schemas/v2/task_group.py</code> <pre><code>class TaskGroupCreateV2Strict(TaskGroupCreateV2):\n    \"\"\"\n    A strict version of TaskGroupCreateV2, to be used for task collection.\n    \"\"\"\n\n    path: AbsolutePathStr\n    version: NonEmptyStr\n    venv_path: AbsolutePathStr\n    python_version: NonEmptyStr\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/workflow/","title":"workflow","text":""},{"location":"reference/fractal_server/app/schemas/v2/workflow/#fractal_server.app.schemas.v2.workflow.WorkflowExportV2","title":"<code>WorkflowExportV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for <code>Workflow</code> export.</p> <p>Attributes:</p> Name Type Description <code>task_list</code> <code>list[WorkflowTaskExportV2]</code> Source code in <code>fractal_server/app/schemas/v2/workflow.py</code> <pre><code>class WorkflowExportV2(BaseModel):\n    \"\"\"\n    Class for `Workflow` export.\n\n    Attributes:\n        task_list:\n    \"\"\"\n\n    name: str\n    task_list: list[WorkflowTaskExportV2]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/workflow/#fractal_server.app.schemas.v2.workflow.WorkflowImportV2","title":"<code>WorkflowImportV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for <code>Workflow</code> import.</p> <p>Attributes:</p> Name Type Description <code>task_list</code> <code>list[WorkflowTaskImportV2]</code> Source code in <code>fractal_server/app/schemas/v2/workflow.py</code> <pre><code>class WorkflowImportV2(BaseModel):\n    \"\"\"\n    Class for `Workflow` import.\n\n    Attributes:\n        task_list:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    name: NonEmptyStr\n    task_list: list[WorkflowTaskImportV2]\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/workflowtask/","title":"workflowtask","text":""},{"location":"reference/fractal_server/app/schemas/v2/workflowtask/#fractal_server.app.schemas.v2.workflowtask.WorkflowTaskImportV2","title":"<code>WorkflowTaskImportV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>fractal_server/app/schemas/v2/workflowtask.py</code> <pre><code>class WorkflowTaskImportV2(BaseModel):\n    model_config = ConfigDict(extra=\"forbid\")\n\n    meta_non_parallel: DictStrAny | None = None\n    meta_parallel: DictStrAny | None = None\n    args_non_parallel: DictStrAny | None = None\n    args_parallel: DictStrAny | None = None\n    type_filters: TypeFilters | None = None\n    input_filters: dict[str, Any] | None = None\n\n    task: TaskImportV2 | TaskImportV2Legacy\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def update_legacy_filters(cls, values: dict):\n        \"\"\"\n        Transform legacy filters (created with fractal-server&lt;2.11.0)\n        into type filters\n        \"\"\"\n        if values.get(\"input_filters\") is not None:\n            if \"type_filters\" in values.keys():\n                raise ValueError(\n                    \"Cannot set filters both through the legacy field \"\n                    \"('filters') and the new one ('type_filters').\"\n                )\n            else:\n                # As of 2.11.0, WorkflowTask do not have attribute filters\n                # any more.\n                if values[\"input_filters\"][\"attributes\"] != {}:\n                    raise ValueError(\n                        \"Cannot set attribute filters for WorkflowTasks.\"\n                    )\n                # Convert legacy filters.types into new type_filters\n                values[\"type_filters\"] = values[\"input_filters\"].get(\n                    \"types\", {}\n                )\n                values[\"input_filters\"] = None\n\n        return values\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/workflowtask/#fractal_server.app.schemas.v2.workflowtask.WorkflowTaskImportV2.update_legacy_filters","title":"<code>update_legacy_filters(values)</code>  <code>classmethod</code>","text":"<p>Transform legacy filters (created with fractal-server&lt;2.11.0) into type filters</p> Source code in <code>fractal_server/app/schemas/v2/workflowtask.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef update_legacy_filters(cls, values: dict):\n    \"\"\"\n    Transform legacy filters (created with fractal-server&lt;2.11.0)\n    into type filters\n    \"\"\"\n    if values.get(\"input_filters\") is not None:\n        if \"type_filters\" in values.keys():\n            raise ValueError(\n                \"Cannot set filters both through the legacy field \"\n                \"('filters') and the new one ('type_filters').\"\n            )\n        else:\n            # As of 2.11.0, WorkflowTask do not have attribute filters\n            # any more.\n            if values[\"input_filters\"][\"attributes\"] != {}:\n                raise ValueError(\n                    \"Cannot set attribute filters for WorkflowTasks.\"\n                )\n            # Convert legacy filters.types into new type_filters\n            values[\"type_filters\"] = values[\"input_filters\"].get(\n                \"types\", {}\n            )\n            values[\"input_filters\"] = None\n\n    return values\n</code></pre>"},{"location":"reference/fractal_server/app/schemas/v2/workflowtask/#fractal_server.app.schemas.v2.workflowtask.WorkflowTaskReplaceV2","title":"<code>WorkflowTaskReplaceV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Used by 'replace-task' endpoint</p> Source code in <code>fractal_server/app/schemas/v2/workflowtask.py</code> <pre><code>class WorkflowTaskReplaceV2(BaseModel):\n    \"\"\"Used by 'replace-task' endpoint\"\"\"\n\n    args_non_parallel: dict[str, Any] | None = None\n    args_parallel: dict[str, Any] | None = None\n</code></pre>"},{"location":"reference/fractal_server/app/security/","title":"security","text":"<p>Auth subsystem</p> <p>This module implements the authorisation/authentication subsystem of the Fractal Server. It is based on the FastAPI Users library with support for the SQLModel database adapter.</p> <p>In particular, this module links the appropriate database models, sets up FastAPIUsers with Barer Token and cookie transports and register local routes. Then, for each OAuth client defined in the Fractal Settings configuration, it registers the client and the relative routes.</p> <p>All routes are registered under the <code>auth/</code> prefix.</p>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync","title":"<code>SQLModelUserDatabaseAsync</code>","text":"<p>               Bases: <code>Generic[UP, ID]</code>, <code>BaseUserDatabase[UP, ID]</code></p> <p>This class is from fastapi_users_db_sqlmodel Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence</p> <p>Database adapter for SQLModel working purely asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>user_model</code> <code>type[UP]</code> <p>SQLModel model of a DB representation of a user.</p> required <code>session</code> <code>AsyncSession</code> <p>SQLAlchemy async session.</p> required Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>class SQLModelUserDatabaseAsync(Generic[UP, ID], BaseUserDatabase[UP, ID]):\n    \"\"\"\n    This class is from fastapi_users_db_sqlmodel\n    Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence\n\n    Database adapter for SQLModel working purely asynchronously.\n\n    Args:\n        user_model: SQLModel model of a DB representation of a user.\n        session: SQLAlchemy async session.\n    \"\"\"\n\n    session: AsyncSession\n    user_model: type[UP]\n    oauth_account_model: type[OAuthAccount] | None = None\n\n    def __init__(\n        self,\n        session: AsyncSession,\n        user_model: type[UP],\n        oauth_account_model: type[OAuthAccount] | None = None,\n    ):\n        self.session = session\n        self.user_model = user_model\n        self.oauth_account_model = oauth_account_model\n\n    async def get(self, id: ID) -&gt; UP | None:\n        \"\"\"Get a single user by id.\"\"\"\n        return await self.session.get(self.user_model, id)\n\n    async def get_by_email(self, email: str) -&gt; UP | None:\n        \"\"\"Get a single user by email.\"\"\"\n        statement = select(self.user_model).where(\n            func.lower(self.user_model.email) == func.lower(email)\n        )\n        results = await self.session.execute(statement)\n        object = results.first()\n        if object is None:\n            return None\n        return object[0]\n\n    async def get_by_oauth_account(\n        self, oauth: str, account_id: str\n    ) -&gt; UP | None:  # noqa\n        \"\"\"Get a single user by OAuth account id.\"\"\"\n        if self.oauth_account_model is None:\n            raise NotImplementedError()\n        statement = (\n            select(self.oauth_account_model)\n            .where(self.oauth_account_model.oauth_name == oauth)\n            .where(self.oauth_account_model.account_id == account_id)\n            .options(selectinload(self.oauth_account_model.user))  # type: ignore  # noqa\n        )\n        results = await self.session.execute(statement)\n        oauth_account = results.first()\n        if oauth_account:\n            user = oauth_account[0].user  # type: ignore\n            return user\n        return None\n\n    async def create(self, create_dict: dict[str, Any]) -&gt; UP:\n        \"\"\"Create a user.\"\"\"\n        user = self.user_model(**create_dict)\n        self.session.add(user)\n        await self.session.commit()\n        await self.session.refresh(user)\n        return user\n\n    async def update(self, user: UP, update_dict: dict[str, Any]) -&gt; UP:\n        for key, value in update_dict.items():\n            setattr(user, key, value)\n        self.session.add(user)\n        await self.session.commit()\n        await self.session.refresh(user)\n        return user\n\n    async def delete(self, user: UP) -&gt; None:\n        await self.session.delete(user)\n        await self.session.commit()\n\n    async def add_oauth_account(\n        self, user: UP, create_dict: dict[str, Any]\n    ) -&gt; UP:  # noqa\n        if self.oauth_account_model is None:\n            raise NotImplementedError()\n\n        oauth_account = self.oauth_account_model(**create_dict)\n        user.oauth_accounts.append(oauth_account)  # type: ignore\n        self.session.add(user)\n\n        await self.session.commit()\n\n        return user\n\n    async def update_oauth_account(\n        self, user: UP, oauth_account: OAP, update_dict: dict[str, Any]\n    ) -&gt; UP:\n        if self.oauth_account_model is None:\n            raise NotImplementedError()\n\n        for key, value in update_dict.items():\n            setattr(oauth_account, key, value)\n        self.session.add(oauth_account)\n        await self.session.commit()\n\n        return user\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.create","title":"<code>create(create_dict)</code>  <code>async</code>","text":"<p>Create a user.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def create(self, create_dict: dict[str, Any]) -&gt; UP:\n    \"\"\"Create a user.\"\"\"\n    user = self.user_model(**create_dict)\n    self.session.add(user)\n    await self.session.commit()\n    await self.session.refresh(user)\n    return user\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.get","title":"<code>get(id)</code>  <code>async</code>","text":"<p>Get a single user by id.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def get(self, id: ID) -&gt; UP | None:\n    \"\"\"Get a single user by id.\"\"\"\n    return await self.session.get(self.user_model, id)\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.get_by_email","title":"<code>get_by_email(email)</code>  <code>async</code>","text":"<p>Get a single user by email.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def get_by_email(self, email: str) -&gt; UP | None:\n    \"\"\"Get a single user by email.\"\"\"\n    statement = select(self.user_model).where(\n        func.lower(self.user_model.email) == func.lower(email)\n    )\n    results = await self.session.execute(statement)\n    object = results.first()\n    if object is None:\n        return None\n    return object[0]\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.get_by_oauth_account","title":"<code>get_by_oauth_account(oauth, account_id)</code>  <code>async</code>","text":"<p>Get a single user by OAuth account id.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def get_by_oauth_account(\n    self, oauth: str, account_id: str\n) -&gt; UP | None:  # noqa\n    \"\"\"Get a single user by OAuth account id.\"\"\"\n    if self.oauth_account_model is None:\n        raise NotImplementedError()\n    statement = (\n        select(self.oauth_account_model)\n        .where(self.oauth_account_model.oauth_name == oauth)\n        .where(self.oauth_account_model.account_id == account_id)\n        .options(selectinload(self.oauth_account_model.user))  # type: ignore  # noqa\n    )\n    results = await self.session.execute(statement)\n    oauth_account = results.first()\n    if oauth_account:\n        user = oauth_account[0].user  # type: ignore\n        return user\n    return None\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.UserManager","title":"<code>UserManager</code>","text":"<p>               Bases: <code>IntegerIDMixin</code>, <code>BaseUserManager[UserOAuth, int]</code></p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>class UserManager(IntegerIDMixin, BaseUserManager[UserOAuth, int]):\n    def __init__(self, user_db):\n        \"\"\"\n        Override `__init__` of `BaseUserManager` to define custom\n        `password_helper`.\n        \"\"\"\n        super().__init__(\n            user_db=user_db,\n            password_helper=password_helper,\n        )\n\n    async def validate_password(self, password: str, user: UserOAuth) -&gt; None:\n        # check password length\n        min_length = 4\n        max_length = 100\n        if len(password) &lt; min_length:\n            raise InvalidPasswordException(\n                f\"The password is too short (minimum length: {min_length}).\"\n            )\n        elif len(password) &gt; max_length:\n            raise InvalidPasswordException(\n                f\"The password is too long (maximum length: {min_length}).\"\n            )\n\n    async def oauth_callback(\n        self: Self,\n        oauth_name: str,\n        access_token: str,\n        account_id: str,\n        account_email: str,\n        expires_at: int | None = None,\n        refresh_token: str | None = None,\n        request: Request | None = None,\n        *,\n        associate_by_email: bool = False,\n        is_verified_by_default: bool = False,\n    ) -&gt; UserOAuth:\n        \"\"\"\n        Handle the callback after a successful OAuth authentication.\n\n        This method extends the corresponding `BaseUserManager` method of\n        &gt; fastapi-users v14.0.1, Copyright (c) 2019 Fran\u00e7ois Voron, MIT License\n\n        If the user already exists with this OAuth account, the token is\n        updated.\n\n        If a user with the same e-mail already exists and `associate_by_email`\n        is True, the OAuth account is associated to this user.\n        Otherwise, the `UserNotExists` exception is raised.\n\n        If the user does not exist, send an email to the Fractal admins (if\n        configured) and respond with a 400 error status. NOTE: This is the\n        function branch where the `fractal-server` implementation deviates\n        from the original `fastapi-users` one.\n\n        :param oauth_name: Name of the OAuth client.\n        :param access_token: Valid access token for the service provider.\n        :param account_id: models.ID of the user on the service provider.\n        :param account_email: E-mail of the user on the service provider.\n        :param expires_at: Optional timestamp at which the access token\n        expires.\n        :param refresh_token: Optional refresh token to get a\n        fresh access token from the service provider.\n        :param request: Optional FastAPI request that\n        triggered the operation, defaults to None\n        :param associate_by_email: If True, any existing user with the same\n        e-mail address will be associated to this user. Defaults to False.\n        :param is_verified_by_default: If True, the `is_verified` flag will be\n        set to `True` on newly created user. Make sure the OAuth Provider you\n        are using does verify the email address before enabling this flag.\n        Defaults to False.\n        :return: A user.\n        \"\"\"\n        from fastapi import HTTPException\n        from fastapi import status\n        from fastapi_users import exceptions\n\n        oauth_account_dict = {\n            \"oauth_name\": oauth_name,\n            \"access_token\": access_token,\n            \"account_id\": account_id,\n            \"account_email\": account_email,\n            \"expires_at\": expires_at,\n            \"refresh_token\": refresh_token,\n        }\n\n        try:\n            user = await self.get_by_oauth_account(oauth_name, account_id)\n        except exceptions.UserNotExists:\n            try:\n                # Associate account\n                user = await self.get_by_email(account_email)\n                if not associate_by_email:\n                    raise exceptions.UserAlreadyExists()\n                user = await self.user_db.add_oauth_account(\n                    user, oauth_account_dict\n                )\n            except exceptions.UserNotExists:\n                # (0) Log\n                logger.warning(\n                    f\"Self-registration attempt by {account_email}.\"\n                )\n\n                # (1) Prepare user-facing error message\n                error_msg = (\n                    \"Thank you for registering for the Fractal service. \"\n                    \"Administrators have been informed to configure your \"\n                    \"account and will get back to you.\"\n                )\n                settings = Inject(get_settings)\n                if settings.FRACTAL_HELP_URL is not None:\n                    error_msg = (\n                        f\"{error_msg}\\n\"\n                        \"You can find more information about the onboarding \"\n                        f\"process at {settings.FRACTAL_HELP_URL}.\"\n                    )\n\n                # (2) Send email to admins\n                email_settings = Inject(get_email_settings)\n                send_fractal_email_or_log_failure(\n                    subject=\"New OAuth self-registration\",\n                    msg=(\n                        f\"User '{account_email}' tried to \"\n                        \"self-register through OAuth.\\n\"\n                        \"Please create the Fractal account manually.\\n\"\n                        \"Here is the error message displayed to the \"\n                        f\"user:\\n{error_msg}\"\n                    ),\n                    email_settings=email_settings.public,\n                )\n\n                # (3) Raise\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=error_msg,\n                )\n        else:\n            # Update oauth\n            for existing_oauth_account in user.oauth_accounts:\n                if (\n                    existing_oauth_account.account_id == account_id\n                    and existing_oauth_account.oauth_name == oauth_name\n                ):\n                    user = await self.user_db.update_oauth_account(\n                        user, existing_oauth_account, oauth_account_dict\n                    )\n\n        return user\n\n    async def on_after_register(\n        self, user: UserOAuth, request: Request | None = None\n    ):\n        settings = Inject(get_settings)\n        logger.info(\n            f\"New-user registration completed ({user.id=}, {user.email=}).\"\n        )\n        async for db in get_async_db():\n            # Note: if `FRACTAL_DEFAULT_GROUP_NAME=None`, this query will\n            # result into `None`\n            settings = Inject(get_settings)\n            stm = select(UserGroup.id).where(\n                UserGroup.name == settings.FRACTAL_DEFAULT_GROUP_NAME\n            )\n            res = await db.execute(stm)\n            default_group_id_or_none = res.scalars().one_or_none()\n            if default_group_id_or_none is not None:\n                link = LinkUserGroup(\n                    user_id=user.id, group_id=default_group_id_or_none\n                )\n                db.add(link)\n                await db.commit()\n                logger.info(\n                    f\"Added {user.email} user to group \"\n                    f\"{default_group_id_or_none=}.\"\n                )\n            elif settings.FRACTAL_DEFAULT_GROUP_NAME is not None:\n                logger.error(\n                    \"No group found with name \"\n                    f\"{settings.FRACTAL_DEFAULT_GROUP_NAME}\"\n                )\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.UserManager.__init__","title":"<code>__init__(user_db)</code>","text":"<p>Override <code>__init__</code> of <code>BaseUserManager</code> to define custom <code>password_helper</code>.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>def __init__(self, user_db):\n    \"\"\"\n    Override `__init__` of `BaseUserManager` to define custom\n    `password_helper`.\n    \"\"\"\n    super().__init__(\n        user_db=user_db,\n        password_helper=password_helper,\n    )\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security.UserManager.oauth_callback","title":"<code>oauth_callback(oauth_name, access_token, account_id, account_email, expires_at=None, refresh_token=None, request=None, *, associate_by_email=False, is_verified_by_default=False)</code>  <code>async</code>","text":"<p>Handle the callback after a successful OAuth authentication.</p> <p>This method extends the corresponding <code>BaseUserManager</code> method of</p> <p>fastapi-users v14.0.1, Copyright (c) 2019 Fran\u00e7ois Voron, MIT License</p> <p>If the user already exists with this OAuth account, the token is updated.</p> <p>If a user with the same e-mail already exists and <code>associate_by_email</code> is True, the OAuth account is associated to this user. Otherwise, the <code>UserNotExists</code> exception is raised.</p> <p>If the user does not exist, send an email to the Fractal admins (if configured) and respond with a 400 error status. NOTE: This is the function branch where the <code>fractal-server</code> implementation deviates from the original <code>fastapi-users</code> one.</p> <p>:param oauth_name: Name of the OAuth client. :param access_token: Valid access token for the service provider. :param account_id: models.ID of the user on the service provider. :param account_email: E-mail of the user on the service provider. :param expires_at: Optional timestamp at which the access token expires. :param refresh_token: Optional refresh token to get a fresh access token from the service provider. :param request: Optional FastAPI request that triggered the operation, defaults to None :param associate_by_email: If True, any existing user with the same e-mail address will be associated to this user. Defaults to False. :param is_verified_by_default: If True, the <code>is_verified</code> flag will be set to <code>True</code> on newly created user. Make sure the OAuth Provider you are using does verify the email address before enabling this flag. Defaults to False. :return: A user.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def oauth_callback(\n    self: Self,\n    oauth_name: str,\n    access_token: str,\n    account_id: str,\n    account_email: str,\n    expires_at: int | None = None,\n    refresh_token: str | None = None,\n    request: Request | None = None,\n    *,\n    associate_by_email: bool = False,\n    is_verified_by_default: bool = False,\n) -&gt; UserOAuth:\n    \"\"\"\n    Handle the callback after a successful OAuth authentication.\n\n    This method extends the corresponding `BaseUserManager` method of\n    &gt; fastapi-users v14.0.1, Copyright (c) 2019 Fran\u00e7ois Voron, MIT License\n\n    If the user already exists with this OAuth account, the token is\n    updated.\n\n    If a user with the same e-mail already exists and `associate_by_email`\n    is True, the OAuth account is associated to this user.\n    Otherwise, the `UserNotExists` exception is raised.\n\n    If the user does not exist, send an email to the Fractal admins (if\n    configured) and respond with a 400 error status. NOTE: This is the\n    function branch where the `fractal-server` implementation deviates\n    from the original `fastapi-users` one.\n\n    :param oauth_name: Name of the OAuth client.\n    :param access_token: Valid access token for the service provider.\n    :param account_id: models.ID of the user on the service provider.\n    :param account_email: E-mail of the user on the service provider.\n    :param expires_at: Optional timestamp at which the access token\n    expires.\n    :param refresh_token: Optional refresh token to get a\n    fresh access token from the service provider.\n    :param request: Optional FastAPI request that\n    triggered the operation, defaults to None\n    :param associate_by_email: If True, any existing user with the same\n    e-mail address will be associated to this user. Defaults to False.\n    :param is_verified_by_default: If True, the `is_verified` flag will be\n    set to `True` on newly created user. Make sure the OAuth Provider you\n    are using does verify the email address before enabling this flag.\n    Defaults to False.\n    :return: A user.\n    \"\"\"\n    from fastapi import HTTPException\n    from fastapi import status\n    from fastapi_users import exceptions\n\n    oauth_account_dict = {\n        \"oauth_name\": oauth_name,\n        \"access_token\": access_token,\n        \"account_id\": account_id,\n        \"account_email\": account_email,\n        \"expires_at\": expires_at,\n        \"refresh_token\": refresh_token,\n    }\n\n    try:\n        user = await self.get_by_oauth_account(oauth_name, account_id)\n    except exceptions.UserNotExists:\n        try:\n            # Associate account\n            user = await self.get_by_email(account_email)\n            if not associate_by_email:\n                raise exceptions.UserAlreadyExists()\n            user = await self.user_db.add_oauth_account(\n                user, oauth_account_dict\n            )\n        except exceptions.UserNotExists:\n            # (0) Log\n            logger.warning(\n                f\"Self-registration attempt by {account_email}.\"\n            )\n\n            # (1) Prepare user-facing error message\n            error_msg = (\n                \"Thank you for registering for the Fractal service. \"\n                \"Administrators have been informed to configure your \"\n                \"account and will get back to you.\"\n            )\n            settings = Inject(get_settings)\n            if settings.FRACTAL_HELP_URL is not None:\n                error_msg = (\n                    f\"{error_msg}\\n\"\n                    \"You can find more information about the onboarding \"\n                    f\"process at {settings.FRACTAL_HELP_URL}.\"\n                )\n\n            # (2) Send email to admins\n            email_settings = Inject(get_email_settings)\n            send_fractal_email_or_log_failure(\n                subject=\"New OAuth self-registration\",\n                msg=(\n                    f\"User '{account_email}' tried to \"\n                    \"self-register through OAuth.\\n\"\n                    \"Please create the Fractal account manually.\\n\"\n                    \"Here is the error message displayed to the \"\n                    f\"user:\\n{error_msg}\"\n                ),\n                email_settings=email_settings.public,\n            )\n\n            # (3) Raise\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=error_msg,\n            )\n    else:\n        # Update oauth\n        for existing_oauth_account in user.oauth_accounts:\n            if (\n                existing_oauth_account.account_id == account_id\n                and existing_oauth_account.oauth_name == oauth_name\n            ):\n                user = await self.user_db.update_oauth_account(\n                    user, existing_oauth_account, oauth_account_dict\n                )\n\n    return user\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security._create_first_group","title":"<code>_create_first_group()</code>","text":"<p>Create a <code>UserGroup</code> named <code>FRACTAL_DEFAULT_GROUP_NAME</code>, if this variable is set and if such a group does not already exist.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>def _create_first_group():\n    \"\"\"\n    Create a `UserGroup` named `FRACTAL_DEFAULT_GROUP_NAME`, if this variable\n    is set and if such a group does not already exist.\n    \"\"\"\n    settings = Inject(get_settings)\n    function_logger = set_logger(\"fractal_server.create_first_group\")\n\n    if settings.FRACTAL_DEFAULT_GROUP_NAME is None:\n        function_logger.info(\n            f\"SKIP because '{settings.FRACTAL_DEFAULT_GROUP_NAME=}'\"\n        )\n        return\n\n    function_logger.info(\n        f\"START, name '{settings.FRACTAL_DEFAULT_GROUP_NAME}'\"\n    )\n    with next(get_sync_db()) as db:\n        group_all = db.execute(\n            select(UserGroup).where(\n                UserGroup.name == settings.FRACTAL_DEFAULT_GROUP_NAME\n            )\n        )\n        if group_all.scalars().one_or_none() is None:\n            first_group = UserGroup(name=settings.FRACTAL_DEFAULT_GROUP_NAME)\n            db.add(first_group)\n            db.commit()\n            function_logger.info(\n                f\"Created group '{settings.FRACTAL_DEFAULT_GROUP_NAME}'\"\n            )\n        else:\n            function_logger.info(\n                f\"Group '{settings.FRACTAL_DEFAULT_GROUP_NAME}' \"\n                \"already exists, skip.\"\n            )\n    function_logger.info(\"END\")\n    close_logger(function_logger)\n</code></pre>"},{"location":"reference/fractal_server/app/security/#fractal_server.app.security._create_first_user","title":"<code>_create_first_user(email, password, project_dir, profile_id=None, is_superuser=False, is_verified=False)</code>  <code>async</code>","text":"<p>Private method to create the first fractal-server user</p> <p>Create a user with the given default arguments and return a message with the relevant information. If the user already exists, for example after a restart, it returns a message to inform that user already exists.</p> <p>WARNING: This function is only meant to create the first user, and then it catches and ignores <code>IntegrityError</code>s (when multiple workers may be trying to concurrently create the first user). This is not the expected behavior for regular user creation, which must rather happen via the /auth/register endpoint.</p> <p>See fastapi_users docs</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>New user's email</p> required <code>password</code> <code>str</code> <p>New user's password</p> required <code>is_superuser</code> <code>bool</code> <p><code>True</code> if the new user is a superuser</p> <code>False</code> <code>is_verified</code> <code>bool</code> <p><code>True</code> if the new user is verified</p> <code>False</code> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def _create_first_user(\n    email: str,\n    password: str,\n    project_dir: str,\n    profile_id: int | None = None,\n    is_superuser: bool = False,\n    is_verified: bool = False,\n) -&gt; None:\n    \"\"\"\n    Private method to create the first fractal-server user\n\n    Create a user with the given default arguments and return a message with\n    the relevant information. If the user already exists, for example after a\n    restart, it returns a message to inform that user already exists.\n\n    **WARNING**: This function is only meant to create the first user, and then\n    it catches and ignores `IntegrityError`s (when multiple workers may be\n    trying to concurrently create the first user). This is not the expected\n    behavior for regular user creation, which must rather happen via the\n    /auth/register endpoint.\n\n    See [fastapi_users docs](https://fastapi-users.github.io/fastapi-users/\n    12.1/cookbook/create-user-programmatically)\n\n    Args:\n        email: New user's email\n        password: New user's password\n        is_superuser: `True` if the new user is a superuser\n        is_verified: `True` if the new user is verified\n    \"\"\"\n    function_logger = set_logger(\"fractal_server.create_first_user\")\n    function_logger.info(f\"START _create_first_user, with email '{email}'\")\n    try:\n        async with get_async_session_context() as session:\n            if is_superuser is True:\n                # If a superuser already exists, exit\n                stm = select(UserOAuth).where(  # noqa\n                    UserOAuth.is_superuser == True  # noqa\n                )  # noqa\n                res = await session.execute(stm)\n                existing_superuser = res.scalars().first()\n                if existing_superuser is not None:\n                    function_logger.info(\n                        f\"'{existing_superuser.email}' superuser already \"\n                        f\"exists, skip creation of '{email}'\"\n                    )\n                    return None\n\n            async with get_user_db_context(session) as user_db:\n                async with get_user_manager_context(user_db) as user_manager:\n                    kwargs = dict(\n                        email=email,\n                        password=password,\n                        project_dir=project_dir,\n                        profile_id=profile_id,\n                        is_superuser=is_superuser,\n                        is_verified=is_verified,\n                    )\n                    user = await user_manager.create(UserCreate(**kwargs))\n                    function_logger.info(f\"User '{user.email}' created\")\n    except UserAlreadyExists:\n        function_logger.warning(f\"User '{email}' already exists\")\n    except Exception as e:\n        function_logger.error(\n            f\"ERROR in _create_first_user, original error {str(e)}\"\n        )\n        raise e\n    finally:\n        function_logger.info(f\"END   _create_first_user, with email '{email}'\")\n</code></pre>"},{"location":"reference/fractal_server/app/security/signup_email/","title":"signup_email","text":""},{"location":"reference/fractal_server/app/security/signup_email/#fractal_server.app.security.signup_email.send_fractal_email_or_log_failure","title":"<code>send_fractal_email_or_log_failure(*, subject, msg, email_settings)</code>","text":"<p>Send an email using the specified settings, or log about failure.</p> Source code in <code>fractal_server/app/security/signup_email.py</code> <pre><code>def send_fractal_email_or_log_failure(\n    *,\n    subject: str,\n    msg: str,\n    email_settings: PublicEmailSettings | None,\n):\n    \"\"\"\n    Send an email using the specified settings, or log about failure.\n    \"\"\"\n\n    if email_settings is None:\n        logger.error(\n            f\"Cannot send email with {subject=}, because {email_settings=}.\"\n        )\n\n    try:\n        logger.info(f\"START sending email with {subject=}.\")\n        mail_msg = EmailMessage()\n        mail_msg.set_content(msg)\n        mail_msg[\"From\"] = formataddr(\n            (email_settings.sender, email_settings.sender)\n        )\n        mail_msg[\"To\"] = \", \".join(\n            [\n                formataddr((recipient, recipient))\n                for recipient in email_settings.recipients\n            ]\n        )\n        mail_msg[\n            \"Subject\"\n        ] = f\"[Fractal, {email_settings.instance_name}] {subject}\"\n        with smtplib.SMTP(\n            email_settings.smtp_server,\n            email_settings.port,\n        ) as server:\n            server.ehlo()\n            if email_settings.use_starttls:\n                server.starttls()\n                server.ehlo()\n            if email_settings.use_login:\n                server.login(\n                    user=email_settings.sender,\n                    password=email_settings.password.get_secret_value(),\n                )\n            server.sendmail(\n                from_addr=email_settings.sender,\n                to_addrs=email_settings.recipients,\n                msg=mail_msg.as_string(),\n            )\n        logger.info(f\"END sending email with {subject=}.\")\n\n    except Exception as e:\n        logger.error(\n            \"Could not send self-registration email, \"\n            f\"original error: {str(e)}.\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/config/","title":"config","text":""},{"location":"reference/fractal_server/config/_data/","title":"_data","text":""},{"location":"reference/fractal_server/config/_data/#fractal_server.config._data.DataSettings","title":"<code>DataSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for the <code>fractal-data</code> integration.</p> Source code in <code>fractal_server/config/_data.py</code> <pre><code>class DataSettings(BaseSettings):\n    \"\"\"\n    Settings for the `fractal-data` integration.\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    FRACTAL_DATA_AUTH_SCHEME: DataAuthScheme = \"none\"\n    \"\"\"\n    Defines how the list of allowed viewer paths is built.\n\n    This variable affects the `GET /auth/current-user/allowed-viewer-paths/`\n    response, which is then consumed by\n    [fractal-data](https://github.com/fractal-analytics-platform/fractal-data).\n\n    Options:\n\n    - \"viewer-paths\": The list of allowed viewer paths will include the user's\n      `project_dir` along with any path defined in user groups' `viewer_paths`\n      attributes.\n    - \"users-folders\": The list will consist of the user's `project_dir` and a\n       user-specific folder. The user folder is constructed by concatenating\n       the base folder `FRACTAL_DATA_BASE_FOLDER` with the user's profile\n       `username`.\n    - \"none\": An empty list will be returned, indicating no access to\n       viewer paths. Useful when vizarr viewer is not used.\n    \"\"\"\n\n    FRACTAL_DATA_BASE_FOLDER: AbsolutePathStr | None = None\n    \"\"\"\n    Base path to Zarr files that will be served by fractal-vizarr-viewer;\n    This variable is required and used only when\n    FRACTAL_DATA_AUTHORIZATION_SCHEME is set to \"users-folders\".\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def check(self: Self) -&gt; Self:\n        \"\"\"\n        `FRACTAL_DATA_BASE_FOLDER` is required when\n        `FRACTAL_DATA_AUTHORIZATION_SCHEME` is set to `\"users-folders\"`.\n        \"\"\"\n        if (\n            self.FRACTAL_DATA_AUTH_SCHEME == DataAuthScheme.USERS_FOLDERS\n            and self.FRACTAL_DATA_BASE_FOLDER is None\n        ):\n            raise ValueError(\n                \"FRACTAL_DATA_BASE_FOLDER is required when \"\n                \"FRACTAL_DATA_AUTH_SCHEME is set to \"\n                \"users-folders\"\n            )\n        return self\n</code></pre>"},{"location":"reference/fractal_server/config/_data/#fractal_server.config._data.DataSettings.FRACTAL_DATA_AUTH_SCHEME","title":"<code>FRACTAL_DATA_AUTH_SCHEME = 'none'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Defines how the list of allowed viewer paths is built.</p> <p>This variable affects the <code>GET /auth/current-user/allowed-viewer-paths/</code> response, which is then consumed by fractal-data.</p> <p>Options:</p> <ul> <li>\"viewer-paths\": The list of allowed viewer paths will include the user's   <code>project_dir</code> along with any path defined in user groups' <code>viewer_paths</code>   attributes.</li> <li>\"users-folders\": The list will consist of the user's <code>project_dir</code> and a    user-specific folder. The user folder is constructed by concatenating    the base folder <code>FRACTAL_DATA_BASE_FOLDER</code> with the user's profile    <code>username</code>.</li> <li>\"none\": An empty list will be returned, indicating no access to    viewer paths. Useful when vizarr viewer is not used.</li> </ul>"},{"location":"reference/fractal_server/config/_data/#fractal_server.config._data.DataSettings.FRACTAL_DATA_BASE_FOLDER","title":"<code>FRACTAL_DATA_BASE_FOLDER = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Base path to Zarr files that will be served by fractal-vizarr-viewer; This variable is required and used only when FRACTAL_DATA_AUTHORIZATION_SCHEME is set to \"users-folders\".</p>"},{"location":"reference/fractal_server/config/_data/#fractal_server.config._data.DataSettings.check","title":"<code>check()</code>","text":"<p><code>FRACTAL_DATA_BASE_FOLDER</code> is required when <code>FRACTAL_DATA_AUTHORIZATION_SCHEME</code> is set to <code>\"users-folders\"</code>.</p> Source code in <code>fractal_server/config/_data.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check(self: Self) -&gt; Self:\n    \"\"\"\n    `FRACTAL_DATA_BASE_FOLDER` is required when\n    `FRACTAL_DATA_AUTHORIZATION_SCHEME` is set to `\"users-folders\"`.\n    \"\"\"\n    if (\n        self.FRACTAL_DATA_AUTH_SCHEME == DataAuthScheme.USERS_FOLDERS\n        and self.FRACTAL_DATA_BASE_FOLDER is None\n    ):\n        raise ValueError(\n            \"FRACTAL_DATA_BASE_FOLDER is required when \"\n            \"FRACTAL_DATA_AUTH_SCHEME is set to \"\n            \"users-folders\"\n        )\n    return self\n</code></pre>"},{"location":"reference/fractal_server/config/_database/","title":"_database","text":""},{"location":"reference/fractal_server/config/_database/#fractal_server.config._database.DatabaseSettings","title":"<code>DatabaseSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Minimal set of configurations needed for operating on the database (e.g for schema migrations).</p> Source code in <code>fractal_server/config/_database.py</code> <pre><code>class DatabaseSettings(BaseSettings):\n    \"\"\"\n    Minimal set of configurations needed for operating on the database (e.g\n    for schema migrations).\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    DB_ECHO: bool = False\n    \"\"\"\n    If `True`, make database operations verbose.\n    \"\"\"\n    POSTGRES_USER: NonEmptyStr | None = None\n    \"\"\"\n    User to use when connecting to the PostgreSQL database.\n    \"\"\"\n    POSTGRES_PASSWORD: SecretStr | None = None\n    \"\"\"\n    Password to use when connecting to the PostgreSQL database.\n    \"\"\"\n    POSTGRES_HOST: NonEmptyStr | None = \"localhost\"\n    \"\"\"\n    URL to the PostgreSQL server or path to a UNIX domain socket.\n    \"\"\"\n    POSTGRES_PORT: NonEmptyStr | None = \"5432\"\n    \"\"\"\n    Port number to use when connecting to the PostgreSQL server.\n    \"\"\"\n    POSTGRES_DB: NonEmptyStr\n    \"\"\"\n    Name of the PostgreSQL database to connect to.\n    \"\"\"\n\n    @property\n    def DATABASE_URL(self) -&gt; URL:\n        if self.POSTGRES_PASSWORD is None:\n            password = None\n        else:\n            password = self.POSTGRES_PASSWORD.get_secret_value()\n\n        url = URL.create(\n            drivername=\"postgresql+psycopg\",\n            username=self.POSTGRES_USER,\n            password=password,\n            host=self.POSTGRES_HOST,\n            port=self.POSTGRES_PORT,\n            database=self.POSTGRES_DB,\n        )\n        return url\n</code></pre>"},{"location":"reference/fractal_server/config/_database/#fractal_server.config._database.DatabaseSettings.DB_ECHO","title":"<code>DB_ECHO = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>True</code>, make database operations verbose.</p>"},{"location":"reference/fractal_server/config/_database/#fractal_server.config._database.DatabaseSettings.POSTGRES_DB","title":"<code>POSTGRES_DB</code>  <code>instance-attribute</code>","text":"<p>Name of the PostgreSQL database to connect to.</p>"},{"location":"reference/fractal_server/config/_database/#fractal_server.config._database.DatabaseSettings.POSTGRES_HOST","title":"<code>POSTGRES_HOST = 'localhost'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>URL to the PostgreSQL server or path to a UNIX domain socket.</p>"},{"location":"reference/fractal_server/config/_database/#fractal_server.config._database.DatabaseSettings.POSTGRES_PASSWORD","title":"<code>POSTGRES_PASSWORD = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Password to use when connecting to the PostgreSQL database.</p>"},{"location":"reference/fractal_server/config/_database/#fractal_server.config._database.DatabaseSettings.POSTGRES_PORT","title":"<code>POSTGRES_PORT = '5432'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Port number to use when connecting to the PostgreSQL server.</p>"},{"location":"reference/fractal_server/config/_database/#fractal_server.config._database.DatabaseSettings.POSTGRES_USER","title":"<code>POSTGRES_USER = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>User to use when connecting to the PostgreSQL database.</p>"},{"location":"reference/fractal_server/config/_email/","title":"_email","text":""},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings","title":"<code>EmailSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Class with settings for email-sending feature.</p> Source code in <code>fractal_server/config/_email.py</code> <pre><code>class EmailSettings(BaseSettings):\n    \"\"\"\n    Class with settings for email-sending feature.\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    FRACTAL_EMAIL_SENDER: EmailStr | None = None\n    \"\"\"\n    Address of the OAuth-signup email sender.\n    \"\"\"\n    FRACTAL_EMAIL_PASSWORD: SecretStr | None = None\n    \"\"\"\n    Password for the OAuth-signup email sender.\n    \"\"\"\n    FRACTAL_EMAIL_SMTP_SERVER: str | None = None\n    \"\"\"\n    SMTP server for the OAuth-signup emails.\n    \"\"\"\n    FRACTAL_EMAIL_SMTP_PORT: int | None = None\n    \"\"\"\n    SMTP server port for the OAuth-signup emails.\n    \"\"\"\n    FRACTAL_EMAIL_INSTANCE_NAME: str | None = None\n    \"\"\"\n    Fractal instance name, to be included in the OAuth-signup emails.\n    \"\"\"\n    FRACTAL_EMAIL_RECIPIENTS: str | None = None\n    \"\"\"\n    Comma-separated list of recipients of the OAuth-signup emails.\n    \"\"\"\n    FRACTAL_EMAIL_USE_STARTTLS: Literal[\"true\", \"false\"] = \"true\"\n    \"\"\"\n    Whether to use StartTLS when using the SMTP server.\n    Accepted values: 'true', 'false'.\n    \"\"\"\n    FRACTAL_EMAIL_USE_LOGIN: Literal[\"true\", \"false\"] = \"true\"\n    \"\"\"\n    Whether to use login when using the SMTP server.\n    If 'true', FRACTAL_EMAIL_PASSWORD  must be provided.\n    Accepted values: 'true', 'false'.\n    \"\"\"\n\n    public: PublicEmailSettings | None = None\n    \"\"\"\n    The validated field which is actually used in `fractal-server\n    (automatically populated upon creation).\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def validate_email_settings(self: Self) -&gt; Self:\n        \"\"\"\n        Set `self.public`.\n        \"\"\"\n\n        email_values = [\n            self.FRACTAL_EMAIL_SENDER,\n            self.FRACTAL_EMAIL_SMTP_SERVER,\n            self.FRACTAL_EMAIL_SMTP_PORT,\n            self.FRACTAL_EMAIL_INSTANCE_NAME,\n            self.FRACTAL_EMAIL_RECIPIENTS,\n        ]\n        if len(set(email_values)) == 1:\n            # All required EMAIL attributes are None\n            pass\n        elif None in email_values:\n            # Not all required EMAIL attributes are set\n            error_msg = (\n                \"Invalid FRACTAL_EMAIL configuration. \"\n                f\"Given values: {email_values}.\"\n            )\n            raise ValueError(error_msg)\n        else:\n            use_starttls = self.FRACTAL_EMAIL_USE_STARTTLS == \"true\"\n            use_login = self.FRACTAL_EMAIL_USE_LOGIN == \"true\"\n\n            if use_login and self.FRACTAL_EMAIL_PASSWORD is None:\n                raise ValueError(\n                    \"'FRACTAL_EMAIL_USE_LOGIN' is 'true' but \"\n                    \"'FRACTAL_EMAIL_PASSWORD' is not provided.\"\n                )\n\n            self.public = PublicEmailSettings(\n                sender=self.FRACTAL_EMAIL_SENDER,\n                recipients=self.FRACTAL_EMAIL_RECIPIENTS.split(\",\"),\n                smtp_server=self.FRACTAL_EMAIL_SMTP_SERVER,\n                port=self.FRACTAL_EMAIL_SMTP_PORT,\n                password=self.FRACTAL_EMAIL_PASSWORD,\n                instance_name=self.FRACTAL_EMAIL_INSTANCE_NAME,\n                use_starttls=use_starttls,\n                use_login=use_login,\n            )\n\n        return self\n</code></pre>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings.FRACTAL_EMAIL_INSTANCE_NAME","title":"<code>FRACTAL_EMAIL_INSTANCE_NAME = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fractal instance name, to be included in the OAuth-signup emails.</p>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings.FRACTAL_EMAIL_PASSWORD","title":"<code>FRACTAL_EMAIL_PASSWORD = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Password for the OAuth-signup email sender.</p>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings.FRACTAL_EMAIL_RECIPIENTS","title":"<code>FRACTAL_EMAIL_RECIPIENTS = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Comma-separated list of recipients of the OAuth-signup emails.</p>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings.FRACTAL_EMAIL_SENDER","title":"<code>FRACTAL_EMAIL_SENDER = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address of the OAuth-signup email sender.</p>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings.FRACTAL_EMAIL_SMTP_PORT","title":"<code>FRACTAL_EMAIL_SMTP_PORT = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>SMTP server port for the OAuth-signup emails.</p>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings.FRACTAL_EMAIL_SMTP_SERVER","title":"<code>FRACTAL_EMAIL_SMTP_SERVER = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>SMTP server for the OAuth-signup emails.</p>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings.FRACTAL_EMAIL_USE_LOGIN","title":"<code>FRACTAL_EMAIL_USE_LOGIN = 'true'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use login when using the SMTP server. If 'true', FRACTAL_EMAIL_PASSWORD  must be provided. Accepted values: 'true', 'false'.</p>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings.FRACTAL_EMAIL_USE_STARTTLS","title":"<code>FRACTAL_EMAIL_USE_STARTTLS = 'true'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use StartTLS when using the SMTP server. Accepted values: 'true', 'false'.</p>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings.public","title":"<code>public = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The validated field which is actually used in `fractal-server (automatically populated upon creation).</p>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.EmailSettings.validate_email_settings","title":"<code>validate_email_settings()</code>","text":"<p>Set <code>self.public</code>.</p> Source code in <code>fractal_server/config/_email.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_email_settings(self: Self) -&gt; Self:\n    \"\"\"\n    Set `self.public`.\n    \"\"\"\n\n    email_values = [\n        self.FRACTAL_EMAIL_SENDER,\n        self.FRACTAL_EMAIL_SMTP_SERVER,\n        self.FRACTAL_EMAIL_SMTP_PORT,\n        self.FRACTAL_EMAIL_INSTANCE_NAME,\n        self.FRACTAL_EMAIL_RECIPIENTS,\n    ]\n    if len(set(email_values)) == 1:\n        # All required EMAIL attributes are None\n        pass\n    elif None in email_values:\n        # Not all required EMAIL attributes are set\n        error_msg = (\n            \"Invalid FRACTAL_EMAIL configuration. \"\n            f\"Given values: {email_values}.\"\n        )\n        raise ValueError(error_msg)\n    else:\n        use_starttls = self.FRACTAL_EMAIL_USE_STARTTLS == \"true\"\n        use_login = self.FRACTAL_EMAIL_USE_LOGIN == \"true\"\n\n        if use_login and self.FRACTAL_EMAIL_PASSWORD is None:\n            raise ValueError(\n                \"'FRACTAL_EMAIL_USE_LOGIN' is 'true' but \"\n                \"'FRACTAL_EMAIL_PASSWORD' is not provided.\"\n            )\n\n        self.public = PublicEmailSettings(\n            sender=self.FRACTAL_EMAIL_SENDER,\n            recipients=self.FRACTAL_EMAIL_RECIPIENTS.split(\",\"),\n            smtp_server=self.FRACTAL_EMAIL_SMTP_SERVER,\n            port=self.FRACTAL_EMAIL_SMTP_PORT,\n            password=self.FRACTAL_EMAIL_PASSWORD,\n            instance_name=self.FRACTAL_EMAIL_INSTANCE_NAME,\n            use_starttls=use_starttls,\n            use_login=use_login,\n        )\n\n    return self\n</code></pre>"},{"location":"reference/fractal_server/config/_email/#fractal_server.config._email.PublicEmailSettings","title":"<code>PublicEmailSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>EmailSettings.public</code>, namely the ready-to-use settings.</p> <p>Attributes:</p> Name Type Description <code>sender</code> <code>EmailStr</code> <p>Sender email address</p> <code>recipients</code> <code>list[EmailStr]</code> <p>List of recipients email address</p> <code>smtp_server</code> <code>str</code> <p>SMTP server address</p> <code>port</code> <code>int</code> <p>SMTP server port</p> <code>password</code> <code>SecretStr | None</code> <p>Sender password</p> <code>instance_name</code> <code>str</code> <p>Name of SMTP server instance</p> <code>use_starttls</code> <code>bool</code> <p>Whether to use the security protocol</p> <code>use_login</code> <code>bool</code> <p>Whether to use login</p> Source code in <code>fractal_server/config/_email.py</code> <pre><code>class PublicEmailSettings(BaseModel):\n    \"\"\"\n    Schema for `EmailSettings.public`, namely the ready-to-use settings.\n\n    Attributes:\n        sender: Sender email address\n        recipients: List of recipients email address\n        smtp_server: SMTP server address\n        port: SMTP server port\n        password: Sender password\n        instance_name: Name of SMTP server instance\n        use_starttls: Whether to use the security protocol\n        use_login: Whether to use login\n    \"\"\"\n\n    sender: EmailStr\n    recipients: list[EmailStr] = Field(min_length=1)\n    smtp_server: str\n    port: int\n    password: SecretStr | None = None\n    instance_name: str\n    use_starttls: bool\n    use_login: bool\n</code></pre>"},{"location":"reference/fractal_server/config/_main/","title":"_main","text":""},{"location":"reference/fractal_server/config/_main/#fractal_server.config._main.Settings","title":"<code>Settings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Contains all the configuration variables for Fractal Server</p> <p>The attributes of this class are set from the environment.</p> Source code in <code>fractal_server/config/_main.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"\n    Contains all the configuration variables for Fractal Server\n\n    The attributes of this class are set from the environment.\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    JWT_EXPIRE_SECONDS: int = 180\n    \"\"\"\n    JWT token lifetime, in seconds.\n    \"\"\"\n\n    JWT_SECRET_KEY: SecretStr\n    \"\"\"\n    JWT secret\n\n    \u26a0\ufe0f **IMPORTANT**: set this variable to a secure string, and do not disclose\n    it.\n    \"\"\"\n\n    COOKIE_EXPIRE_SECONDS: int = 86400\n    \"\"\"\n    Cookie token lifetime, in seconds.\n    \"\"\"\n\n    # Note: we do not use ResourceType here to avoid circular imports\n    FRACTAL_RUNNER_BACKEND: Literal[\n        \"local\", \"slurm_ssh\", \"slurm_sudo\"\n    ] = \"local\"\n    \"\"\"\n    Select which runner backend to use.\n    \"\"\"\n\n    FRACTAL_LOGGING_LEVEL: int = logging.INFO\n    \"\"\"\n    Logging-level threshold for logging\n\n    Only logs of with this level (or higher) will appear in the console logs.\n    \"\"\"\n\n    FRACTAL_API_MAX_JOB_LIST_LENGTH: int = 25\n    \"\"\"\n    Number of ids that can be stored in the `jobsV2` attribute of\n    `app.state`.\n    \"\"\"\n\n    FRACTAL_GRACEFUL_SHUTDOWN_TIME: int = 30\n    \"\"\"\n    Waiting time for the shutdown phase of executors\n    \"\"\"\n\n    FRACTAL_HELP_URL: HttpUrl | None = None\n    \"\"\"\n    The URL of an instance-specific Fractal help page.\n    \"\"\"\n\n    FRACTAL_DEFAULT_GROUP_NAME: Literal[\"All\"] | None = None\n    \"\"\"\n    Name of the default user group.\n\n    If set to `\"All\"`, then the user group with that name is a special user\n    group (e.g. it cannot be deleted, and new users are automatically added\n    to it). If set to `None` (the default value), then user groups are all\n    equivalent, independently on their name.\n    \"\"\"\n</code></pre>"},{"location":"reference/fractal_server/config/_main/#fractal_server.config._main.Settings.COOKIE_EXPIRE_SECONDS","title":"<code>COOKIE_EXPIRE_SECONDS = 86400</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Cookie token lifetime, in seconds.</p>"},{"location":"reference/fractal_server/config/_main/#fractal_server.config._main.Settings.FRACTAL_API_MAX_JOB_LIST_LENGTH","title":"<code>FRACTAL_API_MAX_JOB_LIST_LENGTH = 25</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of ids that can be stored in the <code>jobsV2</code> attribute of <code>app.state</code>.</p>"},{"location":"reference/fractal_server/config/_main/#fractal_server.config._main.Settings.FRACTAL_DEFAULT_GROUP_NAME","title":"<code>FRACTAL_DEFAULT_GROUP_NAME = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the default user group.</p> <p>If set to <code>\"All\"</code>, then the user group with that name is a special user group (e.g. it cannot be deleted, and new users are automatically added to it). If set to <code>None</code> (the default value), then user groups are all equivalent, independently on their name.</p>"},{"location":"reference/fractal_server/config/_main/#fractal_server.config._main.Settings.FRACTAL_GRACEFUL_SHUTDOWN_TIME","title":"<code>FRACTAL_GRACEFUL_SHUTDOWN_TIME = 30</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Waiting time for the shutdown phase of executors</p>"},{"location":"reference/fractal_server/config/_main/#fractal_server.config._main.Settings.FRACTAL_HELP_URL","title":"<code>FRACTAL_HELP_URL = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The URL of an instance-specific Fractal help page.</p>"},{"location":"reference/fractal_server/config/_main/#fractal_server.config._main.Settings.FRACTAL_LOGGING_LEVEL","title":"<code>FRACTAL_LOGGING_LEVEL = logging.INFO</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Logging-level threshold for logging</p> <p>Only logs of with this level (or higher) will appear in the console logs.</p>"},{"location":"reference/fractal_server/config/_main/#fractal_server.config._main.Settings.FRACTAL_RUNNER_BACKEND","title":"<code>FRACTAL_RUNNER_BACKEND = 'local'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Select which runner backend to use.</p>"},{"location":"reference/fractal_server/config/_main/#fractal_server.config._main.Settings.JWT_EXPIRE_SECONDS","title":"<code>JWT_EXPIRE_SECONDS = 180</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>JWT token lifetime, in seconds.</p>"},{"location":"reference/fractal_server/config/_main/#fractal_server.config._main.Settings.JWT_SECRET_KEY","title":"<code>JWT_SECRET_KEY</code>  <code>instance-attribute</code>","text":"<p>JWT secret</p> <p>\u26a0\ufe0f IMPORTANT: set this variable to a secure string, and do not disclose it.</p>"},{"location":"reference/fractal_server/config/_oauth/","title":"_oauth","text":""},{"location":"reference/fractal_server/config/_oauth/#fractal_server.config._oauth.OAuthSettings","title":"<code>OAuthSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Minimal set of configurations needed for operating on the database (e.g for schema migrations).</p> Source code in <code>fractal_server/config/_oauth.py</code> <pre><code>class OAuthSettings(BaseSettings):\n    \"\"\"\n    Minimal set of configurations needed for operating on the database (e.g\n    for schema migrations).\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    OAUTH_CLIENT_NAME: (\n        Annotated[\n            NonEmptyStr,\n            StringConstraints(to_lower=True),\n        ]\n        | None\n    ) = None\n    \"\"\"\n    The name of the client.\n    \"\"\"\n    OAUTH_CLIENT_ID: SecretStr | None = None\n    \"\"\"\n    ID of client.\n    \"\"\"\n    OAUTH_CLIENT_SECRET: SecretStr | None = None\n    \"\"\"\n    Secret to authorise against the identity provider.\n    \"\"\"\n    OAUTH_OIDC_CONFIG_ENDPOINT: SecretStr | None = None\n    \"\"\"\n    OpenID configuration endpoint, for autodiscovery of relevant endpoints.\n    \"\"\"\n    OAUTH_REDIRECT_URL: str | None = None\n    \"\"\"\n    String to be used as `redirect_url` argument in\n    `fastapi_users.get_oauth_router`, and then in\n    `httpx_oauth.integrations.fastapi.OAuth2AuthorizeCallback`\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def check_configuration(self: Self) -&gt; Self:\n        if (\n            self.OAUTH_CLIENT_NAME not in [\"google\", \"github\", None]\n            and self.OAUTH_OIDC_CONFIG_ENDPOINT is None\n        ):\n            raise ValueError(\n                f\"self.OAUTH_OIDC_CONFIG_ENDPOINT=None but \"\n                f\"{self.OAUTH_CLIENT_NAME=}\"\n            )\n        return self\n\n    @property\n    def is_set(self) -&gt; bool:\n        return None not in (\n            self.OAUTH_CLIENT_NAME,\n            self.OAUTH_CLIENT_ID,\n            self.OAUTH_CLIENT_SECRET,\n        )\n</code></pre>"},{"location":"reference/fractal_server/config/_oauth/#fractal_server.config._oauth.OAuthSettings.OAUTH_CLIENT_ID","title":"<code>OAUTH_CLIENT_ID = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ID of client.</p>"},{"location":"reference/fractal_server/config/_oauth/#fractal_server.config._oauth.OAuthSettings.OAUTH_CLIENT_NAME","title":"<code>OAUTH_CLIENT_NAME = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the client.</p>"},{"location":"reference/fractal_server/config/_oauth/#fractal_server.config._oauth.OAuthSettings.OAUTH_CLIENT_SECRET","title":"<code>OAUTH_CLIENT_SECRET = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Secret to authorise against the identity provider.</p>"},{"location":"reference/fractal_server/config/_oauth/#fractal_server.config._oauth.OAuthSettings.OAUTH_OIDC_CONFIG_ENDPOINT","title":"<code>OAUTH_OIDC_CONFIG_ENDPOINT = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>OpenID configuration endpoint, for autodiscovery of relevant endpoints.</p>"},{"location":"reference/fractal_server/config/_oauth/#fractal_server.config._oauth.OAuthSettings.OAUTH_REDIRECT_URL","title":"<code>OAUTH_REDIRECT_URL = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String to be used as <code>redirect_url</code> argument in <code>fastapi_users.get_oauth_router</code>, and then in <code>httpx_oauth.integrations.fastapi.OAuth2AuthorizeCallback</code></p>"},{"location":"reference/fractal_server/config/_settings_config/","title":"_settings_config","text":""},{"location":"reference/fractal_server/images/","title":"images","text":""},{"location":"reference/fractal_server/images/models/","title":"models","text":""},{"location":"reference/fractal_server/images/models/#fractal_server.images.models.SingleImage","title":"<code>SingleImage</code>","text":"<p>               Bases: <code>_SingleImageBase</code></p> <p><code>SingleImageBase</code>, with scalar <code>attributes</code> values (<code>None</code> excluded).</p> Source code in <code>fractal_server/images/models.py</code> <pre><code>class SingleImage(_SingleImageBase):\n    \"\"\"\n    `SingleImageBase`, with scalar `attributes` values (`None` excluded).\n    \"\"\"\n\n    attributes: ImageAttributes = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/fractal_server/images/models/#fractal_server.images.models.SingleImageTaskOutput","title":"<code>SingleImageTaskOutput</code>","text":"<p>               Bases: <code>_SingleImageBase</code></p> <p><code>SingleImageBase</code>, with scalar <code>attributes</code> values (<code>None</code> included).</p> Source code in <code>fractal_server/images/models.py</code> <pre><code>class SingleImageTaskOutput(_SingleImageBase):\n    \"\"\"\n    `SingleImageBase`, with scalar `attributes` values (`None` included).\n    \"\"\"\n\n    attributes: ImageAttributesWithNone = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/fractal_server/images/models/#fractal_server.images.models._SingleImageBase","title":"<code>_SingleImageBase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base for SingleImage and SingleImageTaskOutput.</p> <p>Attributes:</p> Name Type Description <code>zarr_url</code> <code>ZarrUrlStr</code> <code>origin</code> <code>ZarrDirStr | None</code> <code>attributes</code> <code>DictStrAny</code> <code>types</code> <code>ImageTypes</code> Source code in <code>fractal_server/images/models.py</code> <pre><code>class _SingleImageBase(BaseModel):\n    \"\"\"\n    Base for SingleImage and SingleImageTaskOutput.\n\n    Attributes:\n        zarr_url:\n        origin:\n        attributes:\n        types:\n    \"\"\"\n\n    zarr_url: ZarrUrlStr\n    origin: ZarrDirStr | None = None\n\n    attributes: DictStrAny = Field(default_factory=dict)\n    types: ImageTypes = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/fractal_server/images/status_tools/","title":"status_tools","text":""},{"location":"reference/fractal_server/images/status_tools/#fractal_server.images.status_tools._postprocess_image_lists","title":"<code>_postprocess_image_lists(target_images, list_query_url_status)</code>","text":"Source code in <code>fractal_server/images/status_tools.py</code> <pre><code>def _postprocess_image_lists(\n    target_images: list[dict[str, Any]],\n    list_query_url_status: list[tuple[str, str]],\n) -&gt; list[dict[str, Any]]:\n    \"\"\" \"\"\"\n    t_1 = time.perf_counter()\n\n    # Select only processed images that are part of the target image set\n    zarr_url_to_image = {img[\"zarr_url\"]: img for img in target_images}\n    target_zarr_urls = zarr_url_to_image.keys()\n    list_processed_url_status = [\n        url_status\n        for url_status in list_query_url_status\n        if url_status[0] in target_zarr_urls\n    ]\n\n    set_processed_urls = set(\n        url_status[0] for url_status in list_processed_url_status\n    )\n    processed_images_with_status = [\n        _enriched_image(\n            img=zarr_url_to_image[item[0]],\n            status=item[1],\n        )\n        for item in list_processed_url_status\n    ]\n\n    non_processed_urls = target_zarr_urls - set_processed_urls\n    non_processed_images_with_status = [\n        _enriched_image(\n            img=zarr_url_to_image[zarr_url],\n            status=HistoryUnitStatusWithUnset.UNSET,\n        )\n        for zarr_url in non_processed_urls\n    ]\n    t_2 = time.perf_counter()\n    logger.debug(\n        f\"[enrich_images_async] post-processing, elapsed={t_2 - t_1:.5f} s\"\n    )\n\n    return processed_images_with_status + non_processed_images_with_status\n</code></pre>"},{"location":"reference/fractal_server/images/status_tools/#fractal_server.images.status_tools._prepare_query","title":"<code>_prepare_query(*, dataset_id, workflowtask_id)</code>","text":"<p>Note: the query does not include <code>.order_by</code>.</p> Source code in <code>fractal_server/images/status_tools.py</code> <pre><code>def _prepare_query(\n    *,\n    dataset_id: int,\n    workflowtask_id: int,\n) -&gt; Select:\n    \"\"\"\n    Note: the query does not include `.order_by`.\n    \"\"\"\n    stm = (\n        select(HistoryImageCache.zarr_url, HistoryUnit.status)\n        .join(HistoryUnit)\n        .where(HistoryImageCache.dataset_id == dataset_id)\n        .where(HistoryImageCache.workflowtask_id == workflowtask_id)\n        .where(HistoryImageCache.latest_history_unit_id == HistoryUnit.id)\n    )\n    return stm\n</code></pre>"},{"location":"reference/fractal_server/images/status_tools/#fractal_server.images.status_tools.enrich_images_unsorted_async","title":"<code>enrich_images_unsorted_async(*, images, dataset_id, workflowtask_id, db)</code>  <code>async</code>","text":"<p>Enrich images with a status-related attribute.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[dict[str, Any]]</code> <p>The input image list</p> required <code>dataset_id</code> <code>int</code> <p>The dataset ID</p> required <code>workflowtask_id</code> <code>int</code> <p>The workflow-task ID</p> required <code>db</code> <code>AsyncSession</code> <p>An async db session</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>The list of enriched images, not necessarily in the same order as</p> <code>list[dict[str, Any]]</code> <p>the input.</p> Source code in <code>fractal_server/images/status_tools.py</code> <pre><code>async def enrich_images_unsorted_async(\n    *,\n    images: list[dict[str, Any]],\n    dataset_id: int,\n    workflowtask_id: int,\n    db: AsyncSession,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Enrich images with a status-related attribute.\n\n    Args:\n        images: The input image list\n        dataset_id: The dataset ID\n        workflowtask_id: The workflow-task ID\n        db: An async db session\n\n    Returns:\n        The list of enriched images, not necessarily in the same order as\n        the input.\n    \"\"\"\n    t_0 = time.perf_counter()\n    logger.info(\n        f\"[enrich_images_async] START, {dataset_id=}, {workflowtask_id=}\"\n    )\n\n    # Get `(zarr_url, status)` for _all_ processed images (including those that\n    # are not part of the target image set)\n    res = await db.execute(\n        _prepare_query(\n            dataset_id=dataset_id,\n            workflowtask_id=workflowtask_id,\n        )\n    )\n    list_query_url_status = res.all()\n    t_1 = time.perf_counter()\n    logger.debug(f\"[enrich_images_async] query, elapsed={t_1 - t_0:.5f} s\")\n\n    output = _postprocess_image_lists(\n        target_images=images,\n        list_query_url_status=list_query_url_status,\n    )\n\n    return output\n</code></pre>"},{"location":"reference/fractal_server/images/status_tools/#fractal_server.images.status_tools.enrich_images_unsorted_sync","title":"<code>enrich_images_unsorted_sync(*, images, dataset_id, workflowtask_id)</code>","text":"<p>Enrich images with a status-related attribute.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[dict[str, Any]]</code> <p>The input image list</p> required <code>dataset_id</code> <code>int</code> <p>The dataset ID</p> required <code>workflowtask_id</code> <code>int</code> <p>The workflow-task ID</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>The list of enriched images, not necessarily in the same order as</p> <code>list[dict[str, Any]]</code> <p>the input.</p> Source code in <code>fractal_server/images/status_tools.py</code> <pre><code>def enrich_images_unsorted_sync(\n    *,\n    images: list[dict[str, Any]],\n    dataset_id: int,\n    workflowtask_id: int,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Enrich images with a status-related attribute.\n\n\n    Args:\n        images: The input image list\n        dataset_id: The dataset ID\n        workflowtask_id: The workflow-task ID\n\n    Returns:\n        The list of enriched images, not necessarily in the same order as\n        the input.\n    \"\"\"\n\n    t_0 = time.perf_counter()\n    logger.info(\n        f\"[enrich_images_async] START, {dataset_id=}, {workflowtask_id=}\"\n    )\n\n    # Get `(zarr_url, status)` for _all_ processed images (including those that\n    # are not part of the target image set)\n    with next(get_sync_db()) as db:\n        res = db.execute(\n            _prepare_query(\n                dataset_id=dataset_id,\n                workflowtask_id=workflowtask_id,\n            )\n        )\n        list_query_url_status = res.all()\n    t_1 = time.perf_counter()\n    logger.debug(f\"[enrich_images_async] query, elapsed={t_1 - t_0:.5f} s\")\n\n    output = _postprocess_image_lists(\n        target_images=images,\n        list_query_url_status=list_query_url_status,\n    )\n\n    return output\n</code></pre>"},{"location":"reference/fractal_server/images/tools/","title":"tools","text":""},{"location":"reference/fractal_server/images/tools/#fractal_server.images.tools.aggregate_attributes","title":"<code>aggregate_attributes(images)</code>","text":"<p>Given a list of images, this function returns a dictionary of all image attributes, each mapped to a sorted list of existing values.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def aggregate_attributes(images: list[dict[str, Any]]) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Given a list of images, this function returns a dictionary of all image\n    attributes, each mapped to a sorted list of existing values.\n    \"\"\"\n    attributes = {}\n    for image in images:\n        for k, v in image[\"attributes\"].items():\n            attributes.setdefault(k, []).append(v)\n    for k, v in attributes.items():\n        attributes[k] = list(set(v))\n    sorted_attributes = {\n        key: sorted(value) for key, value in attributes.items()\n    }\n    return sorted_attributes\n</code></pre>"},{"location":"reference/fractal_server/images/tools/#fractal_server.images.tools.aggregate_types","title":"<code>aggregate_types(images)</code>","text":"<p>Given a list of images, this function returns a list of all image types.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def aggregate_types(images: list[dict[str, Any]]) -&gt; list[str]:\n    \"\"\"\n    Given a list of images, this function returns a list of all image types.\n    \"\"\"\n    return list({type for image in images for type in image[\"types\"].keys()})\n</code></pre>"},{"location":"reference/fractal_server/images/tools/#fractal_server.images.tools.filter_image_list","title":"<code>filter_image_list(images, type_filters=None, attribute_filters=None)</code>","text":"<p>Compute a sublist with images that match a filter set.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[dict[str, Any]]</code> <p>A list of images.</p> required <code>type_filters</code> <code>dict[str, bool] | None</code> <code>None</code> <code>attribute_filters</code> <code>AttributeFilters | None</code> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of the <code>images</code> elements which match the filter set.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def filter_image_list(\n    images: list[dict[str, Any]],\n    type_filters: dict[str, bool] | None = None,\n    attribute_filters: AttributeFilters | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Compute a sublist with images that match a filter set.\n\n    Args:\n        images: A list of images.\n        type_filters:\n        attribute_filters:\n\n    Returns:\n        List of the `images` elements which match the filter set.\n    \"\"\"\n\n    # When no filter is provided, return all images\n    if type_filters is None and attribute_filters is None:\n        return images\n    actual_type_filters = type_filters or {}\n    actual_attribute_filters = attribute_filters or {}\n\n    filtered_images = [\n        copy(this_image)\n        for this_image in images\n        if match_filter(\n            image=this_image,\n            type_filters=actual_type_filters,\n            attribute_filters=actual_attribute_filters,\n        )\n    ]\n    return filtered_images\n</code></pre>"},{"location":"reference/fractal_server/images/tools/#fractal_server.images.tools.find_image_by_zarr_url","title":"<code>find_image_by_zarr_url(*, images, zarr_url)</code>","text":"<p>Return a copy of the image with a given zarr_url, and its positional index.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[dict[str, Any]]</code> <p>List of images.</p> required <code>zarr_url</code> <code>str</code> <p>Path that the returned image must have.</p> required <p>Returns:</p> Type Description <code>ImageSearch | None</code> <p>The first image from <code>images</code> which has zarr_url equal to <code>zarr_url</code>.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def find_image_by_zarr_url(\n    *,\n    images: list[dict[str, Any]],\n    zarr_url: str,\n) -&gt; ImageSearch | None:\n    \"\"\"\n    Return a copy of the image with a given zarr_url, and its positional index.\n\n    Args:\n        images: List of images.\n        zarr_url: Path that the returned image must have.\n\n    Returns:\n        The first image from `images` which has zarr_url equal to `zarr_url`.\n    \"\"\"\n    image_urls = [img[\"zarr_url\"] for img in images]\n    try:\n        ind = image_urls.index(zarr_url)\n    except ValueError:\n        return None\n    return dict(image=copy(images[ind]), index=ind)\n</code></pre>"},{"location":"reference/fractal_server/images/tools/#fractal_server.images.tools.match_filter","title":"<code>match_filter(*, image, type_filters, attribute_filters)</code>","text":"<p>Find whether an image matches a filter set.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>dict[str, Any]</code> <p>A single image.</p> required <code>type_filters</code> <code>dict[str, bool]</code> required <code>attribute_filters</code> <code>AttributeFilters</code> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the image matches the filter set.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def match_filter(\n    *,\n    image: dict[str, Any],\n    type_filters: dict[str, bool],\n    attribute_filters: AttributeFilters,\n) -&gt; bool:\n    \"\"\"\n    Find whether an image matches a filter set.\n\n    Args:\n        image: A single image.\n        type_filters:\n        attribute_filters:\n\n    Returns:\n        Whether the image matches the filter set.\n    \"\"\"\n\n    # Verify match with types (using a False default)\n    for key, value in type_filters.items():\n        if image[\"types\"].get(key, False) != value:\n            return False\n\n    # Verify match with attributes (only for not-None filters)\n    for key, values in attribute_filters.items():\n        if image[\"attributes\"].get(key) not in values:\n            return False\n\n    return True\n</code></pre>"},{"location":"reference/fractal_server/images/tools/#fractal_server.images.tools.merge_type_filters","title":"<code>merge_type_filters(*, task_input_types, wftask_type_filters)</code>","text":"<p>Merge two type-filters sets, if they are compatible.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def merge_type_filters(\n    *,\n    task_input_types: dict[str, bool],\n    wftask_type_filters: dict[str, bool],\n) -&gt; dict[str, bool]:\n    \"\"\"\n    Merge two type-filters sets, if they are compatible.\n    \"\"\"\n    all_keys = set(task_input_types.keys()) | set(wftask_type_filters.keys())\n    for key in all_keys:\n        if (\n            key in task_input_types.keys()\n            and key in wftask_type_filters.keys()\n            and task_input_types[key] != wftask_type_filters[key]\n        ):\n            raise ValueError(\n                \"Cannot merge type filters \"\n                f\"`{task_input_types}` (from task) \"\n                f\"and `{wftask_type_filters}` (from workflowtask).\"\n            )\n    merged_dict = task_input_types\n    merged_dict.update(wftask_type_filters)\n    return merged_dict\n</code></pre>"},{"location":"reference/fractal_server/runner/","title":"runner","text":""},{"location":"reference/fractal_server/runner/components/","title":"components","text":""},{"location":"reference/fractal_server/runner/exceptions/","title":"exceptions","text":""},{"location":"reference/fractal_server/runner/exceptions/#fractal_server.runner.exceptions.JobExecutionError","title":"<code>JobExecutionError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>JobExecutionError</p> <p>Attributes:</p> Name Type Description <code>info</code> <code>str | None</code> <p>A free field for additional information</p> Source code in <code>fractal_server/runner/exceptions.py</code> <pre><code>class JobExecutionError(RuntimeError):\n    \"\"\"\n    JobExecutionError\n\n    Attributes:\n        info:\n            A free field for additional information\n    \"\"\"\n\n    info: str | None = None\n\n    def __init__(\n        self,\n        *args,\n        info: str | None = None,\n    ):\n        super().__init__(*args)\n        self.info = info\n\n    def assemble_error(self) -&gt; str:\n        if self.info:\n            content = f\"\\n{self.info}\\n\\n\"\n        else:\n            content = str(self)\n        message = f\"JobExecutionError\\n{content}\"\n        return message\n</code></pre>"},{"location":"reference/fractal_server/runner/exceptions/#fractal_server.runner.exceptions.TaskExecutionError","title":"<code>TaskExecutionError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Forwards errors occurred during the execution of a task</p> <p>This error wraps and forwards errors occurred during the execution of tasks, when the exit code is larger than 0 (i.e. the error took place within the task). This error also adds information that is useful to track down and debug the failing task within a workflow.</p> <p>Attributes:</p> Name Type Description <code>workflow_task_id</code> <code>int | None</code> <p>ID of the workflow task that failed.</p> <code>workflow_task_order</code> <code>int | None</code> <p>Order of the task within the workflow.</p> <code>task_name</code> <code>str | None</code> <p>Human readable name of the failing task.</p> Source code in <code>fractal_server/runner/exceptions.py</code> <pre><code>class TaskExecutionError(RuntimeError):\n    \"\"\"\n    Forwards errors occurred during the execution of a task\n\n    This error wraps and forwards errors occurred during the execution of\n    tasks, when the exit code is larger than 0 (i.e. the error took place\n    within the task). This error also adds information that is useful to track\n    down and debug the failing task within a workflow.\n\n    Attributes:\n        workflow_task_id:\n            ID of the workflow task that failed.\n        workflow_task_order:\n            Order of the task within the workflow.\n        task_name:\n            Human readable name of the failing task.\n    \"\"\"\n\n    workflow_task_id: int | None = None\n    workflow_task_order: int | None = None\n    task_name: str | None = None\n\n    def __init__(\n        self,\n        *args,\n        workflow_task_id: int | None = None,\n        workflow_task_order: int | None = None,\n        task_name: str | None = None,\n    ):\n        super().__init__(*args)\n        self.workflow_task_id = workflow_task_id\n        self.workflow_task_order = workflow_task_order\n        self.task_name = task_name\n</code></pre>"},{"location":"reference/fractal_server/runner/filenames/","title":"filenames","text":""},{"location":"reference/fractal_server/runner/set_start_and_last_task_index/","title":"set_start_and_last_task_index","text":""},{"location":"reference/fractal_server/runner/set_start_and_last_task_index/#fractal_server.runner.set_start_and_last_task_index.set_start_and_last_task_index","title":"<code>set_start_and_last_task_index(num_tasks, first_task_index=None, last_task_index=None)</code>","text":"<p>Handle <code>first_task_index</code> and <code>last_task_index</code>, by setting defaults and validating values.</p> num_tasks <p>Total number of tasks in a workflow task list</p> <p>first_task_index:     Positional index of the first task to execute last_task_index:     Positional index of the last task to execute</p> Source code in <code>fractal_server/runner/set_start_and_last_task_index.py</code> <pre><code>def set_start_and_last_task_index(\n    num_tasks: int,\n    first_task_index: int | None = None,\n    last_task_index: int | None = None,\n) -&gt; tuple[int, int]:\n    \"\"\"\n    Handle `first_task_index` and `last_task_index`, by setting defaults and\n    validating values.\n\n    num_tasks:\n        Total number of tasks in a workflow task list\n    first_task_index:\n        Positional index of the first task to execute\n    last_task_index:\n        Positional index of the last task to execute\n    \"\"\"\n    # Set default values\n    if first_task_index is None:\n        first_task_index = 0\n    if last_task_index is None:\n        last_task_index = num_tasks - 1\n\n    # Perform checks\n    if first_task_index &lt; 0:\n        raise ValueError(f\"{first_task_index=} cannot be negative\")\n    if last_task_index &lt; 0:\n        raise ValueError(f\"{last_task_index=} cannot be negative\")\n    if last_task_index &gt; num_tasks - 1:\n        raise ValueError(\n            f\"{last_task_index=} cannot be larger than {(num_tasks-1)=}\"\n        )\n    if first_task_index &gt; last_task_index:\n        raise ValueError(\n            f\"{first_task_index=} cannot be larger than {last_task_index=}\"\n        )\n    return (first_task_index, last_task_index)\n</code></pre>"},{"location":"reference/fractal_server/runner/task_files/","title":"task_files","text":""},{"location":"reference/fractal_server/runner/task_files/#fractal_server.runner.task_files.TaskFiles","title":"<code>TaskFiles</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Files related to a task.</p> <p>Attributes:</p> Name Type Description <code>root_dir_local</code> <code>Path</code> <code>root_dir_remote</code> <code>Path</code> <code>task_name</code> <code>str</code> <code>task_order</code> <code>int</code> <code>component</code> <code>str | None</code> <code>prefix</code> <code>str | None</code> Source code in <code>fractal_server/runner/task_files.py</code> <pre><code>class TaskFiles(BaseModel):\n    \"\"\"\n    Files related to a task.\n\n    Attributes:\n        root_dir_local:\n        root_dir_remote:\n        task_name:\n        task_order:\n        component:\n        prefix:\n    \"\"\"\n\n    # Parent directory\n    root_dir_local: Path\n    root_dir_remote: Path\n\n    # Per-wftask\n    task_name: str\n    task_order: int\n\n    # Per-single-component\n    component: str | None = None\n    prefix: str | None = None\n\n    def _check_component(self):\n        if self.component is None:\n            raise ValueError(\"`component` cannot be None\")\n\n    @property\n    def subfolder_name(self) -&gt; str:\n        order = str(self.task_order or 0)\n        return task_subfolder_name(\n            order=order,\n            task_name=self.task_name,\n        )\n\n    @property\n    def wftask_subfolder_remote(self) -&gt; Path:\n        return self.root_dir_remote / self.subfolder_name\n\n    @property\n    def wftask_subfolder_local(self) -&gt; Path:\n        return self.root_dir_local / self.subfolder_name\n\n    @property\n    def prefix_component(self):\n        if self.prefix is None:\n            return self.component\n        else:\n            return f\"{self.prefix}-{self.component}\"\n\n    @property\n    def log_file_local(self) -&gt; str:\n        self._check_component()\n        return (\n            self.wftask_subfolder_local / f\"{self.prefix_component}-log.txt\"\n        ).as_posix()\n\n    @property\n    def log_file_remote_path(self) -&gt; Path:\n        self._check_component()\n        return (\n            self.wftask_subfolder_remote / f\"{self.prefix_component}-log.txt\"\n        )\n\n    @property\n    def log_file_remote(self) -&gt; str:\n        return self.log_file_remote_path.as_posix()\n\n    @property\n    def args_file_local(self) -&gt; str:\n        self._check_component()\n        return (\n            self.wftask_subfolder_local / f\"{self.prefix_component}-args.json\"\n        ).as_posix()\n\n    @property\n    def args_file_remote_path(self) -&gt; Path:\n        self._check_component()\n        return (\n            self.wftask_subfolder_remote / f\"{self.prefix_component}-args.json\"\n        )\n\n    @property\n    def args_file_remote(self) -&gt; str:\n        return self.args_file_remote_path.as_posix()\n\n    @property\n    def metadiff_file_local(self) -&gt; str:\n        self._check_component()\n        return (\n            self.wftask_subfolder_local\n            / f\"{self.prefix_component}-metadiff.json\"\n        ).as_posix()\n\n    @property\n    def metadiff_file_remote_path(self) -&gt; Path:\n        self._check_component()\n        return (\n            self.wftask_subfolder_remote\n            / f\"{self.prefix_component}-metadiff.json\"\n        )\n\n    @property\n    def metadiff_file_remote(self) -&gt; str:\n        return self.metadiff_file_remote_path.as_posix()\n</code></pre>"},{"location":"reference/fractal_server/runner/task_files/#fractal_server.runner.task_files.enrich_task_files_multisubmit","title":"<code>enrich_task_files_multisubmit(*, tot_tasks, batch_size, base_task_files)</code>","text":"<p>Expand <code>TaskFiles</code> objects with <code>component</code> and <code>prefix</code>.</p> Source code in <code>fractal_server/runner/task_files.py</code> <pre><code>def enrich_task_files_multisubmit(\n    *,\n    tot_tasks: int,\n    batch_size: int,\n    base_task_files: TaskFiles,\n) -&gt; list[TaskFiles]:\n    \"\"\"\n    Expand `TaskFiles` objects with `component` and `prefix`.\n    \"\"\"\n\n    new_list_task_files: list[TaskFiles] = []\n    for absolute_index in range(tot_tasks):\n        ind_batch = absolute_index // batch_size\n        new_list_task_files.append(\n            TaskFiles(\n                **base_task_files.model_dump(\n                    exclude={\n                        \"component\",\n                        \"prefix\",\n                    }\n                ),\n                prefix=f\"{MULTISUBMIT_PREFIX}-{ind_batch:06d}\",\n                component=_index_to_component(absolute_index),\n            )\n        )\n    return new_list_task_files\n</code></pre>"},{"location":"reference/fractal_server/runner/task_files/#fractal_server.runner.task_files.task_subfolder_name","title":"<code>task_subfolder_name(order, task_name)</code>","text":"<p>Get name of task-specific subfolder.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int | str</code> required <code>task_name</code> <code>str</code> required Source code in <code>fractal_server/runner/task_files.py</code> <pre><code>def task_subfolder_name(\n    order: int | str,\n    task_name: str,\n) -&gt; str:\n    \"\"\"\n    Get name of task-specific subfolder.\n\n    Args:\n        order:\n        task_name:\n    \"\"\"\n    task_name_slug = sanitize_string(task_name)\n    return f\"{order}_{task_name_slug}\"\n</code></pre>"},{"location":"reference/fractal_server/runner/versions/","title":"versions","text":""},{"location":"reference/fractal_server/runner/versions/#fractal_server.runner.versions.get_versions","title":"<code>get_versions()</code>","text":"<p>Extract versions of Python and fractal-server.</p> Source code in <code>fractal_server/runner/versions.py</code> <pre><code>def get_versions() -&gt; VersionsType:\n    \"\"\"\n    Extract versions of Python and fractal-server.\n    \"\"\"\n    return dict(\n        python=tuple(sys.version_info[:3]),\n        fractal_server=fractal_server.__VERSION__,\n    )\n</code></pre>"},{"location":"reference/fractal_server/runner/config/","title":"config","text":""},{"location":"reference/fractal_server/runner/config/_local/","title":"_local","text":""},{"location":"reference/fractal_server/runner/config/_local/#fractal_server.runner.config._local.JobRunnerConfigLocal","title":"<code>JobRunnerConfigLocal</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specifications of the local-backend configuration</p> <p>Attributes:</p> Name Type Description <code>parallel_tasks_per_job</code> <code>int | None</code> <p>Maximum number of tasks to be run in parallel as part of a call to <code>FractalThreadPoolExecutor.map</code>; if <code>None</code>, then all tasks will start at the same time.</p> Source code in <code>fractal_server/runner/config/_local.py</code> <pre><code>class JobRunnerConfigLocal(BaseModel):\n    \"\"\"\n    Specifications of the local-backend configuration\n\n    Attributes:\n        parallel_tasks_per_job:\n            Maximum number of tasks to be run in parallel as part of a call to\n            `FractalThreadPoolExecutor.map`; if `None`, then all tasks will\n            start at the same time.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    parallel_tasks_per_job: int | None = None\n\n    @property\n    def batch_size(self) -&gt; int:\n        return self.parallel_tasks_per_job or 1\n</code></pre>"},{"location":"reference/fractal_server/runner/config/_slurm/","title":"_slurm","text":""},{"location":"reference/fractal_server/runner/config/_slurm/#fractal_server.runner.config._slurm.JobRunnerConfigSLURM","title":"<code>JobRunnerConfigSLURM</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Common SLURM configuration.</p> <p>Note: this is a common and abstract class, which gets transformed into more specific configuration objects during job execution.</p> <p>Valid JSON example <pre><code>{\n  \"default_slurm_config\": {\n      \"partition\": \"main\",\n      \"cpus_per_task\": 1\n  },\n  \"gpu_slurm_config\": {\n      \"partition\": \"gpu\",\n      \"extra_lines\": [\"#SBATCH --gres=gpu:v100:1\"]\n  },\n  \"batching_config\": {\n      \"target_cpus_per_job\": 1,\n      \"max_cpus_per_job\": 1,\n      \"target_mem_per_job\": 200,\n      \"max_mem_per_job\": 500,\n      \"target_num_jobs\": 2,\n      \"max_num_jobs\": 4\n  },\n  \"user_local_exports\": {\n      \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n      \"NAPARI_CONFIG\": \"napari_config.json\"\n  }\n}\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>default_slurm_config</code> <code>_SlurmConfigSet</code> <p>Common default options for all tasks.</p> <code>gpu_slurm_config</code> <code>_SlurmConfigSet | None</code> <p>Default configuration for all GPU tasks.</p> <code>batching_config</code> <code>_BatchingConfigSet</code> <p>Configuration of the batching strategy.</p> <code>user_local_exports</code> <code>DictStrStr</code> <p>Key-value pairs to be included as <code>export</code>-ed variables in SLURM submission script, after prepending values with the user's cache directory.</p> Source code in <code>fractal_server/runner/config/_slurm.py</code> <pre><code>class JobRunnerConfigSLURM(BaseModel):\n    \"\"\"\n    Common SLURM configuration.\n\n    Note: this is a common and abstract class, which gets transformed into\n    more specific configuration objects during job execution.\n\n    Valid JSON example\n    ```JSON\n    {\n      \"default_slurm_config\": {\n          \"partition\": \"main\",\n          \"cpus_per_task\": 1\n      },\n      \"gpu_slurm_config\": {\n          \"partition\": \"gpu\",\n          \"extra_lines\": [\"#SBATCH --gres=gpu:v100:1\"]\n      },\n      \"batching_config\": {\n          \"target_cpus_per_job\": 1,\n          \"max_cpus_per_job\": 1,\n          \"target_mem_per_job\": 200,\n          \"max_mem_per_job\": 500,\n          \"target_num_jobs\": 2,\n          \"max_num_jobs\": 4\n      },\n      \"user_local_exports\": {\n          \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n          \"NAPARI_CONFIG\": \"napari_config.json\"\n      }\n    }\n    ```\n\n    Attributes:\n        default_slurm_config:\n            Common default options for all tasks.\n        gpu_slurm_config:\n            Default configuration for all GPU tasks.\n        batching_config:\n            Configuration of the batching strategy.\n        user_local_exports:\n            Key-value pairs to be included as `export`-ed variables in SLURM\n            submission script, after prepending values with the user's cache\n            directory.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    default_slurm_config: _SlurmConfigSet\n    gpu_slurm_config: _SlurmConfigSet | None = None\n    batching_config: _BatchingConfigSet\n    user_local_exports: DictStrStr = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/fractal_server/runner/config/_slurm/#fractal_server.runner.config._slurm._BatchingConfigSet","title":"<code>_BatchingConfigSet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options to configure the batching strategy (that is, how to combine several tasks in a single SLURM job).</p> <p>Attributes:</p> Name Type Description <code>target_cpus_per_job</code> <code>PositiveInt</code> <code>max_cpus_per_job</code> <code>PositiveInt</code> <code>target_mem_per_job</code> <code>MemMBType</code> <p>(see <code>_parse_mem_value</code> for details on allowed values)</p> <code>max_mem_per_job</code> <code>MemMBType</code> <p>(see <code>_parse_mem_value</code> for details on allowed values)</p> <code>target_num_jobs</code> <code>PositiveInt</code> <code>max_num_jobs</code> <code>PositiveInt</code> Source code in <code>fractal_server/runner/config/_slurm.py</code> <pre><code>class _BatchingConfigSet(BaseModel):\n    \"\"\"\n    Options to configure the batching strategy (that is, how to combine\n    several tasks in a single SLURM job).\n\n    Attributes:\n        target_cpus_per_job:\n        max_cpus_per_job:\n        target_mem_per_job:\n            (see `_parse_mem_value` for details on allowed values)\n        max_mem_per_job:\n            (see `_parse_mem_value` for details on allowed values)\n        target_num_jobs:\n        max_num_jobs:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    target_num_jobs: PositiveInt\n    max_num_jobs: PositiveInt\n    target_cpus_per_job: PositiveInt\n    max_cpus_per_job: PositiveInt\n    target_mem_per_job: MemMBType\n    max_mem_per_job: MemMBType\n</code></pre>"},{"location":"reference/fractal_server/runner/config/_slurm/#fractal_server.runner.config._slurm._SlurmConfigSet","title":"<code>_SlurmConfigSet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options for the default or gpu SLURM config.</p> <p>Attributes:</p> Name Type Description <code>partition</code> <code>NonEmptyStr | None</code> <code>cpus_per_task</code> <code>PositiveInt | None</code> <code>mem</code> <code>MemMBType | None</code> <p>See <code>_parse_mem_value</code> for details on allowed values.</p> <code>constraint</code> <code>NonEmptyStr | None</code> <code>gres</code> <code>NonEmptyStr | None</code> <code>time</code> <code>NonEmptyStr | None</code> <code>exclude</code> <code>NonEmptyStr | None</code> <code>nodelist</code> <code>NonEmptyStr | None</code> <code>account</code> <code>NonEmptyStr | None</code> <code>extra_lines</code> <code>list[NonEmptyStr]</code> Source code in <code>fractal_server/runner/config/_slurm.py</code> <pre><code>class _SlurmConfigSet(BaseModel):\n    \"\"\"\n    Options for the default or gpu SLURM config.\n\n    Attributes:\n        partition:\n        cpus_per_task:\n        mem:\n            See `_parse_mem_value` for details on allowed values.\n        constraint:\n        gres:\n        time:\n        exclude:\n        nodelist:\n        account:\n        extra_lines:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    partition: NonEmptyStr | None = None\n    cpus_per_task: PositiveInt | None = None\n    mem: MemMBType | None = None\n    constraint: NonEmptyStr | None = None\n    gres: NonEmptyStr | None = None\n    exclude: NonEmptyStr | None = None\n    nodelist: NonEmptyStr | None = None\n    time: NonEmptyStr | None = None\n    account: NonEmptyStr | None = None\n    extra_lines: list[NonEmptyStr] = Field(default_factory=list)\n    gpus: NonEmptyStr | None = None\n</code></pre>"},{"location":"reference/fractal_server/runner/config/slurm_mem_to_MB/","title":"slurm_mem_to_MB","text":""},{"location":"reference/fractal_server/runner/config/slurm_mem_to_MB/#fractal_server.runner.config.slurm_mem_to_MB.slurm_mem_to_MB","title":"<code>slurm_mem_to_MB(raw_mem)</code>","text":"<p>Convert a memory-specification string into an integer (in MB units), or simply return the input if it is already an integer.</p> <p>Supported units are <code>\"M\", \"G\", \"T\"</code>, with <code>\"M\"</code> being the default; some parsing examples are: <code>\"10M\" -&gt; 10000</code>, <code>\"3G\" -&gt; 3000000</code>.</p> <p>Parameters:</p> Name Type Description Default <code>raw_mem</code> <code>str | int</code> <p>A string (e.g. <code>\"100M\"</code>) or an integer (in MB).</p> required <p>Returns:</p> Type Description <code>int</code> <p>Integer value of memory in MB units.</p> Source code in <code>fractal_server/runner/config/slurm_mem_to_MB.py</code> <pre><code>def slurm_mem_to_MB(raw_mem: str | int) -&gt; int:\n    \"\"\"\n    Convert a memory-specification string into an integer (in MB units), or\n    simply return the input if it is already an integer.\n\n    Supported units are `\"M\", \"G\", \"T\"`, with `\"M\"` being the default; some\n    parsing examples are: `\"10M\" -&gt; 10000`, `\"3G\" -&gt; 3000000`.\n\n    Args:\n        raw_mem:\n            A string (e.g. `\"100M\"`) or an integer (in MB).\n\n    Returns:\n        Integer value of memory in MB units.\n    \"\"\"\n\n    info = f\"[_parse_mem_value] {raw_mem=}\"\n    error_msg = (\n        f\"{info}, invalid specification of memory requirements \"\n        \"(valid examples: 93, 71M, 93G, 71T).\"\n    )\n\n    # Handle integer argument\n    if type(raw_mem) is int:\n        return raw_mem\n\n    # Handle string argument\n    if not raw_mem[0].isdigit():  # fail e.g. for raw_mem=\"M100\"\n        logger.error(error_msg)\n        raise SlurmConfigError(error_msg)\n    if raw_mem.isdigit():\n        mem_MB = int(raw_mem)\n    elif raw_mem.endswith(\"M\"):\n        stripped_raw_mem = raw_mem.strip(\"M\")\n        if not stripped_raw_mem.isdigit():\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        mem_MB = int(stripped_raw_mem)\n    elif raw_mem.endswith(\"G\"):\n        stripped_raw_mem = raw_mem.strip(\"G\")\n        if not stripped_raw_mem.isdigit():\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        mem_MB = int(stripped_raw_mem) * 10**3\n    elif raw_mem.endswith(\"T\"):\n        stripped_raw_mem = raw_mem.strip(\"T\")\n        if not stripped_raw_mem.isdigit():\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        mem_MB = int(stripped_raw_mem) * 10**6\n    else:\n        logger.error(error_msg)\n        raise SlurmConfigError(error_msg)\n\n    logger.debug(f\"{info}, return {mem_MB}\")\n    return mem_MB\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/","title":"executors","text":""},{"location":"reference/fractal_server/runner/executors/base_runner/","title":"base_runner","text":""},{"location":"reference/fractal_server/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.BaseRunner","title":"<code>BaseRunner</code>","text":"<p>Base class for Fractal runners.</p> Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>class BaseRunner:\n    \"\"\"\n    Base class for Fractal runners.\n    \"\"\"\n\n    shared_config: JobRunnerConfigLocal | JobRunnerConfigSLURM\n\n    executor_error_log: str | None = None\n\n    def submit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        parameters: dict[str, Any],\n        history_unit_id: int,\n        task_type: SubmitTaskType,\n        task_files: TaskFiles,\n        user_id: int,\n        config: Any,\n    ) -&gt; tuple[Any, BaseException | None]:\n        \"\"\"\n        Run a single fractal task.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            parameters: Dictionary of parameters.\n            history_unit_id:\n                Database ID of the corresponding `HistoryUnit` entry.\n            task_type: Task type.\n            task_files: `TaskFiles` object.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n        raise NotImplementedError()\n\n    def multisubmit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        list_parameters: list[dict],\n        history_unit_ids: list[int],\n        list_task_files: list[TaskFiles],\n        task_type: MultisubmitTaskType,\n        config: Any,\n        user_id: int,\n    ) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n        \"\"\"\n        Run a parallel fractal task.\n\n        Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n        have the same size. For parallel tasks, this is also the number of\n        input images, while for compound tasks these can differ.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            list_parameters:\n                List of dictionaries of parameters (each one must include\n                `zarr_urls` key).\n            history_unit_ids:\n                Database IDs of the corresponding `HistoryUnit` entries.\n            list_task_files: `TaskFiles` objects.\n            task_type: Task type.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n        raise NotImplementedError()\n\n    def validate_submit_parameters(\n        self,\n        parameters: dict[str, Any],\n        task_type: SubmitTaskType | MultisubmitTaskType,\n    ) -&gt; None:\n        \"\"\"\n        Validate parameters for `submit` method\n\n        Args:\n            parameters: Parameters dictionary.\n            task_type: Task type.s\n        \"\"\"\n        logger.info(\"[validate_submit_parameters] START\")\n        if task_type not in TASK_TYPES_SUBMIT:\n            raise ValueError(f\"Invalid {task_type=} for `submit`.\")\n        if not isinstance(parameters, dict):\n            raise ValueError(\"`parameters` must be a dictionary.\")\n        if task_type in [\n            TaskType.NON_PARALLEL,\n            TaskType.COMPOUND,\n        ]:\n            if \"zarr_urls\" not in parameters.keys():\n                raise ValueError(\n                    f\"No 'zarr_urls' key in in {list(parameters.keys())}\"\n                )\n        elif task_type in [\n            TaskType.CONVERTER_NON_PARALLEL,\n            TaskType.CONVERTER_COMPOUND,\n        ]:\n            if \"zarr_urls\" in parameters.keys():\n                raise ValueError(\n                    f\"Forbidden 'zarr_urls' key in {list(parameters.keys())}\"\n                )\n        logger.info(\"[validate_submit_parameters] END\")\n\n    def validate_multisubmit_parameters(\n        self,\n        *,\n        task_type: MultisubmitTaskType,\n        list_parameters: list[dict[str, Any]],\n        list_task_files: list[TaskFiles],\n        history_unit_ids: list[int],\n    ) -&gt; None:\n        \"\"\"\n        Validate parameters for `multisubmit` method\n\n        Args:\n            task_type: Task type.\n            list_parameters: List of parameters dictionaries.\n            list_task_files:\n            history_unit_ids:\n        \"\"\"\n        if task_type not in TASK_TYPES_MULTISUBMIT:\n            raise ValueError(f\"Invalid {task_type=} for `multisubmit`.\")\n\n        if not isinstance(list_parameters, list):\n            raise ValueError(\"`parameters` must be a list.\")\n\n        if len(list_parameters) != len(list_task_files):\n            raise ValueError(\n                f\"{len(list_task_files)=} differs from \"\n                f\"{len(list_parameters)=}.\"\n            )\n        if len(history_unit_ids) != len(list_parameters):\n            raise ValueError(\n                f\"{len(history_unit_ids)=} differs from \"\n                f\"{len(list_parameters)=}.\"\n            )\n\n        subfolders = {\n            task_file.wftask_subfolder_local for task_file in list_task_files\n        }\n        if len(subfolders) != 1:\n            raise ValueError(f\"More than one subfolders: {subfolders}.\")\n\n        for single_kwargs in list_parameters:\n            if not isinstance(single_kwargs, dict):\n                raise ValueError(\"kwargs itemt must be a dictionary.\")\n            if \"zarr_url\" not in single_kwargs.keys():\n                raise ValueError(\n                    f\"No 'zarr_url' key in in {list(single_kwargs.keys())}\"\n                )\n        if task_type == TaskType.PARALLEL:\n            zarr_urls = [kwargs[\"zarr_url\"] for kwargs in list_parameters]\n            if len(zarr_urls) != len(set(zarr_urls)):\n                raise ValueError(\"Non-unique zarr_urls\")\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.BaseRunner.multisubmit","title":"<code>multisubmit(*, base_command, workflow_task_order, workflow_task_id, task_name, list_parameters, history_unit_ids, list_task_files, task_type, config, user_id)</code>","text":"<p>Run a parallel fractal task.</p> <p>Note: <code>list_parameters</code>, <code>list_task_files</code> and <code>history_unit_ids</code> have the same size. For parallel tasks, this is also the number of input images, while for compound tasks these can differ.</p> <p>Parameters:</p> Name Type Description Default <code>base_command</code> <code>str</code> required <code>workflow_task_order</code> <code>int</code> required <code>workflow_task_id</code> <code>int</code> required <code>task_name</code> <code>str</code> required <code>list_parameters</code> <code>list[dict]</code> <p>List of dictionaries of parameters (each one must include <code>zarr_urls</code> key).</p> required <code>history_unit_ids</code> <code>list[int]</code> <p>Database IDs of the corresponding <code>HistoryUnit</code> entries.</p> required <code>list_task_files</code> <code>list[TaskFiles]</code> <p><code>TaskFiles</code> objects.</p> required <code>task_type</code> <code>MultisubmitTaskType</code> <p>Task type.</p> required <code>config</code> <code>Any</code> <p>Runner-specific parameters.</p> required <code>user_id</code> <code>int</code> required Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>def multisubmit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    list_parameters: list[dict],\n    history_unit_ids: list[int],\n    list_task_files: list[TaskFiles],\n    task_type: MultisubmitTaskType,\n    config: Any,\n    user_id: int,\n) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n    \"\"\"\n    Run a parallel fractal task.\n\n    Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n    have the same size. For parallel tasks, this is also the number of\n    input images, while for compound tasks these can differ.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        list_parameters:\n            List of dictionaries of parameters (each one must include\n            `zarr_urls` key).\n        history_unit_ids:\n            Database IDs of the corresponding `HistoryUnit` entries.\n        list_task_files: `TaskFiles` objects.\n        task_type: Task type.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.BaseRunner.submit","title":"<code>submit(*, base_command, workflow_task_order, workflow_task_id, task_name, parameters, history_unit_id, task_type, task_files, user_id, config)</code>","text":"<p>Run a single fractal task.</p> <p>Parameters:</p> Name Type Description Default <code>base_command</code> <code>str</code> required <code>workflow_task_order</code> <code>int</code> required <code>workflow_task_id</code> <code>int</code> required <code>task_name</code> <code>str</code> required <code>parameters</code> <code>dict[str, Any]</code> <p>Dictionary of parameters.</p> required <code>history_unit_id</code> <code>int</code> <p>Database ID of the corresponding <code>HistoryUnit</code> entry.</p> required <code>task_type</code> <code>SubmitTaskType</code> <p>Task type.</p> required <code>task_files</code> <code>TaskFiles</code> <p><code>TaskFiles</code> object.</p> required <code>config</code> <code>Any</code> <p>Runner-specific parameters.</p> required <code>user_id</code> <code>int</code> required Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>def submit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    parameters: dict[str, Any],\n    history_unit_id: int,\n    task_type: SubmitTaskType,\n    task_files: TaskFiles,\n    user_id: int,\n    config: Any,\n) -&gt; tuple[Any, BaseException | None]:\n    \"\"\"\n    Run a single fractal task.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        parameters: Dictionary of parameters.\n        history_unit_id:\n            Database ID of the corresponding `HistoryUnit` entry.\n        task_type: Task type.\n        task_files: `TaskFiles` object.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.BaseRunner.validate_multisubmit_parameters","title":"<code>validate_multisubmit_parameters(*, task_type, list_parameters, list_task_files, history_unit_ids)</code>","text":"<p>Validate parameters for <code>multisubmit</code> method</p> <p>Parameters:</p> Name Type Description Default <code>task_type</code> <code>MultisubmitTaskType</code> <p>Task type.</p> required <code>list_parameters</code> <code>list[dict[str, Any]]</code> <p>List of parameters dictionaries.</p> required <code>list_task_files</code> <code>list[TaskFiles]</code> required <code>history_unit_ids</code> <code>list[int]</code> required Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>def validate_multisubmit_parameters(\n    self,\n    *,\n    task_type: MultisubmitTaskType,\n    list_parameters: list[dict[str, Any]],\n    list_task_files: list[TaskFiles],\n    history_unit_ids: list[int],\n) -&gt; None:\n    \"\"\"\n    Validate parameters for `multisubmit` method\n\n    Args:\n        task_type: Task type.\n        list_parameters: List of parameters dictionaries.\n        list_task_files:\n        history_unit_ids:\n    \"\"\"\n    if task_type not in TASK_TYPES_MULTISUBMIT:\n        raise ValueError(f\"Invalid {task_type=} for `multisubmit`.\")\n\n    if not isinstance(list_parameters, list):\n        raise ValueError(\"`parameters` must be a list.\")\n\n    if len(list_parameters) != len(list_task_files):\n        raise ValueError(\n            f\"{len(list_task_files)=} differs from \"\n            f\"{len(list_parameters)=}.\"\n        )\n    if len(history_unit_ids) != len(list_parameters):\n        raise ValueError(\n            f\"{len(history_unit_ids)=} differs from \"\n            f\"{len(list_parameters)=}.\"\n        )\n\n    subfolders = {\n        task_file.wftask_subfolder_local for task_file in list_task_files\n    }\n    if len(subfolders) != 1:\n        raise ValueError(f\"More than one subfolders: {subfolders}.\")\n\n    for single_kwargs in list_parameters:\n        if not isinstance(single_kwargs, dict):\n            raise ValueError(\"kwargs itemt must be a dictionary.\")\n        if \"zarr_url\" not in single_kwargs.keys():\n            raise ValueError(\n                f\"No 'zarr_url' key in in {list(single_kwargs.keys())}\"\n            )\n    if task_type == TaskType.PARALLEL:\n        zarr_urls = [kwargs[\"zarr_url\"] for kwargs in list_parameters]\n        if len(zarr_urls) != len(set(zarr_urls)):\n            raise ValueError(\"Non-unique zarr_urls\")\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.BaseRunner.validate_submit_parameters","title":"<code>validate_submit_parameters(parameters, task_type)</code>","text":"<p>Validate parameters for <code>submit</code> method</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict[str, Any]</code> <p>Parameters dictionary.</p> required <code>task_type</code> <code>SubmitTaskType | MultisubmitTaskType</code> <p>Task type.s</p> required Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>def validate_submit_parameters(\n    self,\n    parameters: dict[str, Any],\n    task_type: SubmitTaskType | MultisubmitTaskType,\n) -&gt; None:\n    \"\"\"\n    Validate parameters for `submit` method\n\n    Args:\n        parameters: Parameters dictionary.\n        task_type: Task type.s\n    \"\"\"\n    logger.info(\"[validate_submit_parameters] START\")\n    if task_type not in TASK_TYPES_SUBMIT:\n        raise ValueError(f\"Invalid {task_type=} for `submit`.\")\n    if not isinstance(parameters, dict):\n        raise ValueError(\"`parameters` must be a dictionary.\")\n    if task_type in [\n        TaskType.NON_PARALLEL,\n        TaskType.COMPOUND,\n    ]:\n        if \"zarr_urls\" not in parameters.keys():\n            raise ValueError(\n                f\"No 'zarr_urls' key in in {list(parameters.keys())}\"\n            )\n    elif task_type in [\n        TaskType.CONVERTER_NON_PARALLEL,\n        TaskType.CONVERTER_COMPOUND,\n    ]:\n        if \"zarr_urls\" in parameters.keys():\n            raise ValueError(\n                f\"Forbidden 'zarr_urls' key in {list(parameters.keys())}\"\n            )\n    logger.info(\"[validate_submit_parameters] END\")\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/call_command_wrapper/","title":"call_command_wrapper","text":""},{"location":"reference/fractal_server/runner/executors/call_command_wrapper/#fractal_server.runner.executors.call_command_wrapper.call_command_wrapper","title":"<code>call_command_wrapper(*, cmd, log_path)</code>","text":"<p>Call a command and write its stdout and stderr to files</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>str</code> required <code>log_path</code> <code>str</code> required Source code in <code>fractal_server/runner/executors/call_command_wrapper.py</code> <pre><code>def call_command_wrapper(*, cmd: str, log_path: str) -&gt; None:\n    \"\"\"\n    Call a command and write its stdout and stderr to files\n\n    Args:\n        cmd:\n        log_path:\n    \"\"\"\n    try:\n        validate_cmd(cmd)\n    except ValueError as e:\n        raise TaskExecutionError(f\"Invalid command. Original error: {str(e)}\")\n\n    split_cmd = shlex.split(cmd)\n\n    # Verify that task command is executable\n    if shutil.which(split_cmd[0]) is None:\n        msg = (\n            f'Command \"{split_cmd[0]}\" is not valid. '\n            \"Hint: make sure that it is executable.\"\n        )\n        raise TaskExecutionError(msg)\n\n    with open(log_path, \"w\") as fp_log:\n        try:\n            result = subprocess.run(  # nosec\n                split_cmd,\n                stderr=fp_log,\n                stdout=fp_log,\n            )\n        except Exception as e:\n            # This is likely unreachable\n            raise e\n\n    if result.returncode != 0:\n        stderr = \"\"\n        if os.path.isfile(log_path):\n            with open(log_path) as fp_stderr:\n                stderr = fp_stderr.read()\n        raise TaskExecutionError(\n            f\"Task failed with returncode={result.returncode}.\\n\"\n            f\"STDERR: {stderr}\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/local/","title":"local","text":""},{"location":"reference/fractal_server/runner/executors/local/get_local_config/","title":"get_local_config","text":"<p>Submodule to handle the local-backend configuration for a WorkflowTask</p>"},{"location":"reference/fractal_server/runner/executors/local/get_local_config/#fractal_server.runner.executors.local.get_local_config.get_local_backend_config","title":"<code>get_local_backend_config(shared_config, wftask, which_type, tot_tasks=1)</code>","text":"<p>Prepare a specific <code>LocalBackendConfig</code> configuration.</p> <p>The base configuration is the runner-level <code>shared_config</code> object, based on <code>resource.jobs_runner_config</code>. We then incorporate attributes from <code>wftask.meta_{non_parallel,parallel}</code> - with higher priority.</p> <p>Parameters:</p> Name Type Description Default <code>shared_config</code> <code>JobRunnerConfigLocal</code> <p>Configuration object based on <code>resource.jobs_runner_config</code>.</p> required <code>wftask</code> <code>WorkflowTaskV2</code> <p>WorkflowTaskV2 for which the backend configuration should be prepared.</p> required <code>which_type</code> <code>Literal['non_parallel', 'parallel']</code> <p>Whether we should look at the non-parallel or parallel part of <code>wftask</code>.</p> required <code>tot_tasks</code> <code>int</code> <p>Not used here, only present as a common interface.</p> <code>1</code> <p>Returns:</p> Type Description <code>JobRunnerConfigLocal</code> <p>A ready-to-use local-backend configuration object.</p> Source code in <code>fractal_server/runner/executors/local/get_local_config.py</code> <pre><code>def get_local_backend_config(\n    shared_config: JobRunnerConfigLocal,\n    wftask: WorkflowTaskV2,\n    which_type: Literal[\"non_parallel\", \"parallel\"],\n    tot_tasks: int = 1,\n) -&gt; JobRunnerConfigLocal:\n    \"\"\"\n    Prepare a specific `LocalBackendConfig` configuration.\n\n    The base configuration is the runner-level `shared_config` object, based\n    on `resource.jobs_runner_config`. We then incorporate attributes from\n    `wftask.meta_{non_parallel,parallel}` - with higher priority.\n\n    Args:\n        shared_config:\n            Configuration object based on `resource.jobs_runner_config`.\n        wftask:\n            WorkflowTaskV2 for which the backend configuration should\n            be prepared.\n        which_type:\n            Whether we should look at the non-parallel or parallel part\n            of `wftask`.\n        tot_tasks: Not used here, only present as a common interface.\n\n    Returns:\n        A ready-to-use local-backend configuration object.\n    \"\"\"\n\n    if which_type == \"non_parallel\":\n        wftask_meta = wftask.meta_non_parallel\n    elif which_type == \"parallel\":\n        wftask_meta = wftask.meta_parallel\n    else:\n        raise ValueError(\n            f\"Invalid {which_type=} in `get_local_backend_config`.\"\n        )\n\n    __KEY__ = \"parallel_tasks_per_job\"\n    output = shared_config.model_copy(deep=True)\n    if wftask_meta and __KEY__ in wftask_meta:\n        output.parallel_tasks_per_job = wftask_meta[__KEY__]\n    return output\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/local/runner/","title":"runner","text":""},{"location":"reference/fractal_server/runner/executors/local/runner/#fractal_server.runner.executors.local.runner.LocalRunner","title":"<code>LocalRunner</code>","text":"<p>               Bases: <code>BaseRunner</code></p> Source code in <code>fractal_server/runner/executors/local/runner.py</code> <pre><code>class LocalRunner(BaseRunner):\n    executor: ThreadPoolExecutor\n    root_dir_local: Path\n    shared_config: JobRunnerConfigLocal\n\n    def __init__(\n        self,\n        root_dir_local: Path,\n        resource: Resource,\n        profile: Profile,\n    ):\n        self.root_dir_local = root_dir_local\n        self.root_dir_local.mkdir(parents=True, exist_ok=True)\n        self.executor = ThreadPoolExecutor()\n        logger.debug(\"Create LocalRunner\")\n        self.shared_config = JobRunnerConfigLocal(\n            **resource.jobs_runner_config\n        )\n\n    def __enter__(self):\n        logger.debug(\"Enter LocalRunner\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        logger.debug(\"Exit LocalRunner\")\n        self.executor.shutdown(\n            wait=False,\n            cancel_futures=True,\n        )\n        return self.executor.__exit__(exc_type, exc_val, exc_tb)\n\n    def submit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        parameters: dict[str, Any],\n        history_unit_id: int,\n        task_files: TaskFiles,\n        config: JobRunnerConfigLocal,\n        task_type: SubmitTaskType,\n        user_id: int,\n    ) -&gt; tuple[Any, Exception | None]:\n        \"\"\"\n        Run a single fractal task.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            parameters: Dictionary of parameters.\n            history_unit_id:\n                Database ID of the corresponding `HistoryUnit` entry.\n            task_type: Task type.\n            task_files: `TaskFiles` object.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n        logger.debug(\"[submit] START\")\n\n        try:\n            self.validate_submit_parameters(parameters, task_type=task_type)\n            workdir_local = task_files.wftask_subfolder_local\n            workdir_local.mkdir()\n\n            # SUBMISSION PHASE\n            future = self.executor.submit(\n                run_single_task,\n                base_command=base_command,\n                parameters=parameters,\n                task_files=task_files,\n            )\n        except Exception as e:\n            logger.error(\n                \"[submit] Unexpected exception during submission. \"\n                f\"Original error {str(e)}\"\n            )\n            result = None\n            exception = TaskExecutionError(str(e))\n            with next(get_sync_db()) as db:\n                update_status_of_history_unit(\n                    history_unit_id=history_unit_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n                return None, exception\n\n        # RETRIEVAL PHASE\n        with next(get_sync_db()) as db:\n            try:\n                result = future.result()\n                logger.debug(\"[submit] END with result\")\n                if task_type not in [\n                    TaskType.COMPOUND,\n                    TaskType.CONVERTER_COMPOUND,\n                ]:\n                    update_status_of_history_unit(\n                        history_unit_id=history_unit_id,\n                        status=HistoryUnitStatus.DONE,\n                        db_sync=db,\n                    )\n                return result, None\n            except Exception as e:\n                logger.debug(\"[submit] END with exception\")\n                update_status_of_history_unit(\n                    history_unit_id=history_unit_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n                return None, TaskExecutionError(str(e))\n\n    def multisubmit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        list_parameters: list[dict[str, Any]],\n        history_unit_ids: list[int],\n        list_task_files: list[TaskFiles],\n        task_type: MultisubmitTaskType,\n        config: JobRunnerConfigLocal,\n        user_id: int,\n    ) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n        \"\"\"\n        Run a parallel fractal task.\n\n        Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n        have the same size. For parallel tasks, this is also the number of\n        input images, while for compound tasks these can differ.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            list_parameters:\n                List of dictionaries of parameters (each one must include\n                `zarr_urls` key).\n            history_unit_ids:\n                Database IDs of the corresponding `HistoryUnit` entries.\n            list_task_files: `TaskFiles` objects.\n            task_type: Task type.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n\n        logger.debug(f\"[multisubmit] START, {len(list_parameters)=}\")\n        results: dict[int, Any] = {}\n        exceptions: dict[int, BaseException] = {}\n\n        try:\n            self.validate_multisubmit_parameters(\n                list_parameters=list_parameters,\n                task_type=task_type,\n                list_task_files=list_task_files,\n                history_unit_ids=history_unit_ids,\n            )\n\n            workdir_local = list_task_files[0].wftask_subfolder_local\n            # Note: the `mkdir` is not needed for compound tasks, but it is\n            # needed for parallel tasks\n            workdir_local.mkdir(exist_ok=True)\n\n            # Set `n_elements` and `parallel_tasks_per_job`\n            n_elements = len(list_parameters)\n            parallel_tasks_per_job = config.parallel_tasks_per_job\n            if parallel_tasks_per_job is None:\n                parallel_tasks_per_job = n_elements\n\n        except Exception as e:\n            logger.error(\n                \"[multisubmit] Unexpected exception during preliminary phase. \"\n                f\"Original error {str(e)}\"\n            )\n            exception = TaskExecutionError(str(e))\n            exceptions = {\n                ind: exception for ind in range(len(list_parameters))\n            }\n            if task_type == TaskType.PARALLEL:\n                with next(get_sync_db()) as db:\n                    bulk_update_status_of_history_unit(\n                        history_unit_ids=history_unit_ids,\n                        status=HistoryUnitStatus.FAILED,\n                        db_sync=db,\n                    )\n            return results, exceptions\n\n        # Execute tasks, in chunks of size `parallel_tasks_per_job`\n        for ind_chunk in range(0, n_elements, parallel_tasks_per_job):\n            list_parameters_chunk = list_parameters[\n                ind_chunk : ind_chunk + parallel_tasks_per_job\n            ]\n\n            active_futures: dict[int, Future] = {}\n            for ind_within_chunk, kwargs in enumerate(list_parameters_chunk):\n                positional_index = ind_chunk + ind_within_chunk\n                try:\n                    future = self.executor.submit(\n                        run_single_task,\n                        base_command=base_command,\n                        parameters=list_parameters[positional_index],\n                        task_files=list_task_files[positional_index],\n                    )\n                    active_futures[positional_index] = future\n                except Exception as e:\n                    logger.error(\n                        \"[multisubmit] Unexpected exception during submission.\"\n                        f\" Original error {str(e)}\"\n                    )\n                    current_history_unit_id = history_unit_ids[\n                        positional_index\n                    ]\n                    exceptions[positional_index] = TaskExecutionError(str(e))\n                    if task_type == TaskType.PARALLEL:\n                        with next(get_sync_db()) as db:\n                            update_status_of_history_unit(\n                                history_unit_id=current_history_unit_id,\n                                status=HistoryUnitStatus.FAILED,\n                                db_sync=db,\n                            )\n            while active_futures:\n                finished_futures = [\n                    index_and_future\n                    for index_and_future in active_futures.items()\n                    if not index_and_future[1].running()\n                ]\n                if len(finished_futures) == 0:\n                    continue\n\n                with next(get_sync_db()) as db:\n                    for positional_index, fut in finished_futures:\n                        active_futures.pop(positional_index)\n                        if task_type == TaskType.PARALLEL:\n                            current_history_unit_id = history_unit_ids[\n                                positional_index\n                            ]\n\n                        try:\n                            results[positional_index] = fut.result()\n                            if task_type == TaskType.PARALLEL:\n                                update_status_of_history_unit(\n                                    history_unit_id=current_history_unit_id,\n                                    status=HistoryUnitStatus.DONE,\n                                    db_sync=db,\n                                )\n\n                        except Exception as e:\n                            logger.debug(\n                                \"Multisubmit failed in retrieval \"\n                                \"phase with the following error \"\n                                f\"{str(e)}\"\n                            )\n                            exceptions[positional_index] = TaskExecutionError(\n                                str(e)\n                            )\n                            if task_type == TaskType.PARALLEL:\n                                update_status_of_history_unit(\n                                    history_unit_id=current_history_unit_id,\n                                    status=HistoryUnitStatus.FAILED,\n                                    db_sync=db,\n                                )\n\n        logger.debug(f\"[multisubmit] END, {len(results)=}, {len(exceptions)=}\")\n\n        return results, exceptions\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/local/runner/#fractal_server.runner.executors.local.runner.LocalRunner.multisubmit","title":"<code>multisubmit(*, base_command, workflow_task_order, workflow_task_id, task_name, list_parameters, history_unit_ids, list_task_files, task_type, config, user_id)</code>","text":"<p>Run a parallel fractal task.</p> <p>Note: <code>list_parameters</code>, <code>list_task_files</code> and <code>history_unit_ids</code> have the same size. For parallel tasks, this is also the number of input images, while for compound tasks these can differ.</p> <p>Parameters:</p> Name Type Description Default <code>base_command</code> <code>str</code> required <code>workflow_task_order</code> <code>int</code> required <code>workflow_task_id</code> <code>int</code> required <code>task_name</code> <code>str</code> required <code>list_parameters</code> <code>list[dict[str, Any]]</code> <p>List of dictionaries of parameters (each one must include <code>zarr_urls</code> key).</p> required <code>history_unit_ids</code> <code>list[int]</code> <p>Database IDs of the corresponding <code>HistoryUnit</code> entries.</p> required <code>list_task_files</code> <code>list[TaskFiles]</code> <p><code>TaskFiles</code> objects.</p> required <code>task_type</code> <code>MultisubmitTaskType</code> <p>Task type.</p> required <code>config</code> <code>JobRunnerConfigLocal</code> <p>Runner-specific parameters.</p> required <code>user_id</code> <code>int</code> required Source code in <code>fractal_server/runner/executors/local/runner.py</code> <pre><code>def multisubmit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    list_parameters: list[dict[str, Any]],\n    history_unit_ids: list[int],\n    list_task_files: list[TaskFiles],\n    task_type: MultisubmitTaskType,\n    config: JobRunnerConfigLocal,\n    user_id: int,\n) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n    \"\"\"\n    Run a parallel fractal task.\n\n    Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n    have the same size. For parallel tasks, this is also the number of\n    input images, while for compound tasks these can differ.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        list_parameters:\n            List of dictionaries of parameters (each one must include\n            `zarr_urls` key).\n        history_unit_ids:\n            Database IDs of the corresponding `HistoryUnit` entries.\n        list_task_files: `TaskFiles` objects.\n        task_type: Task type.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n\n    logger.debug(f\"[multisubmit] START, {len(list_parameters)=}\")\n    results: dict[int, Any] = {}\n    exceptions: dict[int, BaseException] = {}\n\n    try:\n        self.validate_multisubmit_parameters(\n            list_parameters=list_parameters,\n            task_type=task_type,\n            list_task_files=list_task_files,\n            history_unit_ids=history_unit_ids,\n        )\n\n        workdir_local = list_task_files[0].wftask_subfolder_local\n        # Note: the `mkdir` is not needed for compound tasks, but it is\n        # needed for parallel tasks\n        workdir_local.mkdir(exist_ok=True)\n\n        # Set `n_elements` and `parallel_tasks_per_job`\n        n_elements = len(list_parameters)\n        parallel_tasks_per_job = config.parallel_tasks_per_job\n        if parallel_tasks_per_job is None:\n            parallel_tasks_per_job = n_elements\n\n    except Exception as e:\n        logger.error(\n            \"[multisubmit] Unexpected exception during preliminary phase. \"\n            f\"Original error {str(e)}\"\n        )\n        exception = TaskExecutionError(str(e))\n        exceptions = {\n            ind: exception for ind in range(len(list_parameters))\n        }\n        if task_type == TaskType.PARALLEL:\n            with next(get_sync_db()) as db:\n                bulk_update_status_of_history_unit(\n                    history_unit_ids=history_unit_ids,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n        return results, exceptions\n\n    # Execute tasks, in chunks of size `parallel_tasks_per_job`\n    for ind_chunk in range(0, n_elements, parallel_tasks_per_job):\n        list_parameters_chunk = list_parameters[\n            ind_chunk : ind_chunk + parallel_tasks_per_job\n        ]\n\n        active_futures: dict[int, Future] = {}\n        for ind_within_chunk, kwargs in enumerate(list_parameters_chunk):\n            positional_index = ind_chunk + ind_within_chunk\n            try:\n                future = self.executor.submit(\n                    run_single_task,\n                    base_command=base_command,\n                    parameters=list_parameters[positional_index],\n                    task_files=list_task_files[positional_index],\n                )\n                active_futures[positional_index] = future\n            except Exception as e:\n                logger.error(\n                    \"[multisubmit] Unexpected exception during submission.\"\n                    f\" Original error {str(e)}\"\n                )\n                current_history_unit_id = history_unit_ids[\n                    positional_index\n                ]\n                exceptions[positional_index] = TaskExecutionError(str(e))\n                if task_type == TaskType.PARALLEL:\n                    with next(get_sync_db()) as db:\n                        update_status_of_history_unit(\n                            history_unit_id=current_history_unit_id,\n                            status=HistoryUnitStatus.FAILED,\n                            db_sync=db,\n                        )\n        while active_futures:\n            finished_futures = [\n                index_and_future\n                for index_and_future in active_futures.items()\n                if not index_and_future[1].running()\n            ]\n            if len(finished_futures) == 0:\n                continue\n\n            with next(get_sync_db()) as db:\n                for positional_index, fut in finished_futures:\n                    active_futures.pop(positional_index)\n                    if task_type == TaskType.PARALLEL:\n                        current_history_unit_id = history_unit_ids[\n                            positional_index\n                        ]\n\n                    try:\n                        results[positional_index] = fut.result()\n                        if task_type == TaskType.PARALLEL:\n                            update_status_of_history_unit(\n                                history_unit_id=current_history_unit_id,\n                                status=HistoryUnitStatus.DONE,\n                                db_sync=db,\n                            )\n\n                    except Exception as e:\n                        logger.debug(\n                            \"Multisubmit failed in retrieval \"\n                            \"phase with the following error \"\n                            f\"{str(e)}\"\n                        )\n                        exceptions[positional_index] = TaskExecutionError(\n                            str(e)\n                        )\n                        if task_type == TaskType.PARALLEL:\n                            update_status_of_history_unit(\n                                history_unit_id=current_history_unit_id,\n                                status=HistoryUnitStatus.FAILED,\n                                db_sync=db,\n                            )\n\n    logger.debug(f\"[multisubmit] END, {len(results)=}, {len(exceptions)=}\")\n\n    return results, exceptions\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/local/runner/#fractal_server.runner.executors.local.runner.LocalRunner.submit","title":"<code>submit(*, base_command, workflow_task_order, workflow_task_id, task_name, parameters, history_unit_id, task_files, config, task_type, user_id)</code>","text":"<p>Run a single fractal task.</p> <p>Parameters:</p> Name Type Description Default <code>base_command</code> <code>str</code> required <code>workflow_task_order</code> <code>int</code> required <code>workflow_task_id</code> <code>int</code> required <code>task_name</code> <code>str</code> required <code>parameters</code> <code>dict[str, Any]</code> <p>Dictionary of parameters.</p> required <code>history_unit_id</code> <code>int</code> <p>Database ID of the corresponding <code>HistoryUnit</code> entry.</p> required <code>task_type</code> <code>SubmitTaskType</code> <p>Task type.</p> required <code>task_files</code> <code>TaskFiles</code> <p><code>TaskFiles</code> object.</p> required <code>config</code> <code>JobRunnerConfigLocal</code> <p>Runner-specific parameters.</p> required <code>user_id</code> <code>int</code> required Source code in <code>fractal_server/runner/executors/local/runner.py</code> <pre><code>def submit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    parameters: dict[str, Any],\n    history_unit_id: int,\n    task_files: TaskFiles,\n    config: JobRunnerConfigLocal,\n    task_type: SubmitTaskType,\n    user_id: int,\n) -&gt; tuple[Any, Exception | None]:\n    \"\"\"\n    Run a single fractal task.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        parameters: Dictionary of parameters.\n        history_unit_id:\n            Database ID of the corresponding `HistoryUnit` entry.\n        task_type: Task type.\n        task_files: `TaskFiles` object.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n    logger.debug(\"[submit] START\")\n\n    try:\n        self.validate_submit_parameters(parameters, task_type=task_type)\n        workdir_local = task_files.wftask_subfolder_local\n        workdir_local.mkdir()\n\n        # SUBMISSION PHASE\n        future = self.executor.submit(\n            run_single_task,\n            base_command=base_command,\n            parameters=parameters,\n            task_files=task_files,\n        )\n    except Exception as e:\n        logger.error(\n            \"[submit] Unexpected exception during submission. \"\n            f\"Original error {str(e)}\"\n        )\n        result = None\n        exception = TaskExecutionError(str(e))\n        with next(get_sync_db()) as db:\n            update_status_of_history_unit(\n                history_unit_id=history_unit_id,\n                status=HistoryUnitStatus.FAILED,\n                db_sync=db,\n            )\n            return None, exception\n\n    # RETRIEVAL PHASE\n    with next(get_sync_db()) as db:\n        try:\n            result = future.result()\n            logger.debug(\"[submit] END with result\")\n            if task_type not in [\n                TaskType.COMPOUND,\n                TaskType.CONVERTER_COMPOUND,\n            ]:\n                update_status_of_history_unit(\n                    history_unit_id=history_unit_id,\n                    status=HistoryUnitStatus.DONE,\n                    db_sync=db,\n                )\n            return result, None\n        except Exception as e:\n            logger.debug(\"[submit] END with exception\")\n            update_status_of_history_unit(\n                history_unit_id=history_unit_id,\n                status=HistoryUnitStatus.FAILED,\n                db_sync=db,\n            )\n            return None, TaskExecutionError(str(e))\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/","title":"slurm_common","text":""},{"location":"reference/fractal_server/runner/executors/slurm_common/_batching/","title":"_batching","text":"<p>Submodule to determine the number of total/parallel tasks per SLURM job.</p>"},{"location":"reference/fractal_server/runner/executors/slurm_common/_batching/#fractal_server.runner.executors.slurm_common._batching._estimate_parallel_tasks_per_job","title":"<code>_estimate_parallel_tasks_per_job(*, cpus_per_task, mem_per_task, max_cpus_per_job, max_mem_per_job)</code>","text":"<p>Compute how many parallel tasks can fit in a given SLURM job</p> <p>Note: If more resources than available are requested, return 1. This assumes that further checks will be performed on the output of the current function, as is the case in the <code>heuristics</code> function below.</p> <p>Parameters:</p> Name Type Description Default <code>cpus_per_task</code> <code>int</code> <p>Number of CPUs needed for one task.</p> required <code>mem_per_task</code> <code>int</code> <p>Memory (in MB) needed for one task.</p> required <code>max_cpus_per_job</code> <code>int</code> <p>Maximum number of CPUs available for one job.</p> required <code>max_mem_per_job</code> <code>int</code> <p>Maximum memory (in MB) available for one job.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of parallel tasks per job</p> Source code in <code>fractal_server/runner/executors/slurm_common/_batching.py</code> <pre><code>def _estimate_parallel_tasks_per_job(\n    *,\n    cpus_per_task: int,\n    mem_per_task: int,\n    max_cpus_per_job: int,\n    max_mem_per_job: int,\n) -&gt; int:\n    \"\"\"\n    Compute how many parallel tasks can fit in a given SLURM job\n\n    Note: If more resources than available are requested, return 1. This\n    assumes that further checks will be performed on the output of the current\n    function, as is the case in the `heuristics` function below.\n\n    Args:\n        cpus_per_task: Number of CPUs needed for one task.\n        mem_per_task: Memory (in MB) needed for one task.\n        max_cpus_per_job: Maximum number of CPUs available for one job.\n        max_mem_per_job: Maximum memory (in MB) available for one job.\n\n    Returns:\n        Number of parallel tasks per job\n    \"\"\"\n    if cpus_per_task &gt; max_cpus_per_job or mem_per_task &gt; max_mem_per_job:\n        return 1\n    val_based_on_cpus = max_cpus_per_job // cpus_per_task\n    val_based_on_mem = max_mem_per_job // mem_per_task\n    return min(val_based_on_cpus, val_based_on_mem)\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/_batching/#fractal_server.runner.executors.slurm_common._batching.heuristics","title":"<code>heuristics(*, tot_tasks, tasks_per_job=None, parallel_tasks_per_job=None, cpus_per_task, mem_per_task, target_cpus_per_job, max_cpus_per_job, target_mem_per_job, max_mem_per_job, target_num_jobs, max_num_jobs)</code>","text":"<p>Heuristically determine parameters for multi-task batching</p> <p>\"In-job queues\" refer to the case where <code>parallel_tasks_per_job&lt;tasks_per_job</code>, that is, where not all tasks of a given SLURM job will be executed at the same time.</p> <p>This function goes through the following branches:</p> <ol> <li>Validate/fix parameters, if they are provided as input.</li> <li>Heuristically determine parameters based on the per-task resource    requirements and on the target amount of per-job resources, without    resorting to in-job queues.</li> <li>Heuristically determine parameters based on the per-task resource    requirements and on the maximum amount of per-job resources, without    resorting to in-job queues.</li> <li>Heuristically determine parameters (based on the per-task resource    requirements and on the maximum amount of per-job resources) and then    introduce in-job queues to satisfy the hard constraint on the maximum    number of jobs.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>tot_tasks</code> <code>int</code> <p>Total number of elements to be processed (e.g. number of images in a OME-NGFF array).</p> required <code>tasks_per_job</code> <code>int | None</code> <p>If <code>tasks_per_job</code> and <code>parallel_tasks_per_job</code> are not <code>None</code>, validate/edit this choice.</p> <code>None</code> <code>parallel_tasks_per_job</code> <code>int | None</code> <p>If <code>tasks_per_job</code> and <code>parallel_tasks_per_job</code> are not <code>None</code>, validate/edit this choice.</p> <code>None</code> <code>cpus_per_task</code> <code>int</code> <p>Number of CPUs needed for each parallel task.</p> required <code>mem_per_task</code> <code>int</code> <p>Memory (in MB) needed for each parallel task.</p> required <code>target_cpus_per_job</code> <code>int</code> <p>Optimal number of CPUs for each SLURM job.</p> required <code>max_cpus_per_job</code> <code>int</code> <p>Maximum number of CPUs for each SLURM job.</p> required <code>target_mem_per_job</code> <code>int</code> <p>Optimal amount of memory (in MB) for each SLURM job.</p> required <code>max_mem_per_job</code> <code>int</code> <p>Maximum amount of memory (in MB) for each SLURM job.</p> required <code>target_num_jobs</code> <code>int</code> <p>Optimal total number of SLURM jobs for a given WorkflowTask.</p> required <code>max_num_jobs</code> <code>int</code> <p>Maximum total number of SLURM jobs for a given WorkflowTask.</p> required <p>Return:     Valid values of <code>tasks_per_job</code> and <code>parallel_tasks_per_job</code>.</p> Source code in <code>fractal_server/runner/executors/slurm_common/_batching.py</code> <pre><code>def heuristics(\n    *,\n    # Number of parallel components (always known)\n    tot_tasks: int,\n    # Optional WorkflowTask attributes:\n    tasks_per_job: int | None = None,\n    parallel_tasks_per_job: int | None = None,\n    # Task requirements (multiple possible sources):\n    cpus_per_task: int,\n    mem_per_task: int,\n    # Fractal configuration variables (soft/hard limits):\n    target_cpus_per_job: int,\n    max_cpus_per_job: int,\n    target_mem_per_job: int,  # in MB\n    max_mem_per_job: int,  # in MB\n    target_num_jobs: int,\n    max_num_jobs: int,\n) -&gt; tuple[int, int]:\n    \"\"\"\n    Heuristically determine parameters for multi-task batching\n\n    \"In-job queues\" refer to the case where\n    `parallel_tasks_per_job&lt;tasks_per_job`, that is, where not all\n    tasks of a given SLURM job will be executed at the same time.\n\n    This function goes through the following branches:\n\n    1. Validate/fix parameters, if they are provided as input.\n    2. Heuristically determine parameters based on the per-task resource\n       requirements and on the target amount of per-job resources, without\n       resorting to in-job queues.\n    3. Heuristically determine parameters based on the per-task resource\n       requirements and on the maximum amount of per-job resources, without\n       resorting to in-job queues.\n    4. Heuristically determine parameters (based on the per-task resource\n       requirements and on the maximum amount of per-job resources) and then\n       introduce in-job queues to satisfy the hard constraint on the maximum\n       number of jobs.\n\n    Args:\n        tot_tasks:\n            Total number of elements to be processed (e.g. number of images in\n            a OME-NGFF array).\n        tasks_per_job:\n            If `tasks_per_job` and `parallel_tasks_per_job` are not\n            `None`, validate/edit this choice.\n        parallel_tasks_per_job:\n            If `tasks_per_job` and `parallel_tasks_per_job` are not\n            `None`, validate/edit this choice.\n        cpus_per_task:\n            Number of CPUs needed for each parallel task.\n        mem_per_task:\n            Memory (in MB) needed for each parallel task.\n        target_cpus_per_job:\n            Optimal number of CPUs for each SLURM job.\n        max_cpus_per_job:\n            Maximum number of CPUs for each SLURM job.\n        target_mem_per_job:\n            Optimal amount of memory (in MB) for each SLURM job.\n        max_mem_per_job:\n            Maximum amount of memory (in MB) for each SLURM job.\n        target_num_jobs:\n            Optimal total number of SLURM jobs for a given WorkflowTask.\n        max_num_jobs:\n            Maximum total number of SLURM jobs for a given WorkflowTask.\n    Return:\n        Valid values of `tasks_per_job` and `parallel_tasks_per_job`.\n    \"\"\"\n    # Preliminary checks\n    if bool(tasks_per_job) != bool(parallel_tasks_per_job):\n        msg = (\n            \"tasks_per_job and parallel_tasks_per_job must \"\n            \"be both set or both unset\"\n        )\n        logger.error(msg)\n        raise SlurmHeuristicsError(msg)\n    if cpus_per_task &gt; max_cpus_per_job:\n        msg = (\n            f\"[heuristics] Requested {cpus_per_task=} \"\n            f\"but {max_cpus_per_job=}.\"\n        )\n        logger.error(msg)\n        raise SlurmHeuristicsError(msg)\n    if mem_per_task &gt; max_mem_per_job:\n        msg = (\n            f\"[heuristics] Requested {mem_per_task=} \"\n            f\"but {max_mem_per_job=}.\"\n        )\n        logger.error(msg)\n        raise SlurmHeuristicsError(msg)\n\n    # Branch 1: validate/update given parameters\n    if tasks_per_job and parallel_tasks_per_job:\n        # Reduce parallel_tasks_per_job if it exceeds tasks_per_job\n        if parallel_tasks_per_job &gt; tasks_per_job:\n            logger.warning(\n                \"[heuristics] Set parallel_tasks_per_job=\"\n                f\"tasks_per_job={tasks_per_job}\"\n            )\n            parallel_tasks_per_job = tasks_per_job\n\n        # Check requested cpus_per_job\n        cpus_per_job = parallel_tasks_per_job * cpus_per_task\n        if cpus_per_job &gt; target_cpus_per_job:\n            logger.warning(\n                f\"[heuristics] Requested {cpus_per_job=} \"\n                f\"but {target_cpus_per_job=}.\"\n            )\n        if cpus_per_job &gt; max_cpus_per_job:\n            msg = (\n                f\"[heuristics] Requested {cpus_per_job=} \"\n                f\"but {max_cpus_per_job=}.\"\n            )\n            logger.error(msg)\n            raise SlurmHeuristicsError(msg)\n\n        # Check requested mem_per_job\n        mem_per_job = parallel_tasks_per_job * mem_per_task\n        if mem_per_job &gt; target_mem_per_job:\n            logger.warning(\n                f\"[heuristics] Requested {mem_per_job=} \"\n                f\"but {target_mem_per_job=}.\"\n            )\n        if mem_per_job &gt; max_mem_per_job:\n            msg = (\n                f\"[heuristics] Requested {mem_per_job=} \"\n                f\"but {max_mem_per_job=}.\"\n            )\n            logger.error(msg)\n            raise SlurmHeuristicsError(msg)\n\n        # Check number of jobs\n        num_jobs = math.ceil(tot_tasks / tasks_per_job)\n        if num_jobs &gt; target_num_jobs:\n            logger.debug(\n                f\"[heuristics] Requested {num_jobs=} \"\n                f\"but {target_num_jobs=}.\"\n            )\n        if num_jobs &gt; max_num_jobs:\n            msg = f\"[heuristics] Requested {num_jobs=} but {max_num_jobs=}.\"\n            logger.error(msg)\n            raise SlurmHeuristicsError(msg)\n        logger.debug(\"[heuristics] Return from branch 1\")\n        return (tasks_per_job, parallel_tasks_per_job)\n\n    # 2: Target-resources-based heuristics, without in-job queues\n    parallel_tasks_per_job = _estimate_parallel_tasks_per_job(\n        cpus_per_task=cpus_per_task,\n        mem_per_task=mem_per_task,\n        max_cpus_per_job=target_cpus_per_job,\n        max_mem_per_job=target_mem_per_job,\n    )\n    tasks_per_job = parallel_tasks_per_job\n    num_jobs = math.ceil(tot_tasks / tasks_per_job)\n    if num_jobs &lt;= target_num_jobs:\n        logger.debug(\"[heuristics] Return from branch 2\")\n        return (tasks_per_job, parallel_tasks_per_job)\n\n    # Branch 3: Max-resources-based heuristics, without in-job queues\n    parallel_tasks_per_job = _estimate_parallel_tasks_per_job(\n        cpus_per_task=cpus_per_task,\n        mem_per_task=mem_per_task,\n        max_cpus_per_job=max_cpus_per_job,\n        max_mem_per_job=max_mem_per_job,\n    )\n    tasks_per_job = parallel_tasks_per_job\n    num_jobs = math.ceil(tot_tasks / tasks_per_job)\n    if num_jobs &lt;= max_num_jobs:\n        logger.debug(\"[heuristics] Return from branch 3\")\n        return (tasks_per_job, parallel_tasks_per_job)\n\n    # Branch 4: Max-resources-based heuristics, with in-job queues\n    parallel_tasks_per_job = _estimate_parallel_tasks_per_job(\n        cpus_per_task=cpus_per_task,\n        mem_per_task=mem_per_task,\n        max_cpus_per_job=max_cpus_per_job,\n        max_mem_per_job=max_mem_per_job,\n    )\n    tasks_per_job = math.ceil(tot_tasks / max_num_jobs)\n    logger.debug(\"[heuristics] Return from branch 4\")\n    return (tasks_per_job, parallel_tasks_per_job)\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/_job_states/","title":"_job_states","text":""},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/","title":"base_slurm_runner","text":""},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner","title":"<code>BaseSlurmRunner</code>","text":"<p>               Bases: <code>BaseRunner</code></p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>class BaseSlurmRunner(BaseRunner):\n    shutdown_file: Path\n    common_script_lines: list[str]\n    user_cache_dir: str\n    root_dir_local: Path\n    root_dir_remote: Path\n    poll_interval: int\n    poll_interval_internal: float\n    jobs: dict[str, SlurmJob]\n    python_worker_interpreter: str\n    slurm_runner_type: Literal[\"ssh\", \"sudo\"]\n    slurm_account: str | None = None\n    shared_config: JobRunnerConfigSLURM\n\n    def __init__(\n        self,\n        *,\n        root_dir_local: Path,\n        root_dir_remote: Path,\n        slurm_runner_type: Literal[\"ssh\", \"sudo\"],\n        python_worker_interpreter: str,\n        poll_interval: int,\n        common_script_lines: list[str] | None = None,\n        user_cache_dir: str,\n        slurm_account: str | None = None,\n    ):\n        self.slurm_runner_type = slurm_runner_type\n        self.root_dir_local = root_dir_local\n        self.root_dir_remote = root_dir_remote\n        self.common_script_lines = common_script_lines or []\n        self._check_slurm_account()\n        self.user_cache_dir = user_cache_dir\n        self.python_worker_interpreter = python_worker_interpreter\n        self.slurm_account = slurm_account\n\n        self.poll_interval = poll_interval\n        self.poll_interval_internal = self.poll_interval / 10.0\n\n        self.check_fractal_server_versions()\n\n        # Create job folders. Note that the local one may or may not exist\n        # depending on whether it is a test or an actual run\n        try:\n            if not self.root_dir_local.is_dir():\n                self._mkdir_local_folder(self.root_dir_local.as_posix())\n            self._mkdir_remote_folder(self.root_dir_remote.as_posix())\n        except Exception as e:\n            error_msg = (\n                f\"Could not mkdir {self.root_dir_local.as_posix()} or \"\n                f\"{self.root_dir_remote.as_posix()}. \"\n                f\"Original error: {str(e)}.\"\n            )\n            logger.error(error_msg)\n            raise RuntimeError(error_msg)\n\n        self.shutdown_file = self.root_dir_local / SHUTDOWN_FILENAME\n        self.jobs = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        return False\n\n    def _run_remote_cmd(self, cmd: str) -&gt; str:\n        raise NotImplementedError(\"Implement in child class.\")\n\n    def run_squeue(self, *, job_ids: list[str]) -&gt; str:\n        raise NotImplementedError(\"Implement in child class.\")\n\n    def _is_squeue_error_recoverable(self, exception: BaseException) -&gt; bool:\n        \"\"\"\n        Determine whether a `squeue` error is considered recoverable.\n\n        A _recoverable_ error is one which will disappear after some time,\n        without any specific action from the `fractal-server` side.\n\n        Note: if this function returns `True` for an error that does not\n        actually recover, this leads to an infinite loop  where\n        `fractal-server` keeps polling `squeue` information forever.\n\n        More info at\n        https://github.com/fractal-analytics-platform/fractal-server/issues/2682\n\n        Args:\n            exception: The exception raised by `self.run_squeue`.\n        Returns:\n            Whether the error is considered recoverable.\n        \"\"\"\n        str_exception = str(exception)\n        if (\n            \"slurm_load_jobs\" in str_exception\n            and \"Socket timed out on send/recv operation\" in str_exception\n        ):\n            return True\n        else:\n            return False\n\n    def _get_finished_jobs(self, job_ids: list[str]) -&gt; set[str]:\n        #  If there is no Slurm job to check, return right away\n        if not job_ids:\n            return set()\n\n        try:\n            stdout = self.run_squeue(job_ids=job_ids)\n            slurm_statuses = {\n                out.split()[0]: out.split()[1] for out in stdout.splitlines()\n            }\n        except Exception as e:\n            logger.warning(\n                \"[_get_finished_jobs] `squeue` failed, \"\n                \"retry with individual job IDs. \"\n                f\"Original error: {str(e)}.\"\n            )\n            slurm_statuses = dict()\n            for job_id in job_ids:\n                try:\n                    stdout = self.run_squeue(job_ids=[job_id])\n                    slurm_statuses.update(\n                        {stdout.split()[0]: stdout.split()[1]}\n                    )\n                except Exception as e:\n                    msg = (\n                        f\"[_get_finished_jobs] `squeue` failed for {job_id=}. \"\n                        f\"Original error: {str(e)}.\"\n                    )\n                    logger.warning(msg)\n                    if self._is_squeue_error_recoverable(e):\n                        logger.warning(\n                            \"[_get_finished_jobs] Recoverable `squeue` \"\n                            f\"error - mark {job_id=} as FRACTAL_UNDEFINED and\"\n                            \" retry later.\"\n                        )\n                        slurm_statuses.update(\n                            {str(job_id): \"FRACTAL_UNDEFINED\"}\n                        )\n                    else:\n                        logger.warning(\n                            \"[_get_finished_jobs] Non-recoverable `squeue`\"\n                            f\"error - mark {job_id=} as completed.\"\n                        )\n                        slurm_statuses.update({str(job_id): \"COMPLETED\"})\n\n        # If a job is not in `squeue` output, mark it as completed.\n        finished_jobs = {\n            job_id\n            for job_id in job_ids\n            if slurm_statuses.get(job_id, \"COMPLETED\") in STATES_FINISHED\n        }\n        return finished_jobs\n\n    def _mkdir_local_folder(self, folder: str) -&gt; None:\n        raise NotImplementedError(\"Implement in child class.\")\n\n    def _mkdir_remote_folder(self, folder: str) -&gt; None:\n        raise NotImplementedError(\"Implement in child class.\")\n\n    def _enrich_slurm_config(\n        self,\n        slurm_config: SlurmConfig,\n    ) -&gt; SlurmConfig:\n        \"\"\"\n        Return an enriched `SlurmConfig` object\n\n        Include `self.account` and `self.common_script_lines` into a\n        `SlurmConfig` object. Extracting this logic into an independent\n        class method is useful to fix issue #2659 (which was due to\n        performing this same operation multiple times rather than once).\n\n        Args:\n            slurm_config: The original `SlurmConfig` object.\n\n        Returns:\n            A new, up-to-date, `SlurmConfig` object.\n        \"\"\"\n\n        new_slurm_config = slurm_config.model_copy(deep=True)\n\n        # Include SLURM account in `slurm_config`.\n        if self.slurm_account is not None:\n            new_slurm_config.account = self.slurm_account\n\n        # Include common_script_lines in extra_lines\n        if len(self.common_script_lines) &gt; 0:\n            logger.debug(\n                f\"Add {self.common_script_lines} to \"\n                f\"{new_slurm_config.extra_lines=}.\"\n            )\n            current_extra_lines = new_slurm_config.extra_lines\n            new_slurm_config.extra_lines = (\n                current_extra_lines + self.common_script_lines\n            )\n\n        return new_slurm_config\n\n    def _prepare_single_slurm_job(\n        self,\n        *,\n        base_command: str,\n        slurm_job: SlurmJob,\n        slurm_config: SlurmConfig,\n    ) -&gt; str:\n        \"\"\"\n        Prepare submission script locally.\n\n        Args:\n            base_command: Base of task executable command.\n            slurm_job: `SlurmJob` object\n            slurm_config: Configuration for SLURM job\n\n        Returns:\n            Command to submit the SLURM job.\n        \"\"\"\n        logger.debug(\"[_prepare_single_slurm_job] START\")\n\n        for task in slurm_job.tasks:\n            # Write input file\n            if self.slurm_runner_type == \"ssh\":\n                args_file_remote = task.task_files.args_file_remote\n            else:\n                args_file_remote = task.task_files.args_file_local\n            metadiff_file_remote = task.task_files.metadiff_file_remote\n            full_command = (\n                f\"{base_command} \"\n                f\"--args-json {args_file_remote} \"\n                f\"--out-json {metadiff_file_remote}\"\n            )\n\n            input_data = RemoteInputData(\n                full_command=full_command,\n                python_version=sys.version_info[:3],\n                fractal_server_version=__VERSION__,\n                metadiff_file_remote=task.task_files.metadiff_file_remote,\n                log_file_remote=task.task_files.log_file_remote,\n            )\n\n            with open(task.input_file_local, \"w\") as f:\n                json.dump(input_data.model_dump(), f, indent=2)\n\n            with open(task.task_files.args_file_local, \"w\") as f:\n                json.dump(task.parameters, f, indent=2)\n\n            logger.debug(\n                \"[_prepare_single_slurm_job] Written \"\n                f\"{task.input_file_local=}\"\n            )\n\n        # Prepare commands to be included in SLURM submission script\n        cmdlines = []\n        for task in slurm_job.tasks:\n            if self.slurm_runner_type == \"ssh\":\n                input_file = task.input_file_remote\n            else:\n                input_file = task.input_file_local\n            output_file = task.output_file_remote\n            cmdlines.append(\n                f\"{self.python_worker_interpreter}\"\n                \" -m fractal_server.runner.\"\n                \"executors.slurm_common.remote \"\n                f\"--input-file {input_file} \"\n                f\"--output-file {output_file}\"\n            )\n\n        # Set ntasks\n        num_tasks_max_running = slurm_config.parallel_tasks_per_job\n        ntasks = min(len(cmdlines), num_tasks_max_running)\n        slurm_config.parallel_tasks_per_job = ntasks\n\n        # Prepare SLURM preamble based on SlurmConfig object\n        script_lines = slurm_config.to_sbatch_preamble(\n            remote_export_dir=self.user_cache_dir\n        )\n\n        # Extend SLURM preamble with variable which are not in SlurmConfig, and\n        # fix their order\n        script_lines.extend(\n            [\n                f\"#SBATCH --err={slurm_job.slurm_stderr_remote}\",\n                f\"#SBATCH --out={slurm_job.slurm_stdout_remote}\",\n                f\"#SBATCH -D {slurm_job.workdir_remote}\",\n            ]\n        )\n        script_lines = slurm_config.sort_script_lines(script_lines)\n        logger.debug(f\"[_prepare_single_slurm_job] {script_lines=}\")\n\n        # Always print output of `uname -n` and `pwd`\n        script_lines.append('\\necho \"Hostname: $(uname -n)\"')\n        script_lines.append('echo \"Current directory: $(pwd)\"')\n        script_lines.append(\n            'echo \"Start time: $(date +\"%Y-%m-%dT%H:%M:%S%z\")\"'\n        )\n\n        # Complete script preamble\n        script_lines.append(\"\\n\")\n\n        # Include command lines\n        mem_per_task_MB = slurm_config.mem_per_task_MB\n        for cmd in cmdlines:\n            script_lines.append(\n                \"srun --ntasks=1 --cpus-per-task=$SLURM_CPUS_PER_TASK \"\n                f\"--mem={mem_per_task_MB}MB \"\n                f\"{cmd} &amp;\"\n            )\n        script_lines.append(\"wait\\n\\n\")\n        script_lines.append(\n            'echo \"End time:   $(date +\"%Y-%m-%dT%H:%M:%S%z\")\"'\n        )\n        script = \"\\n\".join(script_lines)\n\n        # Write submission script\n        with open(slurm_job.slurm_submission_script_local, \"w\") as f:\n            f.write(script)\n        logger.debug(\n            \"[_prepare_single_slurm_job] Written \"\n            f\"{slurm_job.slurm_submission_script_local=}\"\n        )\n\n        if self.slurm_runner_type == \"ssh\":\n            submit_command = (\n                f\"sbatch --parsable {slurm_job.slurm_submission_script_remote}\"\n            )\n        else:\n            submit_command = (\n                f\"sbatch --parsable {slurm_job.slurm_submission_script_local}\"\n            )\n        logger.debug(\"[_prepare_single_slurm_job] END\")\n        return submit_command\n\n    def _send_many_job_inputs(\n        self, *, workdir_local: Path, workdir_remote: Path\n    ) -&gt; None:\n        \"\"\"\n        Placeholder method.\n\n        This method is intentionally left unimplemented in the base class.\n        Subclasses must override it to provide the logic for transferring\n        input data.\n        \"\"\"\n        pass\n\n    def _submit_single_sbatch(\n        self,\n        *,\n        submit_command: str,\n        slurm_job: SlurmJob,\n    ) -&gt; None:\n        \"\"\"\n        Run `sbatch` and add the `slurm_job` to `self.jobs`.\n\n        Args:\n            submit_command:\n                The SLURM submission command prepared in\n                `self._prepare_single_slurm_job`.\n            slurm_job: The `SlurmJob` object.\n        \"\"\"\n\n        logger.debug(\"[_submit_single_sbatch] START\")\n\n        # Submit SLURM job and retrieve job ID\n        logger.debug(f\"[_submit_single_sbatch] Now run {submit_command=}\")\n        sbatch_stdout = self._run_remote_cmd(submit_command)\n        logger.info(f\"[_submit_single_sbatch] {sbatch_stdout=}\")\n        stdout = sbatch_stdout.strip(\"\\n\")\n        submitted_job_id = int(stdout)\n        slurm_job.slurm_job_id = str(submitted_job_id)\n\n        # Add job to self.jobs\n        self.jobs[slurm_job.slurm_job_id] = slurm_job\n        logger.debug(\n            \"[_submit_single_sbatch] Added \"\n            f\"{slurm_job.slurm_job_id} to self.jobs.\"\n        )\n        logger.debug(\"[_submit_single_sbatch] END\")\n\n    def _fetch_artifacts(\n        self,\n        finished_slurm_jobs: list[SlurmJob],\n    ) -&gt; None:\n        raise NotImplementedError(\"Implement in child class.\")\n\n    def _check_slurm_account(self) -&gt; None:\n        \"\"\"\n        Check that SLURM account is not set here in `common_script_lines`.\n        \"\"\"\n        try:\n            invalid_line = next(\n                line\n                for line in self.common_script_lines\n                if line.startswith(\"#SBATCH --account=\")\n            )\n            raise RuntimeError(\n                \"Invalid line in `common_script_lines`: \"\n                f\"'{invalid_line}'.\\n\"\n                \"SLURM account must be set via the request body of the \"\n                \"apply-workflow endpoint, or by modifying the user properties.\"\n            )\n        except StopIteration:\n            pass\n\n    def _postprocess_single_task(\n        self,\n        *,\n        task: SlurmTask,\n        was_job_scancelled: bool = False,\n    ) -&gt; tuple[Any, Exception | None]:\n        try:\n            with open(task.output_file_local) as f:\n                output = json.load(f)\n            success = output[0]\n            if success:\n                # Task succeeded\n                result = output[1]\n                return (result, None)\n            else:\n                # Task failed in a controlled way, and produced an `output`\n                # object which is a dictionary with required keys\n                # `exc_type_name` and `traceback_string` and with optional\n                # keys `workflow_task_order`, `workflow_task_id` and\n                # `task_name`.\n                exc_proxy = output[1]\n                exc_type_name = exc_proxy.get(\"exc_type_name\")\n                logger.debug(\n                    f\"Output file contains a '{exc_type_name}' exception.\"\n                )\n                traceback_string = output[1].get(\"traceback_string\")\n                exception = TaskExecutionError(\n                    traceback_string,\n                    workflow_task_id=task.workflow_task_id,\n                    workflow_task_order=task.workflow_task_order,\n                    task_name=task.task_name,\n                )\n                return (None, exception)\n\n        except Exception as e:\n            exception = JobExecutionError(f\"ERROR, {str(e)}\")\n            # If job was scancelled and task failed, replace\n            # exception with a shutdown-related one.\n            if was_job_scancelled:\n                logger.debug(\n                    \"Replacing exception with a shutdown-related one, \"\n                    f\"for {task.index=}.\"\n                )\n                exception = SHUTDOWN_EXCEPTION\n            return (None, exception)\n        finally:\n            Path(task.input_file_local).unlink(missing_ok=True)\n            Path(task.output_file_local).unlink(missing_ok=True)\n\n    def _extract_slurm_error(self, slurm_job: SlurmJob) -&gt; str | None:\n        \"\"\"\n        Extract stderr of SLURM job, or `None`.\n\n        Note: this method reads the _local_ stderr file, and then it should\n        always be called _after_ fetching remote artifacts (e.g. in an SSH\n        deployment).\n        \"\"\"\n\n        stderr_path = slurm_job.slurm_stderr_local_path\n\n        if not stderr_path.exists():\n            return None\n\n        try:\n            with open(stderr_path) as f:\n                stderr_content = f.read().strip()\n            if stderr_content:\n                return stderr_content\n        except Exception as e:\n            logger.error(f\"Failed to read SLURM stderr file: {e}\")\n\n        return None\n\n    def _set_executor_error_log(self, slurm_jobs: list[SlurmJob]) -&gt; None:\n        \"\"\"\n        If `executor_error_log` is unset, update it based on a list of jobs.\n\n        Notes:\n        1. This method must be executed **after** `_fetch_artifacts`.\n        2. This method only captures the first error it finds.\n        \"\"\"\n        if self.executor_error_log is not None:\n            # `executor_error_log` is already set, exit\n            return\n        for slurm_job in slurm_jobs:\n            slurm_error = self._extract_slurm_error(slurm_job)\n            if slurm_error is not None:\n                logger.warning(f\"SLURM error detected: {slurm_error}\")\n                self.executor_error_log = slurm_error\n                return\n\n    def is_shutdown(self) -&gt; bool:\n        return self.shutdown_file.exists()\n\n    @property\n    def job_ids(self) -&gt; list[str]:\n        return list(self.jobs.keys())\n\n    @property\n    def job_ids_int(self) -&gt; list[int]:\n        return list(map(int, self.jobs.keys()))\n\n    def wait_and_check_shutdown(self) -&gt; list[str]:\n        \"\"\"\n        Wait at most `self.poll_interval`, while also checking for shutdown.\n        \"\"\"\n        # Sleep for `self.poll_interval`, but keep checking for shutdowns\n        start_time = time.perf_counter()\n        # Always wait at least 0.2 (note: this is for cases where\n        # `poll_interval=0`).\n        waiting_time = max(self.poll_interval, 0.2)\n        max_time = start_time + waiting_time\n        logger.debug(\n            \"[wait_and_check_shutdown] \"\n            f\"I will wait at most {self.poll_interval} s, \"\n            f\"in blocks of {self.poll_interval_internal} s.\"\n        )\n\n        while time.perf_counter() &lt; max_time:\n            if self.is_shutdown():\n                logger.info(\"[wait_and_check_shutdown] Shutdown file detected\")\n                scancelled_job_ids = self.scancel_jobs()\n                logger.info(f\"[wait_and_check_shutdown] {scancelled_job_ids=}\")\n                return scancelled_job_ids\n            time.sleep(self.poll_interval_internal)\n\n        logger.debug(\"[wait_and_check_shutdown] No shutdown file detected\")\n        return []\n\n    def _check_no_active_jobs(self):\n        if self.jobs != {}:\n            raise JobExecutionError(\n                \"Unexpected branch: jobs must be empty before new \"\n                \"submissions.\"\n            )\n\n    def submit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        parameters: dict[str, Any],\n        history_unit_id: int,\n        task_files: TaskFiles,\n        config: SlurmConfig,\n        task_type: SubmitTaskType,\n        user_id: int,\n    ) -&gt; tuple[Any, Exception | None]:\n        \"\"\"\n        Run a single fractal task.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            parameters: Dictionary of parameters.\n            history_unit_id:\n                Database ID of the corresponding `HistoryUnit` entry.\n            task_type: Task type.\n            task_files: `TaskFiles` object.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n        logger.debug(\"[submit] START\")\n\n        # Always refresh `executor_error_log` before starting a task\n        self.executor_error_log = None\n\n        config = self._enrich_slurm_config(config)\n\n        try:\n            workdir_local = task_files.wftask_subfolder_local\n            workdir_remote = task_files.wftask_subfolder_remote\n\n            if self.is_shutdown():\n                with next(get_sync_db()) as db:\n                    update_status_of_history_unit(\n                        history_unit_id=history_unit_id,\n                        status=HistoryUnitStatus.FAILED,\n                        db_sync=db,\n                    )\n\n                return None, SHUTDOWN_EXCEPTION\n\n            self._check_no_active_jobs()\n\n            # Validation phase\n            self.validate_submit_parameters(\n                parameters=parameters,\n                task_type=task_type,\n            )\n\n            # Create task subfolder\n            logger.debug(\"[submit] Create local/remote folders - START\")\n            self._mkdir_local_folder(folder=workdir_local.as_posix())\n            self._mkdir_remote_folder(folder=workdir_remote.as_posix())\n            logger.debug(\"[submit] Create local/remote folders - END\")\n\n            # Submission phase\n            slurm_job = SlurmJob(\n                prefix=task_files.prefix,\n                workdir_local=workdir_local,\n                workdir_remote=workdir_remote,\n                tasks=[\n                    SlurmTask(\n                        prefix=task_files.prefix,\n                        index=0,\n                        component=task_files.component,\n                        parameters=parameters,\n                        workdir_remote=workdir_remote,\n                        workdir_local=workdir_local,\n                        task_files=task_files,\n                        workflow_task_order=workflow_task_order,\n                        workflow_task_id=workflow_task_id,\n                        task_name=task_name,\n                    )\n                ],\n            )\n\n            config.parallel_tasks_per_job = 1\n            submit_command = self._prepare_single_slurm_job(\n                base_command=base_command,\n                slurm_job=slurm_job,\n                slurm_config=config,\n            )\n            self._send_many_job_inputs(\n                workdir_local=workdir_local,\n                workdir_remote=workdir_remote,\n            )\n            self._submit_single_sbatch(\n                submit_command=submit_command,\n                slurm_job=slurm_job,\n            )\n            logger.debug(f\"[submit] END submission phase, {self.job_ids=}\")\n\n            create_accounting_record_slurm(\n                user_id=user_id,\n                slurm_job_ids=self.job_ids_int,\n            )\n\n            # Retrieval phase\n            logger.debug(\"[submit] START retrieval phase\")\n            scancelled_job_ids = []\n            while len(self.jobs) &gt; 0:\n                # Look for finished jobs\n                finished_job_ids = self._get_finished_jobs(\n                    job_ids=self.job_ids\n                )\n                logger.debug(f\"[submit] {finished_job_ids=}\")\n                finished_jobs = [\n                    self.jobs[_slurm_job_id]\n                    for _slurm_job_id in finished_job_ids\n                ]\n\n                self._fetch_artifacts(finished_jobs)\n\n                # Extract SLURM errors\n                self._set_executor_error_log(finished_jobs)\n\n                with next(get_sync_db()) as db:\n                    for slurm_job_id in finished_job_ids:\n                        logger.debug(f\"[submit] Now process {slurm_job_id=}\")\n                        slurm_job = self.jobs.pop(slurm_job_id)\n                        was_job_scancelled = slurm_job_id in scancelled_job_ids\n                        result, exception = self._postprocess_single_task(\n                            task=slurm_job.tasks[0],\n                            was_job_scancelled=was_job_scancelled,\n                        )\n\n                        if exception is not None:\n                            update_status_of_history_unit(\n                                history_unit_id=history_unit_id,\n                                status=HistoryUnitStatus.FAILED,\n                                db_sync=db,\n                            )\n                        else:\n                            if task_type not in [\n                                TaskType.COMPOUND,\n                                TaskType.CONVERTER_COMPOUND,\n                            ]:\n                                update_status_of_history_unit(\n                                    history_unit_id=history_unit_id,\n                                    status=HistoryUnitStatus.DONE,\n                                    db_sync=db,\n                                )\n\n                if len(self.jobs) &gt; 0:\n                    scancelled_job_ids = self.wait_and_check_shutdown()\n\n            logger.debug(\"[submit] END\")\n            return result, exception\n\n        except Exception as e:\n            logger.error(\n                f\"[submit] Unexpected exception. Original error: {str(e)}\"\n            )\n            with next(get_sync_db()) as db:\n                update_status_of_history_unit(\n                    history_unit_id=history_unit_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n            self.scancel_jobs()\n            return None, e\n\n    def multisubmit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        list_parameters: list[dict[str, Any]],\n        history_unit_ids: list[int],\n        list_task_files: list[TaskFiles],\n        task_type: MultisubmitTaskType,\n        config: SlurmConfig,\n        user_id: int,\n    ) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n        \"\"\"\n        Run a parallel fractal task.\n\n        Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n        have the same size. For parallel tasks, this is also the number of\n        input images, while for compound tasks these can differ.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            list_parameters:\n                List of dictionaries of parameters (each one must include\n                `zarr_urls` key).\n            history_unit_ids:\n                Database IDs of the corresponding `HistoryUnit` entries.\n            list_task_files: `TaskFiles` objects.\n            task_type: Task type.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n\n        # Always refresh `executor_error_log` before starting a task\n        self.executor_error_log = None\n\n        config = self._enrich_slurm_config(config)\n\n        results: dict[int, Any] = {}\n        exceptions: dict[int, BaseException] = {}\n\n        logger.debug(f\"[multisubmit] START, {len(list_parameters)=}\")\n        try:\n            if self.is_shutdown():\n                if task_type == TaskType.PARALLEL:\n                    with next(get_sync_db()) as db:\n                        bulk_update_status_of_history_unit(\n                            history_unit_ids=history_unit_ids,\n                            status=HistoryUnitStatus.FAILED,\n                            db_sync=db,\n                        )\n                results = {}\n                exceptions = {\n                    ind: SHUTDOWN_EXCEPTION\n                    for ind in range(len(list_parameters))\n                }\n                return results, exceptions\n\n            self._check_no_active_jobs()\n            self.validate_multisubmit_parameters(\n                list_parameters=list_parameters,\n                task_type=task_type,\n                list_task_files=list_task_files,\n                history_unit_ids=history_unit_ids,\n            )\n\n            workdir_local = list_task_files[0].wftask_subfolder_local\n            workdir_remote = list_task_files[0].wftask_subfolder_remote\n\n            # Create local&amp;remote task subfolders\n            if task_type == TaskType.PARALLEL:\n                self._mkdir_local_folder(workdir_local.as_posix())\n                self._mkdir_remote_folder(folder=workdir_remote.as_posix())\n\n            # NOTE: chunking has already taken place in `get_slurm_config`,\n            # so that `config.tasks_per_job` is now set.\n\n            # Divide arguments in batches of `tasks_per_job` tasks each\n            tot_tasks = len(list_parameters)\n            args_batches = []\n            batch_size = config.tasks_per_job\n            for ind_chunk in range(0, tot_tasks, batch_size):\n                args_batches.append(\n                    list_parameters[ind_chunk : ind_chunk + batch_size]  # noqa\n                )\n            if len(args_batches) != math.ceil(\n                tot_tasks / config.tasks_per_job\n            ):\n                raise RuntimeError(\"Something wrong here while batching tasks\")\n\n            # Part 1/3: Iterate over chunks, prepare SlurmJob objects\n            logger.debug(\"[multisubmit] Prepare `SlurmJob`s.\")\n            jobs_to_submit = []\n            for ind_batch, chunk in enumerate(args_batches):\n                # Read prefix based on the first task of this batch\n                prefix = list_task_files[ind_batch * batch_size].prefix\n                tasks = []\n                for ind_chunk, parameters in enumerate(chunk):\n                    index = (ind_batch * batch_size) + ind_chunk\n                    tasks.append(\n                        SlurmTask(\n                            prefix=prefix,\n                            index=index,\n                            component=list_task_files[index].component,\n                            workdir_local=workdir_local,\n                            workdir_remote=workdir_remote,\n                            parameters=parameters,\n                            zarr_url=parameters[\"zarr_url\"],\n                            task_files=list_task_files[index],\n                            workflow_task_order=workflow_task_order,\n                            workflow_task_id=workflow_task_id,\n                            task_name=task_name,\n                        ),\n                    )\n                jobs_to_submit.append(\n                    SlurmJob(\n                        prefix=prefix,\n                        workdir_local=workdir_local,\n                        workdir_remote=workdir_remote,\n                        tasks=tasks,\n                    )\n                )\n\n            submit_commands = []\n            for slurm_job in jobs_to_submit:\n                submit_commands.append(\n                    self._prepare_single_slurm_job(\n                        base_command=base_command,\n                        slurm_job=slurm_job,\n                        slurm_config=config,\n                    )\n                )\n            self._send_many_job_inputs(\n                workdir_local=workdir_local,\n                workdir_remote=workdir_remote,\n            )\n            for slurm_job, submit_command in zip(\n                jobs_to_submit, submit_commands\n            ):\n                self._submit_single_sbatch(\n                    submit_command=submit_command,\n                    slurm_job=slurm_job,\n                )\n\n            logger.info(f\"[multisubmit] END submission phase, {self.job_ids=}\")\n\n            create_accounting_record_slurm(\n                user_id=user_id,\n                slurm_job_ids=self.job_ids_int,\n            )\n\n        except Exception as e:\n            logger.error(\n                \"[multisubmit] Unexpected exception during submission.\"\n                f\" Original error {str(e)}\"\n            )\n            self.scancel_jobs()\n            if task_type == TaskType.PARALLEL:\n                with next(get_sync_db()) as db:\n                    bulk_update_status_of_history_unit(\n                        history_unit_ids=history_unit_ids,\n                        status=HistoryUnitStatus.FAILED,\n                        db_sync=db,\n                    )\n            results: dict[int, Any] = {}\n            exceptions: dict[int, BaseException] = {\n                ind: e for ind in range(len(list_parameters))\n            }\n            return results, exceptions\n\n        # Retrieval phase\n        logger.debug(\"[multisubmit] START retrieval phase\")\n        scancelled_job_ids = []\n        while len(self.jobs) &gt; 0:\n            # Look for finished jobs\n            finished_job_ids = self._get_finished_jobs(job_ids=self.job_ids)\n            logger.debug(f\"[multisubmit] {finished_job_ids=}\")\n            finished_jobs = [\n                self.jobs[_slurm_job_id] for _slurm_job_id in finished_job_ids\n            ]\n\n            fetch_artifacts_exception = None\n            try:\n                self._fetch_artifacts(finished_jobs)\n            except Exception as e:\n                logger.error(\n                    \"[multisubmit] Unexpected exception in \"\n                    \"`_fetch_artifacts`. \"\n                    f\"Original error: {str(e)}\"\n                )\n                fetch_artifacts_exception = e\n\n            # Extract SLURM errors\n            self._set_executor_error_log(finished_jobs)\n\n            with next(get_sync_db()) as db:\n                for slurm_job_id in finished_job_ids:\n                    logger.debug(f\"[multisubmit] Now process {slurm_job_id=}\")\n                    slurm_job = self.jobs.pop(slurm_job_id)\n                    for task in slurm_job.tasks:\n                        logger.debug(\n                            f\"[multisubmit] Now process {task.index=}\"\n                        )\n                        was_job_scancelled = slurm_job_id in scancelled_job_ids\n                        if fetch_artifacts_exception is not None:\n                            result = None\n                            exception = fetch_artifacts_exception\n                        else:\n                            try:\n                                (\n                                    result,\n                                    exception,\n                                ) = self._postprocess_single_task(\n                                    task=task,\n                                    was_job_scancelled=was_job_scancelled,\n                                )\n                            except Exception as e:\n                                logger.error(\n                                    \"[multisubmit] Unexpected exception in \"\n                                    \"`_postprocess_single_task`. \"\n                                    f\"Original error: {str(e)}\"\n                                )\n                                result = None\n                                exception = e\n                        # Note: the relevant done/failed check is based on\n                        # whether `exception is None`. The fact that\n                        # `result is None` is not relevant for this purpose.\n                        if exception is not None:\n                            exceptions[task.index] = exception\n                            if task_type == TaskType.PARALLEL:\n                                update_status_of_history_unit(\n                                    history_unit_id=history_unit_ids[\n                                        task.index\n                                    ],\n                                    status=HistoryUnitStatus.FAILED,\n                                    db_sync=db,\n                                )\n                        else:\n                            results[task.index] = result\n                            if task_type == TaskType.PARALLEL:\n                                update_status_of_history_unit(\n                                    history_unit_id=history_unit_ids[\n                                        task.index\n                                    ],\n                                    status=HistoryUnitStatus.DONE,\n                                    db_sync=db,\n                                )\n\n            if len(self.jobs) &gt; 0:\n                scancelled_job_ids = self.wait_and_check_shutdown()\n\n        logger.debug(\"[multisubmit] END\")\n        return results, exceptions\n\n    def check_fractal_server_versions(self) -&gt; None:\n        \"\"\"\n        Compare fractal-server versions of local/remote Python interpreters.\n        \"\"\"\n\n        # Skip check when the local and remote interpreters are the same\n        # (notably for some sudo-slurm deployments)\n        if self.python_worker_interpreter == sys.executable:\n            return\n\n        # Fetch remote fractal-server version\n        cmd = (\n            f\"{self.python_worker_interpreter} \"\n            \"-m fractal_server.runner.versions\"\n        )\n        stdout = self._run_remote_cmd(cmd)\n        remote_version = json.loads(stdout.strip(\"\\n\"))[\"fractal_server\"]\n\n        # Verify local/remote version match\n        if remote_version != __VERSION__:\n            error_msg = (\n                \"Fractal-server version mismatch.\\n\"\n                \"Local interpreter: \"\n                f\"({sys.executable}): {__VERSION__}.\\n\"\n                \"Remote interpreter: \"\n                f\"({self.python_worker_interpreter}): {remote_version}.\"\n            )\n            logger.error(error_msg)\n            raise RuntimeError(error_msg)\n\n    def scancel_jobs(self) -&gt; list[str]:\n        logger.info(\"[scancel_jobs] START\")\n        scancelled_job_ids = self.job_ids\n        if self.jobs:\n            scancel_string = \" \".join(scancelled_job_ids)\n            scancel_cmd = f\"scancel {scancel_string}\"\n            logger.warning(f\"[scancel_jobs] {scancel_string}\")\n            try:\n                self._run_remote_cmd(scancel_cmd)\n            except Exception as e:\n                logger.error(\n                    \"[scancel_jobs] `scancel` command failed. \"\n                    f\"Original error:\\n{str(e)}\"\n                )\n        logger.info(\"[scancel_jobs] END\")\n        return scancelled_job_ids\n\n    def validate_slurm_jobs_workdirs(\n        self,\n        slurm_jobs: list[SlurmJob],\n    ) -&gt; None:\n        \"\"\"\n        Check that a list of `SlurmJob`s have homogeneous working folders.\n        \"\"\"\n        set_workdir_local = {_job.workdir_local for _job in slurm_jobs}\n        set_workdir_remote = {_job.workdir_remote for _job in slurm_jobs}\n        if len(set_workdir_local) &gt; 1:\n            raise ValueError(f\"Non-unique values in {set_workdir_local=}.\")\n        if len(set_workdir_remote) &gt; 1:\n            raise ValueError(f\"Non-unique values in {set_workdir_remote=}.\")\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._check_slurm_account","title":"<code>_check_slurm_account()</code>","text":"<p>Check that SLURM account is not set here in <code>common_script_lines</code>.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _check_slurm_account(self) -&gt; None:\n    \"\"\"\n    Check that SLURM account is not set here in `common_script_lines`.\n    \"\"\"\n    try:\n        invalid_line = next(\n            line\n            for line in self.common_script_lines\n            if line.startswith(\"#SBATCH --account=\")\n        )\n        raise RuntimeError(\n            \"Invalid line in `common_script_lines`: \"\n            f\"'{invalid_line}'.\\n\"\n            \"SLURM account must be set via the request body of the \"\n            \"apply-workflow endpoint, or by modifying the user properties.\"\n        )\n    except StopIteration:\n        pass\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._enrich_slurm_config","title":"<code>_enrich_slurm_config(slurm_config)</code>","text":"<p>Return an enriched <code>SlurmConfig</code> object</p> <p>Include <code>self.account</code> and <code>self.common_script_lines</code> into a <code>SlurmConfig</code> object. Extracting this logic into an independent class method is useful to fix issue #2659 (which was due to performing this same operation multiple times rather than once).</p> <p>Parameters:</p> Name Type Description Default <code>slurm_config</code> <code>SlurmConfig</code> <p>The original <code>SlurmConfig</code> object.</p> required <p>Returns:</p> Type Description <code>SlurmConfig</code> <p>A new, up-to-date, <code>SlurmConfig</code> object.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _enrich_slurm_config(\n    self,\n    slurm_config: SlurmConfig,\n) -&gt; SlurmConfig:\n    \"\"\"\n    Return an enriched `SlurmConfig` object\n\n    Include `self.account` and `self.common_script_lines` into a\n    `SlurmConfig` object. Extracting this logic into an independent\n    class method is useful to fix issue #2659 (which was due to\n    performing this same operation multiple times rather than once).\n\n    Args:\n        slurm_config: The original `SlurmConfig` object.\n\n    Returns:\n        A new, up-to-date, `SlurmConfig` object.\n    \"\"\"\n\n    new_slurm_config = slurm_config.model_copy(deep=True)\n\n    # Include SLURM account in `slurm_config`.\n    if self.slurm_account is not None:\n        new_slurm_config.account = self.slurm_account\n\n    # Include common_script_lines in extra_lines\n    if len(self.common_script_lines) &gt; 0:\n        logger.debug(\n            f\"Add {self.common_script_lines} to \"\n            f\"{new_slurm_config.extra_lines=}.\"\n        )\n        current_extra_lines = new_slurm_config.extra_lines\n        new_slurm_config.extra_lines = (\n            current_extra_lines + self.common_script_lines\n        )\n\n    return new_slurm_config\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._extract_slurm_error","title":"<code>_extract_slurm_error(slurm_job)</code>","text":"<p>Extract stderr of SLURM job, or <code>None</code>.</p> <p>Note: this method reads the local stderr file, and then it should always be called after fetching remote artifacts (e.g. in an SSH deployment).</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _extract_slurm_error(self, slurm_job: SlurmJob) -&gt; str | None:\n    \"\"\"\n    Extract stderr of SLURM job, or `None`.\n\n    Note: this method reads the _local_ stderr file, and then it should\n    always be called _after_ fetching remote artifacts (e.g. in an SSH\n    deployment).\n    \"\"\"\n\n    stderr_path = slurm_job.slurm_stderr_local_path\n\n    if not stderr_path.exists():\n        return None\n\n    try:\n        with open(stderr_path) as f:\n            stderr_content = f.read().strip()\n        if stderr_content:\n            return stderr_content\n    except Exception as e:\n        logger.error(f\"Failed to read SLURM stderr file: {e}\")\n\n    return None\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._is_squeue_error_recoverable","title":"<code>_is_squeue_error_recoverable(exception)</code>","text":"<p>Determine whether a <code>squeue</code> error is considered recoverable.</p> <p>A recoverable error is one which will disappear after some time, without any specific action from the <code>fractal-server</code> side.</p> <p>Note: if this function returns <code>True</code> for an error that does not actually recover, this leads to an infinite loop  where <code>fractal-server</code> keeps polling <code>squeue</code> information forever.</p> <p>More info at https://github.com/fractal-analytics-platform/fractal-server/issues/2682</p> <p>Parameters:</p> Name Type Description Default <code>exception</code> <code>BaseException</code> <p>The exception raised by <code>self.run_squeue</code>.</p> required <p>Returns:     Whether the error is considered recoverable.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _is_squeue_error_recoverable(self, exception: BaseException) -&gt; bool:\n    \"\"\"\n    Determine whether a `squeue` error is considered recoverable.\n\n    A _recoverable_ error is one which will disappear after some time,\n    without any specific action from the `fractal-server` side.\n\n    Note: if this function returns `True` for an error that does not\n    actually recover, this leads to an infinite loop  where\n    `fractal-server` keeps polling `squeue` information forever.\n\n    More info at\n    https://github.com/fractal-analytics-platform/fractal-server/issues/2682\n\n    Args:\n        exception: The exception raised by `self.run_squeue`.\n    Returns:\n        Whether the error is considered recoverable.\n    \"\"\"\n    str_exception = str(exception)\n    if (\n        \"slurm_load_jobs\" in str_exception\n        and \"Socket timed out on send/recv operation\" in str_exception\n    ):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._prepare_single_slurm_job","title":"<code>_prepare_single_slurm_job(*, base_command, slurm_job, slurm_config)</code>","text":"<p>Prepare submission script locally.</p> <p>Parameters:</p> Name Type Description Default <code>base_command</code> <code>str</code> <p>Base of task executable command.</p> required <code>slurm_job</code> <code>SlurmJob</code> <p><code>SlurmJob</code> object</p> required <code>slurm_config</code> <code>SlurmConfig</code> <p>Configuration for SLURM job</p> required <p>Returns:</p> Type Description <code>str</code> <p>Command to submit the SLURM job.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _prepare_single_slurm_job(\n    self,\n    *,\n    base_command: str,\n    slurm_job: SlurmJob,\n    slurm_config: SlurmConfig,\n) -&gt; str:\n    \"\"\"\n    Prepare submission script locally.\n\n    Args:\n        base_command: Base of task executable command.\n        slurm_job: `SlurmJob` object\n        slurm_config: Configuration for SLURM job\n\n    Returns:\n        Command to submit the SLURM job.\n    \"\"\"\n    logger.debug(\"[_prepare_single_slurm_job] START\")\n\n    for task in slurm_job.tasks:\n        # Write input file\n        if self.slurm_runner_type == \"ssh\":\n            args_file_remote = task.task_files.args_file_remote\n        else:\n            args_file_remote = task.task_files.args_file_local\n        metadiff_file_remote = task.task_files.metadiff_file_remote\n        full_command = (\n            f\"{base_command} \"\n            f\"--args-json {args_file_remote} \"\n            f\"--out-json {metadiff_file_remote}\"\n        )\n\n        input_data = RemoteInputData(\n            full_command=full_command,\n            python_version=sys.version_info[:3],\n            fractal_server_version=__VERSION__,\n            metadiff_file_remote=task.task_files.metadiff_file_remote,\n            log_file_remote=task.task_files.log_file_remote,\n        )\n\n        with open(task.input_file_local, \"w\") as f:\n            json.dump(input_data.model_dump(), f, indent=2)\n\n        with open(task.task_files.args_file_local, \"w\") as f:\n            json.dump(task.parameters, f, indent=2)\n\n        logger.debug(\n            \"[_prepare_single_slurm_job] Written \"\n            f\"{task.input_file_local=}\"\n        )\n\n    # Prepare commands to be included in SLURM submission script\n    cmdlines = []\n    for task in slurm_job.tasks:\n        if self.slurm_runner_type == \"ssh\":\n            input_file = task.input_file_remote\n        else:\n            input_file = task.input_file_local\n        output_file = task.output_file_remote\n        cmdlines.append(\n            f\"{self.python_worker_interpreter}\"\n            \" -m fractal_server.runner.\"\n            \"executors.slurm_common.remote \"\n            f\"--input-file {input_file} \"\n            f\"--output-file {output_file}\"\n        )\n\n    # Set ntasks\n    num_tasks_max_running = slurm_config.parallel_tasks_per_job\n    ntasks = min(len(cmdlines), num_tasks_max_running)\n    slurm_config.parallel_tasks_per_job = ntasks\n\n    # Prepare SLURM preamble based on SlurmConfig object\n    script_lines = slurm_config.to_sbatch_preamble(\n        remote_export_dir=self.user_cache_dir\n    )\n\n    # Extend SLURM preamble with variable which are not in SlurmConfig, and\n    # fix their order\n    script_lines.extend(\n        [\n            f\"#SBATCH --err={slurm_job.slurm_stderr_remote}\",\n            f\"#SBATCH --out={slurm_job.slurm_stdout_remote}\",\n            f\"#SBATCH -D {slurm_job.workdir_remote}\",\n        ]\n    )\n    script_lines = slurm_config.sort_script_lines(script_lines)\n    logger.debug(f\"[_prepare_single_slurm_job] {script_lines=}\")\n\n    # Always print output of `uname -n` and `pwd`\n    script_lines.append('\\necho \"Hostname: $(uname -n)\"')\n    script_lines.append('echo \"Current directory: $(pwd)\"')\n    script_lines.append(\n        'echo \"Start time: $(date +\"%Y-%m-%dT%H:%M:%S%z\")\"'\n    )\n\n    # Complete script preamble\n    script_lines.append(\"\\n\")\n\n    # Include command lines\n    mem_per_task_MB = slurm_config.mem_per_task_MB\n    for cmd in cmdlines:\n        script_lines.append(\n            \"srun --ntasks=1 --cpus-per-task=$SLURM_CPUS_PER_TASK \"\n            f\"--mem={mem_per_task_MB}MB \"\n            f\"{cmd} &amp;\"\n        )\n    script_lines.append(\"wait\\n\\n\")\n    script_lines.append(\n        'echo \"End time:   $(date +\"%Y-%m-%dT%H:%M:%S%z\")\"'\n    )\n    script = \"\\n\".join(script_lines)\n\n    # Write submission script\n    with open(slurm_job.slurm_submission_script_local, \"w\") as f:\n        f.write(script)\n    logger.debug(\n        \"[_prepare_single_slurm_job] Written \"\n        f\"{slurm_job.slurm_submission_script_local=}\"\n    )\n\n    if self.slurm_runner_type == \"ssh\":\n        submit_command = (\n            f\"sbatch --parsable {slurm_job.slurm_submission_script_remote}\"\n        )\n    else:\n        submit_command = (\n            f\"sbatch --parsable {slurm_job.slurm_submission_script_local}\"\n        )\n    logger.debug(\"[_prepare_single_slurm_job] END\")\n    return submit_command\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._send_many_job_inputs","title":"<code>_send_many_job_inputs(*, workdir_local, workdir_remote)</code>","text":"<p>Placeholder method.</p> <p>This method is intentionally left unimplemented in the base class. Subclasses must override it to provide the logic for transferring input data.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _send_many_job_inputs(\n    self, *, workdir_local: Path, workdir_remote: Path\n) -&gt; None:\n    \"\"\"\n    Placeholder method.\n\n    This method is intentionally left unimplemented in the base class.\n    Subclasses must override it to provide the logic for transferring\n    input data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._set_executor_error_log","title":"<code>_set_executor_error_log(slurm_jobs)</code>","text":"<p>If <code>executor_error_log</code> is unset, update it based on a list of jobs.</p> <p>Notes: 1. This method must be executed after <code>_fetch_artifacts</code>. 2. This method only captures the first error it finds.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _set_executor_error_log(self, slurm_jobs: list[SlurmJob]) -&gt; None:\n    \"\"\"\n    If `executor_error_log` is unset, update it based on a list of jobs.\n\n    Notes:\n    1. This method must be executed **after** `_fetch_artifacts`.\n    2. This method only captures the first error it finds.\n    \"\"\"\n    if self.executor_error_log is not None:\n        # `executor_error_log` is already set, exit\n        return\n    for slurm_job in slurm_jobs:\n        slurm_error = self._extract_slurm_error(slurm_job)\n        if slurm_error is not None:\n            logger.warning(f\"SLURM error detected: {slurm_error}\")\n            self.executor_error_log = slurm_error\n            return\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._submit_single_sbatch","title":"<code>_submit_single_sbatch(*, submit_command, slurm_job)</code>","text":"<p>Run <code>sbatch</code> and add the <code>slurm_job</code> to <code>self.jobs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>submit_command</code> <code>str</code> <p>The SLURM submission command prepared in <code>self._prepare_single_slurm_job</code>.</p> required <code>slurm_job</code> <code>SlurmJob</code> <p>The <code>SlurmJob</code> object.</p> required Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _submit_single_sbatch(\n    self,\n    *,\n    submit_command: str,\n    slurm_job: SlurmJob,\n) -&gt; None:\n    \"\"\"\n    Run `sbatch` and add the `slurm_job` to `self.jobs`.\n\n    Args:\n        submit_command:\n            The SLURM submission command prepared in\n            `self._prepare_single_slurm_job`.\n        slurm_job: The `SlurmJob` object.\n    \"\"\"\n\n    logger.debug(\"[_submit_single_sbatch] START\")\n\n    # Submit SLURM job and retrieve job ID\n    logger.debug(f\"[_submit_single_sbatch] Now run {submit_command=}\")\n    sbatch_stdout = self._run_remote_cmd(submit_command)\n    logger.info(f\"[_submit_single_sbatch] {sbatch_stdout=}\")\n    stdout = sbatch_stdout.strip(\"\\n\")\n    submitted_job_id = int(stdout)\n    slurm_job.slurm_job_id = str(submitted_job_id)\n\n    # Add job to self.jobs\n    self.jobs[slurm_job.slurm_job_id] = slurm_job\n    logger.debug(\n        \"[_submit_single_sbatch] Added \"\n        f\"{slurm_job.slurm_job_id} to self.jobs.\"\n    )\n    logger.debug(\"[_submit_single_sbatch] END\")\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner.check_fractal_server_versions","title":"<code>check_fractal_server_versions()</code>","text":"<p>Compare fractal-server versions of local/remote Python interpreters.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def check_fractal_server_versions(self) -&gt; None:\n    \"\"\"\n    Compare fractal-server versions of local/remote Python interpreters.\n    \"\"\"\n\n    # Skip check when the local and remote interpreters are the same\n    # (notably for some sudo-slurm deployments)\n    if self.python_worker_interpreter == sys.executable:\n        return\n\n    # Fetch remote fractal-server version\n    cmd = (\n        f\"{self.python_worker_interpreter} \"\n        \"-m fractal_server.runner.versions\"\n    )\n    stdout = self._run_remote_cmd(cmd)\n    remote_version = json.loads(stdout.strip(\"\\n\"))[\"fractal_server\"]\n\n    # Verify local/remote version match\n    if remote_version != __VERSION__:\n        error_msg = (\n            \"Fractal-server version mismatch.\\n\"\n            \"Local interpreter: \"\n            f\"({sys.executable}): {__VERSION__}.\\n\"\n            \"Remote interpreter: \"\n            f\"({self.python_worker_interpreter}): {remote_version}.\"\n        )\n        logger.error(error_msg)\n        raise RuntimeError(error_msg)\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner.multisubmit","title":"<code>multisubmit(*, base_command, workflow_task_order, workflow_task_id, task_name, list_parameters, history_unit_ids, list_task_files, task_type, config, user_id)</code>","text":"<p>Run a parallel fractal task.</p> <p>Note: <code>list_parameters</code>, <code>list_task_files</code> and <code>history_unit_ids</code> have the same size. For parallel tasks, this is also the number of input images, while for compound tasks these can differ.</p> <p>Parameters:</p> Name Type Description Default <code>base_command</code> <code>str</code> required <code>workflow_task_order</code> <code>int</code> required <code>workflow_task_id</code> <code>int</code> required <code>task_name</code> <code>str</code> required <code>list_parameters</code> <code>list[dict[str, Any]]</code> <p>List of dictionaries of parameters (each one must include <code>zarr_urls</code> key).</p> required <code>history_unit_ids</code> <code>list[int]</code> <p>Database IDs of the corresponding <code>HistoryUnit</code> entries.</p> required <code>list_task_files</code> <code>list[TaskFiles]</code> <p><code>TaskFiles</code> objects.</p> required <code>task_type</code> <code>MultisubmitTaskType</code> <p>Task type.</p> required <code>config</code> <code>SlurmConfig</code> <p>Runner-specific parameters.</p> required <code>user_id</code> <code>int</code> required Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def multisubmit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    list_parameters: list[dict[str, Any]],\n    history_unit_ids: list[int],\n    list_task_files: list[TaskFiles],\n    task_type: MultisubmitTaskType,\n    config: SlurmConfig,\n    user_id: int,\n) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n    \"\"\"\n    Run a parallel fractal task.\n\n    Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n    have the same size. For parallel tasks, this is also the number of\n    input images, while for compound tasks these can differ.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        list_parameters:\n            List of dictionaries of parameters (each one must include\n            `zarr_urls` key).\n        history_unit_ids:\n            Database IDs of the corresponding `HistoryUnit` entries.\n        list_task_files: `TaskFiles` objects.\n        task_type: Task type.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n\n    # Always refresh `executor_error_log` before starting a task\n    self.executor_error_log = None\n\n    config = self._enrich_slurm_config(config)\n\n    results: dict[int, Any] = {}\n    exceptions: dict[int, BaseException] = {}\n\n    logger.debug(f\"[multisubmit] START, {len(list_parameters)=}\")\n    try:\n        if self.is_shutdown():\n            if task_type == TaskType.PARALLEL:\n                with next(get_sync_db()) as db:\n                    bulk_update_status_of_history_unit(\n                        history_unit_ids=history_unit_ids,\n                        status=HistoryUnitStatus.FAILED,\n                        db_sync=db,\n                    )\n            results = {}\n            exceptions = {\n                ind: SHUTDOWN_EXCEPTION\n                for ind in range(len(list_parameters))\n            }\n            return results, exceptions\n\n        self._check_no_active_jobs()\n        self.validate_multisubmit_parameters(\n            list_parameters=list_parameters,\n            task_type=task_type,\n            list_task_files=list_task_files,\n            history_unit_ids=history_unit_ids,\n        )\n\n        workdir_local = list_task_files[0].wftask_subfolder_local\n        workdir_remote = list_task_files[0].wftask_subfolder_remote\n\n        # Create local&amp;remote task subfolders\n        if task_type == TaskType.PARALLEL:\n            self._mkdir_local_folder(workdir_local.as_posix())\n            self._mkdir_remote_folder(folder=workdir_remote.as_posix())\n\n        # NOTE: chunking has already taken place in `get_slurm_config`,\n        # so that `config.tasks_per_job` is now set.\n\n        # Divide arguments in batches of `tasks_per_job` tasks each\n        tot_tasks = len(list_parameters)\n        args_batches = []\n        batch_size = config.tasks_per_job\n        for ind_chunk in range(0, tot_tasks, batch_size):\n            args_batches.append(\n                list_parameters[ind_chunk : ind_chunk + batch_size]  # noqa\n            )\n        if len(args_batches) != math.ceil(\n            tot_tasks / config.tasks_per_job\n        ):\n            raise RuntimeError(\"Something wrong here while batching tasks\")\n\n        # Part 1/3: Iterate over chunks, prepare SlurmJob objects\n        logger.debug(\"[multisubmit] Prepare `SlurmJob`s.\")\n        jobs_to_submit = []\n        for ind_batch, chunk in enumerate(args_batches):\n            # Read prefix based on the first task of this batch\n            prefix = list_task_files[ind_batch * batch_size].prefix\n            tasks = []\n            for ind_chunk, parameters in enumerate(chunk):\n                index = (ind_batch * batch_size) + ind_chunk\n                tasks.append(\n                    SlurmTask(\n                        prefix=prefix,\n                        index=index,\n                        component=list_task_files[index].component,\n                        workdir_local=workdir_local,\n                        workdir_remote=workdir_remote,\n                        parameters=parameters,\n                        zarr_url=parameters[\"zarr_url\"],\n                        task_files=list_task_files[index],\n                        workflow_task_order=workflow_task_order,\n                        workflow_task_id=workflow_task_id,\n                        task_name=task_name,\n                    ),\n                )\n            jobs_to_submit.append(\n                SlurmJob(\n                    prefix=prefix,\n                    workdir_local=workdir_local,\n                    workdir_remote=workdir_remote,\n                    tasks=tasks,\n                )\n            )\n\n        submit_commands = []\n        for slurm_job in jobs_to_submit:\n            submit_commands.append(\n                self._prepare_single_slurm_job(\n                    base_command=base_command,\n                    slurm_job=slurm_job,\n                    slurm_config=config,\n                )\n            )\n        self._send_many_job_inputs(\n            workdir_local=workdir_local,\n            workdir_remote=workdir_remote,\n        )\n        for slurm_job, submit_command in zip(\n            jobs_to_submit, submit_commands\n        ):\n            self._submit_single_sbatch(\n                submit_command=submit_command,\n                slurm_job=slurm_job,\n            )\n\n        logger.info(f\"[multisubmit] END submission phase, {self.job_ids=}\")\n\n        create_accounting_record_slurm(\n            user_id=user_id,\n            slurm_job_ids=self.job_ids_int,\n        )\n\n    except Exception as e:\n        logger.error(\n            \"[multisubmit] Unexpected exception during submission.\"\n            f\" Original error {str(e)}\"\n        )\n        self.scancel_jobs()\n        if task_type == TaskType.PARALLEL:\n            with next(get_sync_db()) as db:\n                bulk_update_status_of_history_unit(\n                    history_unit_ids=history_unit_ids,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n        results: dict[int, Any] = {}\n        exceptions: dict[int, BaseException] = {\n            ind: e for ind in range(len(list_parameters))\n        }\n        return results, exceptions\n\n    # Retrieval phase\n    logger.debug(\"[multisubmit] START retrieval phase\")\n    scancelled_job_ids = []\n    while len(self.jobs) &gt; 0:\n        # Look for finished jobs\n        finished_job_ids = self._get_finished_jobs(job_ids=self.job_ids)\n        logger.debug(f\"[multisubmit] {finished_job_ids=}\")\n        finished_jobs = [\n            self.jobs[_slurm_job_id] for _slurm_job_id in finished_job_ids\n        ]\n\n        fetch_artifacts_exception = None\n        try:\n            self._fetch_artifacts(finished_jobs)\n        except Exception as e:\n            logger.error(\n                \"[multisubmit] Unexpected exception in \"\n                \"`_fetch_artifacts`. \"\n                f\"Original error: {str(e)}\"\n            )\n            fetch_artifacts_exception = e\n\n        # Extract SLURM errors\n        self._set_executor_error_log(finished_jobs)\n\n        with next(get_sync_db()) as db:\n            for slurm_job_id in finished_job_ids:\n                logger.debug(f\"[multisubmit] Now process {slurm_job_id=}\")\n                slurm_job = self.jobs.pop(slurm_job_id)\n                for task in slurm_job.tasks:\n                    logger.debug(\n                        f\"[multisubmit] Now process {task.index=}\"\n                    )\n                    was_job_scancelled = slurm_job_id in scancelled_job_ids\n                    if fetch_artifacts_exception is not None:\n                        result = None\n                        exception = fetch_artifacts_exception\n                    else:\n                        try:\n                            (\n                                result,\n                                exception,\n                            ) = self._postprocess_single_task(\n                                task=task,\n                                was_job_scancelled=was_job_scancelled,\n                            )\n                        except Exception as e:\n                            logger.error(\n                                \"[multisubmit] Unexpected exception in \"\n                                \"`_postprocess_single_task`. \"\n                                f\"Original error: {str(e)}\"\n                            )\n                            result = None\n                            exception = e\n                    # Note: the relevant done/failed check is based on\n                    # whether `exception is None`. The fact that\n                    # `result is None` is not relevant for this purpose.\n                    if exception is not None:\n                        exceptions[task.index] = exception\n                        if task_type == TaskType.PARALLEL:\n                            update_status_of_history_unit(\n                                history_unit_id=history_unit_ids[\n                                    task.index\n                                ],\n                                status=HistoryUnitStatus.FAILED,\n                                db_sync=db,\n                            )\n                    else:\n                        results[task.index] = result\n                        if task_type == TaskType.PARALLEL:\n                            update_status_of_history_unit(\n                                history_unit_id=history_unit_ids[\n                                    task.index\n                                ],\n                                status=HistoryUnitStatus.DONE,\n                                db_sync=db,\n                            )\n\n        if len(self.jobs) &gt; 0:\n            scancelled_job_ids = self.wait_and_check_shutdown()\n\n    logger.debug(\"[multisubmit] END\")\n    return results, exceptions\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner.submit","title":"<code>submit(*, base_command, workflow_task_order, workflow_task_id, task_name, parameters, history_unit_id, task_files, config, task_type, user_id)</code>","text":"<p>Run a single fractal task.</p> <p>Parameters:</p> Name Type Description Default <code>base_command</code> <code>str</code> required <code>workflow_task_order</code> <code>int</code> required <code>workflow_task_id</code> <code>int</code> required <code>task_name</code> <code>str</code> required <code>parameters</code> <code>dict[str, Any]</code> <p>Dictionary of parameters.</p> required <code>history_unit_id</code> <code>int</code> <p>Database ID of the corresponding <code>HistoryUnit</code> entry.</p> required <code>task_type</code> <code>SubmitTaskType</code> <p>Task type.</p> required <code>task_files</code> <code>TaskFiles</code> <p><code>TaskFiles</code> object.</p> required <code>config</code> <code>SlurmConfig</code> <p>Runner-specific parameters.</p> required <code>user_id</code> <code>int</code> required Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def submit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    parameters: dict[str, Any],\n    history_unit_id: int,\n    task_files: TaskFiles,\n    config: SlurmConfig,\n    task_type: SubmitTaskType,\n    user_id: int,\n) -&gt; tuple[Any, Exception | None]:\n    \"\"\"\n    Run a single fractal task.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        parameters: Dictionary of parameters.\n        history_unit_id:\n            Database ID of the corresponding `HistoryUnit` entry.\n        task_type: Task type.\n        task_files: `TaskFiles` object.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n    logger.debug(\"[submit] START\")\n\n    # Always refresh `executor_error_log` before starting a task\n    self.executor_error_log = None\n\n    config = self._enrich_slurm_config(config)\n\n    try:\n        workdir_local = task_files.wftask_subfolder_local\n        workdir_remote = task_files.wftask_subfolder_remote\n\n        if self.is_shutdown():\n            with next(get_sync_db()) as db:\n                update_status_of_history_unit(\n                    history_unit_id=history_unit_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n\n            return None, SHUTDOWN_EXCEPTION\n\n        self._check_no_active_jobs()\n\n        # Validation phase\n        self.validate_submit_parameters(\n            parameters=parameters,\n            task_type=task_type,\n        )\n\n        # Create task subfolder\n        logger.debug(\"[submit] Create local/remote folders - START\")\n        self._mkdir_local_folder(folder=workdir_local.as_posix())\n        self._mkdir_remote_folder(folder=workdir_remote.as_posix())\n        logger.debug(\"[submit] Create local/remote folders - END\")\n\n        # Submission phase\n        slurm_job = SlurmJob(\n            prefix=task_files.prefix,\n            workdir_local=workdir_local,\n            workdir_remote=workdir_remote,\n            tasks=[\n                SlurmTask(\n                    prefix=task_files.prefix,\n                    index=0,\n                    component=task_files.component,\n                    parameters=parameters,\n                    workdir_remote=workdir_remote,\n                    workdir_local=workdir_local,\n                    task_files=task_files,\n                    workflow_task_order=workflow_task_order,\n                    workflow_task_id=workflow_task_id,\n                    task_name=task_name,\n                )\n            ],\n        )\n\n        config.parallel_tasks_per_job = 1\n        submit_command = self._prepare_single_slurm_job(\n            base_command=base_command,\n            slurm_job=slurm_job,\n            slurm_config=config,\n        )\n        self._send_many_job_inputs(\n            workdir_local=workdir_local,\n            workdir_remote=workdir_remote,\n        )\n        self._submit_single_sbatch(\n            submit_command=submit_command,\n            slurm_job=slurm_job,\n        )\n        logger.debug(f\"[submit] END submission phase, {self.job_ids=}\")\n\n        create_accounting_record_slurm(\n            user_id=user_id,\n            slurm_job_ids=self.job_ids_int,\n        )\n\n        # Retrieval phase\n        logger.debug(\"[submit] START retrieval phase\")\n        scancelled_job_ids = []\n        while len(self.jobs) &gt; 0:\n            # Look for finished jobs\n            finished_job_ids = self._get_finished_jobs(\n                job_ids=self.job_ids\n            )\n            logger.debug(f\"[submit] {finished_job_ids=}\")\n            finished_jobs = [\n                self.jobs[_slurm_job_id]\n                for _slurm_job_id in finished_job_ids\n            ]\n\n            self._fetch_artifacts(finished_jobs)\n\n            # Extract SLURM errors\n            self._set_executor_error_log(finished_jobs)\n\n            with next(get_sync_db()) as db:\n                for slurm_job_id in finished_job_ids:\n                    logger.debug(f\"[submit] Now process {slurm_job_id=}\")\n                    slurm_job = self.jobs.pop(slurm_job_id)\n                    was_job_scancelled = slurm_job_id in scancelled_job_ids\n                    result, exception = self._postprocess_single_task(\n                        task=slurm_job.tasks[0],\n                        was_job_scancelled=was_job_scancelled,\n                    )\n\n                    if exception is not None:\n                        update_status_of_history_unit(\n                            history_unit_id=history_unit_id,\n                            status=HistoryUnitStatus.FAILED,\n                            db_sync=db,\n                        )\n                    else:\n                        if task_type not in [\n                            TaskType.COMPOUND,\n                            TaskType.CONVERTER_COMPOUND,\n                        ]:\n                            update_status_of_history_unit(\n                                history_unit_id=history_unit_id,\n                                status=HistoryUnitStatus.DONE,\n                                db_sync=db,\n                            )\n\n            if len(self.jobs) &gt; 0:\n                scancelled_job_ids = self.wait_and_check_shutdown()\n\n        logger.debug(\"[submit] END\")\n        return result, exception\n\n    except Exception as e:\n        logger.error(\n            f\"[submit] Unexpected exception. Original error: {str(e)}\"\n        )\n        with next(get_sync_db()) as db:\n            update_status_of_history_unit(\n                history_unit_id=history_unit_id,\n                status=HistoryUnitStatus.FAILED,\n                db_sync=db,\n            )\n        self.scancel_jobs()\n        return None, e\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner.validate_slurm_jobs_workdirs","title":"<code>validate_slurm_jobs_workdirs(slurm_jobs)</code>","text":"<p>Check that a list of <code>SlurmJob</code>s have homogeneous working folders.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def validate_slurm_jobs_workdirs(\n    self,\n    slurm_jobs: list[SlurmJob],\n) -&gt; None:\n    \"\"\"\n    Check that a list of `SlurmJob`s have homogeneous working folders.\n    \"\"\"\n    set_workdir_local = {_job.workdir_local for _job in slurm_jobs}\n    set_workdir_remote = {_job.workdir_remote for _job in slurm_jobs}\n    if len(set_workdir_local) &gt; 1:\n        raise ValueError(f\"Non-unique values in {set_workdir_local=}.\")\n    if len(set_workdir_remote) &gt; 1:\n        raise ValueError(f\"Non-unique values in {set_workdir_remote=}.\")\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner.wait_and_check_shutdown","title":"<code>wait_and_check_shutdown()</code>","text":"<p>Wait at most <code>self.poll_interval</code>, while also checking for shutdown.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def wait_and_check_shutdown(self) -&gt; list[str]:\n    \"\"\"\n    Wait at most `self.poll_interval`, while also checking for shutdown.\n    \"\"\"\n    # Sleep for `self.poll_interval`, but keep checking for shutdowns\n    start_time = time.perf_counter()\n    # Always wait at least 0.2 (note: this is for cases where\n    # `poll_interval=0`).\n    waiting_time = max(self.poll_interval, 0.2)\n    max_time = start_time + waiting_time\n    logger.debug(\n        \"[wait_and_check_shutdown] \"\n        f\"I will wait at most {self.poll_interval} s, \"\n        f\"in blocks of {self.poll_interval_internal} s.\"\n    )\n\n    while time.perf_counter() &lt; max_time:\n        if self.is_shutdown():\n            logger.info(\"[wait_and_check_shutdown] Shutdown file detected\")\n            scancelled_job_ids = self.scancel_jobs()\n            logger.info(f\"[wait_and_check_shutdown] {scancelled_job_ids=}\")\n            return scancelled_job_ids\n        time.sleep(self.poll_interval_internal)\n\n    logger.debug(\"[wait_and_check_shutdown] No shutdown file detected\")\n    return []\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/get_slurm_config/","title":"get_slurm_config","text":""},{"location":"reference/fractal_server/runner/executors/slurm_common/get_slurm_config/#fractal_server.runner.executors.slurm_common.get_slurm_config._get_slurm_config_internal","title":"<code>_get_slurm_config_internal(shared_config, wftask, which_type)</code>","text":"<p>Prepare a specific <code>SlurmConfig</code> configuration.</p> <p>The base configuration is the runner-level <code>shared_config</code> object, based on <code>resource.jobs_runner_config</code> (note that GPU-specific properties take priority, when <code>needs_gpu=True</code>). We then incorporate attributes from <code>wftask.meta_{non_parallel,parallel}</code> - with higher priority.</p> <p>Parameters:</p> Name Type Description Default <code>shared_config</code> <code>JobRunnerConfigSLURM</code> <p>Configuration object based on <code>resource.jobs_runner_config</code>.</p> required <code>wftask</code> <code>WorkflowTaskV2</code> <p>WorkflowTaskV2 for which the backend configuration should be prepared.</p> required <code>which_type</code> <code>Literal['non_parallel', 'parallel']</code> <p>Whether we should look at the non-parallel or parallel part of <code>wftask</code>.</p> required <code>tot_tasks</code> <p>Not used here, only present as a common interface.</p> required <p>Returns:</p> Type Description <code>SlurmConfig</code> <p>A ready-to-use <code>SlurmConfig</code> object.</p> Source code in <code>fractal_server/runner/executors/slurm_common/get_slurm_config.py</code> <pre><code>def _get_slurm_config_internal(\n    shared_config: JobRunnerConfigSLURM,\n    wftask: WorkflowTaskV2,\n    which_type: Literal[\"non_parallel\", \"parallel\"],\n) -&gt; SlurmConfig:\n    \"\"\"\n\n    Prepare a specific `SlurmConfig` configuration.\n\n    The base configuration is the runner-level `shared_config` object, based\n    on `resource.jobs_runner_config` (note that GPU-specific properties take\n    priority, when `needs_gpu=True`). We then incorporate attributes from\n    `wftask.meta_{non_parallel,parallel}` - with higher priority.\n\n    Args:\n        shared_config:\n            Configuration object based on `resource.jobs_runner_config`.\n        wftask:\n            WorkflowTaskV2 for which the backend configuration should\n            be prepared.\n        which_type:\n            Whether we should look at the non-parallel or parallel part\n            of `wftask`.\n        tot_tasks: Not used here, only present as a common interface.\n\n    Returns:\n        A ready-to-use `SlurmConfig` object.\n    \"\"\"\n\n    if which_type == \"non_parallel\":\n        wftask_meta = wftask.meta_non_parallel\n    elif which_type == \"parallel\":\n        wftask_meta = wftask.meta_parallel\n    else:\n        raise ValueError(\n            f\"get_slurm_config received invalid argument {which_type=}.\"\n        )\n\n    logger.debug(\n        f\"[get_slurm_config] WorkflowTask meta attribute: {wftask_meta=}\"\n    )\n\n    # Start from `shared_config`\n    slurm_dict = shared_config.default_slurm_config.model_dump(\n        exclude_unset=True, exclude={\"mem\"}\n    )\n    if shared_config.default_slurm_config.mem:\n        slurm_dict[\"mem_per_task_MB\"] = shared_config.default_slurm_config.mem\n\n    # Incorporate slurm_env.batching_config\n    for key, value in shared_config.batching_config.model_dump().items():\n        slurm_dict[key] = value\n\n    # Incorporate slurm_env.user_local_exports\n    slurm_dict[\"user_local_exports\"] = shared_config.user_local_exports\n\n    # GPU-related options\n    # Notes about priority:\n    # 1. This block of definitions takes priority over other definitions from\n    #    slurm_env which are not under the `needs_gpu` subgroup\n    # 2. This block of definitions has lower priority than whatever comes next\n    #    (i.e. from WorkflowTask.meta_parallel).\n    if wftask_meta is not None:\n        needs_gpu = interpret_as_bool(wftask_meta.get(\"needs_gpu\", False))\n    else:\n        needs_gpu = False\n    logger.debug(f\"[get_slurm_config] {needs_gpu=}\")\n    if needs_gpu and shared_config.gpu_slurm_config is not None:\n        for key, value in shared_config.gpu_slurm_config.model_dump(\n            exclude_unset=True, exclude={\"mem\"}\n        ).items():\n            slurm_dict[key] = value\n        if shared_config.gpu_slurm_config.mem:\n            slurm_dict[\"mem_per_task_MB\"] = shared_config.gpu_slurm_config.mem\n\n    # Number of CPUs per task, for multithreading\n    if wftask_meta is not None and \"cpus_per_task\" in wftask_meta:\n        cpus_per_task = int(wftask_meta[\"cpus_per_task\"])\n        slurm_dict[\"cpus_per_task\"] = cpus_per_task\n\n    # Required memory per task, in MB\n    if wftask_meta is not None and \"mem\" in wftask_meta:\n        raw_mem = wftask_meta[\"mem\"]\n        mem_per_task_MB = slurm_mem_to_MB(raw_mem)\n        slurm_dict[\"mem_per_task_MB\"] = mem_per_task_MB\n\n    # Job name\n    job_name = wftask.task.name.replace(\" \", \"_\")\n    slurm_dict[\"job_name\"] = job_name\n\n    # Optional SLURM arguments and extra lines\n    if wftask_meta is not None:\n        account = wftask_meta.get(\"account\", None)\n        if account is not None:\n            error_msg = (\n                f\"Invalid {account=} property in WorkflowTask `meta` \"\n                \"attribute.\\n\"\n                \"SLURM account must be set in the request body of the \"\n                \"apply-workflow endpoint, or by modifying the user properties.\"\n            )\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        for key in [\n            \"time\",\n            \"gres\",\n            \"gpus\",\n            \"constraint\",\n            \"nodelist\",\n            \"exclude\",\n        ]:\n            value = wftask_meta.get(key, None)\n            if value is not None:\n                slurm_dict[key] = value\n    if wftask_meta is not None:\n        extra_lines = wftask_meta.get(\"extra_lines\", [])\n    else:\n        extra_lines = []\n    extra_lines = slurm_dict.get(\"extra_lines\", []) + extra_lines\n    if len(set(extra_lines)) != len(extra_lines):\n        logger.debug(\n            f\"[get_slurm_config] Removing repeated elements in {extra_lines=}.\"\n        )\n        extra_lines = list(set(extra_lines))\n    slurm_dict[\"extra_lines\"] = extra_lines\n\n    # Job-batching parameters (if None, they will be determined heuristically)\n    if wftask_meta is not None:\n        tasks_per_job = wftask_meta.get(\"tasks_per_job\", None)\n        parallel_tasks_per_job = wftask_meta.get(\n            \"parallel_tasks_per_job\", None\n        )\n    else:\n        tasks_per_job = None\n        parallel_tasks_per_job = None\n    slurm_dict[\"tasks_per_job\"] = tasks_per_job\n    slurm_dict[\"parallel_tasks_per_job\"] = parallel_tasks_per_job\n\n    # Put everything together\n    logger.debug(\n        f\"[get_slurm_config] Create SlurmConfig object based on {slurm_dict=}\"\n    )\n    slurm_config = SlurmConfig(**slurm_dict)\n\n    return slurm_config\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/remote/","title":"remote","text":""},{"location":"reference/fractal_server/runner/executors/slurm_common/remote/#fractal_server.runner.executors.slurm_common.remote.FractalVersionMismatch","title":"<code>FractalVersionMismatch</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Custom exception for version mismatch</p> Source code in <code>fractal_server/runner/executors/slurm_common/remote.py</code> <pre><code>class FractalVersionMismatch(RuntimeError):\n    \"\"\"\n    Custom exception for version mismatch\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/remote/#fractal_server.runner.executors.slurm_common.remote.worker","title":"<code>worker(*, in_fname, out_fname)</code>","text":"<p>Execute a job, possibly on a remote node.</p> <p>Parameters:</p> Name Type Description Default <code>in_fname</code> <code>str</code> <p>Absolute path to the input file (must be readable).</p> required <code>out_fname</code> <code>str</code> <p>Absolute path of the output file (must be writeable).</p> required Source code in <code>fractal_server/runner/executors/slurm_common/remote.py</code> <pre><code>def worker(\n    *,\n    in_fname: str,\n    out_fname: str,\n) -&gt; None:\n    \"\"\"\n    Execute a job, possibly on a remote node.\n\n    Args:\n        in_fname: Absolute path to the input file (must be readable).\n        out_fname: Absolute path of the output file (must be writeable).\n    \"\"\"\n\n    # Create output folder, if missing\n    out_dir = os.path.dirname(out_fname)\n    if not os.path.exists(out_dir):\n        os.mkdir(out_dir)\n\n    # Execute the job and capture exceptions\n    try:\n        with open(in_fname) as f:\n            input_data = json.load(f)\n\n        # Fractal-server version must be identical\n        server_fractal_server_version = input_data[\"fractal_server_version\"]\n        worker_fractal_server_version = __VERSION__\n        if worker_fractal_server_version != server_fractal_server_version:\n            raise FractalVersionMismatch(\n                f\"{server_fractal_server_version=} but \"\n                f\"{worker_fractal_server_version=}\"\n            )\n\n        # Get `worker_python_version` as a `list` since this is the type of\n        # `server_python_version` after a JSON dump/load round trip.\n        worker_python_version = list(sys.version_info[:3])\n\n        # Print a warning for Python version mismatch\n        server_python_version = input_data[\"python_version\"]\n        if worker_python_version != server_python_version:\n            if worker_python_version[:2] != server_python_version[:2]:\n                print(\n                    \"WARNING: \"\n                    f\"{server_python_version=} but {worker_python_version=}.\"\n                )\n\n        # Extract some useful paths\n        metadiff_file_remote = input_data[\"metadiff_file_remote\"]\n        log_path = input_data[\"log_file_remote\"]\n\n        # Execute command\n        full_command = input_data[\"full_command\"]\n        call_command_wrapper(cmd=full_command, log_path=log_path)\n\n        try:\n            with open(metadiff_file_remote) as f:\n                out_meta = json.load(f)\n            result = (True, out_meta)\n        except FileNotFoundError:\n            # Command completed, but it produced no metadiff file\n            result = (True, None)\n\n    except Exception as e:\n        # Exception objects are not serialisable. Here we save the relevant\n        # exception contents in a serializable dictionary. Note that whenever\n        # the task failed \"properly\", the exception is a `TaskExecutionError`\n        # and it has additional attributes.\n        import traceback\n\n        exc_type, exc_value, traceback_obj = sys.exc_info()\n        traceback_obj = traceback_obj.tb_next\n        traceback_list = traceback.format_exception(\n            exc_type,\n            exc_value,\n            traceback_obj,\n        )\n        traceback_string = \"\".join(traceback_list)\n        exc_proxy = dict(\n            exc_type_name=type(e).__name__,\n            traceback_string=traceback_string,\n        )\n        result = (False, exc_proxy)\n\n    # Write output file\n    with open(out_fname, \"w\") as f:\n        json.dump(result, f, indent=2)\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/slurm_config/","title":"slurm_config","text":"<p>Submodule to handle the SLURM configuration for a WorkflowTask</p>"},{"location":"reference/fractal_server/runner/executors/slurm_common/slurm_config/#fractal_server.runner.executors.slurm_common.slurm_config.SlurmConfig","title":"<code>SlurmConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Abstraction for SLURM parameters</p> <p>NOTE: <code>SlurmConfig</code> objects are created internally in <code>fractal-server</code>, and they are not meant to be initialized by the user; the same holds for <code>SlurmConfig</code> attributes (e.g. <code>mem_per_task_MB</code>), which are not meant to be part of the superuser-defined <code>resource.jobs_runner_config</code> JSON field.</p> <p>Part of the attributes map directly to some of the SLURM attributes (see https://slurm.schedmd.com/sbatch.html), e.g. <code>partition</code>. Other attributes are metaparameters which are needed in fractal-server to combine multiple tasks in the same SLURM job (e.g. <code>parallel_tasks_per_job</code> or <code>max_num_jobs</code>).</p> <p>Attributes:</p> Name Type Description <code>partition</code> <code>str</code> <p>Corresponds to SLURM option.</p> <code>cpus_per_task</code> <code>int</code> <p>Corresponds to SLURM option.</p> <code>mem_per_task_MB</code> <code>int</code> <p>Corresponds to <code>mem</code> SLURM option.</p> <code>job_name</code> <code>str | None</code> <p>Corresponds to <code>name</code> SLURM option.</p> <code>constraint</code> <code>str | None</code> <p>Corresponds to SLURM option.</p> <code>gres</code> <code>str | None</code> <p>Corresponds to SLURM option.</p> <code>account</code> <code>str | None</code> <p>Corresponds to SLURM option.</p> <code>gpus</code> <code>str | None</code> <p>Corresponds to SLURM option.</p> <code>time</code> <code>str | None</code> <p>Corresponds to SLURM option (WARNING: not fully supported).</p> <code>nodelist</code> <code>str | None</code> <p>Corresponds to SLURM option.</p> <code>exclude</code> <code>str | None</code> <p>Corresponds to SLURM option.</p> <code>prefix</code> <code>str</code> <p>Prefix of configuration lines in SLURM submission scripts.</p> <code>shebang_line</code> <code>str</code> <p>Shebang line for SLURM submission scripts.</p> <code>extra_lines</code> <code>list[str]</code> <p>Additional lines to include in SLURM submission scripts.</p> <code>tasks_per_job</code> <code>int | None</code> <p>Number of tasks for each SLURM job.</p> <code>parallel_tasks_per_job</code> <code>int | None</code> <p>Number of tasks to run in parallel for                     each SLURM job.</p> <code>target_cpus_per_job</code> <code>int</code> <p>Optimal number of CPUs to be requested in each                  SLURM job.</p> <code>max_cpus_per_job</code> <code>int</code> <p>Maximum number of CPUs that can be requested in each               SLURM job.</p> <code>target_mem_per_job</code> <code>int</code> <p>Optimal amount of memory (in MB) to be requested in                 each SLURM job.</p> <code>max_mem_per_job</code> <code>int</code> <p>Maximum amount of memory (in MB) that can be requested              in each SLURM job.</p> <code>target_num_jobs</code> <code>int</code> <p>Optimal number of SLURM jobs for a given WorkflowTask.</p> <code>max_num_jobs</code> <code>int</code> <p>Maximum number of SLURM jobs for a given WorkflowTask.</p> <code>user_local_exports</code> <code>dict[str, str]</code> <p>Key-value pairs to be included as <code>export</code>-ed variables in SLURM submission script, after prepending values with the user's cache directory.</p> Source code in <code>fractal_server/runner/executors/slurm_common/slurm_config.py</code> <pre><code>class SlurmConfig(BaseModel):\n    \"\"\"\n    Abstraction for SLURM parameters\n\n    **NOTE**: `SlurmConfig` objects are created internally in `fractal-server`,\n    and they are not meant to be initialized by the user; the same holds for\n    `SlurmConfig` attributes (e.g. `mem_per_task_MB`), which are not meant to\n    be part of the superuser-defined `resource.jobs_runner_config` JSON field.\n\n    Part of the attributes map directly to some of the SLURM attributes (see\n    https://slurm.schedmd.com/sbatch.html), e.g. `partition`. Other attributes\n    are metaparameters which are needed in fractal-server to combine multiple\n    tasks in the same SLURM job (e.g. `parallel_tasks_per_job` or\n    `max_num_jobs`).\n\n    Attributes:\n        partition: Corresponds to SLURM option.\n        cpus_per_task: Corresponds to SLURM option.\n        mem_per_task_MB: Corresponds to `mem` SLURM option.\n        job_name: Corresponds to `name` SLURM option.\n        constraint: Corresponds to SLURM option.\n        gres: Corresponds to SLURM option.\n        account: Corresponds to SLURM option.\n        gpus: Corresponds to SLURM option.\n        time: Corresponds to SLURM option (WARNING: not fully supported).\n        nodelist: Corresponds to SLURM option.\n        exclude: Corresponds to SLURM option.\n        prefix: Prefix of configuration lines in SLURM submission scripts.\n        shebang_line: Shebang line for SLURM submission scripts.\n        extra_lines: Additional lines to include in SLURM submission scripts.\n        tasks_per_job: Number of tasks for each SLURM job.\n        parallel_tasks_per_job: Number of tasks to run in parallel for\n                                each SLURM job.\n        target_cpus_per_job: Optimal number of CPUs to be requested in each\n                             SLURM job.\n        max_cpus_per_job: Maximum number of CPUs that can be requested in each\n                          SLURM job.\n        target_mem_per_job: Optimal amount of memory (in MB) to be requested in\n                            each SLURM job.\n        max_mem_per_job: Maximum amount of memory (in MB) that can be requested\n                         in each SLURM job.\n        target_num_jobs: Optimal number of SLURM jobs for a given WorkflowTask.\n        max_num_jobs: Maximum number of SLURM jobs for a given WorkflowTask.\n        user_local_exports:\n            Key-value pairs to be included as `export`-ed variables in SLURM\n            submission script, after prepending values with the user's cache\n            directory.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    # Required SLURM parameters (note that the integer attributes are those\n    # that will need to scale up with the number of parallel tasks per job)\n    partition: str\n    cpus_per_task: int\n    mem_per_task_MB: int\n    prefix: str = \"#SBATCH\"\n    shebang_line: str = \"#!/bin/sh\"\n\n    # Optional SLURM parameters\n    job_name: str | None = None\n    constraint: str | None = None\n    gres: str | None = None\n    gpus: str | None = None\n    time: str | None = None\n    account: str | None = None\n    nodelist: str | None = None\n    exclude: str | None = None\n\n    # Free-field attribute for extra lines to be added to the SLURM job\n    # preamble\n    extra_lines: list[str] = Field(default_factory=list)\n\n    # Variables that will be `export`ed in the SLURM submission script\n    user_local_exports: dict[str, str] = Field(default_factory=dict)\n\n    # Metaparameters needed to combine multiple tasks in each SLURM job\n    tasks_per_job: int | None = None\n    parallel_tasks_per_job: int | None = None\n    target_cpus_per_job: int\n    max_cpus_per_job: int\n    target_mem_per_job: int\n    max_mem_per_job: int\n    target_num_jobs: int\n    max_num_jobs: int\n\n    def _sorted_extra_lines(self) -&gt; list[str]:\n        \"\"\"\n        Return a copy of `self.extra_lines`, where lines starting with\n        `self.prefix` are listed first.\n        \"\"\"\n\n        def _no_prefix(_line):\n            if _line.startswith(self.prefix):\n                return 0\n            else:\n                return 1\n\n        return sorted(self.extra_lines, key=_no_prefix)\n\n    def sort_script_lines(self, script_lines: list[str]) -&gt; list[str]:\n        \"\"\"\n        Return a copy of `script_lines`, where lines are sorted as in:\n\n        1. `self.shebang_line` (if present);\n        2. Lines starting with `self.prefix`;\n        3. Other lines.\n\n        Args:\n            script_lines:\n        \"\"\"\n\n        def _sorting_function(_line):\n            if _line == self.shebang_line:\n                return 0\n            elif _line.startswith(self.prefix):\n                return 1\n            else:\n                return 2\n\n        return sorted(script_lines, key=_sorting_function)\n\n    def to_sbatch_preamble(\n        self,\n        remote_export_dir: str,\n    ) -&gt; list[str]:\n        \"\"\"\n        Compile `SlurmConfig` object into the preamble of a SLURM submission\n        script.\n\n        Args:\n            remote_export_dir:\n                Base directory for exports defined in\n                `self.user_local_exports`.\n        \"\"\"\n        if self.parallel_tasks_per_job is None:\n            raise ValueError(\n                \"SlurmConfig.sbatch_preamble requires that \"\n                f\"{self.parallel_tasks_per_job=} is not None.\"\n            )\n        if len(self.extra_lines) != len(set(self.extra_lines)):\n            raise ValueError(f\"{self.extra_lines=} contains repetitions\")\n\n        mem_per_job_MB = self.parallel_tasks_per_job * self.mem_per_task_MB\n        lines = [\n            self.shebang_line,\n            f\"{self.prefix} --partition={self.partition}\",\n            f\"{self.prefix} --ntasks={self.parallel_tasks_per_job}\",\n            f\"{self.prefix} --cpus-per-task={self.cpus_per_task}\",\n            f\"{self.prefix} --mem={mem_per_job_MB}M\",\n        ]\n        for key in [\n            \"job_name\",\n            \"constraint\",\n            \"gres\",\n            \"gpus\",\n            \"time\",\n            \"account\",\n            \"exclude\",\n            \"nodelist\",\n        ]:\n            value = getattr(self, key)\n            if value is not None:\n                # Handle the `time` parameter\n                if key == \"time\" and self.parallel_tasks_per_job &gt; 1:\n                    # NOTE: see issue #1632\n                    logger.warning(\n                        f\"`time` SLURM parameter is set to {self.time}, \"\n                        \"but this does not take into account the number of \"\n                        f\"SLURM tasks ({self.parallel_tasks_per_job}).\"\n                    )\n                option = key.replace(\"_\", \"-\")\n                lines.append(f\"{self.prefix} --{option}={value}\")\n\n        for line in self._sorted_extra_lines():\n            lines.append(line)\n\n        if self.user_local_exports:\n            for key, value in self.user_local_exports.items():\n                tmp_value = str(Path(remote_export_dir) / value)\n                lines.append(f\"export {key}={tmp_value}\")\n\n        \"\"\"\n        FIXME export SRUN_CPUS_PER_TASK\n        # From https://slurm.schedmd.com/sbatch.html: Beginning with 22.05,\n        # srun will not inherit the --cpus-per-task value requested by salloc\n        # or sbatch.  It must be requested again with the call to srun or set\n        # with the SRUN_CPUS_PER_TASK environment variable if desired for the\n        # task(s).\n        if config.cpus_per_task:\n            #additional_setup_lines.append(\n                f\"export SRUN_CPUS_PER_TASK={config.cpus_per_task}\"\n            )\n        \"\"\"\n\n        return lines\n\n    @property\n    def batch_size(self) -&gt; int:\n        return self.tasks_per_job\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/slurm_config/#fractal_server.runner.executors.slurm_common.slurm_config.SlurmConfig._sorted_extra_lines","title":"<code>_sorted_extra_lines()</code>","text":"<p>Return a copy of <code>self.extra_lines</code>, where lines starting with <code>self.prefix</code> are listed first.</p> Source code in <code>fractal_server/runner/executors/slurm_common/slurm_config.py</code> <pre><code>def _sorted_extra_lines(self) -&gt; list[str]:\n    \"\"\"\n    Return a copy of `self.extra_lines`, where lines starting with\n    `self.prefix` are listed first.\n    \"\"\"\n\n    def _no_prefix(_line):\n        if _line.startswith(self.prefix):\n            return 0\n        else:\n            return 1\n\n    return sorted(self.extra_lines, key=_no_prefix)\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/slurm_config/#fractal_server.runner.executors.slurm_common.slurm_config.SlurmConfig.sort_script_lines","title":"<code>sort_script_lines(script_lines)</code>","text":"<p>Return a copy of <code>script_lines</code>, where lines are sorted as in:</p> <ol> <li><code>self.shebang_line</code> (if present);</li> <li>Lines starting with <code>self.prefix</code>;</li> <li>Other lines.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>script_lines</code> <code>list[str]</code> required Source code in <code>fractal_server/runner/executors/slurm_common/slurm_config.py</code> <pre><code>def sort_script_lines(self, script_lines: list[str]) -&gt; list[str]:\n    \"\"\"\n    Return a copy of `script_lines`, where lines are sorted as in:\n\n    1. `self.shebang_line` (if present);\n    2. Lines starting with `self.prefix`;\n    3. Other lines.\n\n    Args:\n        script_lines:\n    \"\"\"\n\n    def _sorting_function(_line):\n        if _line == self.shebang_line:\n            return 0\n        elif _line.startswith(self.prefix):\n            return 1\n        else:\n            return 2\n\n    return sorted(script_lines, key=_sorting_function)\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/slurm_config/#fractal_server.runner.executors.slurm_common.slurm_config.SlurmConfig.to_sbatch_preamble","title":"<code>to_sbatch_preamble(remote_export_dir)</code>","text":"<p>Compile <code>SlurmConfig</code> object into the preamble of a SLURM submission script.</p> <p>Parameters:</p> Name Type Description Default <code>remote_export_dir</code> <code>str</code> <p>Base directory for exports defined in <code>self.user_local_exports</code>.</p> required Source code in <code>fractal_server/runner/executors/slurm_common/slurm_config.py</code> <pre><code>def to_sbatch_preamble(\n    self,\n    remote_export_dir: str,\n) -&gt; list[str]:\n    \"\"\"\n    Compile `SlurmConfig` object into the preamble of a SLURM submission\n    script.\n\n    Args:\n        remote_export_dir:\n            Base directory for exports defined in\n            `self.user_local_exports`.\n    \"\"\"\n    if self.parallel_tasks_per_job is None:\n        raise ValueError(\n            \"SlurmConfig.sbatch_preamble requires that \"\n            f\"{self.parallel_tasks_per_job=} is not None.\"\n        )\n    if len(self.extra_lines) != len(set(self.extra_lines)):\n        raise ValueError(f\"{self.extra_lines=} contains repetitions\")\n\n    mem_per_job_MB = self.parallel_tasks_per_job * self.mem_per_task_MB\n    lines = [\n        self.shebang_line,\n        f\"{self.prefix} --partition={self.partition}\",\n        f\"{self.prefix} --ntasks={self.parallel_tasks_per_job}\",\n        f\"{self.prefix} --cpus-per-task={self.cpus_per_task}\",\n        f\"{self.prefix} --mem={mem_per_job_MB}M\",\n    ]\n    for key in [\n        \"job_name\",\n        \"constraint\",\n        \"gres\",\n        \"gpus\",\n        \"time\",\n        \"account\",\n        \"exclude\",\n        \"nodelist\",\n    ]:\n        value = getattr(self, key)\n        if value is not None:\n            # Handle the `time` parameter\n            if key == \"time\" and self.parallel_tasks_per_job &gt; 1:\n                # NOTE: see issue #1632\n                logger.warning(\n                    f\"`time` SLURM parameter is set to {self.time}, \"\n                    \"but this does not take into account the number of \"\n                    f\"SLURM tasks ({self.parallel_tasks_per_job}).\"\n                )\n            option = key.replace(\"_\", \"-\")\n            lines.append(f\"{self.prefix} --{option}={value}\")\n\n    for line in self._sorted_extra_lines():\n        lines.append(line)\n\n    if self.user_local_exports:\n        for key, value in self.user_local_exports.items():\n            tmp_value = str(Path(remote_export_dir) / value)\n            lines.append(f\"export {key}={tmp_value}\")\n\n    \"\"\"\n    FIXME export SRUN_CPUS_PER_TASK\n    # From https://slurm.schedmd.com/sbatch.html: Beginning with 22.05,\n    # srun will not inherit the --cpus-per-task value requested by salloc\n    # or sbatch.  It must be requested again with the call to srun or set\n    # with the SRUN_CPUS_PER_TASK environment variable if desired for the\n    # task(s).\n    if config.cpus_per_task:\n        #additional_setup_lines.append(\n            f\"export SRUN_CPUS_PER_TASK={config.cpus_per_task}\"\n        )\n    \"\"\"\n\n    return lines\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_common/slurm_job_task_models/","title":"slurm_job_task_models","text":""},{"location":"reference/fractal_server/runner/executors/slurm_ssh/","title":"slurm_ssh","text":""},{"location":"reference/fractal_server/runner/executors/slurm_ssh/run_subprocess/","title":"run_subprocess","text":""},{"location":"reference/fractal_server/runner/executors/slurm_ssh/runner/","title":"runner","text":""},{"location":"reference/fractal_server/runner/executors/slurm_ssh/runner/#fractal_server.runner.executors.slurm_ssh.runner.SlurmSSHRunner","title":"<code>SlurmSSHRunner</code>","text":"<p>               Bases: <code>BaseSlurmRunner</code></p> Source code in <code>fractal_server/runner/executors/slurm_ssh/runner.py</code> <pre><code>class SlurmSSHRunner(BaseSlurmRunner):\n    fractal_ssh: FractalSSH\n\n    def __init__(\n        self,\n        *,\n        # Common\n        root_dir_local: Path,\n        root_dir_remote: Path,\n        common_script_lines: list[str] | None = None,\n        resource: Resource,\n        # Specific\n        slurm_account: str | None = None,\n        profile: Profile,\n        user_cache_dir: str,\n        fractal_ssh: FractalSSH,\n    ) -&gt; None:\n        \"\"\"\n        Set parameters that are the same for different Fractal tasks and for\n        different SLURM jobs/tasks.\n        \"\"\"\n        self.fractal_ssh = fractal_ssh\n        self.shared_config = JobRunnerConfigSLURM(\n            **resource.jobs_runner_config\n        )\n        logger.warning(self.fractal_ssh)\n\n        # Check SSH connection and try to recover from a closed-socket error\n        self.fractal_ssh.check_connection()\n        super().__init__(\n            slurm_runner_type=\"ssh\",\n            root_dir_local=root_dir_local,\n            root_dir_remote=root_dir_remote,\n            common_script_lines=common_script_lines,\n            user_cache_dir=user_cache_dir,\n            poll_interval=resource.jobs_poll_interval,\n            python_worker_interpreter=resource.jobs_slurm_python_worker,\n            slurm_account=slurm_account,\n        )\n\n    def _mkdir_local_folder(self, folder: str) -&gt; None:\n        Path(folder).mkdir(parents=True)\n\n    def _mkdir_remote_folder(self, folder: str):\n        self.fractal_ssh.mkdir(\n            folder=folder,\n            parents=True,\n        )\n\n    def _fetch_artifacts(\n        self,\n        finished_slurm_jobs: list[SlurmJob],\n    ) -&gt; None:\n        \"\"\"\n        Fetch artifacts for a list of SLURM jobs.\n        \"\"\"\n\n        # Check length\n        if len(finished_slurm_jobs) == 0:\n            logger.debug(f\"[_fetch_artifacts] EXIT ({finished_slurm_jobs=}).\")\n            return None\n\n        t_0 = time.perf_counter()\n        logger.debug(\n            f\"[_fetch_artifacts] START ({len(finished_slurm_jobs)=}).\"\n        )\n\n        # Extract `workdir_remote` and `workdir_local`\n        self.validate_slurm_jobs_workdirs(finished_slurm_jobs)\n        workdir_local = finished_slurm_jobs[0].workdir_local\n        workdir_remote = finished_slurm_jobs[0].workdir_remote\n\n        # Define local/remote tarfile paths\n        tarfile_path_local = workdir_local.with_suffix(\".tar.gz\").as_posix()\n        tarfile_path_remote = workdir_remote.with_suffix(\".tar.gz\").as_posix()\n\n        # Create file list\n        # NOTE: see issue 2483\n        filelist = []\n        for _slurm_job in finished_slurm_jobs:\n            _single_job_filelist = [\n                _slurm_job.slurm_stdout_remote_path.name,\n                _slurm_job.slurm_stderr_remote_path.name,\n            ]\n            for task in _slurm_job.tasks:\n                _single_job_filelist.extend(\n                    [\n                        task.output_file_remote_path.name,\n                        task.task_files.log_file_remote_path.name,\n                        task.task_files.metadiff_file_remote_path.name,\n                    ]\n                )\n            filelist.extend(_single_job_filelist)\n        filelist_string = \"\\n\".join(filelist)\n        elapsed = time.perf_counter() - t_0\n        logger.debug(\n            \"[_fetch_artifacts] Created filelist \"\n            f\"({len(filelist)=}, from start: {elapsed=:.3f} s).\"\n        )\n\n        # Write filelist to file remotely\n        tmp_filelist_path = workdir_remote / f\"filelist_{time.time()}.txt\"\n        self.fractal_ssh.write_remote_file(\n            path=tmp_filelist_path.as_posix(),\n            content=f\"{filelist_string}\\n\",\n        )\n        elapsed = time.perf_counter() - t_0\n        logger.debug(\n            f\"[_fetch_artifacts] File list written to {tmp_filelist_path} \"\n            f\"(from start: {elapsed=:.3f} s).\"\n        )\n\n        # Create remote tarfile\n        t_0_tar = time.perf_counter()\n        tar_command = get_tar_compression_cmd(\n            subfolder_path=workdir_remote,\n            filelist_path=tmp_filelist_path,\n        )\n        self.fractal_ssh.run_command(cmd=tar_command)\n        t_1_tar = time.perf_counter()\n        logger.info(\n            f\"[_fetch_artifacts] Remote archive {tarfile_path_remote} created\"\n            f\" - elapsed={t_1_tar - t_0_tar:.3f} s\"\n        )\n\n        # Fetch tarfile\n        t_0_get = time.perf_counter()\n        self.fractal_ssh.fetch_file(\n            remote=tarfile_path_remote,\n            local=tarfile_path_local,\n        )\n        t_1_get = time.perf_counter()\n        logger.info(\n            \"[_fetch_artifacts] Subfolder archive transferred back \"\n            f\"to {tarfile_path_local}\"\n            f\" - elapsed={t_1_get - t_0_get:.3f} s\"\n        )\n\n        # Extract tarfile locally\n        target_dir, cmd_tar = get_tar_extraction_cmd(Path(tarfile_path_local))\n        target_dir.mkdir(exist_ok=True)\n        run_subprocess(cmd=cmd_tar, logger_name=logger.name)\n        Path(tarfile_path_local).unlink(missing_ok=True)\n\n        t_1 = time.perf_counter()\n        logger.info(f\"[_fetch_artifacts] End - elapsed={t_1 - t_0:.3f} s\")\n\n    def _run_remote_cmd(self, cmd: str) -&gt; str:\n        stdout = self.fractal_ssh.run_command(cmd=cmd)\n        return stdout\n\n    def _send_many_job_inputs(\n        self, *, workdir_local: Path, workdir_remote: Path\n    ) -&gt; None:\n        \"\"\"\n        Compress, transfer, and extract a local working directory onto a remote\n        host.\n\n        This method creates a temporary `.tar.gz` archive of the given\n        `workdir_local`, transfers it to the remote machine via the configured\n        SSH connection, extracts it into `workdir_remote`, and removes the\n        temporary archive from both local and remote filesystems.\n        \"\"\"\n\n        logger.debug(\"[_send_many_job_inputs] START\")\n\n        tar_path_local = workdir_local.with_suffix(\".tar.gz\")\n        tar_name = Path(tar_path_local).name\n        tar_path_remote = workdir_remote.parent / tar_name\n\n        tar_compression_cmd = get_tar_compression_cmd(\n            subfolder_path=workdir_local, filelist_path=None\n        )\n        _, tar_extraction_cmd = get_tar_extraction_cmd(\n            archive_path=tar_path_remote\n        )\n        rm_tar_cmd = f\"rm {tar_path_remote.as_posix()}\"\n\n        try:\n            run_subprocess(tar_compression_cmd, logger_name=logger.name)\n            logger.debug(\n                \"[_send_many_job_inputs] \"\n                f\"{workdir_local=} compressed to {tar_path_local=}.\"\n            )\n            self.fractal_ssh.send_file(\n                local=tar_path_local.as_posix(),\n                remote=tar_path_remote.as_posix(),\n            )\n            logger.debug(\n                \"[_send_many_job_inputs] \"\n                f\"{tar_path_local=} sent via SSH to {tar_path_remote=}.\"\n            )\n            self.fractal_ssh.run_command(cmd=tar_extraction_cmd)\n            logger.debug(\n                \"[_send_many_job_inputs] \"\n                f\"{tar_path_remote=} extracted to {workdir_remote=}.\"\n            )\n            self.fractal_ssh.run_command(cmd=rm_tar_cmd)\n            logger.debug(\n                \"[_send_many_job_inputs] \"\n                f\"{tar_path_remote=} removed from remote server.\"\n            )\n        except Exception as e:\n            raise e\n        finally:\n            Path(tar_path_local).unlink(missing_ok=True)\n            logger.debug(f\"[_send_many_job_inputs] {tar_path_local=} removed.\")\n\n        logger.debug(\"[_send_many_job_inputs] END.\")\n\n    def run_squeue(self, *, job_ids: list[str]) -&gt; str:\n        \"\"\"\n        Run `squeue` for a set of SLURM job IDs.\n\n        Different scenarios:\n\n        1. When `squeue -j` succeeds (with exit code 0), return its stdout.\n        2. When `squeue -j` fails (typical example:\n           `squeue -j {invalid_job_id}` fails with exit code 1), re-raise.\n           The error will be handled upstream.\n        3. When the SSH command fails because another thread is keeping the\n           lock of the `FractalSSH` object for a long time, mock the standard\n           output of the `squeue` command so that it looks like jobs are not\n           completed yet.\n        4. When the SSH command fails for other reasons, despite a forgiving\n           setup (7 connection attempts with base waiting interval of 2\n           seconds, with a cumulative timeout of 126 seconds), return an empty\n           string. This will be treated upstream as an empty `squeu` output,\n           indirectly resulting in marking the job as completed.\n        \"\"\"\n\n        if len(job_ids) == 0:\n            return \"\"\n\n        job_id_single_str = \",\".join([str(j) for j in job_ids])\n        cmd = (\n            \"squeue --noheader --format='%i %T' --states=all \"\n            f\"--jobs={job_id_single_str}\"\n        )\n\n        try:\n            stdout = self.fractal_ssh.run_command(\n                cmd=cmd,\n            )\n            return stdout\n        except FractalSSHCommandError as e:\n            raise e\n        except FractalSSHTimeoutError:\n            logger.warning(\n                \"[run_squeue] Could not acquire lock, use stdout placeholder.\"\n            )\n            FAKE_STATUS = \"FRACTAL_STATUS_PLACEHOLDER\"\n            placeholder_stdout = \"\\n\".join(\n                [f\"{job_id} {FAKE_STATUS}\" for job_id in job_ids]\n            )\n            return placeholder_stdout\n        except Exception as e:\n            logger.error(f\"Ignoring `squeue` command failure {e}\")\n            return \"\"\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_ssh/runner/#fractal_server.runner.executors.slurm_ssh.runner.SlurmSSHRunner.__init__","title":"<code>__init__(*, root_dir_local, root_dir_remote, common_script_lines=None, resource, slurm_account=None, profile, user_cache_dir, fractal_ssh)</code>","text":"<p>Set parameters that are the same for different Fractal tasks and for different SLURM jobs/tasks.</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/runner.py</code> <pre><code>def __init__(\n    self,\n    *,\n    # Common\n    root_dir_local: Path,\n    root_dir_remote: Path,\n    common_script_lines: list[str] | None = None,\n    resource: Resource,\n    # Specific\n    slurm_account: str | None = None,\n    profile: Profile,\n    user_cache_dir: str,\n    fractal_ssh: FractalSSH,\n) -&gt; None:\n    \"\"\"\n    Set parameters that are the same for different Fractal tasks and for\n    different SLURM jobs/tasks.\n    \"\"\"\n    self.fractal_ssh = fractal_ssh\n    self.shared_config = JobRunnerConfigSLURM(\n        **resource.jobs_runner_config\n    )\n    logger.warning(self.fractal_ssh)\n\n    # Check SSH connection and try to recover from a closed-socket error\n    self.fractal_ssh.check_connection()\n    super().__init__(\n        slurm_runner_type=\"ssh\",\n        root_dir_local=root_dir_local,\n        root_dir_remote=root_dir_remote,\n        common_script_lines=common_script_lines,\n        user_cache_dir=user_cache_dir,\n        poll_interval=resource.jobs_poll_interval,\n        python_worker_interpreter=resource.jobs_slurm_python_worker,\n        slurm_account=slurm_account,\n    )\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_ssh/runner/#fractal_server.runner.executors.slurm_ssh.runner.SlurmSSHRunner._fetch_artifacts","title":"<code>_fetch_artifacts(finished_slurm_jobs)</code>","text":"<p>Fetch artifacts for a list of SLURM jobs.</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/runner.py</code> <pre><code>def _fetch_artifacts(\n    self,\n    finished_slurm_jobs: list[SlurmJob],\n) -&gt; None:\n    \"\"\"\n    Fetch artifacts for a list of SLURM jobs.\n    \"\"\"\n\n    # Check length\n    if len(finished_slurm_jobs) == 0:\n        logger.debug(f\"[_fetch_artifacts] EXIT ({finished_slurm_jobs=}).\")\n        return None\n\n    t_0 = time.perf_counter()\n    logger.debug(\n        f\"[_fetch_artifacts] START ({len(finished_slurm_jobs)=}).\"\n    )\n\n    # Extract `workdir_remote` and `workdir_local`\n    self.validate_slurm_jobs_workdirs(finished_slurm_jobs)\n    workdir_local = finished_slurm_jobs[0].workdir_local\n    workdir_remote = finished_slurm_jobs[0].workdir_remote\n\n    # Define local/remote tarfile paths\n    tarfile_path_local = workdir_local.with_suffix(\".tar.gz\").as_posix()\n    tarfile_path_remote = workdir_remote.with_suffix(\".tar.gz\").as_posix()\n\n    # Create file list\n    # NOTE: see issue 2483\n    filelist = []\n    for _slurm_job in finished_slurm_jobs:\n        _single_job_filelist = [\n            _slurm_job.slurm_stdout_remote_path.name,\n            _slurm_job.slurm_stderr_remote_path.name,\n        ]\n        for task in _slurm_job.tasks:\n            _single_job_filelist.extend(\n                [\n                    task.output_file_remote_path.name,\n                    task.task_files.log_file_remote_path.name,\n                    task.task_files.metadiff_file_remote_path.name,\n                ]\n            )\n        filelist.extend(_single_job_filelist)\n    filelist_string = \"\\n\".join(filelist)\n    elapsed = time.perf_counter() - t_0\n    logger.debug(\n        \"[_fetch_artifacts] Created filelist \"\n        f\"({len(filelist)=}, from start: {elapsed=:.3f} s).\"\n    )\n\n    # Write filelist to file remotely\n    tmp_filelist_path = workdir_remote / f\"filelist_{time.time()}.txt\"\n    self.fractal_ssh.write_remote_file(\n        path=tmp_filelist_path.as_posix(),\n        content=f\"{filelist_string}\\n\",\n    )\n    elapsed = time.perf_counter() - t_0\n    logger.debug(\n        f\"[_fetch_artifacts] File list written to {tmp_filelist_path} \"\n        f\"(from start: {elapsed=:.3f} s).\"\n    )\n\n    # Create remote tarfile\n    t_0_tar = time.perf_counter()\n    tar_command = get_tar_compression_cmd(\n        subfolder_path=workdir_remote,\n        filelist_path=tmp_filelist_path,\n    )\n    self.fractal_ssh.run_command(cmd=tar_command)\n    t_1_tar = time.perf_counter()\n    logger.info(\n        f\"[_fetch_artifacts] Remote archive {tarfile_path_remote} created\"\n        f\" - elapsed={t_1_tar - t_0_tar:.3f} s\"\n    )\n\n    # Fetch tarfile\n    t_0_get = time.perf_counter()\n    self.fractal_ssh.fetch_file(\n        remote=tarfile_path_remote,\n        local=tarfile_path_local,\n    )\n    t_1_get = time.perf_counter()\n    logger.info(\n        \"[_fetch_artifacts] Subfolder archive transferred back \"\n        f\"to {tarfile_path_local}\"\n        f\" - elapsed={t_1_get - t_0_get:.3f} s\"\n    )\n\n    # Extract tarfile locally\n    target_dir, cmd_tar = get_tar_extraction_cmd(Path(tarfile_path_local))\n    target_dir.mkdir(exist_ok=True)\n    run_subprocess(cmd=cmd_tar, logger_name=logger.name)\n    Path(tarfile_path_local).unlink(missing_ok=True)\n\n    t_1 = time.perf_counter()\n    logger.info(f\"[_fetch_artifacts] End - elapsed={t_1 - t_0:.3f} s\")\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_ssh/runner/#fractal_server.runner.executors.slurm_ssh.runner.SlurmSSHRunner._send_many_job_inputs","title":"<code>_send_many_job_inputs(*, workdir_local, workdir_remote)</code>","text":"<p>Compress, transfer, and extract a local working directory onto a remote host.</p> <p>This method creates a temporary <code>.tar.gz</code> archive of the given <code>workdir_local</code>, transfers it to the remote machine via the configured SSH connection, extracts it into <code>workdir_remote</code>, and removes the temporary archive from both local and remote filesystems.</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/runner.py</code> <pre><code>def _send_many_job_inputs(\n    self, *, workdir_local: Path, workdir_remote: Path\n) -&gt; None:\n    \"\"\"\n    Compress, transfer, and extract a local working directory onto a remote\n    host.\n\n    This method creates a temporary `.tar.gz` archive of the given\n    `workdir_local`, transfers it to the remote machine via the configured\n    SSH connection, extracts it into `workdir_remote`, and removes the\n    temporary archive from both local and remote filesystems.\n    \"\"\"\n\n    logger.debug(\"[_send_many_job_inputs] START\")\n\n    tar_path_local = workdir_local.with_suffix(\".tar.gz\")\n    tar_name = Path(tar_path_local).name\n    tar_path_remote = workdir_remote.parent / tar_name\n\n    tar_compression_cmd = get_tar_compression_cmd(\n        subfolder_path=workdir_local, filelist_path=None\n    )\n    _, tar_extraction_cmd = get_tar_extraction_cmd(\n        archive_path=tar_path_remote\n    )\n    rm_tar_cmd = f\"rm {tar_path_remote.as_posix()}\"\n\n    try:\n        run_subprocess(tar_compression_cmd, logger_name=logger.name)\n        logger.debug(\n            \"[_send_many_job_inputs] \"\n            f\"{workdir_local=} compressed to {tar_path_local=}.\"\n        )\n        self.fractal_ssh.send_file(\n            local=tar_path_local.as_posix(),\n            remote=tar_path_remote.as_posix(),\n        )\n        logger.debug(\n            \"[_send_many_job_inputs] \"\n            f\"{tar_path_local=} sent via SSH to {tar_path_remote=}.\"\n        )\n        self.fractal_ssh.run_command(cmd=tar_extraction_cmd)\n        logger.debug(\n            \"[_send_many_job_inputs] \"\n            f\"{tar_path_remote=} extracted to {workdir_remote=}.\"\n        )\n        self.fractal_ssh.run_command(cmd=rm_tar_cmd)\n        logger.debug(\n            \"[_send_many_job_inputs] \"\n            f\"{tar_path_remote=} removed from remote server.\"\n        )\n    except Exception as e:\n        raise e\n    finally:\n        Path(tar_path_local).unlink(missing_ok=True)\n        logger.debug(f\"[_send_many_job_inputs] {tar_path_local=} removed.\")\n\n    logger.debug(\"[_send_many_job_inputs] END.\")\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_ssh/runner/#fractal_server.runner.executors.slurm_ssh.runner.SlurmSSHRunner.run_squeue","title":"<code>run_squeue(*, job_ids)</code>","text":"<p>Run <code>squeue</code> for a set of SLURM job IDs.</p> <p>Different scenarios:</p> <ol> <li>When <code>squeue -j</code> succeeds (with exit code 0), return its stdout.</li> <li>When <code>squeue -j</code> fails (typical example:    <code>squeue -j {invalid_job_id}</code> fails with exit code 1), re-raise.    The error will be handled upstream.</li> <li>When the SSH command fails because another thread is keeping the    lock of the <code>FractalSSH</code> object for a long time, mock the standard    output of the <code>squeue</code> command so that it looks like jobs are not    completed yet.</li> <li>When the SSH command fails for other reasons, despite a forgiving    setup (7 connection attempts with base waiting interval of 2    seconds, with a cumulative timeout of 126 seconds), return an empty    string. This will be treated upstream as an empty <code>squeu</code> output,    indirectly resulting in marking the job as completed.</li> </ol> Source code in <code>fractal_server/runner/executors/slurm_ssh/runner.py</code> <pre><code>def run_squeue(self, *, job_ids: list[str]) -&gt; str:\n    \"\"\"\n    Run `squeue` for a set of SLURM job IDs.\n\n    Different scenarios:\n\n    1. When `squeue -j` succeeds (with exit code 0), return its stdout.\n    2. When `squeue -j` fails (typical example:\n       `squeue -j {invalid_job_id}` fails with exit code 1), re-raise.\n       The error will be handled upstream.\n    3. When the SSH command fails because another thread is keeping the\n       lock of the `FractalSSH` object for a long time, mock the standard\n       output of the `squeue` command so that it looks like jobs are not\n       completed yet.\n    4. When the SSH command fails for other reasons, despite a forgiving\n       setup (7 connection attempts with base waiting interval of 2\n       seconds, with a cumulative timeout of 126 seconds), return an empty\n       string. This will be treated upstream as an empty `squeu` output,\n       indirectly resulting in marking the job as completed.\n    \"\"\"\n\n    if len(job_ids) == 0:\n        return \"\"\n\n    job_id_single_str = \",\".join([str(j) for j in job_ids])\n    cmd = (\n        \"squeue --noheader --format='%i %T' --states=all \"\n        f\"--jobs={job_id_single_str}\"\n    )\n\n    try:\n        stdout = self.fractal_ssh.run_command(\n            cmd=cmd,\n        )\n        return stdout\n    except FractalSSHCommandError as e:\n        raise e\n    except FractalSSHTimeoutError:\n        logger.warning(\n            \"[run_squeue] Could not acquire lock, use stdout placeholder.\"\n        )\n        FAKE_STATUS = \"FRACTAL_STATUS_PLACEHOLDER\"\n        placeholder_stdout = \"\\n\".join(\n            [f\"{job_id} {FAKE_STATUS}\" for job_id in job_ids]\n        )\n        return placeholder_stdout\n    except Exception as e:\n        logger.error(f\"Ignoring `squeue` command failure {e}\")\n        return \"\"\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_ssh/tar_commands/","title":"tar_commands","text":"<p>Prepare tar commands for task-subfolder compression/extraction.</p>"},{"location":"reference/fractal_server/runner/executors/slurm_ssh/tar_commands/#fractal_server.runner.executors.slurm_ssh.tar_commands.get_tar_compression_cmd","title":"<code>get_tar_compression_cmd(subfolder_path, filelist_path)</code>","text":"<p>Prepare command to compress e.g. <code>/path/dir</code> into <code>/path/dir.tar.gz</code>.</p> <p>Note that <code>/path/dir.tar.gz</code> may already exist. In this case, it will be overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>subfolder_path</code> <code>Path</code> <p>Absolute path to the folder to compress.</p> required <code>filelist_path</code> <code>Path | None</code> <p>If set, to be used in the <code>--files-from</code> option.</p> required <p>Returns:</p> Type Description <code>str</code> <p>tar command</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/tar_commands.py</code> <pre><code>def get_tar_compression_cmd(\n    subfolder_path: Path,\n    filelist_path: Path | None,\n) -&gt; str:\n    \"\"\"\n    Prepare command to compress e.g. `/path/dir` into `/path/dir.tar.gz`.\n\n    Note that `/path/dir.tar.gz` may already exist. In this case, it will\n    be overwritten.\n\n    Args:\n        subfolder_path: Absolute path to the folder to compress.\n        filelist_path: If set, to be used in the `--files-from` option.\n\n    Returns:\n        tar command\n    \"\"\"\n    tarfile_path = subfolder_path.with_suffix(\".tar.gz\")\n    if filelist_path is None:\n        cmd_tar = (\n            f\"tar -c -z \"\n            f\"-f {tarfile_path} \"\n            f\"--directory={subfolder_path.as_posix()} \"\n            \".\"\n        )\n    else:\n        cmd_tar = (\n            f\"tar -c -z -f {tarfile_path} \"\n            f\"--directory={subfolder_path.as_posix()} \"\n            f\"--files-from={filelist_path.as_posix()} --ignore-failed-read\"\n        )\n\n    return cmd_tar\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_ssh/tar_commands/#fractal_server.runner.executors.slurm_ssh.tar_commands.get_tar_extraction_cmd","title":"<code>get_tar_extraction_cmd(archive_path)</code>","text":"<p>Prepare command to extract e.g. <code>/path/dir.tar.gz</code> into <code>/path/dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>archive_path</code> <code>Path</code> <p>Absolute path to the tar.gz archive.</p> required <p>Returns:</p> Type Description <code>tuple[Path, str]</code> <p>Target extraction folder and tar command</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/tar_commands.py</code> <pre><code>def get_tar_extraction_cmd(archive_path: Path) -&gt; tuple[Path, str]:\n    \"\"\"\n    Prepare command to extract e.g. `/path/dir.tar.gz` into `/path/dir`.\n\n    Args:\n        archive_path: Absolute path to the tar.gz archive.\n\n    Returns:\n        Target extraction folder and tar command\n    \"\"\"\n\n    # Prepare subfolder path\n    if archive_path.suffixes[-2:] != [\".tar\", \".gz\"]:\n        raise ValueError(\n            \"Archive path must end with `.tar.gz` \"\n            f\"(given: {archive_path.as_posix()})\"\n        )\n    subfolder_path = archive_path.with_suffix(\"\").with_suffix(\"\")\n\n    cmd_tar = (\n        f\"tar -xzvf {archive_path} --directory={subfolder_path.as_posix()}\"\n    )\n    return subfolder_path, cmd_tar\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_sudo/","title":"slurm_sudo","text":""},{"location":"reference/fractal_server/runner/executors/slurm_sudo/_subprocess_run_as_user/","title":"_subprocess_run_as_user","text":"<p>Run simple commands as another user</p> <p>This module provides a set of tools similar to <code>subprocess.run</code>, <code>glob.glob</code> or <code>os.path.exists</code>, but extended so that they can be executed on behalf of another user. Note that this requires appropriate sudo permissions.</p>"},{"location":"reference/fractal_server/runner/executors/slurm_sudo/_subprocess_run_as_user/#fractal_server.runner.executors.slurm_sudo._subprocess_run_as_user._mkdir_as_user","title":"<code>_mkdir_as_user(*, folder, user)</code>","text":"<p>Create a folder as a different user</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Absolute path to the folder</p> required <code>user</code> <code>str</code> <p>User to be impersonated</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if <code>user</code> is not correctly defined, or if subprocess           returncode is not 0.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/_subprocess_run_as_user.py</code> <pre><code>def _mkdir_as_user(*, folder: str, user: str) -&gt; None:\n    \"\"\"\n    Create a folder as a different user\n\n    Args:\n        folder: Absolute path to the folder\n        user: User to be impersonated\n\n    Raises:\n        RuntimeError: if `user` is not correctly defined, or if subprocess\n                      returncode is not 0.\n    \"\"\"\n    if not user:\n        raise RuntimeError(f\"{user=} not allowed in _mkdir_as_user\")\n\n    cmd = f\"mkdir -p {folder}\"\n    _run_command_as_user(cmd=cmd, user=user, check=True)\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_sudo/_subprocess_run_as_user/#fractal_server.runner.executors.slurm_sudo._subprocess_run_as_user._run_command_as_user","title":"<code>_run_command_as_user(*, cmd, user=None, check=False)</code>","text":"<p>Use <code>sudo -u</code> to impersonate another user and run a command</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>str</code> <p>Command to be run</p> required <code>user</code> <code>str | None</code> <p>User to be impersonated</p> <code>None</code> <code>check</code> <code>bool</code> <p>If <code>True</code>, check that <code>returncode=0</code> and fail otherwise.</p> <code>False</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if <code>check=True</code> and returncode is non-zero.</p> <p>Returns:</p> Name Type Description <code>res</code> <code>CompletedProcess</code> <p>The return value from <code>subprocess.run</code>.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/_subprocess_run_as_user.py</code> <pre><code>def _run_command_as_user(\n    *,\n    cmd: str,\n    user: str | None = None,\n    check: bool = False,\n) -&gt; subprocess.CompletedProcess:\n    \"\"\"\n    Use `sudo -u` to impersonate another user and run a command\n\n    Args:\n        cmd: Command to be run\n        user: User to be impersonated\n        check: If `True`, check that `returncode=0` and fail otherwise.\n\n    Raises:\n        RuntimeError: if `check=True` and returncode is non-zero.\n\n    Returns:\n        res: The return value from `subprocess.run`.\n    \"\"\"\n    validate_cmd(cmd)\n    logger.debug(f'[_run_command_as_user] {user=}, cmd=\"{cmd}\"')\n    if user:\n        new_cmd = f\"sudo --set-home --non-interactive -u {user} {cmd}\"\n    else:\n        new_cmd = cmd\n    res = subprocess.run(  # nosec\n        shlex.split(new_cmd),\n        capture_output=True,\n        encoding=\"utf-8\",\n    )\n    logger.debug(f\"[_run_command_as_user] {res.returncode=}\")\n    logger.debug(f\"[_run_command_as_user] {res.stdout=}\")\n    logger.debug(f\"[_run_command_as_user] {res.stderr=}\")\n\n    if check and not res.returncode == 0:\n        raise RuntimeError(\n            f\"{cmd=}\\n\\n{res.returncode=}\\n\\n{res.stdout=}\\n\\n{res.stderr=}\\n\"\n        )\n\n    return res\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_sudo/runner/","title":"runner","text":""},{"location":"reference/fractal_server/runner/executors/slurm_sudo/runner/#fractal_server.runner.executors.slurm_sudo.runner.SudoSlurmRunner","title":"<code>SudoSlurmRunner</code>","text":"<p>               Bases: <code>BaseSlurmRunner</code></p> Source code in <code>fractal_server/runner/executors/slurm_sudo/runner.py</code> <pre><code>class SudoSlurmRunner(BaseSlurmRunner):\n    slurm_user: str\n    slurm_account: str | None = None\n\n    def __init__(\n        self,\n        *,\n        # Common\n        root_dir_local: Path,\n        root_dir_remote: Path,\n        common_script_lines: list[str] | None = None,\n        resource: Resource,\n        # Specific\n        profile: Profile,\n        user_cache_dir: str,\n        slurm_account: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Set parameters that are the same for different Fractal tasks and for\n        different SLURM jobs/tasks.\n        \"\"\"\n\n        self.slurm_user = profile.username\n        self.shared_config = JobRunnerConfigSLURM(\n            **resource.jobs_runner_config\n        )\n\n        super().__init__(\n            slurm_runner_type=\"sudo\",\n            root_dir_local=root_dir_local,\n            root_dir_remote=root_dir_remote,\n            common_script_lines=common_script_lines,\n            user_cache_dir=user_cache_dir,\n            poll_interval=resource.jobs_poll_interval,\n            python_worker_interpreter=resource.jobs_slurm_python_worker,\n            slurm_account=slurm_account,\n        )\n\n    def _mkdir_local_folder(self, folder: str) -&gt; None:\n        original_umask = os.umask(0)\n        Path(folder).mkdir(parents=True, mode=0o755)\n        os.umask(original_umask)\n\n    def _mkdir_remote_folder(self, folder: str) -&gt; None:\n        _mkdir_as_user(folder=folder, user=self.slurm_user)\n\n    def _fetch_artifacts_single_job(self, job: SlurmJob) -&gt; None:\n        \"\"\"\n        Fetch artifacts for a single SLURM jobs.\n        \"\"\"\n        logger.debug(\n            f\"[_fetch_artifacts_single_job] {job.slurm_job_id=} START\"\n        )\n        source_target_list = [\n            (job.slurm_stdout_remote, job.slurm_stdout_local),\n            (job.slurm_stderr_remote, job.slurm_stderr_local),\n        ]\n        for task in job.tasks:\n            source_target_list.extend(\n                [\n                    (\n                        task.output_file_remote,\n                        task.output_file_local,\n                    ),\n                    (\n                        task.task_files.log_file_remote,\n                        task.task_files.log_file_local,\n                    ),\n                    (\n                        task.task_files.metadiff_file_remote,\n                        task.task_files.metadiff_file_local,\n                    ),\n                ]\n            )\n\n        for source, target in source_target_list:\n            try:\n                res = _run_command_as_user(\n                    cmd=f\"cat {source}\",\n                    user=self.slurm_user,\n                    check=True,\n                )\n                # Write local file\n                with open(target, \"w\") as f:\n                    f.write(res.stdout)\n                logger.debug(\n                    f\"[_fetch_artifacts_single_job] Copied {source} into \"\n                    f\"{target}\"\n                )\n            except RuntimeError as e:\n                logger.warning(\n                    f\"SKIP copy {source} into {target}. \"\n                    f\"Original error: {str(e)}\"\n                )\n        logger.debug(f\"[_fetch_artifacts_single_job] {job.slurm_job_id=} END\")\n\n    def _fetch_artifacts(\n        self,\n        finished_slurm_jobs: list[SlurmJob],\n    ) -&gt; None:\n        \"\"\"\n        Fetch artifacts for a list of SLURM jobs.\n        \"\"\"\n        MAX_NUM_THREADS = 12\n        THREAD_NAME_PREFIX = \"fetch_artifacts\"\n        logger.debug(\n            \"[_fetch_artifacts] START \"\n            f\"({MAX_NUM_THREADS=}, {len(finished_slurm_jobs)=}).\"\n        )\n        with ThreadPoolExecutor(\n            max_workers=MAX_NUM_THREADS,\n            thread_name_prefix=THREAD_NAME_PREFIX,\n        ) as executor:\n            executor.map(\n                self._fetch_artifacts_single_job,\n                finished_slurm_jobs,\n            )\n        logger.debug(\"[_fetch_artifacts] END.\")\n\n    def _run_remote_cmd(self, cmd: str) -&gt; str:\n        res = _run_command_as_user(\n            cmd=cmd,\n            user=self.slurm_user,\n            check=True,\n        )\n        return res.stdout\n\n    def run_squeue(self, *, job_ids: list[str]) -&gt; str:\n        \"\"\"\n        Run `squeue` for a set of SLURM job IDs.\n        \"\"\"\n\n        if len(job_ids) == 0:\n            return \"\"\n\n        job_id_single_str = \",\".join([str(j) for j in job_ids])\n        cmd = (\n            \"squeue --noheader --format='%i %T' --states=all \"\n            f\"--jobs {job_id_single_str}\"\n        )\n        res = _subprocess_run_or_raise(cmd)\n        return res.stdout\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_sudo/runner/#fractal_server.runner.executors.slurm_sudo.runner.SudoSlurmRunner.__init__","title":"<code>__init__(*, root_dir_local, root_dir_remote, common_script_lines=None, resource, profile, user_cache_dir, slurm_account=None)</code>","text":"<p>Set parameters that are the same for different Fractal tasks and for different SLURM jobs/tasks.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/runner.py</code> <pre><code>def __init__(\n    self,\n    *,\n    # Common\n    root_dir_local: Path,\n    root_dir_remote: Path,\n    common_script_lines: list[str] | None = None,\n    resource: Resource,\n    # Specific\n    profile: Profile,\n    user_cache_dir: str,\n    slurm_account: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Set parameters that are the same for different Fractal tasks and for\n    different SLURM jobs/tasks.\n    \"\"\"\n\n    self.slurm_user = profile.username\n    self.shared_config = JobRunnerConfigSLURM(\n        **resource.jobs_runner_config\n    )\n\n    super().__init__(\n        slurm_runner_type=\"sudo\",\n        root_dir_local=root_dir_local,\n        root_dir_remote=root_dir_remote,\n        common_script_lines=common_script_lines,\n        user_cache_dir=user_cache_dir,\n        poll_interval=resource.jobs_poll_interval,\n        python_worker_interpreter=resource.jobs_slurm_python_worker,\n        slurm_account=slurm_account,\n    )\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_sudo/runner/#fractal_server.runner.executors.slurm_sudo.runner.SudoSlurmRunner._fetch_artifacts","title":"<code>_fetch_artifacts(finished_slurm_jobs)</code>","text":"<p>Fetch artifacts for a list of SLURM jobs.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/runner.py</code> <pre><code>def _fetch_artifacts(\n    self,\n    finished_slurm_jobs: list[SlurmJob],\n) -&gt; None:\n    \"\"\"\n    Fetch artifacts for a list of SLURM jobs.\n    \"\"\"\n    MAX_NUM_THREADS = 12\n    THREAD_NAME_PREFIX = \"fetch_artifacts\"\n    logger.debug(\n        \"[_fetch_artifacts] START \"\n        f\"({MAX_NUM_THREADS=}, {len(finished_slurm_jobs)=}).\"\n    )\n    with ThreadPoolExecutor(\n        max_workers=MAX_NUM_THREADS,\n        thread_name_prefix=THREAD_NAME_PREFIX,\n    ) as executor:\n        executor.map(\n            self._fetch_artifacts_single_job,\n            finished_slurm_jobs,\n        )\n    logger.debug(\"[_fetch_artifacts] END.\")\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_sudo/runner/#fractal_server.runner.executors.slurm_sudo.runner.SudoSlurmRunner._fetch_artifacts_single_job","title":"<code>_fetch_artifacts_single_job(job)</code>","text":"<p>Fetch artifacts for a single SLURM jobs.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/runner.py</code> <pre><code>def _fetch_artifacts_single_job(self, job: SlurmJob) -&gt; None:\n    \"\"\"\n    Fetch artifacts for a single SLURM jobs.\n    \"\"\"\n    logger.debug(\n        f\"[_fetch_artifacts_single_job] {job.slurm_job_id=} START\"\n    )\n    source_target_list = [\n        (job.slurm_stdout_remote, job.slurm_stdout_local),\n        (job.slurm_stderr_remote, job.slurm_stderr_local),\n    ]\n    for task in job.tasks:\n        source_target_list.extend(\n            [\n                (\n                    task.output_file_remote,\n                    task.output_file_local,\n                ),\n                (\n                    task.task_files.log_file_remote,\n                    task.task_files.log_file_local,\n                ),\n                (\n                    task.task_files.metadiff_file_remote,\n                    task.task_files.metadiff_file_local,\n                ),\n            ]\n        )\n\n    for source, target in source_target_list:\n        try:\n            res = _run_command_as_user(\n                cmd=f\"cat {source}\",\n                user=self.slurm_user,\n                check=True,\n            )\n            # Write local file\n            with open(target, \"w\") as f:\n                f.write(res.stdout)\n            logger.debug(\n                f\"[_fetch_artifacts_single_job] Copied {source} into \"\n                f\"{target}\"\n            )\n        except RuntimeError as e:\n            logger.warning(\n                f\"SKIP copy {source} into {target}. \"\n                f\"Original error: {str(e)}\"\n            )\n    logger.debug(f\"[_fetch_artifacts_single_job] {job.slurm_job_id=} END\")\n</code></pre>"},{"location":"reference/fractal_server/runner/executors/slurm_sudo/runner/#fractal_server.runner.executors.slurm_sudo.runner.SudoSlurmRunner.run_squeue","title":"<code>run_squeue(*, job_ids)</code>","text":"<p>Run <code>squeue</code> for a set of SLURM job IDs.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/runner.py</code> <pre><code>def run_squeue(self, *, job_ids: list[str]) -&gt; str:\n    \"\"\"\n    Run `squeue` for a set of SLURM job IDs.\n    \"\"\"\n\n    if len(job_ids) == 0:\n        return \"\"\n\n    job_id_single_str = \",\".join([str(j) for j in job_ids])\n    cmd = (\n        \"squeue --noheader --format='%i %T' --states=all \"\n        f\"--jobs {job_id_single_str}\"\n    )\n    res = _subprocess_run_or_raise(cmd)\n    return res.stdout\n</code></pre>"},{"location":"reference/fractal_server/runner/v2/","title":"v2","text":""},{"location":"reference/fractal_server/runner/v2/_local/","title":"_local","text":""},{"location":"reference/fractal_server/runner/v2/_local/#fractal_server.runner.v2._local.process_workflow","title":"<code>process_workflow(*, job_id, workflow, dataset, workflow_dir_local, workflow_dir_remote=None, first_task_index=None, last_task_index=None, logger_name, job_attribute_filters, job_type_filters, user_id, resource, profile, user_cache_dir=None, fractal_ssh=None, slurm_account=None, worker_init=None)</code>","text":"<p>Run a workflow through a local backend.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>Job ID.</p> required <code>workflow</code> <code>WorkflowV2</code> <p>Workflow to be run</p> required <code>dataset</code> <code>DatasetV2</code> <p>Dataset to be used.</p> required <code>workflow_dir_local</code> <code>Path</code> <p>Local working directory for this job.</p> required <code>workflow_dir_remote</code> <code>Path | None</code> <p>Remote working directory for this job - only relevant for <code>slurm_sudo</code> and <code>slurm_ssh</code> backends.</p> <code>None</code> <code>first_task_index</code> <code>int | None</code> <p>Positional index of the first task to execute; if <code>None</code>, start from <code>0</code>.</p> <code>None</code> <code>last_task_index</code> <code>int | None</code> <p>Positional index of the last task to execute; if <code>None</code>, proceed until the last task.</p> <code>None</code> <code>logger_name</code> <code>str</code> <p>Logger name</p> required <code>user_id</code> <code>int</code> <p>User ID.</p> required <code>resource</code> <code>Resource</code> <p>Computational resource for running this job.</p> required <code>profile</code> <code>Profile</code> <p>Computational profile for running this job.</p> required <code>user_cache_dir</code> <code>str | None</code> <p>User-writeable folder (typically a subfolder of <code>project_dir</code>). Only relevant for <code>slurm_sudo</code> and <code>slurm_ssh</code> backends.</p> <code>None</code> <code>fractal_ssh</code> <code>FractalSSH | None</code> <p><code>FractalSSH</code> object, only relevant for the <code>slurm_ssh</code> backend.</p> <code>None</code> <code>slurm_account</code> <code>str | None</code> <p>SLURM account to set. Only relevant for <code>slurm_sudo</code> and <code>slurm_ssh</code> backends.</p> <code>None</code> <code>worker_init</code> <code>str | None</code> <p>Additional preamble lines for SLURM submission script. Only relevant for <code>slurm_sudo</code> and <code>slurm_ssh</code> backends.</p> <code>None</code> Source code in <code>fractal_server/runner/v2/_local.py</code> <pre><code>def process_workflow(\n    *,\n    job_id: int,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    workflow_dir_local: Path,\n    workflow_dir_remote: Path | None = None,\n    first_task_index: int | None = None,\n    last_task_index: int | None = None,\n    logger_name: str,\n    job_attribute_filters: AttributeFilters,\n    job_type_filters: dict[str, bool],\n    user_id: int,\n    resource: Resource,\n    profile: Profile,\n    user_cache_dir: str | None = None,\n    fractal_ssh: FractalSSH | None = None,\n    slurm_account: str | None = None,\n    worker_init: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Run a workflow through a local backend.\n\n    Args:\n        job_id: Job ID.\n        workflow: Workflow to be run\n        dataset: Dataset to be used.\n        workflow_dir_local: Local working directory for this job.\n        workflow_dir_remote:\n            Remote working directory for this job - only relevant for\n            `slurm_sudo` and `slurm_ssh` backends.\n        first_task_index:\n            Positional index of the first task to execute; if `None`, start\n            from `0`.\n        last_task_index:\n            Positional index of the last task to execute; if `None`, proceed\n            until the last task.\n        logger_name: Logger name\n        user_id: User ID.\n        resource: Computational resource for running this job.\n        profile: Computational profile for running this job.\n        user_cache_dir:\n            User-writeable folder (typically a subfolder of `project_dir`).\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        fractal_ssh:\n            `FractalSSH` object, only relevant for the `slurm_ssh` backend.\n        slurm_account:\n            SLURM account to set.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        worker_init:\n            Additional preamble lines for SLURM submission script.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n    \"\"\"\n\n    if workflow_dir_remote and (workflow_dir_remote != workflow_dir_local):\n        raise NotImplementedError(\n            \"Local backend does not support different directories \"\n            f\"{workflow_dir_local=} and {workflow_dir_remote=}\"\n        )\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    with LocalRunner(\n        root_dir_local=workflow_dir_local,\n        resource=resource,\n        profile=profile,\n    ) as runner:\n        execute_tasks_v2(\n            wf_task_list=workflow.task_list[\n                first_task_index : (last_task_index + 1)\n            ],\n            dataset=dataset,\n            job_id=job_id,\n            runner=runner,\n            workflow_dir_local=workflow_dir_local,\n            workflow_dir_remote=workflow_dir_local,\n            logger_name=logger_name,\n            get_runner_config=get_local_backend_config,\n            job_attribute_filters=job_attribute_filters,\n            job_type_filters=job_type_filters,\n            user_id=user_id,\n        )\n</code></pre>"},{"location":"reference/fractal_server/runner/v2/_slurm_ssh/","title":"_slurm_ssh","text":"<p>Slurm Backend</p> <p>This backend runs fractal workflows in a SLURM cluster.</p>"},{"location":"reference/fractal_server/runner/v2/_slurm_ssh/#fractal_server.runner.v2._slurm_ssh.process_workflow","title":"<code>process_workflow(*, job_id, workflow, dataset, workflow_dir_local, workflow_dir_remote=None, first_task_index=None, last_task_index=None, logger_name, job_attribute_filters, job_type_filters, user_id, resource, profile, fractal_ssh=None, slurm_account=None, worker_init=None, user_cache_dir)</code>","text":"<p>Run a workflow through a <code>slurm_ssh</code> backend.</p> <pre><code>Args:\njob_id: Job ID.\nworkflow: Workflow to be run\ndataset: Dataset to be used.\nworkflow_dir_local: Local working directory for this job.\nworkflow_dir_remote:\n    Remote working directory for this job - only relevant for\n    `slurm_sudo` and `slurm_ssh` backends.\nfirst_task_index:\n    Positional index of the first task to execute; if `None`, start\n    from `0`.\nlast_task_index:\n    Positional index of the last task to execute; if `None`, proceed\n    until the last task.\nlogger_name: Logger name\nuser_id: User ID.\nresource: Computational resource for running this job.\nprofile: Computational profile for running this job.\nuser_cache_dir:\n    User-writeable folder (typically a subfolder of `project_dir`).\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\nfractal_ssh:\n    `FractalSSH` object, only relevant for the `slurm_ssh` backend.\nslurm_account:\n    SLURM account to set.\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\nworker_init:\n    Additional preamble lines for SLURM submission script.\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n</code></pre> Source code in <code>fractal_server/runner/v2/_slurm_ssh.py</code> <pre><code>def process_workflow(\n    *,\n    job_id: int,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    workflow_dir_local: Path,\n    workflow_dir_remote: Path | None = None,\n    first_task_index: int | None = None,\n    last_task_index: int | None = None,\n    logger_name: str,\n    job_attribute_filters: AttributeFilters,\n    job_type_filters: dict[str, bool],\n    user_id: int,\n    resource: Resource,\n    profile: Profile,\n    fractal_ssh: FractalSSH | None = None,\n    slurm_account: str | None = None,\n    worker_init: str | None = None,\n    user_cache_dir: str,\n) -&gt; None:\n    \"\"\"\n    Run a workflow through a `slurm_ssh` backend.\n\n        Args:\n        job_id: Job ID.\n        workflow: Workflow to be run\n        dataset: Dataset to be used.\n        workflow_dir_local: Local working directory for this job.\n        workflow_dir_remote:\n            Remote working directory for this job - only relevant for\n            `slurm_sudo` and `slurm_ssh` backends.\n        first_task_index:\n            Positional index of the first task to execute; if `None`, start\n            from `0`.\n        last_task_index:\n            Positional index of the last task to execute; if `None`, proceed\n            until the last task.\n        logger_name: Logger name\n        user_id: User ID.\n        resource: Computational resource for running this job.\n        profile: Computational profile for running this job.\n        user_cache_dir:\n            User-writeable folder (typically a subfolder of `project_dir`).\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        fractal_ssh:\n            `FractalSSH` object, only relevant for the `slurm_ssh` backend.\n        slurm_account:\n            SLURM account to set.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        worker_init:\n            Additional preamble lines for SLURM submission script.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n    \"\"\"\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    if isinstance(worker_init, str):\n        worker_init = worker_init.split(\"\\n\")\n\n    with SlurmSSHRunner(\n        fractal_ssh=fractal_ssh,\n        root_dir_local=workflow_dir_local,\n        root_dir_remote=workflow_dir_remote,\n        slurm_account=slurm_account,\n        resource=resource,\n        profile=profile,\n        common_script_lines=worker_init,\n        user_cache_dir=user_cache_dir,\n    ) as runner:\n        execute_tasks_v2(\n            wf_task_list=workflow.task_list[\n                first_task_index : (last_task_index + 1)\n            ],\n            dataset=dataset,\n            job_id=job_id,\n            runner=runner,\n            workflow_dir_local=workflow_dir_local,\n            workflow_dir_remote=workflow_dir_remote,\n            logger_name=logger_name,\n            get_runner_config=get_slurm_config,\n            job_attribute_filters=job_attribute_filters,\n            job_type_filters=job_type_filters,\n            user_id=user_id,\n        )\n</code></pre>"},{"location":"reference/fractal_server/runner/v2/_slurm_sudo/","title":"_slurm_sudo","text":"<p>Slurm Backend</p> <p>This backend runs fractal workflows in a SLURM cluster.</p>"},{"location":"reference/fractal_server/runner/v2/_slurm_sudo/#fractal_server.runner.v2._slurm_sudo.process_workflow","title":"<code>process_workflow(*, job_id, workflow, dataset, workflow_dir_local, workflow_dir_remote=None, first_task_index=None, last_task_index=None, logger_name, job_attribute_filters, job_type_filters, user_id, resource, profile, user_cache_dir, slurm_account=None, worker_init=None, fractal_ssh=None)</code>","text":"<p>Run a workflow through a <code>slurm_sudo</code> backend.</p> <pre><code>Args:\njob_id: Job ID.\nworkflow: Workflow to be run\ndataset: Dataset to be used.\nworkflow_dir_local: Local working directory for this job.\nworkflow_dir_remote:\n    Remote working directory for this job - only relevant for\n    `slurm_sudo` and `slurm_ssh` backends.\nfirst_task_index:\n    Positional index of the first task to execute; if `None`, start\n    from `0`.\nlast_task_index:\n    Positional index of the last task to execute; if `None`, proceed\n    until the last task.\nlogger_name: Logger name\nuser_id: User ID.\nresource: Computational resource for running this job.\nprofile: Computational profile for running this job.\nuser_cache_dir:\n    User-writeable folder (typically a subfolder of `project_dir`).\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\nfractal_ssh:\n    `FractalSSH` object, only relevant for the `slurm_ssh` backend.\nslurm_account:\n    SLURM account to set.\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\nworker_init:\n    Additional preamble lines for SLURM submission script.\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n</code></pre> Source code in <code>fractal_server/runner/v2/_slurm_sudo.py</code> <pre><code>def process_workflow(\n    *,\n    job_id: int,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    workflow_dir_local: Path,\n    workflow_dir_remote: Path | None = None,\n    first_task_index: int | None = None,\n    last_task_index: int | None = None,\n    logger_name: str,\n    job_attribute_filters: AttributeFilters,\n    job_type_filters: dict[str, bool],\n    user_id: int,\n    resource: Resource,\n    profile: Profile,\n    user_cache_dir: str,\n    slurm_account: str | None = None,\n    worker_init: str | None = None,\n    fractal_ssh: FractalSSH | None = None,\n) -&gt; None:\n    \"\"\"\n    Run a workflow through a `slurm_sudo` backend.\n\n        Args:\n        job_id: Job ID.\n        workflow: Workflow to be run\n        dataset: Dataset to be used.\n        workflow_dir_local: Local working directory for this job.\n        workflow_dir_remote:\n            Remote working directory for this job - only relevant for\n            `slurm_sudo` and `slurm_ssh` backends.\n        first_task_index:\n            Positional index of the first task to execute; if `None`, start\n            from `0`.\n        last_task_index:\n            Positional index of the last task to execute; if `None`, proceed\n            until the last task.\n        logger_name: Logger name\n        user_id: User ID.\n        resource: Computational resource for running this job.\n        profile: Computational profile for running this job.\n        user_cache_dir:\n            User-writeable folder (typically a subfolder of `project_dir`).\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        fractal_ssh:\n            `FractalSSH` object, only relevant for the `slurm_ssh` backend.\n        slurm_account:\n            SLURM account to set.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        worker_init:\n            Additional preamble lines for SLURM submission script.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n    \"\"\"\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    if isinstance(worker_init, str):\n        worker_init = worker_init.split(\"\\n\")\n\n    with SudoSlurmRunner(\n        root_dir_local=workflow_dir_local,\n        root_dir_remote=workflow_dir_remote,\n        common_script_lines=worker_init,\n        resource=resource,\n        profile=profile,\n        user_cache_dir=user_cache_dir,\n        slurm_account=slurm_account,\n    ) as runner:\n        execute_tasks_v2(\n            wf_task_list=workflow.task_list[\n                first_task_index : (last_task_index + 1)\n            ],\n            dataset=dataset,\n            job_id=job_id,\n            runner=runner,\n            workflow_dir_local=workflow_dir_local,\n            workflow_dir_remote=workflow_dir_remote,\n            logger_name=logger_name,\n            get_runner_config=get_slurm_config,\n            job_attribute_filters=job_attribute_filters,\n            job_type_filters=job_type_filters,\n            user_id=user_id,\n        )\n</code></pre>"},{"location":"reference/fractal_server/runner/v2/db_tools/","title":"db_tools","text":""},{"location":"reference/fractal_server/runner/v2/db_tools/#fractal_server.runner.v2.db_tools.bulk_upsert_image_cache_fast","title":"<code>bulk_upsert_image_cache_fast(*, list_upsert_objects, db)</code>","text":"<p>Insert or update many objects into <code>HistoryImageCache</code> and commit</p> <p>This function is an optimized version of</p> <pre><code>for obj in list_upsert_objects:\n    db.merge(**obj)\ndb.commit()\n</code></pre> <p>See docs at https://docs.sqlalchemy.org/en/20/dialects/postgresql.html#insert-on-conflict-upsert</p> <p>NOTE: we tried to replace <code>index_elements</code> with <code>constraint=\"pk_historyimagecache\"</code>, but it did not work as expected.</p> <p>Parameters:</p> Name Type Description Default <code>list_upsert_objects</code> <code>list[dict[str, Any]]</code> <p>List of dictionaries for objects to be upsert-ed.</p> required <code>db</code> <code>Session</code> <p>A sync database session</p> required Source code in <code>fractal_server/runner/v2/db_tools.py</code> <pre><code>def bulk_upsert_image_cache_fast(\n    *,\n    list_upsert_objects: list[dict[str, Any]],\n    db: Session,\n) -&gt; None:\n    \"\"\"\n    Insert or update many objects into `HistoryImageCache` and commit\n\n    This function is an optimized version of\n\n    ```python\n    for obj in list_upsert_objects:\n        db.merge(**obj)\n    db.commit()\n    ```\n\n    See docs at\n    https://docs.sqlalchemy.org/en/20/dialects/postgresql.html#insert-on-conflict-upsert\n\n    NOTE: we tried to replace `index_elements` with\n    `constraint=\"pk_historyimagecache\"`, but it did not work as expected.\n\n    Args:\n        list_upsert_objects:\n            List of dictionaries for objects to be upsert-ed.\n        db: A sync database session\n    \"\"\"\n    len_list_upsert_objects = len(list_upsert_objects)\n\n    logger.debug(f\"[bulk_upsert_image_cache_fast] {len_list_upsert_objects=}.\")\n\n    if len_list_upsert_objects == 0:\n        return None\n\n    for ind in range(0, len_list_upsert_objects, _CHUNK_SIZE):\n        stmt = pg_insert(HistoryImageCache).values(\n            list_upsert_objects[ind : ind + _CHUNK_SIZE]\n        )\n        stmt = stmt.on_conflict_do_update(\n            index_elements=[\n                HistoryImageCache.zarr_url,\n                HistoryImageCache.dataset_id,\n                HistoryImageCache.workflowtask_id,\n            ],\n            set_=dict(\n                latest_history_unit_id=stmt.excluded.latest_history_unit_id\n            ),\n        )\n        db.execute(stmt)\n        db.commit()\n</code></pre>"},{"location":"reference/fractal_server/runner/v2/deduplicate_list/","title":"deduplicate_list","text":""},{"location":"reference/fractal_server/runner/v2/deduplicate_list/#fractal_server.runner.v2.deduplicate_list.deduplicate_list","title":"<code>deduplicate_list(this_list)</code>","text":"<p>Custom replacement for <code>set(this_list)</code>, when items are non-hashable.</p> Source code in <code>fractal_server/runner/v2/deduplicate_list.py</code> <pre><code>def deduplicate_list(\n    this_list: list[T],\n) -&gt; list[T]:\n    \"\"\"\n    Custom replacement for `set(this_list)`, when items are non-hashable.\n    \"\"\"\n    new_list_dict = []\n    new_list_objs = []\n    for this_obj in this_list:\n        this_dict = this_obj.model_dump()\n        if this_dict not in new_list_dict:\n            new_list_dict.append(this_dict)\n            new_list_objs.append(this_obj)\n    return new_list_objs\n</code></pre>"},{"location":"reference/fractal_server/runner/v2/merge_outputs/","title":"merge_outputs","text":""},{"location":"reference/fractal_server/runner/v2/runner/","title":"runner","text":""},{"location":"reference/fractal_server/runner/v2/runner/#fractal_server.runner.v2.runner._remove_status_from_attributes","title":"<code>_remove_status_from_attributes(images)</code>","text":"<p>Drop attribute <code>IMAGE_STATUS_KEY</code> from all images.</p> Source code in <code>fractal_server/runner/v2/runner.py</code> <pre><code>def _remove_status_from_attributes(\n    images: list[dict[str, Any]],\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Drop attribute `IMAGE_STATUS_KEY` from all images.\n    \"\"\"\n    images_copy = deepcopy(images)\n    [img[\"attributes\"].pop(IMAGE_STATUS_KEY, None) for img in images_copy]\n    return images_copy\n</code></pre>"},{"location":"reference/fractal_server/runner/v2/runner/#fractal_server.runner.v2.runner.get_origin_attribute_and_types","title":"<code>get_origin_attribute_and_types(*, origin_url, images)</code>","text":"<p>Search for origin image and extract its attributes/types.</p> Source code in <code>fractal_server/runner/v2/runner.py</code> <pre><code>def get_origin_attribute_and_types(\n    *,\n    origin_url: str,\n    images: list[dict[str, Any]],\n) -&gt; tuple[dict[str, Any], dict[str, bool]]:\n    \"\"\"\n    Search for origin image and extract its attributes/types.\n    \"\"\"\n    origin_img_search = find_image_by_zarr_url(\n        images=images,\n        zarr_url=origin_url,\n    )\n    if origin_img_search is None:\n        updated_attributes = {}\n        updated_types = {}\n    else:\n        origin_image = origin_img_search[\"image\"]\n        updated_attributes = copy(origin_image[\"attributes\"])\n        updated_types = copy(origin_image[\"types\"])\n    return updated_attributes, updated_types\n</code></pre>"},{"location":"reference/fractal_server/runner/v2/runner_functions/","title":"runner_functions","text":""},{"location":"reference/fractal_server/runner/v2/runner_functions/#fractal_server.runner.v2.runner_functions.run_v2_task_non_parallel","title":"<code>run_v2_task_non_parallel(*, images, zarr_dir, task, wftask, workflow_dir_local, workflow_dir_remote, runner, get_runner_config, dataset_id, history_run_id, task_type, user_id)</code>","text":"<p>This runs server-side (see <code>executor</code> argument)</p> Source code in <code>fractal_server/runner/v2/runner_functions.py</code> <pre><code>def run_v2_task_non_parallel(\n    *,\n    images: list[dict[str, Any]],\n    zarr_dir: str,\n    task: TaskV2,\n    wftask: WorkflowTaskV2,\n    workflow_dir_local: Path,\n    workflow_dir_remote: Path,\n    runner: BaseRunner,\n    get_runner_config: GetRunnerConfigType,\n    dataset_id: int,\n    history_run_id: int,\n    task_type: Literal[TaskType.NON_PARALLEL, TaskType.CONVERTER_NON_PARALLEL],\n    user_id: int,\n) -&gt; tuple[dict[int, SubmissionOutcome], int]:\n    \"\"\"\n    This runs server-side (see `executor` argument)\n    \"\"\"\n\n    if task_type not in [\n        TaskType.NON_PARALLEL,\n        TaskType.CONVERTER_NON_PARALLEL,\n    ]:\n        raise ValueError(\n            f\"Invalid {task_type=} for `run_v2_task_non_parallel`.\"\n        )\n\n    # Get TaskFiles object\n    task_files = TaskFiles(\n        root_dir_local=workflow_dir_local,\n        root_dir_remote=workflow_dir_remote,\n        task_order=wftask.order,\n        task_name=wftask.task.name,\n        component=\"\",\n        prefix=SUBMIT_PREFIX,\n    )\n\n    runner_config = get_runner_config(\n        shared_config=runner.shared_config,\n        wftask=wftask,\n        which_type=\"non_parallel\",\n        tot_tasks=1,\n    )\n\n    function_kwargs = {\n        \"zarr_dir\": zarr_dir,\n        **(wftask.args_non_parallel or {}),\n    }\n    if task_type == TaskType.NON_PARALLEL:\n        function_kwargs[\"zarr_urls\"] = [img[\"zarr_url\"] for img in images]\n\n    # Database History operations\n    with next(get_sync_db()) as db:\n        if task_type == TaskType.NON_PARALLEL:\n            zarr_urls = function_kwargs[\"zarr_urls\"]\n        elif task_type == TaskType.CONVERTER_NON_PARALLEL:\n            zarr_urls = []\n\n        history_unit = HistoryUnit(\n            history_run_id=history_run_id,\n            status=HistoryUnitStatus.SUBMITTED,\n            logfile=task_files.log_file_local,\n            zarr_urls=zarr_urls,\n        )\n        db.add(history_unit)\n        db.commit()\n        db.refresh(history_unit)\n        logger.debug(\n            \"[run_v2_task_non_parallel] Created `HistoryUnit` with \"\n            f\"{history_run_id=}.\"\n        )\n        history_unit_id = history_unit.id\n        bulk_upsert_image_cache_fast(\n            db=db,\n            list_upsert_objects=[\n                dict(\n                    workflowtask_id=wftask.id,\n                    dataset_id=dataset_id,\n                    zarr_url=zarr_url,\n                    latest_history_unit_id=history_unit_id,\n                )\n                for zarr_url in history_unit.zarr_urls\n            ],\n        )\n\n    result, exception = runner.submit(\n        base_command=task.command_non_parallel,\n        workflow_task_order=wftask.order,\n        workflow_task_id=wftask.task_id,\n        task_name=wftask.task.name,\n        parameters=function_kwargs,\n        task_type=task_type,\n        task_files=task_files,\n        history_unit_id=history_unit_id,\n        config=runner_config,\n        user_id=user_id,\n    )\n\n    positional_index = 0\n    num_tasks = 1\n\n    outcome = {\n        positional_index: _process_task_output(\n            result=result,\n            exception=exception,\n        )\n    }\n    # NOTE: Here we don't have to handle the\n    # `outcome[0].exception is not None` branch, since for non_parallel\n    # tasks it was already handled within submit\n    if outcome[0].invalid_output:\n        with next(get_sync_db()) as db:\n            update_status_of_history_unit(\n                history_unit_id=history_unit_id,\n                status=HistoryUnitStatus.FAILED,\n                db_sync=db,\n            )\n    return outcome, num_tasks\n</code></pre>"},{"location":"reference/fractal_server/runner/v2/submit_workflow/","title":"submit_workflow","text":"<p>Runner backend subsystem root V2</p> <p>This module is the single entry point to the runner backend subsystem V2. Other subsystems should only import this module and not its submodules or the individual backends.</p>"},{"location":"reference/fractal_server/runner/v2/submit_workflow/#fractal_server.runner.v2.submit_workflow.submit_workflow","title":"<code>submit_workflow(*, workflow_id, dataset_id, job_id, user_id, user_cache_dir, resource, profile, worker_init=None, fractal_ssh=None)</code>","text":"<p>Prepares a workflow and applies it to a dataset</p> <p>This function wraps the process_workflow one, which is different for each backend (e.g. local or slurm backend).</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>int</code> <p>ID of the workflow being applied</p> required <code>dataset_id</code> <code>int</code> <p>Dataset ID</p> required <code>job_id</code> <code>int</code> <p>Id of the job record which stores the state for the current workflow application.</p> required <code>user_id</code> <code>int</code> <p>User ID.</p> required <code>worker_init</code> <code>str | None</code> <p>Custom executor parameters that get parsed before the execution of each task.</p> <code>None</code> <code>user_cache_dir</code> <code>str</code> <p>Cache directory (namely a path where the user can write). For <code>slurm_sudo</code> backend, this is both a base directory for <code>job.working_dir_user</code>. For <code>slurm_sudo</code> and <code>slurm_ssh</code> backends, this is used for <code>user_local_exports</code>.</p> required <code>resource</code> <code>Resource</code> <p>Computational resource to be used for this job (e.g. a SLURM cluster).</p> required <code>profile</code> <code>Profile</code> <p>Computational profile to be used for this job.</p> required <code>fractal_ssh</code> <code>FractalSSH | None</code> <p>SSH object, for when <code>resource.type = \"slurm_ssh\"</code>.</p> <code>None</code> Source code in <code>fractal_server/runner/v2/submit_workflow.py</code> <pre><code>def submit_workflow(\n    *,\n    workflow_id: int,\n    dataset_id: int,\n    job_id: int,\n    user_id: int,\n    user_cache_dir: str,\n    resource: Resource,\n    profile: Profile,\n    worker_init: str | None = None,\n    fractal_ssh: FractalSSH | None = None,\n) -&gt; None:\n    \"\"\"\n    Prepares a workflow and applies it to a dataset\n\n    This function wraps the process_workflow one, which is different for each\n    backend (e.g. local or slurm backend).\n\n    Args:\n        workflow_id:\n            ID of the workflow being applied\n        dataset_id:\n            Dataset ID\n        job_id:\n            Id of the job record which stores the state for the current\n            workflow application.\n        user_id:\n            User ID.\n        worker_init:\n            Custom executor parameters that get parsed before the execution of\n            each task.\n        user_cache_dir:\n            Cache directory (namely a path where the user can write). For\n            `slurm_sudo` backend, this is both a base directory for\n            `job.working_dir_user`. For `slurm_sudo` and `slurm_ssh` backends,\n            this is used for `user_local_exports`.\n        resource:\n            Computational resource to be used for this job (e.g. a SLURM\n            cluster).\n        profile:\n           Computational profile to be used for this job.\n        fractal_ssh: SSH object, for when `resource.type = \"slurm_ssh\"`.\n    \"\"\"\n    # Declare runner backend and set `process_workflow` function\n    logger_name = f\"WF{workflow_id}_job{job_id}\"\n    logger = set_logger(logger_name=logger_name)\n\n    with next(DB.get_sync_db()) as db_sync:\n        try:\n            job: JobV2 | None = db_sync.get(JobV2, job_id)\n            dataset: DatasetV2 | None = db_sync.get(DatasetV2, dataset_id)\n            workflow: WorkflowV2 | None = db_sync.get(WorkflowV2, workflow_id)\n        except Exception as e:\n            logger.error(\n                f\"Error connecting to the database. Original error: {str(e)}\"\n            )\n            reset_logger_handlers(logger)\n            return\n\n        if job is None:\n            logger.error(f\"JobV2 {job_id} does not exist\")\n            reset_logger_handlers(logger)\n            return\n        if dataset is None or workflow is None:\n            log_msg = \"\"\n            if not dataset:\n                log_msg += f\"Cannot fetch dataset {dataset_id} from database\\n\"\n            if not workflow:\n                log_msg += (\n                    f\"Cannot fetch workflow {workflow_id} from database\\n\"\n                )\n            fail_job(\n                db=db_sync, job=job, log_msg=log_msg, logger_name=logger_name\n            )\n            return\n\n        try:\n            # Define local/remote folders, and create local folder\n            local_job_dir = Path(job.working_dir)\n            remote_job_dir = Path(job.working_dir_user)\n            match resource.type:\n                case ResourceType.LOCAL:\n                    local_job_dir.mkdir(parents=True, exist_ok=False)\n                case ResourceType.SLURM_SUDO:\n                    original_umask = os.umask(0)\n                    local_job_dir.mkdir(\n                        parents=True, mode=0o755, exist_ok=False\n                    )\n                    os.umask(original_umask)\n                case ResourceType.SLURM_SSH:\n                    local_job_dir.mkdir(parents=True, exist_ok=False)\n\n        except Exception as e:\n            error_type = type(e).__name__\n            fail_job(\n                db=db_sync,\n                job=job,\n                log_msg=(\n                    f\"{error_type} error while creating local job folder.\"\n                    f\" Original error: {str(e)}\"\n                ),\n                logger_name=logger_name,\n                emit_log=True,\n            )\n            return\n\n        # After Session.commit() is called, either explicitly or when using a\n        # context manager, all objects associated with the Session are expired.\n        # https://docs.sqlalchemy.org/en/14/orm/\n        #   session_basics.html#opening-and-closing-a-session\n        # https://docs.sqlalchemy.org/en/14/orm/\n        #   session_state_management.html#refreshing-expiring\n\n        # See issue #928:\n        # https://github.com/fractal-analytics-platform/\n        #   fractal-server/issues/928\n\n        db_sync.refresh(dataset)\n        db_sync.refresh(workflow)\n        for wftask in workflow.task_list:\n            db_sync.refresh(wftask)\n\n        # Write logs\n        log_file_path = local_job_dir / WORKFLOW_LOG_FILENAME\n        logger = set_logger(\n            logger_name=logger_name,\n            log_file_path=log_file_path,\n        )\n        logger.info(\n            f'Start execution of workflow \"{workflow.name}\"; '\n            f\"more logs at {str(log_file_path)}\"\n        )\n        logger.debug(f\"fractal_server.__VERSION__: {__VERSION__}\")\n        logger.debug(f\"Resource name: {resource.name}\")\n        logger.debug(f\"Profile name: {profile.name}\")\n        logger.debug(f\"Username: {profile.username}\")\n        if resource.type in [ResourceType.SLURM_SUDO, ResourceType.SLURM_SSH]:\n            logger.debug(f\"slurm_account: {job.slurm_account}\")\n            logger.debug(f\"worker_init: {worker_init}\")\n        logger.debug(f\"job.id: {job.id}\")\n        logger.debug(f\"job.working_dir: {job.working_dir}\")\n        logger.debug(f\"job.working_dir_user: {job.working_dir_user}\")\n        logger.debug(f\"job.first_task_index: {job.first_task_index}\")\n        logger.debug(f\"job.last_task_index: {job.last_task_index}\")\n        logger.debug(f'START workflow \"{workflow.name}\"')\n        job_working_dir = job.working_dir\n\n    try:\n        process_workflow: ProcessWorkflowType\n        match resource.type:\n            case ResourceType.LOCAL:\n                process_workflow = local_process_workflow\n                backend_specific_kwargs = {}\n            case ResourceType.SLURM_SUDO:\n                process_workflow = slurm_sudo_process_workflow\n                backend_specific_kwargs = dict(\n                    slurm_account=job.slurm_account,\n                )\n            case ResourceType.SLURM_SSH:\n                process_workflow = slurm_ssh_process_workflow\n                backend_specific_kwargs = dict(\n                    fractal_ssh=fractal_ssh,\n                    slurm_account=job.slurm_account,\n                )\n\n        process_workflow(\n            workflow=workflow,\n            dataset=dataset,\n            job_id=job_id,\n            user_id=user_id,\n            workflow_dir_local=local_job_dir,\n            workflow_dir_remote=remote_job_dir,\n            logger_name=logger_name,\n            worker_init=worker_init,\n            first_task_index=job.first_task_index,\n            last_task_index=job.last_task_index,\n            job_attribute_filters=job.attribute_filters,\n            job_type_filters=job.type_filters,\n            resource=resource,\n            profile=profile,\n            user_cache_dir=user_cache_dir,\n            **backend_specific_kwargs,\n        )\n\n        logger.info(\n            f'End execution of workflow \"{workflow.name}\"; '\n            f\"more logs at {str(log_file_path)}\"\n        )\n        logger.debug(f'END workflow \"{workflow.name}\"')\n\n        # Update job DB entry\n        with next(DB.get_sync_db()) as db_sync:\n            job = db_sync.get(JobV2, job_id)\n            job.status = JobStatusTypeV2.DONE\n            job.end_timestamp = get_timestamp()\n            with log_file_path.open(\"r\") as f:\n                logs = f.read()\n            job.log = logs\n            db_sync.merge(job)\n            db_sync.commit()\n\n    except JobExecutionError as e:\n        logger.debug(f'FAILED workflow \"{workflow.name}\", JobExecutionError.')\n        logger.info(f'Workflow \"{workflow.name}\" failed (JobExecutionError).')\n        with next(DB.get_sync_db()) as db_sync:\n            job = db_sync.get(JobV2, job_id)\n            fail_job(\n                db=db_sync,\n                job=job,\n                log_msg=(\n                    f\"JOB ERROR in Fractal job {job.id}:\\n\"\n                    f\"TRACEBACK:\\n{e.assemble_error()}\"\n                ),\n                logger_name=logger_name,\n            )\n\n    except Exception:\n        logger.debug(f'FAILED workflow \"{workflow.name}\", unknown error.')\n        logger.info(f'Workflow \"{workflow.name}\" failed (unkwnon error).')\n\n        current_traceback = traceback.format_exc()\n        with next(DB.get_sync_db()) as db_sync:\n            job = db_sync.get(JobV2, job_id)\n            fail_job(\n                db=db_sync,\n                job=job,\n                log_msg=(\n                    f\"UNKNOWN ERROR in Fractal job {job.id}\\n\"\n                    f\"TRACEBACK:\\n{current_traceback}\"\n                ),\n                logger_name=logger_name,\n            )\n\n    finally:\n        reset_logger_handlers(logger)\n        _zip_folder_to_file_and_remove(folder=job_working_dir)\n</code></pre>"},{"location":"reference/fractal_server/runner/v2/task_interface/","title":"task_interface","text":""},{"location":"reference/fractal_server/ssh/","title":"ssh","text":"<p>The <code>fractal_server.ssh</code> subpackage is meant as a layer in front of some SSH library (e.g. <code>fabric</code> or <code>asyncssh</code>).</p>"},{"location":"reference/fractal_server/ssh/_fabric/","title":"_fabric","text":""},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH","title":"<code>FractalSSH</code>","text":"<p>Wrapper of <code>fabric.Connection</code> object, enriched with locks.</p> <p>Note: methods marked as <code>_unsafe</code> should not be used directly, since they do not enforce locking.</p> <p>Attributes:</p> Name Type Description <code>_lock</code> <code>Lock</code> <code>_connection</code> <code>Connection</code> <code>default_lock_timeout</code> <code>float</code> <code>sftp_get_prefetch</code> <code>bool</code> <code>sftp_get_max_requests</code> <code>int</code> <code>logger_name</code> <code>str</code> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>class FractalSSH:\n    \"\"\"\n    Wrapper of `fabric.Connection` object, enriched with locks.\n\n    Note: methods marked as `_unsafe` should not be used directly,\n    since they do not enforce locking.\n\n    Attributes:\n        _lock:\n        _connection:\n        default_lock_timeout:\n        sftp_get_prefetch:\n        sftp_get_max_requests:\n        logger_name:\n    \"\"\"\n\n    _lock: Lock\n    _connection: Connection\n    default_lock_timeout: float\n    sftp_get_prefetch: bool\n    sftp_get_max_requests: int\n    logger_name: str\n    _pid: int\n\n    def __init__(\n        self,\n        connection: Connection,\n        default_timeout: float = 500.0,\n        sftp_get_prefetch: bool = False,\n        sftp_get_max_requests: int = 64,\n        logger_name: str = __name__,\n    ):\n        self._lock = Lock()\n        self._connection = connection\n        self.default_lock_timeout = default_timeout\n        self.sftp_get_prefetch = sftp_get_prefetch\n        self.sftp_get_max_requests = sftp_get_max_requests\n        self.logger_name = logger_name\n        set_logger(self.logger_name)\n        set_logger(SSH_MONITORING_LOGGER_NAME)\n        self._pid = os.getpid()\n\n    @property\n    def is_connected(self) -&gt; bool:\n        return self._connection.is_connected\n\n    @property\n    def logger(self) -&gt; logging.Logger:\n        return get_logger(self.logger_name)\n\n    def log_and_raise(self, *, e: Exception, message: str) -&gt; None:\n        \"\"\"\n        Log and re-raise an exception from a FractalSSH method.\n\n        Args:\n            message: Additional message to be logged.\n            e: Original exception\n        \"\"\"\n        try:\n            self.logger.error(message)\n            self.logger.error(f\"Original Error {type(e)} : \\n{str(e)}\")\n            # Handle the specific case of `NoValidConnectionsError`s from\n            # paramiko, which store relevant information in the `errors`\n            # attribute\n            if hasattr(e, \"errors\"):\n                self.logger.error(f\"{type(e)=}\")\n                for err in e.errors:\n                    self.logger.error(f\"{err}\")\n        except Exception as exception:\n            # Handle unexpected cases, e.g. (1) `e` has no `type`, or\n            # (2) `errors` is not iterable.\n            self.logger.error(\n                \"Unexpected Error while handling exception above: \"\n                f\"{str(exception)}\"\n            )\n\n        raise e\n\n    def _run(\n        self,\n        *args,\n        label: str,\n        lock_timeout: float | None = None,\n        **kwargs,\n    ) -&gt; Any:\n        actual_lock_timeout = self.default_lock_timeout\n        if lock_timeout is not None:\n            actual_lock_timeout = lock_timeout\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=label,\n            timeout=actual_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            return self._connection.run(*args, **kwargs)\n\n    def _sftp_unsafe(self) -&gt; paramiko.sftp_client.SFTPClient:\n        \"\"\"\n        This is marked as unsafe because you should only use its methods\n        after acquiring a lock.\n        \"\"\"\n        return self._connection.sftp()\n\n    @retry_if_socket_error\n    def read_remote_json_file(self, filepath: str) -&gt; dict[str, Any]:\n        self.logger.info(f\"START reading remote JSON file {filepath}.\")\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            timeout=self.default_lock_timeout,\n            logger_name=self.logger_name,\n            pid=self._pid,\n            label=f\"read_remote_json_file({filepath})\",\n        ):\n            try:\n                with self._sftp_unsafe().open(filepath, \"r\") as f:\n                    data = json.load(f)\n            except Exception as e:\n                self.log_and_raise(\n                    e=e,\n                    message=(\n                        f\"Error in `read_remote_json_file`, for {filepath=}.\"\n                    ),\n                )\n        self.logger.info(f\"END reading remote JSON file {filepath}.\")\n        return data\n\n    @retry_if_socket_error\n    def read_remote_text_file(self, filepath: str) -&gt; dict[str, Any]:\n        \"\"\"\n        Read a remote text file into a string.\n\n        Note from paramiko docs:\n        &gt; The Python 'b' flag is ignored, since SSH treats all files as binary.\n        \"\"\"\n        self.logger.info(f\"START reading remote text file {filepath}.\")\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=f\"read_remote_text_file({filepath})\",\n            timeout=self.default_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            try:\n                with self._sftp_unsafe().open(filepath, \"r\") as f:\n                    data = f.read().decode()\n            except Exception as e:\n                self.log_and_raise(\n                    e=e,\n                    message=(\n                        f\"Error in `read_remote_text_file`, for {filepath=}.\"\n                    ),\n                )\n        self.logger.info(f\"END reading remote text file {filepath}.\")\n        return data\n\n    def check_connection(self) -&gt; None:\n        \"\"\"\n        Open the SSH connection and handle exceptions.\n\n        This method should always be called at the beginning of background\n        operations that use FractalSSH, so that:\n\n        1. We try to restore unusable connections (e.g. due to closed socket).\n        2. We provide an informative error if connection cannot be established.\n        \"\"\"\n        self.logger.debug(\n            f\"[check_connection] {self._connection.is_connected=}\"\n        )\n        if self._connection.is_connected:\n            # Even if the connection appears open, it could be broken for\n            # external reasons (e.g. the socket is closed because the SSH\n            # server was restarted). In these cases, we catch the error and\n            # try to re-open the connection.\n            try:\n                self.logger.info(\n                    \"[check_connection] Run dummy command to check connection.\"\n                )\n                # Run both an SFTP and an SSH command, as they correspond to\n                # different sockets\n                self.remote_exists(\"/dummy/path/\")\n                self.run_command(cmd=\"whoami\")\n                self.logger.info(\n                    \"[check_connection] SSH connection is already OK, exit.\"\n                )\n                return\n            except (OSError, EOFError) as e:\n                self.logger.warning(\n                    f\"[check_connection] Detected error {str(e)}, re-open.\"\n                )\n        # Try opening the connection (if it was closed) or to re-open it (if\n        # an error happened).\n        self.refresh_connection()\n\n    def refresh_connection(self) -&gt; None:\n        try:\n            self.close()\n            with _acquire_lock_with_timeout(\n                lock=self._lock,\n                label=\"FractalSSH._connection.{open,open_sftp}()\",\n                timeout=self.default_lock_timeout,\n                logger_name=self.logger_name,\n                pid=self._pid,\n            ):\n                self._connection.open()\n                self._connection.client.open_sftp()\n                self.logger.info(\n                    \"[check_connection] SSH connection opened, exit.\"\n                )\n\n        except Exception as e:\n            raise RuntimeError(\n                f\"Cannot open SSH connection. Original error:\\n{str(e)}\"\n            )\n\n    def close(self) -&gt; None:\n        \"\"\"\n        Aggressively close `self._connection`.\n\n        When `Connection.is_connected` is `False`, `Connection.close()` does\n        not call `Connection.client.close()`. Thus we do this explicitly here,\n        because we observed cases where `is_connected=False` but the underlying\n        `Transport` object was not closed.\n        \"\"\"\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=\"FractalSSH._connection.close()\",\n            timeout=self.default_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            self._connection.close()\n            if self._connection.client is not None:\n                self._connection.client.close()\n        close_logger(get_logger(self.logger_name))\n        close_logger(get_logger(SSH_MONITORING_LOGGER_NAME))\n\n    @retry_if_socket_error\n    def run_command(\n        self,\n        *,\n        cmd: str,\n        allow_char: str | None = None,\n        lock_timeout: int | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Run a command within an open SSH connection.\n\n        Args:\n            cmd: Command to be run\n            allow_char: Forbidden chars to allow for this command\n            max_attempts:\n            base_interval:\n            lock_timeout:\n\n        Returns:\n            Standard output of the command, if successful.\n        \"\"\"\n\n        validate_cmd(cmd, allow_char=allow_char)\n\n        actual_lock_timeout = self.default_lock_timeout\n        if lock_timeout is not None:\n            actual_lock_timeout = lock_timeout\n\n        t_0 = time.perf_counter()\n        try:\n            # Case 1: Command runs successfully\n            res = self._run(\n                cmd,\n                label=cmd,\n                lock_timeout=actual_lock_timeout,\n                hide=True,\n                in_stream=False,\n            )\n            t_1 = time.perf_counter()\n            self.logger.info(\n                f\"END   running '{cmd}' over SSH, elapsed={t_1 - t_0:.3f}\"\n            )\n            self.logger.debug(\"STDOUT:\")\n            self.logger.debug(res.stdout)\n            self.logger.debug(\"STDERR:\")\n            self.logger.debug(res.stderr)\n            return res.stdout\n        # Case 2: Command fails with a connection error\n        except NoValidConnectionsError as e:\n            raise NoValidConnectionsError(errors=e.errors)\n        except UnexpectedExit as e:\n            # Case 3: Command fails with an actual error\n            error_msg = (\n                f\"Running command `{cmd}` over SSH failed.\\n\"\n                f\"Original error:\\n{str(e)}.\"\n            )\n            self.logger.error(error_msg)\n            raise FractalSSHCommandError(error_msg)\n        except FractalSSHTimeoutError as e:\n            raise e\n        except Exception as e:\n            self.logger.error(\n                f\"Running command `{cmd}` over SSH failed.\\n\"\n                f\"Original Error:\\n{str(e)}.\"\n            )\n            raise FractalSSHUnknownError(f\"{type(e)}: {str(e)}\")\n\n    @retry_if_socket_error\n    def send_file(\n        self,\n        *,\n        local: str,\n        remote: str,\n        lock_timeout: float | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Transfer a file via SSH\n\n        Args:\n            local: Local path to file.\n            remote: Target path on remote host.\n            lock_timeout: Timeout for lock acquisition (overrides default).\n        \"\"\"\n        try:\n            self.logger.info(\n                f\"[send_file] START transfer of '{local}' over SSH.\"\n            )\n            actual_lock_timeout = self.default_lock_timeout\n            if lock_timeout is not None:\n                actual_lock_timeout = lock_timeout\n            with _acquire_lock_with_timeout(\n                lock=self._lock,\n                label=f\"send_file({local},{remote})\",\n                timeout=actual_lock_timeout,\n                pid=self._pid,\n                logger_name=self.logger_name,\n            ):\n                self._sftp_unsafe().put(local, remote)\n            self.logger.info(\n                f\"[send_file] END transfer of '{local}' over SSH.\"\n            )\n        except Exception as e:\n            self.log_and_raise(\n                e=e,\n                message=(\n                    \"Error in `send_file`, while \"\n                    f\"transferring {local=} to {remote=}.\"\n                ),\n            )\n\n    @retry_if_socket_error\n    def fetch_file(\n        self,\n        *,\n        local: str,\n        remote: str,\n        lock_timeout: float | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Transfer a file via SSH\n\n        Args:\n            local: Local path to file.\n            remote: Target path on remote host.\n            lock_timeout: Timeout for lock acquisition (overrides default).\n        \"\"\"\n        try:\n            prefix = \"[fetch_file] \"\n            self.logger.info(f\"{prefix} START fetching '{remote}' over SSH.\")\n            actual_lock_timeout = self.default_lock_timeout\n            if lock_timeout is not None:\n                actual_lock_timeout = lock_timeout\n            with _acquire_lock_with_timeout(\n                lock=self._lock,\n                label=f\"fetch_file({local},{remote})\",\n                timeout=actual_lock_timeout,\n                pid=self._pid,\n                logger_name=self.logger_name,\n            ):\n                self._sftp_unsafe().get(\n                    remote,\n                    local,\n                    prefetch=self.sftp_get_prefetch,\n                    max_concurrent_prefetch_requests=self.sftp_get_max_requests,  # noqa E501\n                )\n            self.logger.info(f\"{prefix} END fetching '{remote}' over SSH.\")\n        except Exception as e:\n            self.log_and_raise(\n                e=e,\n                message=(\n                    \"Error in `fetch_file`, while \"\n                    f\"Transferring {remote=} to {local=}.\"\n                ),\n            )\n\n    def mkdir(self, *, folder: str, parents: bool = True) -&gt; None:\n        \"\"\"\n        Create a folder remotely via SSH.\n\n        Args:\n            folder:\n            parents:\n        \"\"\"\n        if parents:\n            cmd = f\"mkdir -p {folder}\"\n        else:\n            cmd = f\"mkdir {folder}\"\n        self.run_command(cmd=cmd)\n\n    def remove_folder(\n        self,\n        *,\n        folder: str,\n        safe_root: str,\n    ) -&gt; None:\n        \"\"\"\n        Removes a folder remotely via SSH.\n\n        This functions calls `rm -r`, after a few checks on `folder`.\n\n        Args:\n            folder: Absolute path to a folder that should be removed.\n            safe_root: If `folder` is not a subfolder of the absolute\n                `safe_root` path, raise an error.\n        \"\"\"\n        validate_cmd(folder)\n        validate_cmd(safe_root)\n\n        if \" \" in folder:\n            raise ValueError(f\"folder='{folder}' includes whitespace.\")\n        elif \" \" in safe_root:\n            raise ValueError(f\"safe_root='{safe_root}' includes whitespace.\")\n        elif not Path(folder).is_absolute():\n            raise ValueError(f\"{folder=} is not an absolute path.\")\n        elif not Path(safe_root).is_absolute():\n            raise ValueError(f\"{safe_root=} is not an absolute path.\")\n        elif not (\n            Path(folder).resolve().is_relative_to(Path(safe_root).resolve())\n        ):\n            raise ValueError(f\"{folder=} is not a subfolder of {safe_root=}.\")\n        else:\n            cmd = f\"rm -r {folder}\"\n            self.run_command(cmd=cmd)\n\n    @retry_if_socket_error\n    def write_remote_file(\n        self,\n        *,\n        path: str,\n        content: str,\n        lock_timeout: float | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Open a remote file via SFTP and write it.\n\n        Args:\n            path: Absolute path of remote file.\n            content: Contents to be written to file.\n            lock_timeout: Timeout for lock acquisition (overrides default).\n        \"\"\"\n        t_start = time.perf_counter()\n        self.logger.info(f\"[write_remote_file] START ({path}).\")\n        actual_lock_timeout = self.default_lock_timeout\n        if lock_timeout is not None:\n            actual_lock_timeout = lock_timeout\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=f\"write_remote_file({path})\",\n            timeout=actual_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            try:\n                with self._sftp_unsafe().open(filename=path, mode=\"w\") as f:\n                    f.write(content)\n            except Exception as e:\n                self.log_and_raise(\n                    e=e, message=f\"Error in `write_remote_file`, for {path=}.\"\n                )\n\n        elapsed = time.perf_counter() - t_start\n        self.logger.info(f\"[write_remote_file] END, {elapsed=} s ({path}).\")\n\n    @retry_if_socket_error\n    def remote_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Return whether a remote file/folder exists\n        \"\"\"\n        self.logger.info(f\"START remote_file_exists {path}\")\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=f\"remote_file_exists({path})\",\n            timeout=self.default_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            try:\n                self._sftp_unsafe().stat(path)\n                self.logger.info(f\"END   remote_file_exists {path} / True\")\n                return True\n            except FileNotFoundError:\n                self.logger.info(f\"END   remote_file_exists {path} / False\")\n                return False\n            except Exception as e:\n                self.log_and_raise(\n                    e=e, message=f\"Error in `remote_exists`, for {path=}.\"\n                )\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH._sftp_unsafe","title":"<code>_sftp_unsafe()</code>","text":"<p>This is marked as unsafe because you should only use its methods after acquiring a lock.</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def _sftp_unsafe(self) -&gt; paramiko.sftp_client.SFTPClient:\n    \"\"\"\n    This is marked as unsafe because you should only use its methods\n    after acquiring a lock.\n    \"\"\"\n    return self._connection.sftp()\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.check_connection","title":"<code>check_connection()</code>","text":"<p>Open the SSH connection and handle exceptions.</p> <p>This method should always be called at the beginning of background operations that use FractalSSH, so that:</p> <ol> <li>We try to restore unusable connections (e.g. due to closed socket).</li> <li>We provide an informative error if connection cannot be established.</li> </ol> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def check_connection(self) -&gt; None:\n    \"\"\"\n    Open the SSH connection and handle exceptions.\n\n    This method should always be called at the beginning of background\n    operations that use FractalSSH, so that:\n\n    1. We try to restore unusable connections (e.g. due to closed socket).\n    2. We provide an informative error if connection cannot be established.\n    \"\"\"\n    self.logger.debug(\n        f\"[check_connection] {self._connection.is_connected=}\"\n    )\n    if self._connection.is_connected:\n        # Even if the connection appears open, it could be broken for\n        # external reasons (e.g. the socket is closed because the SSH\n        # server was restarted). In these cases, we catch the error and\n        # try to re-open the connection.\n        try:\n            self.logger.info(\n                \"[check_connection] Run dummy command to check connection.\"\n            )\n            # Run both an SFTP and an SSH command, as they correspond to\n            # different sockets\n            self.remote_exists(\"/dummy/path/\")\n            self.run_command(cmd=\"whoami\")\n            self.logger.info(\n                \"[check_connection] SSH connection is already OK, exit.\"\n            )\n            return\n        except (OSError, EOFError) as e:\n            self.logger.warning(\n                f\"[check_connection] Detected error {str(e)}, re-open.\"\n            )\n    # Try opening the connection (if it was closed) or to re-open it (if\n    # an error happened).\n    self.refresh_connection()\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.close","title":"<code>close()</code>","text":"<p>Aggressively close <code>self._connection</code>.</p> <p>When <code>Connection.is_connected</code> is <code>False</code>, <code>Connection.close()</code> does not call <code>Connection.client.close()</code>. Thus we do this explicitly here, because we observed cases where <code>is_connected=False</code> but the underlying <code>Transport</code> object was not closed.</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"\n    Aggressively close `self._connection`.\n\n    When `Connection.is_connected` is `False`, `Connection.close()` does\n    not call `Connection.client.close()`. Thus we do this explicitly here,\n    because we observed cases where `is_connected=False` but the underlying\n    `Transport` object was not closed.\n    \"\"\"\n    with _acquire_lock_with_timeout(\n        lock=self._lock,\n        label=\"FractalSSH._connection.close()\",\n        timeout=self.default_lock_timeout,\n        pid=self._pid,\n        logger_name=self.logger_name,\n    ):\n        self._connection.close()\n        if self._connection.client is not None:\n            self._connection.client.close()\n    close_logger(get_logger(self.logger_name))\n    close_logger(get_logger(SSH_MONITORING_LOGGER_NAME))\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.fetch_file","title":"<code>fetch_file(*, local, remote, lock_timeout=None)</code>","text":"<p>Transfer a file via SSH</p> <p>Parameters:</p> Name Type Description Default <code>local</code> <code>str</code> <p>Local path to file.</p> required <code>remote</code> <code>str</code> <p>Target path on remote host.</p> required <code>lock_timeout</code> <code>float | None</code> <p>Timeout for lock acquisition (overrides default).</p> <code>None</code> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef fetch_file(\n    self,\n    *,\n    local: str,\n    remote: str,\n    lock_timeout: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Transfer a file via SSH\n\n    Args:\n        local: Local path to file.\n        remote: Target path on remote host.\n        lock_timeout: Timeout for lock acquisition (overrides default).\n    \"\"\"\n    try:\n        prefix = \"[fetch_file] \"\n        self.logger.info(f\"{prefix} START fetching '{remote}' over SSH.\")\n        actual_lock_timeout = self.default_lock_timeout\n        if lock_timeout is not None:\n            actual_lock_timeout = lock_timeout\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=f\"fetch_file({local},{remote})\",\n            timeout=actual_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            self._sftp_unsafe().get(\n                remote,\n                local,\n                prefetch=self.sftp_get_prefetch,\n                max_concurrent_prefetch_requests=self.sftp_get_max_requests,  # noqa E501\n            )\n        self.logger.info(f\"{prefix} END fetching '{remote}' over SSH.\")\n    except Exception as e:\n        self.log_and_raise(\n            e=e,\n            message=(\n                \"Error in `fetch_file`, while \"\n                f\"Transferring {remote=} to {local=}.\"\n            ),\n        )\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.log_and_raise","title":"<code>log_and_raise(*, e, message)</code>","text":"<p>Log and re-raise an exception from a FractalSSH method.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Additional message to be logged.</p> required <code>e</code> <code>Exception</code> <p>Original exception</p> required Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def log_and_raise(self, *, e: Exception, message: str) -&gt; None:\n    \"\"\"\n    Log and re-raise an exception from a FractalSSH method.\n\n    Args:\n        message: Additional message to be logged.\n        e: Original exception\n    \"\"\"\n    try:\n        self.logger.error(message)\n        self.logger.error(f\"Original Error {type(e)} : \\n{str(e)}\")\n        # Handle the specific case of `NoValidConnectionsError`s from\n        # paramiko, which store relevant information in the `errors`\n        # attribute\n        if hasattr(e, \"errors\"):\n            self.logger.error(f\"{type(e)=}\")\n            for err in e.errors:\n                self.logger.error(f\"{err}\")\n    except Exception as exception:\n        # Handle unexpected cases, e.g. (1) `e` has no `type`, or\n        # (2) `errors` is not iterable.\n        self.logger.error(\n            \"Unexpected Error while handling exception above: \"\n            f\"{str(exception)}\"\n        )\n\n    raise e\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.mkdir","title":"<code>mkdir(*, folder, parents=True)</code>","text":"<p>Create a folder remotely via SSH.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> required <code>parents</code> <code>bool</code> <code>True</code> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def mkdir(self, *, folder: str, parents: bool = True) -&gt; None:\n    \"\"\"\n    Create a folder remotely via SSH.\n\n    Args:\n        folder:\n        parents:\n    \"\"\"\n    if parents:\n        cmd = f\"mkdir -p {folder}\"\n    else:\n        cmd = f\"mkdir {folder}\"\n    self.run_command(cmd=cmd)\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.read_remote_text_file","title":"<code>read_remote_text_file(filepath)</code>","text":"<p>Read a remote text file into a string.</p> <p>Note from paramiko docs:</p> <p>The Python 'b' flag is ignored, since SSH treats all files as binary.</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef read_remote_text_file(self, filepath: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Read a remote text file into a string.\n\n    Note from paramiko docs:\n    &gt; The Python 'b' flag is ignored, since SSH treats all files as binary.\n    \"\"\"\n    self.logger.info(f\"START reading remote text file {filepath}.\")\n    with _acquire_lock_with_timeout(\n        lock=self._lock,\n        label=f\"read_remote_text_file({filepath})\",\n        timeout=self.default_lock_timeout,\n        pid=self._pid,\n        logger_name=self.logger_name,\n    ):\n        try:\n            with self._sftp_unsafe().open(filepath, \"r\") as f:\n                data = f.read().decode()\n        except Exception as e:\n            self.log_and_raise(\n                e=e,\n                message=(\n                    f\"Error in `read_remote_text_file`, for {filepath=}.\"\n                ),\n            )\n    self.logger.info(f\"END reading remote text file {filepath}.\")\n    return data\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.remote_exists","title":"<code>remote_exists(path)</code>","text":"<p>Return whether a remote file/folder exists</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef remote_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Return whether a remote file/folder exists\n    \"\"\"\n    self.logger.info(f\"START remote_file_exists {path}\")\n    with _acquire_lock_with_timeout(\n        lock=self._lock,\n        label=f\"remote_file_exists({path})\",\n        timeout=self.default_lock_timeout,\n        pid=self._pid,\n        logger_name=self.logger_name,\n    ):\n        try:\n            self._sftp_unsafe().stat(path)\n            self.logger.info(f\"END   remote_file_exists {path} / True\")\n            return True\n        except FileNotFoundError:\n            self.logger.info(f\"END   remote_file_exists {path} / False\")\n            return False\n        except Exception as e:\n            self.log_and_raise(\n                e=e, message=f\"Error in `remote_exists`, for {path=}.\"\n            )\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.remove_folder","title":"<code>remove_folder(*, folder, safe_root)</code>","text":"<p>Removes a folder remotely via SSH.</p> <p>This functions calls <code>rm -r</code>, after a few checks on <code>folder</code>.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Absolute path to a folder that should be removed.</p> required <code>safe_root</code> <code>str</code> <p>If <code>folder</code> is not a subfolder of the absolute <code>safe_root</code> path, raise an error.</p> required Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def remove_folder(\n    self,\n    *,\n    folder: str,\n    safe_root: str,\n) -&gt; None:\n    \"\"\"\n    Removes a folder remotely via SSH.\n\n    This functions calls `rm -r`, after a few checks on `folder`.\n\n    Args:\n        folder: Absolute path to a folder that should be removed.\n        safe_root: If `folder` is not a subfolder of the absolute\n            `safe_root` path, raise an error.\n    \"\"\"\n    validate_cmd(folder)\n    validate_cmd(safe_root)\n\n    if \" \" in folder:\n        raise ValueError(f\"folder='{folder}' includes whitespace.\")\n    elif \" \" in safe_root:\n        raise ValueError(f\"safe_root='{safe_root}' includes whitespace.\")\n    elif not Path(folder).is_absolute():\n        raise ValueError(f\"{folder=} is not an absolute path.\")\n    elif not Path(safe_root).is_absolute():\n        raise ValueError(f\"{safe_root=} is not an absolute path.\")\n    elif not (\n        Path(folder).resolve().is_relative_to(Path(safe_root).resolve())\n    ):\n        raise ValueError(f\"{folder=} is not a subfolder of {safe_root=}.\")\n    else:\n        cmd = f\"rm -r {folder}\"\n        self.run_command(cmd=cmd)\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.run_command","title":"<code>run_command(*, cmd, allow_char=None, lock_timeout=None)</code>","text":"<p>Run a command within an open SSH connection.</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>str</code> <p>Command to be run</p> required <code>allow_char</code> <code>str | None</code> <p>Forbidden chars to allow for this command</p> <code>None</code> <code>max_attempts</code> required <code>base_interval</code> required <code>lock_timeout</code> <code>int | None</code> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Standard output of the command, if successful.</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef run_command(\n    self,\n    *,\n    cmd: str,\n    allow_char: str | None = None,\n    lock_timeout: int | None = None,\n) -&gt; str:\n    \"\"\"\n    Run a command within an open SSH connection.\n\n    Args:\n        cmd: Command to be run\n        allow_char: Forbidden chars to allow for this command\n        max_attempts:\n        base_interval:\n        lock_timeout:\n\n    Returns:\n        Standard output of the command, if successful.\n    \"\"\"\n\n    validate_cmd(cmd, allow_char=allow_char)\n\n    actual_lock_timeout = self.default_lock_timeout\n    if lock_timeout is not None:\n        actual_lock_timeout = lock_timeout\n\n    t_0 = time.perf_counter()\n    try:\n        # Case 1: Command runs successfully\n        res = self._run(\n            cmd,\n            label=cmd,\n            lock_timeout=actual_lock_timeout,\n            hide=True,\n            in_stream=False,\n        )\n        t_1 = time.perf_counter()\n        self.logger.info(\n            f\"END   running '{cmd}' over SSH, elapsed={t_1 - t_0:.3f}\"\n        )\n        self.logger.debug(\"STDOUT:\")\n        self.logger.debug(res.stdout)\n        self.logger.debug(\"STDERR:\")\n        self.logger.debug(res.stderr)\n        return res.stdout\n    # Case 2: Command fails with a connection error\n    except NoValidConnectionsError as e:\n        raise NoValidConnectionsError(errors=e.errors)\n    except UnexpectedExit as e:\n        # Case 3: Command fails with an actual error\n        error_msg = (\n            f\"Running command `{cmd}` over SSH failed.\\n\"\n            f\"Original error:\\n{str(e)}.\"\n        )\n        self.logger.error(error_msg)\n        raise FractalSSHCommandError(error_msg)\n    except FractalSSHTimeoutError as e:\n        raise e\n    except Exception as e:\n        self.logger.error(\n            f\"Running command `{cmd}` over SSH failed.\\n\"\n            f\"Original Error:\\n{str(e)}.\"\n        )\n        raise FractalSSHUnknownError(f\"{type(e)}: {str(e)}\")\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.send_file","title":"<code>send_file(*, local, remote, lock_timeout=None)</code>","text":"<p>Transfer a file via SSH</p> <p>Parameters:</p> Name Type Description Default <code>local</code> <code>str</code> <p>Local path to file.</p> required <code>remote</code> <code>str</code> <p>Target path on remote host.</p> required <code>lock_timeout</code> <code>float | None</code> <p>Timeout for lock acquisition (overrides default).</p> <code>None</code> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef send_file(\n    self,\n    *,\n    local: str,\n    remote: str,\n    lock_timeout: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Transfer a file via SSH\n\n    Args:\n        local: Local path to file.\n        remote: Target path on remote host.\n        lock_timeout: Timeout for lock acquisition (overrides default).\n    \"\"\"\n    try:\n        self.logger.info(\n            f\"[send_file] START transfer of '{local}' over SSH.\"\n        )\n        actual_lock_timeout = self.default_lock_timeout\n        if lock_timeout is not None:\n            actual_lock_timeout = lock_timeout\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=f\"send_file({local},{remote})\",\n            timeout=actual_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            self._sftp_unsafe().put(local, remote)\n        self.logger.info(\n            f\"[send_file] END transfer of '{local}' over SSH.\"\n        )\n    except Exception as e:\n        self.log_and_raise(\n            e=e,\n            message=(\n                \"Error in `send_file`, while \"\n                f\"transferring {local=} to {remote=}.\"\n            ),\n        )\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.write_remote_file","title":"<code>write_remote_file(*, path, content, lock_timeout=None)</code>","text":"<p>Open a remote file via SFTP and write it.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Absolute path of remote file.</p> required <code>content</code> <code>str</code> <p>Contents to be written to file.</p> required <code>lock_timeout</code> <code>float | None</code> <p>Timeout for lock acquisition (overrides default).</p> <code>None</code> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef write_remote_file(\n    self,\n    *,\n    path: str,\n    content: str,\n    lock_timeout: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Open a remote file via SFTP and write it.\n\n    Args:\n        path: Absolute path of remote file.\n        content: Contents to be written to file.\n        lock_timeout: Timeout for lock acquisition (overrides default).\n    \"\"\"\n    t_start = time.perf_counter()\n    self.logger.info(f\"[write_remote_file] START ({path}).\")\n    actual_lock_timeout = self.default_lock_timeout\n    if lock_timeout is not None:\n        actual_lock_timeout = lock_timeout\n    with _acquire_lock_with_timeout(\n        lock=self._lock,\n        label=f\"write_remote_file({path})\",\n        timeout=actual_lock_timeout,\n        pid=self._pid,\n        logger_name=self.logger_name,\n    ):\n        try:\n            with self._sftp_unsafe().open(filename=path, mode=\"w\") as f:\n                f.write(content)\n        except Exception as e:\n            self.log_and_raise(\n                e=e, message=f\"Error in `write_remote_file`, for {path=}.\"\n            )\n\n    elapsed = time.perf_counter() - t_start\n    self.logger.info(f\"[write_remote_file] END, {elapsed=} s ({path}).\")\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList","title":"<code>FractalSSHList</code>","text":"<p>Collection of <code>FractalSSH</code> objects</p> <p>Attributes are all private, and access to this collection must be through methods (mostly the <code>get</code> one).</p> <p>Attributes:</p> Name Type Description <code>_data</code> <code>dict[tuple[str, str, str], FractalSSH]</code> <p>Mapping of unique keys (the SSH-credentials tuples) to <code>FractalSSH</code> objects.</p> <code>_lock</code> <code>Lock</code> <p>A <code>threading.Lock object</code>, to be acquired when changing <code>_data</code>.</p> <code>_timeout</code> <code>float</code> <p>Timeout for <code>_lock</code> acquisition.</p> <code>_logger_name</code> <code>str</code> <p>Logger name.</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>class FractalSSHList:\n    \"\"\"\n    Collection of `FractalSSH` objects\n\n    Attributes are all private, and access to this collection must be\n    through methods (mostly the `get` one).\n\n    Attributes:\n        _data:\n            Mapping of unique keys (the SSH-credentials tuples) to\n            `FractalSSH` objects.\n        _lock:\n            A `threading.Lock object`, to be acquired when changing `_data`.\n        _timeout: Timeout for `_lock` acquisition.\n        _logger_name: Logger name.\n    \"\"\"\n\n    _data: dict[tuple[str, str, str], FractalSSH]\n    _lock: Lock\n    _timeout: float\n    _logger_name: str\n    _pid: int\n\n    def __init__(\n        self,\n        *,\n        timeout: float = 5.0,\n        logger_name: str = \"fractal_server.FractalSSHList\",\n    ):\n        self._lock = Lock()\n        self._data = {}\n        self._timeout = timeout\n        self._logger_name = logger_name\n        set_logger(self._logger_name)\n        self._pid = os.getpid()\n\n    @property\n    def logger(self) -&gt; logging.Logger:\n        \"\"\"\n        This property exists so that we never have to propagate the\n        `Logger` object.\n        \"\"\"\n        return get_logger(self._logger_name)\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"\n        Number of current key-value pairs in `self._data`.\n        \"\"\"\n        return len(self._data.values())\n\n    def get(self, *, host: str, user: str, key_path: str) -&gt; FractalSSH:\n        \"\"\"\n        Get the `FractalSSH` for the current credentials, or create one.\n\n        Note: Changing `_data` requires acquiring `_lock`.\n\n        Args:\n            host:\n            user:\n            key_path:\n        \"\"\"\n        key = (host, user, key_path)\n        fractal_ssh = self._data.get(key, None)\n        if fractal_ssh is not None:\n            self.logger.info(\n                f\"Return existing FractalSSH object for {user}@{host}\"\n            )\n            return fractal_ssh\n        else:\n            self.logger.info(f\"Add new FractalSSH object for {user}@{host}\")\n            connection = Connection(\n                host=host,\n                user=user,\n                forward_agent=False,\n                connect_kwargs={\n                    \"key_filename\": key_path,\n                    \"look_for_keys\": False,\n                    \"banner_timeout\": 30,\n                    \"auth_timeout\": 30,  # default value\n                    \"channel_timeout\": 60 * 60,  # default value\n                },\n            )\n            with _acquire_lock_with_timeout(\n                lock=self._lock,\n                label=\"FractalSSHList.get\",\n                timeout=self._timeout,\n                pid=self._pid,\n                logger_name=self._logger_name,\n            ):\n                self._data[key] = FractalSSH(connection=connection)\n                return self._data[key]\n\n    def contains(\n        self,\n        *,\n        host: str,\n        user: str,\n        key_path: str,\n    ) -&gt; bool:\n        \"\"\"\n        Return whether a given key is present in the collection.\n\n        Args:\n            host:\n            user:\n            key_path:\n        \"\"\"\n        key = (host, user, key_path)\n        return key in self._data.keys()\n\n    def remove(\n        self,\n        *,\n        host: str,\n        user: str,\n        key_path: str,\n    ) -&gt; None:\n        \"\"\"\n        Remove a key from `_data` and close the corresponding connection.\n\n        Note: Changing `_data` requires acquiring `_lock`.\n\n        Args:\n            host:\n            user:\n            key_path:\n        \"\"\"\n        key = (host, user, key_path)\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            timeout=self._timeout,\n            label=\"FractalSSHList.remove\",\n            pid=self._pid,\n            logger_name=self._logger_name,\n        ):\n            self.logger.info(\n                f\"Removing FractalSSH object for {user}@{host} \"\n                \"from collection.\"\n            )\n            fractal_ssh_obj = self._data.pop(key)\n            self.logger.info(\n                f\"Closing FractalSSH object for {user}@{host} \"\n                f\"({fractal_ssh_obj.is_connected=}).\"\n            )\n            fractal_ssh_obj.close()\n\n    def close_all(self, *, timeout: float = 5.0):\n        \"\"\"\n        Close all `FractalSSH` objects in the collection.\n\n        Args:\n            timeout:\n                Timeout for `FractalSSH._lock` acquisition, to be obtained\n                before closing.\n        \"\"\"\n        for key, fractal_ssh_obj in self._data.items():\n            host, user, _ = key[:]\n            self.logger.info(\n                f\"Closing FractalSSH object for {user}@{host} \"\n                f\"({fractal_ssh_obj.is_connected=}).\"\n            )\n            fractal_ssh_obj.close()\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.logger","title":"<code>logger</code>  <code>property</code>","text":"<p>This property exists so that we never have to propagate the <code>Logger</code> object.</p>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.size","title":"<code>size</code>  <code>property</code>","text":"<p>Number of current key-value pairs in <code>self._data</code>.</p>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.close_all","title":"<code>close_all(*, timeout=5.0)</code>","text":"<p>Close all <code>FractalSSH</code> objects in the collection.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Timeout for <code>FractalSSH._lock</code> acquisition, to be obtained before closing.</p> <code>5.0</code> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def close_all(self, *, timeout: float = 5.0):\n    \"\"\"\n    Close all `FractalSSH` objects in the collection.\n\n    Args:\n        timeout:\n            Timeout for `FractalSSH._lock` acquisition, to be obtained\n            before closing.\n    \"\"\"\n    for key, fractal_ssh_obj in self._data.items():\n        host, user, _ = key[:]\n        self.logger.info(\n            f\"Closing FractalSSH object for {user}@{host} \"\n            f\"({fractal_ssh_obj.is_connected=}).\"\n        )\n        fractal_ssh_obj.close()\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.contains","title":"<code>contains(*, host, user, key_path)</code>","text":"<p>Return whether a given key is present in the collection.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> required <code>user</code> <code>str</code> required <code>key_path</code> <code>str</code> required Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def contains(\n    self,\n    *,\n    host: str,\n    user: str,\n    key_path: str,\n) -&gt; bool:\n    \"\"\"\n    Return whether a given key is present in the collection.\n\n    Args:\n        host:\n        user:\n        key_path:\n    \"\"\"\n    key = (host, user, key_path)\n    return key in self._data.keys()\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.get","title":"<code>get(*, host, user, key_path)</code>","text":"<p>Get the <code>FractalSSH</code> for the current credentials, or create one.</p> <p>Note: Changing <code>_data</code> requires acquiring <code>_lock</code>.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> required <code>user</code> <code>str</code> required <code>key_path</code> <code>str</code> required Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def get(self, *, host: str, user: str, key_path: str) -&gt; FractalSSH:\n    \"\"\"\n    Get the `FractalSSH` for the current credentials, or create one.\n\n    Note: Changing `_data` requires acquiring `_lock`.\n\n    Args:\n        host:\n        user:\n        key_path:\n    \"\"\"\n    key = (host, user, key_path)\n    fractal_ssh = self._data.get(key, None)\n    if fractal_ssh is not None:\n        self.logger.info(\n            f\"Return existing FractalSSH object for {user}@{host}\"\n        )\n        return fractal_ssh\n    else:\n        self.logger.info(f\"Add new FractalSSH object for {user}@{host}\")\n        connection = Connection(\n            host=host,\n            user=user,\n            forward_agent=False,\n            connect_kwargs={\n                \"key_filename\": key_path,\n                \"look_for_keys\": False,\n                \"banner_timeout\": 30,\n                \"auth_timeout\": 30,  # default value\n                \"channel_timeout\": 60 * 60,  # default value\n            },\n        )\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=\"FractalSSHList.get\",\n            timeout=self._timeout,\n            pid=self._pid,\n            logger_name=self._logger_name,\n        ):\n            self._data[key] = FractalSSH(connection=connection)\n            return self._data[key]\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.remove","title":"<code>remove(*, host, user, key_path)</code>","text":"<p>Remove a key from <code>_data</code> and close the corresponding connection.</p> <p>Note: Changing <code>_data</code> requires acquiring <code>_lock</code>.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> required <code>user</code> <code>str</code> required <code>key_path</code> <code>str</code> required Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def remove(\n    self,\n    *,\n    host: str,\n    user: str,\n    key_path: str,\n) -&gt; None:\n    \"\"\"\n    Remove a key from `_data` and close the corresponding connection.\n\n    Note: Changing `_data` requires acquiring `_lock`.\n\n    Args:\n        host:\n        user:\n        key_path:\n    \"\"\"\n    key = (host, user, key_path)\n    with _acquire_lock_with_timeout(\n        lock=self._lock,\n        timeout=self._timeout,\n        label=\"FractalSSHList.remove\",\n        pid=self._pid,\n        logger_name=self._logger_name,\n    ):\n        self.logger.info(\n            f\"Removing FractalSSH object for {user}@{host} \"\n            \"from collection.\"\n        )\n        fractal_ssh_obj = self._data.pop(key)\n        self.logger.info(\n            f\"Closing FractalSSH object for {user}@{host} \"\n            f\"({fractal_ssh_obj.is_connected=}).\"\n        )\n        fractal_ssh_obj.close()\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric.SingleUseFractalSSH","title":"<code>SingleUseFractalSSH(*, ssh_config, logger_name)</code>","text":"<p>Get a new FractalSSH object (with a fresh connection).</p> <p>Parameters:</p> Name Type Description Default <code>ssh_config</code> <code>SSHConfig</code> required <code>logger_name</code> <code>str</code> required Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@contextmanager\ndef SingleUseFractalSSH(\n    *,\n    ssh_config: SSHConfig,\n    logger_name: str,\n) -&gt; Generator[FractalSSH, Any, None]:\n    \"\"\"\n    Get a new FractalSSH object (with a fresh connection).\n\n    Args:\n        ssh_config:\n        logger_name:\n    \"\"\"\n    _fractal_ssh_list = FractalSSHList(logger_name=logger_name)\n    _fractal_ssh = _fractal_ssh_list.get(**ssh_config.model_dump())\n    yield _fractal_ssh\n    _fractal_ssh.close()\n</code></pre>"},{"location":"reference/fractal_server/ssh/_fabric/#fractal_server.ssh._fabric._acquire_lock_with_timeout","title":"<code>_acquire_lock_with_timeout(lock, label, timeout, pid, logger_name)</code>","text":"<p>Given a <code>threading.Lock</code> object, try to acquire it within a given timeout.</p> <p>Parameters:</p> Name Type Description Default <code>lock</code> <code>Lock</code> required <code>label</code> <code>str</code> required <code>timeout</code> <code>float</code> required <code>logger_name</code> <code>str</code> required Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@contextmanager\ndef _acquire_lock_with_timeout(\n    lock: Lock,\n    label: str,\n    timeout: float,\n    pid: int,\n    logger_name: str,\n) -&gt; Generator[Literal[True], Any, None]:\n    \"\"\"\n    Given a `threading.Lock` object, try to acquire it within a given timeout.\n\n    Args:\n        lock:\n        label:\n        timeout:\n        logger_name:\n    \"\"\"\n    logger = get_logger(logger_name)\n    ssh_logger = get_logger(SSH_MONITORING_LOGGER_NAME)\n    logger.info(f\"Trying to acquire lock for '{label}', with {timeout=}\")\n    t_lock_request = time.perf_counter()\n    result = lock.acquire(timeout=timeout)\n    try:\n        if not result:\n            logger.error(f\"Lock for '{label}' was *not* acquired.\")\n            raise FractalSSHTimeoutError(\n                f\"Failed to acquire lock for '{label}' within \"\n                f\"{timeout} seconds\"\n            )\n        t_lock_acquisition = time.perf_counter()\n        elapsed = t_lock_acquisition - t_lock_request\n        logger.info(f\"Lock for '{label}' was acquired - {elapsed=:.4f} s\")\n        yield result\n    finally:\n        if result:\n            lock.release()\n            logger.info(f\"Lock for '{label}' was released.\")\n            t_lock_release = time.perf_counter()\n            lock_was_acquired = 1\n        else:\n            t_lock_release = time.perf_counter()\n            t_lock_acquisition = t_lock_release\n            lock_was_acquired = 0\n        lock_waiting_time = t_lock_acquisition - t_lock_request\n        lock_holding_time = t_lock_release - t_lock_acquisition\n        ssh_logger.info(\n            f\"{pid} {lock_waiting_time:.6e} {lock_holding_time:.6e} {lock_was_acquired} {label.replace(' ', '_')}\"  # noqa\n        )\n</code></pre>"},{"location":"reference/fractal_server/tasks/","title":"tasks","text":"<p><code>tasks</code> module</p>"},{"location":"reference/fractal_server/tasks/utils/","title":"utils","text":""},{"location":"reference/fractal_server/tasks/config/","title":"config","text":""},{"location":"reference/fractal_server/tasks/config/_pixi/","title":"_pixi","text":""},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig","title":"<code>PixiSLURMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters that are passed directly to a <code>sbatch</code> command.</p> <p>See https://slurm.schedmd.com/sbatch.html.</p> Source code in <code>fractal_server/tasks/config/_pixi.py</code> <pre><code>class PixiSLURMConfig(BaseModel):\n    \"\"\"\n    Parameters that are passed directly to a `sbatch` command.\n\n    See https://slurm.schedmd.com/sbatch.html.\n    \"\"\"\n\n    partition: NonEmptyStr\n    \"\"\"\n    `-p, --partition=&lt;partition_names&gt;`\n    \"\"\"\n    cpus: PositiveInt\n    \"\"\"\n    `-c, --cpus-per-task=&lt;ncpus&gt;\n    \"\"\"\n    mem: Annotated[NonEmptyStr, AfterValidator(_check_pixi_slurm_memory)]\n    \"\"\"\n    `--mem=&lt;size&gt;[units]` (examples: `\"10M\"`, `\"10G\"`).\n    From `sbatch` docs: Specify the real memory required per node. Default\n    units are megabytes. Different units can be specified using the suffix\n    [K|M|G|T].\n    \"\"\"\n    time: NonEmptyStr\n    \"\"\"\n    `-t, --time=&lt;time&gt;`.\n    From `sbatch` docs: \"A time limit of zero requests that no time limit be\n    imposed. Acceptable time formats include \"minutes\", \"minutes:seconds\",\n    \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and\n    \"days-hours:minutes:seconds\".\n    \"\"\"\n</code></pre>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig.cpus","title":"<code>cpus</code>  <code>instance-attribute</code>","text":"<p>`-c, --cpus-per-task="},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig.mem","title":"<code>mem</code>  <code>instance-attribute</code>","text":"<p><code>--mem=&lt;size&gt;[units]</code> (examples: <code>\"10M\"</code>, <code>\"10G\"</code>). From <code>sbatch</code> docs: Specify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T].</p>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig.partition","title":"<code>partition</code>  <code>instance-attribute</code>","text":"<p><code>-p, --partition=&lt;partition_names&gt;</code></p>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig.time","title":"<code>time</code>  <code>instance-attribute</code>","text":"<p><code>-t, --time=&lt;time&gt;</code>. From <code>sbatch</code> docs: \"A time limit of zero requests that no time limit be imposed. Acceptable time formats include \"minutes\", \"minutes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-hours:minutes:seconds\".</p>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings","title":"<code>TasksPixiSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for <code>pixi</code> Task collection.</p> Source code in <code>fractal_server/tasks/config/_pixi.py</code> <pre><code>class TasksPixiSettings(BaseModel):\n    \"\"\"\n    Configuration for `pixi` Task collection.\n    \"\"\"\n\n    versions: DictStrStr\n    \"\"\"\n    Dictionary mapping `pixi` versions (e.g. `0.47.0`) to the corresponding\n    folders (e.g. `/somewhere/pixi/0.47.0` - if the binary is\n    `/somewhere/pixi/0.47.0/bin/pixi`).\n    \"\"\"\n    default_version: str\n    \"\"\"\n    Default task-collection `pixi` version.\n    \"\"\"\n    PIXI_CONCURRENT_SOLVES: int = 4\n    \"\"\"\n    Value of\n    [`--concurrent-solves`](https://pixi.sh/latest/reference/cli/pixi/install/#arg---concurrent-solves)\n    for `pixi install`.\n    \"\"\"\n    PIXI_CONCURRENT_DOWNLOADS: int = 4\n    \"\"\"\n    Value of\n    [`--concurrent-downloads`](https://pixi.sh/latest/reference/cli/pixi/install/#arg---concurrent-downloads)\n    for `pixi install`.\n    \"\"\"\n    TOKIO_WORKER_THREADS: int = 2\n    \"\"\"\n    From\n    [Tokio documentation](\n    https://docs.rs/tokio/latest/tokio/#cpu-bound-tasks-and-blocking-code\n    )\n    :\n\n        The core threads are where all asynchronous code runs,\n        and Tokio will by default spawn one for each CPU core.\n        You can use the environment variable `TOKIO_WORKER_THREADS` to override\n        the default value.\n    \"\"\"\n    DEFAULT_ENVIRONMENT: str = \"default\"\n    \"\"\"\n    Default pixi environment name.\n    \"\"\"\n    DEFAULT_PLATFORM: str = \"linux-64\"\n    \"\"\"\n    Default platform for pixi.\n    \"\"\"\n    SLURM_CONFIG: PixiSLURMConfig | None = None\n    \"\"\"\n    Required when using `pixi` in a SSH/SLURM deployment.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def check_pixi_settings(self):\n        if self.default_version not in self.versions:\n            raise ValueError(\n                f\"Default version '{self.default_version}' not in \"\n                f\"available version {list(self.versions.keys())}.\"\n            )\n\n        pixi_base_dir = Path(self.versions[self.default_version]).parent\n\n        for key, value in self.versions.items():\n            pixi_path = Path(value)\n\n            if pixi_path.parent != pixi_base_dir:\n                raise ValueError(\n                    f\"{pixi_path=} is not located within the {pixi_base_dir=}.\"\n                )\n            if pixi_path.name != key:\n                raise ValueError(f\"{pixi_path.name=} is not equal to {key=}\")\n\n        return self\n</code></pre>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.DEFAULT_ENVIRONMENT","title":"<code>DEFAULT_ENVIRONMENT = 'default'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Default pixi environment name.</p>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.DEFAULT_PLATFORM","title":"<code>DEFAULT_PLATFORM = 'linux-64'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Default platform for pixi.</p>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.PIXI_CONCURRENT_DOWNLOADS","title":"<code>PIXI_CONCURRENT_DOWNLOADS = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Value of <code>--concurrent-downloads</code> for <code>pixi install</code>.</p>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.PIXI_CONCURRENT_SOLVES","title":"<code>PIXI_CONCURRENT_SOLVES = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Value of <code>--concurrent-solves</code> for <code>pixi install</code>.</p>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.SLURM_CONFIG","title":"<code>SLURM_CONFIG = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Required when using <code>pixi</code> in a SSH/SLURM deployment.</p>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.TOKIO_WORKER_THREADS","title":"<code>TOKIO_WORKER_THREADS = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>From Tokio documentation :</p> <pre><code>The core threads are where all asynchronous code runs,\nand Tokio will by default spawn one for each CPU core.\nYou can use the environment variable `TOKIO_WORKER_THREADS` to override\nthe default value.\n</code></pre>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.default_version","title":"<code>default_version</code>  <code>instance-attribute</code>","text":"<p>Default task-collection <code>pixi</code> version.</p>"},{"location":"reference/fractal_server/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.versions","title":"<code>versions</code>  <code>instance-attribute</code>","text":"<p>Dictionary mapping <code>pixi</code> versions (e.g. <code>0.47.0</code>) to the corresponding folders (e.g. <code>/somewhere/pixi/0.47.0</code> - if the binary is <code>/somewhere/pixi/0.47.0/bin/pixi</code>).</p>"},{"location":"reference/fractal_server/tasks/config/_python/","title":"_python","text":""},{"location":"reference/fractal_server/tasks/config/_python/#fractal_server.tasks.config._python.TasksPythonSettings","title":"<code>TasksPythonSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the Python base interpreters to be used for task venvs.</p> <p>For task collection to work, there must be one or more base Python interpreters available on your system.</p> Source code in <code>fractal_server/tasks/config/_python.py</code> <pre><code>class TasksPythonSettings(BaseModel):\n    \"\"\"\n    Configuration for the Python base interpreters to be used for task venvs.\n\n    For task collection to work, there must be one or more base Python\n    interpreters available on your system.\n    \"\"\"\n\n    default_version: NonEmptyStr\n    \"\"\"\n    Default task-collection Python version (must be a key of `versions`).\n    \"\"\"\n    versions: dict[\n        Literal[\n            \"3.9\",\n            \"3.10\",\n            \"3.11\",\n            \"3.12\",\n            \"3.13\",\n            \"3.14\",\n        ],\n        AbsolutePathStr,\n    ]\n    \"\"\"\n    Dictionary mapping Python versions to the corresponding interpreters.\n    \"\"\"\n\n    pip_cache_dir: AbsolutePathStr | None = None\n    \"\"\"\n    Argument for `--cache-dir` option of `pip install`, if set.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def _validate_versions(self) -&gt; Self:\n        if self.default_version not in self.versions.keys():\n            raise ValueError(\n                f\"The default Python version ('{self.default_version}') is \"\n                f\"not available versions in {list(self.versions.keys())}.\"\n            )\n\n        return self\n</code></pre>"},{"location":"reference/fractal_server/tasks/config/_python/#fractal_server.tasks.config._python.TasksPythonSettings.default_version","title":"<code>default_version</code>  <code>instance-attribute</code>","text":"<p>Default task-collection Python version (must be a key of <code>versions</code>).</p>"},{"location":"reference/fractal_server/tasks/config/_python/#fractal_server.tasks.config._python.TasksPythonSettings.pip_cache_dir","title":"<code>pip_cache_dir = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Argument for <code>--cache-dir</code> option of <code>pip install</code>, if set.</p>"},{"location":"reference/fractal_server/tasks/config/_python/#fractal_server.tasks.config._python.TasksPythonSettings.versions","title":"<code>versions</code>  <code>instance-attribute</code>","text":"<p>Dictionary mapping Python versions to the corresponding interpreters.</p>"},{"location":"reference/fractal_server/tasks/v2/","title":"v2","text":""},{"location":"reference/fractal_server/tasks/v2/utils_background/","title":"utils_background","text":""},{"location":"reference/fractal_server/tasks/v2/utils_background/#fractal_server.tasks.v2.utils_background.prepare_tasks_metadata","title":"<code>prepare_tasks_metadata(*, package_manifest, package_root, python_bin=None, project_python_wrapper=None, package_version=None)</code>","text":"<p>Based on the package manifest and additional info, prepare the task list.</p> <p>Parameters:</p> Name Type Description Default <code>package_manifest</code> <code>ManifestV2</code> required <code>package_root</code> <code>Path</code> required <code>package_version</code> <code>str | None</code> <code>None</code> <code>python_bin</code> <code>Path | None</code> <code>None</code> <code>project_python_wrapper</code> <code>Path | None</code> <code>None</code> Source code in <code>fractal_server/tasks/v2/utils_background.py</code> <pre><code>def prepare_tasks_metadata(\n    *,\n    package_manifest: ManifestV2,\n    package_root: Path,\n    python_bin: Path | None = None,\n    project_python_wrapper: Path | None = None,\n    package_version: str | None = None,\n) -&gt; list[TaskCreateV2]:\n    \"\"\"\n    Based on the package manifest and additional info, prepare the task list.\n\n    Args:\n        package_manifest:\n        package_root:\n        package_version:\n        python_bin:\n        project_python_wrapper:\n    \"\"\"\n\n    if bool(project_python_wrapper is None) == bool(python_bin is None):\n        raise UnreachableBranchError(\n            f\"Either {project_python_wrapper} or {python_bin} must be set.\"\n        )\n\n    if python_bin is not None:\n        actual_python = python_bin\n    else:\n        actual_python = project_python_wrapper\n\n    task_list = []\n    for _task in package_manifest.task_list:\n        # Set non-command attributes\n        task_attributes = {}\n        if package_version is not None:\n            task_attributes[\"version\"] = package_version\n        if package_manifest.has_args_schemas:\n            task_attributes[\n                \"args_schema_version\"\n            ] = package_manifest.args_schema_version\n        # Set command attributes\n        if _task.executable_non_parallel is not None:\n            non_parallel_path = package_root / _task.executable_non_parallel\n            cmd_non_parallel = (\n                f\"{actual_python.as_posix()} {non_parallel_path.as_posix()}\"\n            )\n            task_attributes[\"command_non_parallel\"] = cmd_non_parallel\n        if _task.executable_parallel is not None:\n            parallel_path = package_root / _task.executable_parallel\n            cmd_parallel = (\n                f\"{actual_python.as_posix()} {parallel_path.as_posix()}\"\n            )\n            task_attributes[\"command_parallel\"] = cmd_parallel\n        # Create object\n        task_obj = TaskCreateV2(\n            **_task.model_dump(\n                exclude={\n                    \"executable_non_parallel\",\n                    \"executable_parallel\",\n                }\n            ),\n            **task_attributes,\n            authors=package_manifest.authors,\n        )\n        task_list.append(task_obj)\n    return task_list\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/utils_database/","title":"utils_database","text":""},{"location":"reference/fractal_server/tasks/v2/utils_database/#fractal_server.tasks.v2.utils_database.create_db_tasks_and_update_task_group_async","title":"<code>create_db_tasks_and_update_task_group_async(*, task_group_id, task_list, db)</code>  <code>async</code>","text":"<p>Create a <code>TaskGroupV2</code> with N <code>TaskV2</code>s, and insert them into the database.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> <p>ID of an existing <code>TaskGroupV2</code> object.</p> required <code>task_list</code> <code>list[TaskCreateV2]</code> <p>List of <code>TaskCreateV2</code> objects to be inserted into the db.</p> required <code>db</code> <code>AsyncSession</code> <p>Synchronous database session</p> required <p>Returns:</p> Type Description <code>TaskGroupV2</code> <p>Updated <code>TaskGroupV2</code> object.</p> Source code in <code>fractal_server/tasks/v2/utils_database.py</code> <pre><code>async def create_db_tasks_and_update_task_group_async(\n    *,\n    task_group_id: int,\n    task_list: list[TaskCreateV2],\n    db: AsyncSession,\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Create a `TaskGroupV2` with N `TaskV2`s, and insert them into the database.\n\n    Args:\n        task_group_id: ID of an existing `TaskGroupV2` object.\n        task_list: List of `TaskCreateV2` objects to be inserted into the db.\n        db: Synchronous database session\n\n    Returns:\n        Updated `TaskGroupV2` object.\n    \"\"\"\n    actual_task_list = [TaskV2(**task.model_dump()) for task in task_list]\n    task_group = await db.get(TaskGroupV2, task_group_id)\n    task_group.task_list = actual_task_list\n    db.add(task_group)\n    await db.commit()\n    await db.refresh(task_group)\n\n    return task_group\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/utils_database/#fractal_server.tasks.v2.utils_database.create_db_tasks_and_update_task_group_sync","title":"<code>create_db_tasks_and_update_task_group_sync(*, task_group_id, task_list, db)</code>","text":"<p>Create a <code>TaskGroupV2</code> with N <code>TaskV2</code>s, and insert them into the database.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> <p>ID of an existing <code>TaskGroupV2</code> object.</p> required <code>task_list</code> <code>list[TaskCreateV2]</code> <p>List of <code>TaskCreateV2</code> objects to be inserted into the db.</p> required <code>db</code> <code>Session</code> <p>Synchronous database session</p> required <p>Returns:</p> Type Description <code>TaskGroupV2</code> <p>Updated <code>TaskGroupV2</code> object.</p> Source code in <code>fractal_server/tasks/v2/utils_database.py</code> <pre><code>def create_db_tasks_and_update_task_group_sync(\n    *,\n    task_group_id: int,\n    task_list: list[TaskCreateV2],\n    db: DBSyncSession,\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Create a `TaskGroupV2` with N `TaskV2`s, and insert them into the database.\n\n    Args:\n        task_group_id: ID of an existing `TaskGroupV2` object.\n        task_list: List of `TaskCreateV2` objects to be inserted into the db.\n        db: Synchronous database session\n\n    Returns:\n        Updated `TaskGroupV2` object.\n    \"\"\"\n    actual_task_list = [TaskV2(**task.model_dump()) for task in task_list]\n    task_group = db.get(TaskGroupV2, task_group_id)\n    task_group.task_list = actual_task_list\n    db.add(task_group)\n    db.commit()\n    db.refresh(task_group)\n\n    return task_group\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/utils_package_names/","title":"utils_package_names","text":""},{"location":"reference/fractal_server/tasks/v2/utils_package_names/#fractal_server.tasks.v2.utils_package_names._parse_wheel_filename","title":"<code>_parse_wheel_filename(wheel_filename)</code>","text":"<p>Extract distribution and version from a wheel filename.</p> <p>The structure of a wheel filename is fixed, and it must start with <code>{distribution}-{version}</code> (see https://packaging.python.org/en/latest/specifications/binary-distribution-format ).</p> <p>Note that we transform exceptions in <code>ValueError</code>s, since this function is also used within Pydantic validators.</p> Source code in <code>fractal_server/tasks/v2/utils_package_names.py</code> <pre><code>def _parse_wheel_filename(wheel_filename: str) -&gt; dict[str, str]:\n    \"\"\"\n    Extract distribution and version from a wheel filename.\n\n    The structure of a wheel filename is fixed, and it must start with\n    `{distribution}-{version}` (see\n    https://packaging.python.org/en/latest/specifications/binary-distribution-format\n    ).\n\n    Note that we transform exceptions in `ValueError`s, since this function is\n    also used within Pydantic validators.\n    \"\"\"\n    if \"/\" in wheel_filename:\n        raise ValueError(\n            \"[_parse_wheel_filename] Input must be a filename, not a full \"\n            f\"path (given: {wheel_filename}).\"\n        )\n    try:\n        parts = wheel_filename.split(\"-\")\n        return dict(distribution=parts[0], version=parts[1])\n    except Exception as e:\n        raise ValueError(\n            f\"Invalid {wheel_filename=}. Original error: {str(e)}.\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/utils_package_names/#fractal_server.tasks.v2.utils_package_names.compare_package_names","title":"<code>compare_package_names(*, pkg_name_pip_show, pkg_name_task_group, logger_name)</code>","text":"<p>Compare the package names from <code>pip show</code> and from the db.</p> Source code in <code>fractal_server/tasks/v2/utils_package_names.py</code> <pre><code>def compare_package_names(\n    *,\n    pkg_name_pip_show: str,\n    pkg_name_task_group: str,\n    logger_name: str,\n) -&gt; None:\n    \"\"\"\n    Compare the package names from `pip show` and from the db.\n    \"\"\"\n    logger = get_logger(logger_name)\n\n    if pkg_name_pip_show == pkg_name_task_group:\n        return\n\n    logger.warning(\n        f\"Package name mismatch: \"\n        f\"{pkg_name_task_group=}, {pkg_name_pip_show=}.\"\n    )\n    normalized_pkg_name_pip = normalize_package_name(pkg_name_pip_show)\n    normalized_pkg_name_taskgroup = normalize_package_name(pkg_name_task_group)\n    if normalized_pkg_name_pip != normalized_pkg_name_taskgroup:\n        error_msg = (\n            f\"Package name mismatch persists, after normalization: \"\n            f\"{pkg_name_task_group=}, \"\n            f\"{pkg_name_pip_show=}.\"\n        )\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/utils_package_names/#fractal_server.tasks.v2.utils_package_names.normalize_package_name","title":"<code>normalize_package_name(name)</code>","text":"<p>Implement PyPa specifications for package-name normalization</p> <p>The name should be lowercased with all runs of the characters <code>.</code>, <code>-</code>, or <code>_</code> replaced with a single <code>-</code> character. This can be implemented in Python with the re module. (https://packaging.python.org/en/latest/specifications/name-normalization)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The non-normalized package name.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized package name.</p> Source code in <code>fractal_server/tasks/v2/utils_package_names.py</code> <pre><code>def normalize_package_name(name: str) -&gt; str:\n    \"\"\"\n    Implement PyPa specifications for package-name normalization\n\n    The name should be lowercased with all runs of the characters `.`, `-`,\n    or `_` replaced with a single `-` character. This can be implemented in\n    Python with the re module.\n    (https://packaging.python.org/en/latest/specifications/name-normalization)\n\n    Args:\n        name: The non-normalized package name.\n\n    Returns:\n        The normalized package name.\n    \"\"\"\n    return re.sub(r\"[-_.]+\", \"-\", name).lower()\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/utils_pixi/","title":"utils_pixi","text":""},{"location":"reference/fractal_server/tasks/v2/utils_pixi/#fractal_server.tasks.v2.utils_pixi.parse_collect_stdout","title":"<code>parse_collect_stdout(stdout)</code>","text":"<p>Parse standard output of <code>pixi/1_collect.sh</code></p> Source code in <code>fractal_server/tasks/v2/utils_pixi.py</code> <pre><code>def parse_collect_stdout(stdout: str) -&gt; ParsedOutput:\n    \"\"\"\n    Parse standard output of `pixi/1_collect.sh`\n    \"\"\"\n    searches = [\n        (\"Package folder:\", \"package_root\"),\n        (\"Disk usage:\", \"venv_size\"),\n        (\"Number of files:\", \"venv_file_number\"),\n        (\"Project Python wrapper:\", \"project_python_wrapper\"),\n    ]\n    stdout_lines = stdout.splitlines()\n    attributes = dict()\n    for search, attribute_name in searches:\n        matching_lines = [_line for _line in stdout_lines if search in _line]\n        if len(matching_lines) == 0:\n            raise ValueError(f\"String '{search}' not found in stdout.\")\n        elif len(matching_lines) &gt; 1:\n            raise ValueError(\n                f\"String '{search}' found too many times \"\n                f\"({len(matching_lines)}).\"\n            )\n        else:\n            actual_line = matching_lines[0]\n            attribute_value = actual_line.split(search)[-1].strip(\" \")\n            attributes[attribute_name] = attribute_value\n    return attributes\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/utils_pixi/#fractal_server.tasks.v2.utils_pixi.simplify_pyproject_toml","title":"<code>simplify_pyproject_toml(*, original_toml_string, pixi_environment, pixi_platform)</code>","text":"<p>Simplify <code>pyproject.toml</code> contents to make <code>pixi install</code> less heavy.</p> <p>Parameters:</p> Name Type Description Default <code>original_toml_string</code> <code>str</code> <p>Original <code>pyproject.toml</code> contents</p> required <code>pixi_environment</code> <code>str</code> <p>Name of the pixi environment to use (e.g. <code>default</code>).</p> required <code>pixi_platform</code> <code>str</code> <p>Name of the platform (e.g. <code>linux-64</code>)</p> required <p>Returns:</p> Type Description <code>str</code> <p>New <code>pyproject.toml</code> contents</p> Source code in <code>fractal_server/tasks/v2/utils_pixi.py</code> <pre><code>def simplify_pyproject_toml(\n    *,\n    original_toml_string: str,\n    pixi_environment: str,\n    pixi_platform: str,\n) -&gt; str:\n    \"\"\"\n    Simplify `pyproject.toml` contents to make `pixi install` less heavy.\n\n    Args:\n        original_toml_string: Original `pyproject.toml` contents\n        pixi_environment: Name of the pixi environment to use (e.g. `default`).\n        pixi_platform: Name of the platform (e.g. `linux-64`)\n\n    Returns:\n        New `pyproject.toml` contents\n    \"\"\"\n\n    # Parse TOML string\n    data = tomllib.loads(original_toml_string)\n    try:\n        pixi_data = data[\"tool\"][\"pixi\"]\n    except KeyError:\n        logger.warning(\n            \"KeyError when looking for tool/pixi path. Return original value.\"\n        )\n        return original_toml_string\n\n    # Use a single platform (or skip, if not set)\n    try:\n        pixi_data[\"workspace\"][\"platforms\"] = [pixi_platform]\n    except KeyError:\n        logger.info(\"KeyError for workspace/platforms - skip.\")\n    try:\n        pixi_data[\"project\"][\"platforms\"] = [pixi_platform]\n    except KeyError:\n        logger.info(\"KeyError for project/platforms - skip.\")\n\n    # Keep a single environment (or skip, if not set)\n    try:\n        current_environments = pixi_data[\"environments\"]\n        pixi_data[\"environments\"] = {\n            key: value\n            for key, value in current_environments.items()\n            if key == pixi_environment\n        }\n        if pixi_data[\"environments\"] == {}:\n            raise ValueError(\n                f\"No '{pixi_environment}' pixi environment found.\"\n            )\n    except KeyError:\n        logger.info(\"KeyError for workspace/platforms - skip.\")\n\n    # Drop pixi.tasks\n    pixi_data.pop(\"tasks\", None)\n\n    # Prepare and validate new `pyprojectl.toml` contents\n    data[\"tool\"][\"pixi\"] = pixi_data\n    new_toml_string = tomli_w.dumps(data)\n    tomllib.loads(new_toml_string)\n\n    return new_toml_string\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/utils_python_interpreter/","title":"utils_python_interpreter","text":""},{"location":"reference/fractal_server/tasks/v2/utils_python_interpreter/#fractal_server.tasks.v2.utils_python_interpreter.get_python_interpreter","title":"<code>get_python_interpreter(python_version, resource)</code>","text":"<p>Return the path to the Python interpreter</p> <p>Parameters:</p> Name Type Description Default <code>python_version</code> <code>str</code> <p>Python version</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the python version requested is not available on the         host.</p> <p>Returns:</p> Name Type Description <code>interpreter</code> <code>str</code> <p>string representing the python executable or its path</p> Source code in <code>fractal_server/tasks/v2/utils_python_interpreter.py</code> <pre><code>def get_python_interpreter(\n    python_version: str,\n    resource: Resource,\n) -&gt; str:\n    \"\"\"\n    Return the path to the Python interpreter\n\n    Args:\n        python_version: Python version\n\n    Raises:\n        ValueError: If the python version requested is not available on the\n                    host.\n\n    Returns:\n        interpreter: string representing the python executable or its path\n    \"\"\"\n\n    python_path = resource.tasks_python_config[\"versions\"].get(python_version)\n    if python_path is None:\n        raise ValueError(f\"Requested {python_version=} is not available.\")\n    return python_path\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/utils_templates/","title":"utils_templates","text":""},{"location":"reference/fractal_server/tasks/v2/utils_templates/#fractal_server.tasks.v2.utils_templates.customize_template","title":"<code>customize_template(*, template_name, replacements, script_path)</code>","text":"<p>Customize a bash-script template and write it to disk.</p> <p>Parameters:</p> Name Type Description Default <code>template_name</code> <code>str</code> <p>Name of the template that will be customized.</p> required <code>replacements</code> <code>set[tuple[str, str]]</code> <p>List of replacements for template customization.</p> required <code>script_path</code> <code>str</code> <p>Local path where the customized template will be written.</p> required Source code in <code>fractal_server/tasks/v2/utils_templates.py</code> <pre><code>def customize_template(\n    *,\n    template_name: str,\n    replacements: set[tuple[str, str]],\n    script_path: str,\n) -&gt; None:\n    \"\"\"\n    Customize a bash-script template and write it to disk.\n\n    Args:\n        template_name: Name of the template that will be customized.\n        replacements: List of replacements for template customization.\n        script_path: Local path where the customized template will be written.\n    \"\"\"\n    _check_pixi_frozen_option(replacements=replacements)\n\n    # Read template\n    template_path = TEMPLATES_DIR / template_name\n    with template_path.open(\"r\") as f:\n        template_data = f.read()\n    # Customize template\n    script_data = template_data\n    for old_new in replacements:\n        script_data = script_data.replace(old_new[0], old_new[1])\n    # Create parent folder if needed\n    Path(script_path).parent.mkdir(exist_ok=True)\n    # Write script locally\n    with open(script_path, \"w\") as f:\n        f.write(script_data)\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/utils_templates/#fractal_server.tasks.v2.utils_templates.parse_script_pip_show_stdout","title":"<code>parse_script_pip_show_stdout(stdout)</code>","text":"<p>Parse standard output of 4_pip_show.sh</p> Source code in <code>fractal_server/tasks/v2/utils_templates.py</code> <pre><code>def parse_script_pip_show_stdout(stdout: str) -&gt; dict[str, str]:\n    \"\"\"\n    Parse standard output of 4_pip_show.sh\n    \"\"\"\n    searches = [\n        (\"Python interpreter:\", \"python_bin\"),\n        (\"Package name:\", \"package_name\"),\n        (\"Package version:\", \"package_version\"),\n        (\"Package parent folder:\", \"package_root_parent\"),\n        (\"Manifest absolute path:\", \"manifest_path\"),\n    ]\n    stdout_lines = stdout.splitlines()\n    attributes = dict()\n    for search, attribute_name in searches:\n        matching_lines = [_line for _line in stdout_lines if search in _line]\n        if len(matching_lines) == 0:\n            raise ValueError(f\"String '{search}' not found in stdout.\")\n        elif len(matching_lines) &gt; 1:\n            raise ValueError(\n                f\"String '{search}' found too many times \"\n                f\"({len(matching_lines)}).\"\n            )\n        else:\n            actual_line = matching_lines[0]\n            attribute_value = actual_line.split(search)[-1].strip(\" \")\n            attributes[attribute_name] = attribute_value\n    return attributes\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/local/","title":"local","text":""},{"location":"reference/fractal_server/tasks/v2/local/_utils/","title":"_utils","text":""},{"location":"reference/fractal_server/tasks/v2/local/_utils/#fractal_server.tasks.v2.local._utils._customize_and_run_template","title":"<code>_customize_and_run_template(template_filename, replacements, script_dir, logger_name, prefix)</code>","text":"<p>Customize one of the template bash scripts.</p> <p>Parameters:</p> Name Type Description Default <code>template_filename</code> <code>str</code> <p>Filename of the template file (ends with \".sh\").</p> required <code>replacements</code> <code>list[tuple[str, str]]</code> <p>Dictionary of replacements.</p> required <code>script_dir</code> <code>str</code> <p>Local folder where the script will be placed.</p> required <code>prefix</code> <code>int</code> <p>Prefix for the script filename.</p> required Source code in <code>fractal_server/tasks/v2/local/_utils.py</code> <pre><code>def _customize_and_run_template(\n    template_filename: str,\n    replacements: list[tuple[str, str]],\n    script_dir: str,\n    logger_name: str,\n    prefix: int,\n) -&gt; str:\n    \"\"\"\n    Customize one of the template bash scripts.\n\n    Args:\n        template_filename: Filename of the template file (ends with \".sh\").\n        replacements: Dictionary of replacements.\n        script_dir: Local folder where the script will be placed.\n        prefix: Prefix for the script filename.\n    \"\"\"\n    logger = get_logger(logger_name=logger_name)\n    logger.debug(f\"_customize_and_run_template {template_filename} - START\")\n\n    # Prepare name and path of script\n    if not template_filename.endswith(\".sh\"):\n        raise ValueError(\n            f\"Invalid {template_filename=} (it must end with '.sh').\"\n        )\n\n    script_filename = f\"{prefix}_{template_filename}\"\n    script_path_local = Path(script_dir) / script_filename\n    # Read template\n    customize_template(\n        template_name=template_filename,\n        replacements=replacements,\n        script_path=script_path_local,\n    )\n    cmd = f\"bash {script_path_local}\"\n    logger.debug(f\"Now run '{cmd}' \")\n    stdout = execute_command_sync(command=cmd, logger_name=logger_name)\n    logger.debug(f\"_customize_and_run_template {template_filename} - END\")\n    return stdout\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/local/_utils/#fractal_server.tasks.v2.local._utils.check_task_files_exist","title":"<code>check_task_files_exist(task_list)</code>","text":"<p>Check that the modules listed in task commands point to existing files.</p> <p>Parameters:</p> Name Type Description Default <code>task_list</code> <code>list[TaskCreateV2]</code> required Source code in <code>fractal_server/tasks/v2/local/_utils.py</code> <pre><code>def check_task_files_exist(task_list: list[TaskCreateV2]) -&gt; None:\n    \"\"\"\n    Check that the modules listed in task commands point to existing files.\n\n    Args:\n        task_list:\n    \"\"\"\n\n    for _task in task_list:\n        if _task.command_non_parallel is not None:\n            _task_path = _task.command_non_parallel.split()[-1]\n            if not Path(_task_path).exists():\n                raise FileNotFoundError(\n                    f\"Task `{_task.name}` has `command_non_parallel` \"\n                    f\"pointing to missing file `{_task_path}`.\"\n                )\n        if _task.command_parallel is not None:\n            _task_path = _task.command_parallel.split()[-1]\n            if not Path(_task_path).exists():\n                raise FileNotFoundError(\n                    f\"Task `{_task.name}` has `command_parallel` \"\n                    f\"pointing to missing file `{_task_path}`.\"\n                )\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/local/_utils/#fractal_server.tasks.v2.local._utils.edit_pyproject_toml_in_place_local","title":"<code>edit_pyproject_toml_in_place_local(pyproject_toml_path, resource)</code>","text":"<p>Wrapper of <code>simplify_pyproject_toml</code>, with I/O.</p> Source code in <code>fractal_server/tasks/v2/local/_utils.py</code> <pre><code>def edit_pyproject_toml_in_place_local(\n    pyproject_toml_path: Path,\n    resource: Resource,\n) -&gt; None:\n    \"\"\"\n    Wrapper of `simplify_pyproject_toml`, with I/O.\n    \"\"\"\n\n    # Read `pyproject.toml`\n    with pyproject_toml_path.open() as f:\n        pyproject_contents = f.read()\n\n    # Simplify contents\n    new_pyproject_contents = simplify_pyproject_toml(\n        original_toml_string=pyproject_contents,\n        pixi_environment=resource.tasks_pixi_config[\"DEFAULT_ENVIRONMENT\"],\n        pixi_platform=resource.tasks_pixi_config[\"DEFAULT_PLATFORM\"],\n    )\n    # Write new `pyproject.toml`\n    with pyproject_toml_path.open(\"w\") as f:\n        f.write(new_pyproject_contents)\n    logger.debug(\n        f\"Replaced local {pyproject_toml_path.as_posix()} \"\n        \"with simplified version.\"\n    )\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/local/collect/","title":"collect","text":""},{"location":"reference/fractal_server/tasks/v2/local/collect/#fractal_server.tasks.v2.local.collect.collect_local","title":"<code>collect_local(*, task_group_activity_id, task_group_id, resource, profile, wheel_file=None)</code>","text":"<p>Collect a task package.</p> <p>This function runs as a background task, therefore exceptions must be handled.</p> <p>NOTE:  since this function is sync, it runs within a thread - due to starlette/fastapi handling of background tasks (see https://github.com/encode/starlette/blob/master/starlette/background.py).</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required <code>resource</code> <code>Resource</code> <p>Resource</p> required <code>wheel_file</code> <code>FractalUploadedFile | None</code> <code>None</code> Source code in <code>fractal_server/tasks/v2/local/collect.py</code> <pre><code>def collect_local(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n    wheel_file: FractalUploadedFile | None = None,\n) -&gt; None:\n    \"\"\"\n    Collect a task package.\n\n    This function runs as a background task, therefore exceptions must be\n    handled.\n\n    NOTE:  since this function is sync, it runs within a thread - due to\n    starlette/fastapi handling of background tasks (see\n    https://github.com/encode/starlette/blob/master/starlette/background.py).\n\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource: Resource\n        wheel_file:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.info(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            # Check that the (local) task_group path does exist\n            if Path(task_group.path).exists():\n                error_msg = f\"{task_group.path} already exists.\"\n                logger.error(error_msg)\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=FileExistsError(error_msg),\n                    db=db,\n                )\n                return\n\n            try:\n                # Create task_group.path folder\n                Path(task_group.path).mkdir(parents=True)\n                logger.info(f\"Created {task_group.path}\")\n\n                # Write wheel file and set task_group.archive_path\n                if wheel_file is not None:\n                    archive_path = (\n                        Path(task_group.path) / wheel_file.filename\n                    ).as_posix()\n                    logger.info(\n                        f\"Write wheel-file contents into {archive_path}\"\n                    )\n                    with open(archive_path, \"wb\") as f:\n                        f.write(wheel_file.contents)\n                    task_group.archive_path = archive_path\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n\n                # Prepare replacements for templates\n                python_bin = get_python_interpreter(\n                    python_version=task_group.python_version,\n                    resource=resource,\n                )\n                replacements = get_collection_replacements(\n                    task_group=task_group,\n                    python_bin=python_bin,\n                    resource=resource,\n                )\n\n                # Prepare common arguments for `_customize_and_run_template``\n                common_args = dict(\n                    replacements=replacements,\n                    script_dir=(\n                        Path(task_group.path) / SCRIPTS_SUBFOLDER\n                    ).as_posix(),\n                    prefix=(\n                        f\"{int(time.time())}_\"\n                        f\"{TaskGroupActivityActionV2.COLLECT}\"\n                    ),\n                    logger_name=LOGGER_NAME,\n                )\n\n                # Set status to ONGOING and refresh logs\n                activity.status = TaskGroupActivityStatusV2.ONGOING\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 1\n                stdout = _customize_and_run_template(\n                    template_filename=\"1_create_venv.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 2\n                stdout = _customize_and_run_template(\n                    template_filename=\"2_pip_install.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 3\n                pip_freeze_stdout = _customize_and_run_template(\n                    template_filename=\"3_pip_freeze.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 4\n                stdout = _customize_and_run_template(\n                    template_filename=\"4_pip_show.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 5\n                venv_info = _customize_and_run_template(\n                    template_filename=\"5_get_venv_size_and_file_number.sh\",\n                    **common_args,\n                )\n                venv_size, venv_file_number = venv_info.split()\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                pkg_attrs = parse_script_pip_show_stdout(stdout)\n                for key, value in pkg_attrs.items():\n                    logger.debug(f\"Parsed from pip-show: {key}={value}\")\n                # Check package_name match between pip show and task-group\n                task_group = db.get(TaskGroupV2, task_group_id)\n                package_name_pip_show = pkg_attrs.get(\"package_name\")\n                package_name_task_group = task_group.pkg_name\n                compare_package_names(\n                    pkg_name_pip_show=package_name_pip_show,\n                    pkg_name_task_group=package_name_task_group,\n                    logger_name=LOGGER_NAME,\n                )\n                # Extract/drop parsed attributes\n                package_name = package_name_task_group\n                python_bin = pkg_attrs.pop(\"python_bin\")\n                package_root_parent = pkg_attrs.pop(\"package_root_parent\")\n\n                # TODO : Use more robust logic to determine `package_root`.\n                # Examples: use `importlib.util.find_spec`, or parse the\n                # output of `pip show --files {package_name}`.\n                package_name_underscore = package_name.replace(\"-\", \"_\")\n                package_root = (\n                    Path(package_root_parent) / package_name_underscore\n                ).as_posix()\n\n                # Read and validate manifest file\n                manifest_path = pkg_attrs.pop(\"manifest_path\")\n                logger.info(f\"now loading {manifest_path=}\")\n                with open(manifest_path) as json_data:\n                    pkg_manifest_dict = json.load(json_data)\n                logger.info(f\"loaded {manifest_path=}\")\n                logger.info(\"now validating manifest content\")\n                pkg_manifest = ManifestV2(**pkg_manifest_dict)\n                logger.info(\"validated manifest content\")\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                logger.info(\"_prepare_tasks_metadata - start\")\n                task_list = prepare_tasks_metadata(\n                    package_manifest=pkg_manifest,\n                    package_version=task_group.version,\n                    package_root=Path(package_root),\n                    python_bin=Path(python_bin),\n                )\n                check_task_files_exist(task_list=task_list)\n                logger.info(\"_prepare_tasks_metadata - end\")\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                logger.info(\"create_db_tasks_and_update_task_group - \" \"start\")\n                create_db_tasks_and_update_task_group_sync(\n                    task_list=task_list,\n                    task_group_id=task_group.id,\n                    db=db,\n                )\n                logger.info(\"create_db_tasks_and_update_task_group - end\")\n\n                # Update task_group data\n                logger.info(\n                    \"Add env_info, venv_size and venv_file_number \"\n                    \"to TaskGroupV2 - start\"\n                )\n                task_group.env_info = pip_freeze_stdout\n                task_group.venv_size_in_kB = int(venv_size)\n                task_group.venv_file_number = int(venv_file_number)\n                task_group = add_commit_refresh(obj=task_group, db=db)\n                logger.info(\n                    \"Add env_info, venv_size and venv_file_number \"\n                    \"to TaskGroupV2 - end\"\n                )\n\n                # Finalize (write metadata to DB)\n                logger.info(\"finalising - START\")\n                activity.status = TaskGroupActivityStatusV2.OK\n                activity.timestamp_ended = get_timestamp()\n                activity = add_commit_refresh(obj=activity, db=db)\n                logger.info(\"finalising - END\")\n                logger.info(\"END\")\n\n                reset_logger_handlers(logger)\n\n            except Exception as collection_e:\n                # Delete corrupted package dir\n                try:\n                    logger.info(f\"Now delete folder {task_group.path}\")\n                    shutil.rmtree(task_group.path)\n                    logger.info(f\"Deleted folder {task_group.path}\")\n                except Exception as rm_e:\n                    logger.error(\n                        \"Removing folder failed.\\n\"\n                        f\"Original error:\\n{str(rm_e)}\"\n                    )\n\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=collection_e,\n                    db=db,\n                )\n        return\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/local/collect_pixi/","title":"collect_pixi","text":""},{"location":"reference/fractal_server/tasks/v2/local/deactivate/","title":"deactivate","text":""},{"location":"reference/fractal_server/tasks/v2/local/deactivate/#fractal_server.tasks.v2.local.deactivate.deactivate_local","title":"<code>deactivate_local(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Deactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required <code>resource</code> <code>Resource</code> required Source code in <code>fractal_server/tasks/v2/local/deactivate.py</code> <pre><code>def deactivate_local(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Deactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            # Check that the (local) task_group venv_path does exist\n            if not Path(task_group.venv_path).exists():\n                error_msg = f\"{task_group.venv_path} does not exist.\"\n                logger.error(error_msg)\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=FileNotFoundError(error_msg),\n                    db=db,\n                )\n                return\n\n            try:\n                activity.status = TaskGroupActivityStatusV2.ONGOING\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                if task_group.env_info is None:\n                    logger.warning(\n                        \"Recreate pip-freeze information, since \"\n                        f\"{task_group.env_info=}. NOTE: this should only \"\n                        \"happen for task groups created before 2.9.0.\"\n                    )\n                    # Prepare replacements for templates\n                    replacements = get_collection_replacements(\n                        task_group=task_group,\n                        python_bin=\"/not/applicable\",\n                        resource=resource,\n                    )\n\n                    # Prepare common arguments for _customize_and_run_template\n                    common_args = dict(\n                        replacements=replacements,\n                        script_dir=(\n                            Path(task_group.path) / SCRIPTS_SUBFOLDER\n                        ).as_posix(),\n                        prefix=(\n                            f\"{int(time.time())}_\"\n                            f\"{TaskGroupActivityActionV2.DEACTIVATE}\"\n                        ),\n                        logger_name=LOGGER_NAME,\n                    )\n\n                    # Update pip-freeze data\n                    pip_freeze_stdout = _customize_and_run_template(\n                        template_filename=\"3_pip_freeze.sh\",\n                        **common_args,\n                    )\n                    logger.info(\"Add pip freeze stdout to TaskGroupV2 - start\")\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n                    task_group.env_info = pip_freeze_stdout\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n                    logger.info(\"Add pip freeze stdout to TaskGroupV2 - end\")\n\n                # Handle some specific cases for wheel-file case\n                if task_group.origin == TaskGroupV2OriginEnum.WHEELFILE:\n                    logger.info(\n                        f\"Handle specific cases for {task_group.origin=}.\"\n                    )\n\n                    # Blocking situation: `archive_path` is not set or points\n                    # to a missing path\n                    if (\n                        task_group.archive_path is None\n                        or not Path(task_group.archive_path).exists()\n                    ):\n                        error_msg = (\n                            \"Invalid wheel path for task group with \"\n                            f\"{task_group_id=}. {task_group.archive_path=} is \"\n                            \"unset or does not exist.\"\n                        )\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileNotFoundError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    # Recoverable situation: `archive_path` was not yet copied\n                    # over to the correct server-side folder\n                    archive_path_parent_dir = Path(\n                        task_group.archive_path\n                    ).parent\n                    if archive_path_parent_dir != Path(task_group.path):\n                        logger.warning(\n                            f\"{archive_path_parent_dir.as_posix()} differs \"\n                            f\"from {task_group.path}. NOTE: this should only \"\n                            \"happen for task groups created before 2.9.0.\"\n                        )\n\n                        if task_group.archive_path not in task_group.env_info:\n                            raise ValueError(\n                                f\"Cannot find {task_group.archive_path=} in \"\n                                \"pip-freeze data. Exit.\"\n                            )\n\n                        logger.info(\n                            f\"Now copy wheel file into {task_group.path}.\"\n                        )\n                        new_archive_path = (\n                            Path(task_group.path)\n                            / Path(task_group.archive_path).name\n                        ).as_posix()\n                        shutil.copy(task_group.archive_path, new_archive_path)\n                        logger.info(\n                            f\"Copied wheel file to {new_archive_path}.\"\n                        )\n\n                        task_group.archive_path = new_archive_path\n                        new_pip_freeze = task_group.env_info.replace(\n                            task_group.archive_path,\n                            new_archive_path,\n                        )\n                        task_group.env_info = new_pip_freeze\n                        task_group = add_commit_refresh(obj=task_group, db=db)\n                        logger.info(\n                            \"Updated `archive_path` and `env_info` \"\n                            \"task-group attributes.\"\n                        )\n\n                # Fail if `pip_freeze` includes \"github.com\", see\n                # https://github.com/fractal-analytics-platform/fractal-server/issues/2142\n                for forbidden_string in FORBIDDEN_DEPENDENCY_STRINGS:\n                    if forbidden_string in task_group.env_info:\n                        raise ValueError(\n                            \"Deactivation and reactivation of task packages \"\n                            f\"with direct {forbidden_string} dependencies \"\n                            \"are not currently supported. Exit.\"\n                        )\n\n                # We now have all required information for reactivating the\n                # virtual environment at a later point\n\n                # Actually mark the task group as non-active\n                logger.info(\"Now setting `active=False`.\")\n                task_group.active = False\n                task_group = add_commit_refresh(obj=task_group, db=db)\n\n                # Proceed with deactivation\n                logger.info(f\"Now removing {task_group.venv_path}.\")\n                shutil.rmtree(task_group.venv_path)\n                logger.info(f\"All good, {task_group.venv_path} removed.\")\n                activity.status = TaskGroupActivityStatusV2.OK\n                activity.log = get_current_log(log_file_path)\n                activity.timestamp_ended = get_timestamp()\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                reset_logger_handlers(logger)\n\n            except Exception as e:\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=e,\n                    db=db,\n                )\n        return\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/local/deactivate_pixi/","title":"deactivate_pixi","text":""},{"location":"reference/fractal_server/tasks/v2/local/deactivate_pixi/#fractal_server.tasks.v2.local.deactivate_pixi.deactivate_local_pixi","title":"<code>deactivate_local_pixi(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Deactivate a pixi task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required Source code in <code>fractal_server/tasks/v2/local/deactivate_pixi.py</code> <pre><code>def deactivate_local_pixi(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Deactivate a pixi task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            source_dir = Path(task_group.path, SOURCE_DIR_NAME)\n            if not source_dir.exists():\n                error_msg = f\"'{source_dir.as_posix()}' does not exist.\"\n                logger.error(error_msg)\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=FileNotFoundError(error_msg),\n                    db=db,\n                )\n                return\n\n            try:\n                activity.status = TaskGroupActivityStatusV2.ONGOING\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Actually mark the task group as non-active\n                logger.info(\"Now setting `active=False`.\")\n                task_group.active = False\n                task_group = add_commit_refresh(obj=task_group, db=db)\n\n                # Proceed with deactivation\n                logger.info(f\"Now removing '{source_dir.as_posix()}'.\")\n                shutil.rmtree(source_dir)\n                logger.info(f\"All good, '{source_dir.as_posix()}' removed.\")\n                activity.status = TaskGroupActivityStatusV2.OK\n                activity.log = get_current_log(log_file_path)\n                activity.timestamp_ended = get_timestamp()\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                reset_logger_handlers(logger)\n\n            except Exception as e:\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=e,\n                    db=db,\n                )\n        return\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/local/delete/","title":"delete","text":""},{"location":"reference/fractal_server/tasks/v2/local/reactivate/","title":"reactivate","text":""},{"location":"reference/fractal_server/tasks/v2/local/reactivate/#fractal_server.tasks.v2.local.reactivate.reactivate_local","title":"<code>reactivate_local(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Reactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required <code>resource</code> <code>Resource</code> required Source code in <code>fractal_server/tasks/v2/local/reactivate.py</code> <pre><code>def reactivate_local(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Reactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            # Check that the (local) task_group venv_path does not exist\n            if Path(task_group.venv_path).exists():\n                error_msg = f\"{task_group.venv_path} already exists.\"\n                logger.error(error_msg)\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=FileExistsError(error_msg),\n                    db=db,\n                )\n                return\n\n            try:\n                activity.status = TaskGroupActivityStatusV2.ONGOING\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Prepare replacements for templates\n                python_bin = get_python_interpreter(\n                    python_version=task_group.python_version,\n                    resource=resource,\n                )\n                replacements = get_collection_replacements(\n                    task_group=task_group,\n                    python_bin=python_bin,\n                    resource=resource,\n                )\n                with open(f\"{tmpdir}/pip_freeze.txt\", \"w\") as f:\n                    f.write(task_group.env_info)\n                replacements.append(\n                    (\"__PIP_FREEZE_FILE__\", f\"{tmpdir}/pip_freeze.txt\")\n                )\n                # Prepare common arguments for `_customize_and_run_template`\n                common_args = dict(\n                    replacements=replacements,\n                    script_dir=(\n                        Path(task_group.path) / SCRIPTS_SUBFOLDER\n                    ).as_posix(),\n                    prefix=(\n                        f\"{int(time.time())}_\"\n                        f\"{TaskGroupActivityActionV2.REACTIVATE}\"\n                    ),\n                    logger_name=LOGGER_NAME,\n                )\n\n                logger.debug(\"start - create venv\")\n                _customize_and_run_template(\n                    template_filename=\"1_create_venv.sh\",\n                    **common_args,\n                )\n                logger.debug(\"end - create venv\")\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                logger.debug(\"start - install from pip freeze\")\n                _customize_and_run_template(\n                    template_filename=\"6_pip_install_from_freeze.sh\",\n                    **common_args,\n                )\n                logger.debug(\"end - install from pip freeze\")\n                activity.log = get_current_log(log_file_path)\n                activity.status = TaskGroupActivityStatusV2.OK\n                activity.timestamp_ended = get_timestamp()\n                activity = add_commit_refresh(obj=activity, db=db)\n                task_group.active = True\n                task_group = add_commit_refresh(obj=task_group, db=db)\n                logger.debug(\"END\")\n\n                reset_logger_handlers(logger)\n\n            except Exception as reactivate_e:\n                # Delete corrupted venv_path\n                try:\n                    logger.info(f\"Now delete folder {task_group.venv_path}\")\n                    shutil.rmtree(task_group.venv_path)\n                    logger.info(f\"Deleted folder {task_group.venv_path}\")\n                except Exception as rm_e:\n                    logger.error(\n                        \"Removing folder failed.\\n\"\n                        f\"Original error:\\n{str(rm_e)}\"\n                    )\n\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=reactivate_e,\n                    db=db,\n                )\n        return\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/local/reactivate_pixi/","title":"reactivate_pixi","text":""},{"location":"reference/fractal_server/tasks/v2/local/reactivate_pixi/#fractal_server.tasks.v2.local.reactivate_pixi.reactivate_local_pixi","title":"<code>reactivate_local_pixi(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Reactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required <code>resource</code> <code>Resource</code> required Source code in <code>fractal_server/tasks/v2/local/reactivate_pixi.py</code> <pre><code>def reactivate_local_pixi(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Reactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            source_dir = Path(task_group.path, SOURCE_DIR_NAME).as_posix()\n            if Path(source_dir).exists():\n                error_msg = f\"{source_dir} already exists.\"\n                logger.error(error_msg)\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=FileExistsError(error_msg),\n                    db=db,\n                )\n                return\n\n            try:\n                activity.status = TaskGroupActivityStatusV2.ONGOING\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                common_args = dict(\n                    replacements={\n                        (\n                            \"__PIXI_HOME__\",\n                            resource.tasks_pixi_config[\"versions\"][\n                                task_group.pixi_version\n                            ],\n                        ),\n                        (\"__PACKAGE_DIR__\", task_group.path),\n                        (\"__TAR_GZ_PATH__\", task_group.archive_path),\n                        (\n                            \"__IMPORT_PACKAGE_NAME__\",\n                            task_group.pkg_name.replace(\"-\", \"_\"),\n                        ),\n                        (\"__SOURCE_DIR_NAME__\", SOURCE_DIR_NAME),\n                        (\"__FROZEN_OPTION__\", \"--frozen\"),\n                        (\n                            \"__TOKIO_WORKER_THREADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"TOKIO_WORKER_THREADS\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_SOLVES__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_SOLVES\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_DOWNLOADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_DOWNLOADS\"\n                                ]\n                            ),\n                        ),\n                    },\n                    script_dir=Path(\n                        task_group.path, SCRIPTS_SUBFOLDER\n                    ).as_posix(),\n                    prefix=(\n                        f\"{int(time.time())}_\"\n                        f\"{TaskGroupActivityActionV2.REACTIVATE}\"\n                    ),\n                    logger_name=LOGGER_NAME,\n                )\n\n                # Run script 1 - extract tar.gz into `source_dir`\n                _customize_and_run_template(\n                    template_filename=\"pixi_1_extract.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Simplify `pyproject.toml`\n                pyproject_toml_path = Path(source_dir, \"pyproject.toml\")\n                edit_pyproject_toml_in_place_local(\n                    pyproject_toml_path, resource=resource\n                )\n\n                # Write pixi.lock into `source_dir`\n                logger.debug(f\"start - writing {source_dir}/pixi.lock\")\n                with Path(source_dir, \"pixi.lock\").open(\"w\") as f:\n                    f.write(task_group.env_info)\n                logger.debug(f\"end - writing {source_dir}/pixi.lock\")\n\n                # Run script 2 - run pixi-install command\n                _customize_and_run_template(\n                    template_filename=\"pixi_2_install.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 3 - post-install\n                _customize_and_run_template(\n                    template_filename=\"pixi_3_post_install.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Make task folder 755\n                source_dir = Path(task_group.path, SOURCE_DIR_NAME).as_posix()\n                command = f\"chmod -R 755 {source_dir}\"\n                execute_command_sync(\n                    command=command,\n                    logger_name=LOGGER_NAME,\n                )\n\n                activity.log = get_current_log(log_file_path)\n                activity.status = TaskGroupActivityStatusV2.OK\n                activity.timestamp_ended = get_timestamp()\n                activity = add_commit_refresh(obj=activity, db=db)\n                task_group.active = True\n                task_group = add_commit_refresh(obj=task_group, db=db)\n                logger.debug(\"END\")\n\n                reset_logger_handlers(logger)\n\n            except Exception as reactivate_e:\n                # Delete corrupted source_dir\n                try:\n                    logger.info(f\"Now delete folder {source_dir}\")\n                    shutil.rmtree(source_dir)\n                    logger.info(f\"Deleted folder {source_dir}\")\n                except Exception as rm_e:\n                    logger.error(\n                        \"Removing folder failed. \"\n                        f\"Original error: {str(rm_e)}\"\n                    )\n\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=reactivate_e,\n                    db=db,\n                )\n        return\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/","title":"ssh","text":""},{"location":"reference/fractal_server/tasks/v2/ssh/_pixi_slurm_ssh/","title":"_pixi_slurm_ssh","text":""},{"location":"reference/fractal_server/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh._get_workdir_remote","title":"<code>_get_workdir_remote(script_paths)</code>","text":"<p>Check that there is one and only one <code>workdir</code>, and return it.</p> <p>Note: The <code>is_absolute</code> check is to filter out a <code>chmod</code> command.</p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def _get_workdir_remote(script_paths: list[str]) -&gt; str:\n    \"\"\"\n    Check that there is one and only one `workdir`, and return it.\n\n    Note: The `is_absolute` check is to filter out a `chmod` command.\n    \"\"\"\n    workdirs = [\n        Path(script_path).parent.as_posix()\n        for script_path in script_paths\n        if Path(script_path).is_absolute()\n    ]\n    if not len(set(workdirs)) == 1:\n        raise ValueError(f\"Invalid {script_paths=}.\")\n    return workdirs[0]\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh._log_change_of_job_state","title":"<code>_log_change_of_job_state(*, old_state, new_state, logger_name)</code>","text":"<p>Emit a log for state changes.</p> <p>Parameters:</p> Name Type Description Default <code>old_state</code> <code>str | None</code> required <code>new_state</code> <code>str</code> required <code>logger_name</code> <code>str</code> required Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def _log_change_of_job_state(\n    *,\n    old_state: str | None,\n    new_state: str,\n    logger_name: str,\n) -&gt; None:\n    \"\"\"\n    Emit a log for state changes.\n\n    Args:\n        old_state:\n        new_state:\n        logger_name:\n    \"\"\"\n    if new_state != old_state:\n        logger = get_logger(logger_name=logger_name)\n        logger.debug(\n            f\"SLURM-job state changed from {old_state=} to {new_state=}.\"\n        )\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh._read_file_if_exists","title":"<code>_read_file_if_exists(*, fractal_ssh, path)</code>","text":"<p>Read a remote file if it exists, or return an empty string.</p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def _read_file_if_exists(\n    *,\n    fractal_ssh: FractalSSH,\n    path: str,\n) -&gt; str:\n    \"\"\"\n    Read a remote file if it exists, or return an empty string.\n    \"\"\"\n    if fractal_ssh.remote_exists(path=path):\n        return fractal_ssh.read_remote_text_file(path)\n    else:\n        return \"\"\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh._run_squeue","title":"<code>_run_squeue(*, fractal_ssh, squeue_cmd, logger_name)</code>","text":"<p>Run a <code>squeue</code> command and handle exceptions.</p> <p>Parameters:</p> Name Type Description Default <code>fractal_ssh</code> <code>FractalSSH</code> required <code>logger_name</code> <code>str</code> required <code>squeue_cmd</code> <code>str</code> required Return <p>state: The SLURM-job state.</p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def _run_squeue(\n    *,\n    fractal_ssh: FractalSSH,\n    squeue_cmd: str,\n    logger_name: str,\n) -&gt; str:\n    \"\"\"\n    Run a `squeue` command and handle exceptions.\n\n    Args:\n        fractal_ssh:\n        logger_name:\n        squeue_cmd:\n\n    Return:\n        state: The SLURM-job state.\n    \"\"\"\n    try:\n        cmd_stdout = fractal_ssh.run_command(cmd=squeue_cmd)\n        state = cmd_stdout.strip().split()[1]\n        return state\n    except Exception as e:\n        logger = get_logger(logger_name=logger_name)\n        logger.info(f\"`squeue` command failed (original error: {e})\")\n        return FRACTAL_SQUEUE_ERROR_STATE\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh._verify_success_file_exists","title":"<code>_verify_success_file_exists(*, fractal_ssh, success_file_remote, logger_name, stderr_remote)</code>","text":"<p>Fail if the success sentinel file does not exist remotely.</p> <p>Note: the <code>FractalSSH</code> methods in this function may fail, and such failures are not handled in this function. Any such failure, however, will lead to a \"failed\" task-group lifecycle activity (because it will raise an exception from within <code>run_script_on_remote_slurm</code>, which will then be handled at the calling-function level.</p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def _verify_success_file_exists(\n    *,\n    fractal_ssh: FractalSSH,\n    success_file_remote: str,\n    logger_name: str,\n    stderr_remote: str,\n) -&gt; None:\n    \"\"\"\n    Fail if the success sentinel file does not exist remotely.\n\n    Note: the `FractalSSH` methods in this function may fail, and such failures\n    are not handled in this function. Any such failure, however, will lead to\n    a \"failed\" task-group lifecycle activity (because it will raise an\n    exception from within `run_script_on_remote_slurm`, which will then be\n    handled at the calling-function level.\n    \"\"\"\n    if not fractal_ssh.remote_exists(path=success_file_remote):\n        logger = get_logger(logger_name=logger_name)\n        error_msg = f\"{success_file_remote=} missing.\"\n        logger.info(error_msg)\n\n        stderr = _read_file_if_exists(\n            fractal_ssh=fractal_ssh, path=stderr_remote\n        )\n        if stderr:\n            logger.info(f\"SLURM-job stderr:\\n{stderr}\")\n        raise RuntimeError(error_msg)\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh.run_script_on_remote_slurm","title":"<code>run_script_on_remote_slurm(*, script_paths, slurm_config, fractal_ssh, logger_name, log_file_path, prefix, db, activity, poll_interval)</code>","text":"<p>Run a <code>pixi install</code> script as a SLURM job.</p> <p>NOTE: This is called from within a try/except, thus we can use exceptions as a mechanism to propagate failure/errors.</p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def run_script_on_remote_slurm(\n    *,\n    script_paths: list[str],\n    slurm_config: dict[str, Any],\n    fractal_ssh: FractalSSH,\n    logger_name: str,\n    log_file_path: Path,\n    prefix: str,\n    db: Session,\n    activity: TaskGroupActivityV2,\n    poll_interval: int,\n):\n    \"\"\"\n    Run a `pixi install` script as a SLURM job.\n\n    NOTE: This is called from within a try/except, thus we can use exceptions\n    as a mechanism to propagate failure/errors.\n    \"\"\"\n\n    slurm_config_obj = PixiSLURMConfig(**slurm_config)\n\n    logger = get_logger(logger_name=logger_name)\n\n    # (1) Prepare remote submission script\n    workdir_remote = _get_workdir_remote(script_paths)\n    submission_script_remote = os.path.join(\n        workdir_remote, f\"{prefix}-submit.sh\"\n    )\n    stderr_remote = os.path.join(workdir_remote, f\"{prefix}-err.txt\")\n    stdout_remote = os.path.join(workdir_remote, f\"{prefix}-out.txt\")\n    success_file_remote = os.path.join(workdir_remote, f\"{prefix}-success.txt\")\n    script_lines = [\n        \"#!/bin/bash\",\n        f\"#SBATCH --partition={slurm_config_obj.partition}\",\n        f\"#SBATCH --cpus-per-task={slurm_config_obj.cpus}\",\n        f\"#SBATCH --mem={slurm_config_obj.mem}\",\n        f\"#SBATCH --time={slurm_config_obj.time}\",\n        f\"#SBATCH --err={stderr_remote}\",\n        f\"#SBATCH --out={stdout_remote}\",\n        f\"#SBATCH -D {workdir_remote}\",\n        \"\",\n    ]\n    for script_path in script_paths:\n        script_lines.append(f\"bash {script_path}\")\n    script_lines.append(f\"touch {success_file_remote}\")\n\n    script_contents = \"\\n\".join(script_lines)\n    fractal_ssh.write_remote_file(\n        path=submission_script_remote,\n        content=script_contents,\n    )\n    logger.debug(f\"Written {submission_script_remote=}.\")\n\n    activity.log = get_current_log(log_file_path)\n    activity = add_commit_refresh(obj=activity, db=db)\n\n    # (2) Submit SLURM job\n    logger.debug(\"Now submit SLURM job.\")\n    sbatch_cmd = f\"sbatch --parsable {submission_script_remote}\"\n    try:\n        stdout = fractal_ssh.run_command(cmd=sbatch_cmd)\n        job_id = int(stdout)\n        logger.debug(f\"SLURM-job submission successful ({job_id=}).\")\n    except Exception as e:\n        logger.error(\n            (\n                f\"Submission of {submission_script_remote} failed. \"\n                f\"Original error: {str(e)}\"\n            )\n        )\n        raise e\n    finally:\n        activity.log = get_current_log(log_file_path)\n        activity = add_commit_refresh(obj=activity, db=db)\n\n    # (3) Monitor job\n    squeue_cmd = (\n        f\"squeue --noheader --format='%i %T' --states=all --jobs={job_id}\"\n    )\n    logger.debug(f\"Start monitoring job with {squeue_cmd=}.\")\n    old_state = None\n    while True:\n        new_state = _run_squeue(\n            fractal_ssh=fractal_ssh,\n            squeue_cmd=squeue_cmd,\n            logger_name=logger_name,\n        )\n        _log_change_of_job_state(\n            old_state=old_state,\n            new_state=new_state,\n            logger_name=logger_name,\n        )\n        activity.log = get_current_log(log_file_path)\n        activity = add_commit_refresh(obj=activity, db=db)\n        if new_state in STATES_FINISHED:\n            logger.debug(f\"Exit retrieval loop (state={new_state}).\")\n            break\n        old_state = new_state\n        time.sleep(poll_interval)\n\n    _verify_success_file_exists(\n        fractal_ssh=fractal_ssh,\n        logger_name=logger_name,\n        success_file_remote=success_file_remote,\n        stderr_remote=stderr_remote,\n    )\n\n    stdout = _read_file_if_exists(\n        fractal_ssh=fractal_ssh,\n        path=stdout_remote,\n    )\n\n    logger.info(\"SLURM-job execution completed successfully, continue.\")\n    activity.log = get_current_log(log_file_path)\n    activity = add_commit_refresh(obj=activity, db=db)\n\n    return stdout\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/_utils/","title":"_utils","text":""},{"location":"reference/fractal_server/tasks/v2/ssh/_utils/#fractal_server.tasks.v2.ssh._utils._copy_wheel_file_ssh","title":"<code>_copy_wheel_file_ssh(*, task_group, fractal_ssh, logger_name)</code>","text":"<p>Handle the situation where <code>task_group.archive_path</code> is not part of <code>task_group.path</code>, by copying <code>archive_path</code> into <code>path</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>The new <code>archive_path</code>.</p> Source code in <code>fractal_server/tasks/v2/ssh/_utils.py</code> <pre><code>def _copy_wheel_file_ssh(\n    *,\n    task_group: TaskGroupV2,\n    fractal_ssh: FractalSSH,\n    logger_name: str,\n) -&gt; str:\n    \"\"\"\n    Handle the situation where `task_group.archive_path` is not part of\n    `task_group.path`, by copying `archive_path` into `path`.\n\n    Returns:\n        The new `archive_path`.\n    \"\"\"\n    logger = get_logger(logger_name=logger_name)\n    source = task_group.archive_path\n    dest = (\n        Path(task_group.path) / Path(task_group.archive_path).name\n    ).as_posix()\n    cmd = f\"cp {source} {dest}\"\n    logger.debug(f\"[_copy_wheel_file_ssh] START {source=} {dest=}\")\n    fractal_ssh.run_command(cmd=cmd)\n    logger.debug(f\"[_copy_wheel_file_ssh] END {source=} {dest=}\")\n    return dest\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/_utils/#fractal_server.tasks.v2.ssh._utils._customize_and_run_template","title":"<code>_customize_and_run_template(*, template_filename, replacements, script_dir_local, script_dir_remote, prefix, fractal_ssh, logger_name)</code>","text":"<p>Customize one of the template bash scripts, transfer it to the remote host via SFTP and then run it via SSH.</p> <p>Parameters:</p> Name Type Description Default <code>template_filename</code> <code>str</code> <p>Filename of the template file (ends with \".sh\").</p> required <code>replacements</code> <code>set[tuple[str, str]]</code> <p>Dictionary of replacements.</p> required <code>script_dir_remote</code> <code>str</code> <p>Remote scripts directory</p> required <code>script_dir_local</code> <code>str</code> <p>Local folder where the script will be placed.</p> required <code>prefix</code> <code>str</code> <p>Prefix for the script filename.</p> required <code>fractal_ssh</code> <code>FractalSSH</code> <p>FractalSSH object</p> required Source code in <code>fractal_server/tasks/v2/ssh/_utils.py</code> <pre><code>def _customize_and_run_template(\n    *,\n    template_filename: str,\n    replacements: set[tuple[str, str]],\n    script_dir_local: str,\n    script_dir_remote: str,\n    prefix: str,\n    fractal_ssh: FractalSSH,\n    logger_name: str,\n) -&gt; str:\n    \"\"\"\n    Customize one of the template bash scripts, transfer it to the remote host\n    via SFTP and then run it via SSH.\n\n    Args:\n        template_filename: Filename of the template file (ends with \".sh\").\n        replacements: Dictionary of replacements.\n        script_dir_remote: Remote scripts directory\n        script_dir_local: Local folder where the script will be placed.\n        prefix: Prefix for the script filename.\n        fractal_ssh: FractalSSH object\n    \"\"\"\n    logger = get_logger(logger_name=logger_name)\n    logger.debug(f\"_customize_and_run_template {template_filename} - START\")\n\n    script_path_remote = _customize_and_send_template(\n        template_filename=template_filename,\n        replacements=replacements,\n        script_dir_local=script_dir_local,\n        script_dir_remote=script_dir_remote,\n        prefix=prefix,\n        fractal_ssh=fractal_ssh,\n        logger_name=logger_name,\n    )\n\n    # Execute script remotely\n    cmd = f\"bash {script_path_remote}\"\n    logger.debug(f\"Now run '{cmd}' over SSH.\")\n    stdout = fractal_ssh.run_command(cmd=cmd)\n\n    logger.debug(f\"_customize_and_run_template {template_filename} - END\")\n    return stdout\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/_utils/#fractal_server.tasks.v2.ssh._utils._customize_and_send_template","title":"<code>_customize_and_send_template(*, template_filename, replacements, script_dir_local, script_dir_remote, prefix, fractal_ssh, logger_name)</code>","text":"<p>Customize a template bash scripts and transfer it to the remote host.</p> <p>Parameters:</p> Name Type Description Default <code>template_filename</code> <code>str</code> <p>Filename of the template file (ends with \".sh\").</p> required <code>replacements</code> <code>set[tuple[str, str]]</code> <p>Dictionary of replacements.</p> required <code>script_dir_local</code> <code>str</code> <p>Local folder where the script will be placed.</p> required <code>script_dir_remote</code> <code>str</code> <p>Remote scripts directory</p> required <code>prefix</code> <code>str</code> <p>Prefix for the script filename.</p> required <code>fractal_ssh</code> <code>FractalSSH</code> <p>FractalSSH object</p> required Source code in <code>fractal_server/tasks/v2/ssh/_utils.py</code> <pre><code>def _customize_and_send_template(\n    *,\n    template_filename: str,\n    replacements: set[tuple[str, str]],\n    script_dir_local: str,\n    script_dir_remote: str,\n    prefix: str,\n    fractal_ssh: FractalSSH,\n    logger_name: str,\n) -&gt; str:\n    \"\"\"\n    Customize a template bash scripts and transfer it to the remote host.\n\n    Args:\n        template_filename: Filename of the template file (ends with \".sh\").\n        replacements: Dictionary of replacements.\n        script_dir_local: Local folder where the script will be placed.\n        script_dir_remote: Remote scripts directory\n        prefix: Prefix for the script filename.\n        fractal_ssh: FractalSSH object\n    \"\"\"\n    logger = get_logger(logger_name=logger_name)\n    logger.debug(f\"_customize_and_run_template {template_filename} - START\")\n    # Prepare name and path of script\n    if not template_filename.endswith(\".sh\"):\n        raise ValueError(\n            f\"Invalid {template_filename=} (it must end with '.sh').\"\n        )\n    script_filename = f\"{prefix}_{template_filename}\"\n    script_path_local = (Path(script_dir_local) / script_filename).as_posix()\n\n    customize_template(\n        template_name=template_filename,\n        replacements=replacements,\n        script_path=script_path_local,\n    )\n\n    # Transfer script to remote host\n    script_path_remote = os.path.join(\n        script_dir_remote,\n        script_filename,\n    )\n    logger.debug(f\"Now transfer {script_path_local=} over SSH.\")\n    fractal_ssh.send_file(\n        local=script_path_local,\n        remote=script_path_remote,\n    )\n    return script_path_remote\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/_utils/#fractal_server.tasks.v2.ssh._utils.check_ssh_or_fail_and_cleanup","title":"<code>check_ssh_or_fail_and_cleanup(*, fractal_ssh, task_group, task_group_activity, logger_name, log_file_path, db)</code>","text":"<p>Check SSH connection.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether SSH connection is OK.</p> Source code in <code>fractal_server/tasks/v2/ssh/_utils.py</code> <pre><code>def check_ssh_or_fail_and_cleanup(\n    *,\n    fractal_ssh: FractalSSH,\n    task_group: TaskGroupV2,\n    task_group_activity: TaskGroupActivityV2,\n    logger_name: str,\n    log_file_path: Path,\n    db: Session,\n) -&gt; bool:\n    \"\"\"\n    Check SSH connection.\n\n    Returns:\n        Whether SSH connection is OK.\n    \"\"\"\n    try:\n        fractal_ssh.check_connection()\n        return True\n    except Exception as e:\n        logger = get_logger(logger_name=logger_name)\n        logger.error(\n            \"Cannot establish SSH connection. \" f\"Original error: {str(e)}\"\n        )\n        fail_and_cleanup(\n            task_group=task_group,\n            task_group_activity=task_group_activity,\n            logger_name=logger_name,\n            log_file_path=log_file_path,\n            exception=e,\n            db=db,\n        )\n        return False\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/_utils/#fractal_server.tasks.v2.ssh._utils.edit_pyproject_toml_in_place_ssh","title":"<code>edit_pyproject_toml_in_place_ssh(*, fractal_ssh, pyproject_toml_path, resource)</code>","text":"<p>Wrapper of <code>simplify_pyproject_toml</code>, with I/O.</p> Source code in <code>fractal_server/tasks/v2/ssh/_utils.py</code> <pre><code>def edit_pyproject_toml_in_place_ssh(\n    *,\n    fractal_ssh: FractalSSH,\n    pyproject_toml_path: Path,\n    resource: Resource,\n) -&gt; None:\n    \"\"\"\n    Wrapper of `simplify_pyproject_toml`, with I/O.\n    \"\"\"\n\n    # Read `pyproject.toml`\n    pyproject_contents = fractal_ssh.read_remote_text_file(\n        pyproject_toml_path.as_posix()\n    )\n\n    # Simplify contents\n    new_pyproject_contents = simplify_pyproject_toml(\n        original_toml_string=pyproject_contents,\n        pixi_environment=resource.tasks_pixi_config[\"DEFAULT_ENVIRONMENT\"],\n        pixi_platform=resource.tasks_pixi_config[\"DEFAULT_PLATFORM\"],\n    )\n    # Write new `pyproject.toml`\n    fractal_ssh.write_remote_file(\n        path=pyproject_toml_path.as_posix(),\n        content=new_pyproject_contents,\n    )\n    logger.debug(\n        f\"Replaced remote {pyproject_toml_path.as_posix()} \"\n        \"with simplified version.\"\n    )\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/collect/","title":"collect","text":""},{"location":"reference/fractal_server/tasks/v2/ssh/collect/#fractal_server.tasks.v2.ssh.collect.collect_ssh","title":"<code>collect_ssh(*, task_group_id, task_group_activity_id, resource, profile, wheel_file=None)</code>","text":"<p>Collect a task package over SSH</p> <p>This function runs as a background task, therefore exceptions must be handled.</p> <p>NOTE: since this function is sync, it runs within a thread - due to starlette/fastapi handling of background tasks (see https://github.com/encode/starlette/blob/master/starlette/background.py).</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required <code>ssh_config</code> required <code>resource</code> <code>Resource</code> required <code>wheel_file</code> <code>FractalUploadedFile | None</code> <code>None</code> Source code in <code>fractal_server/tasks/v2/ssh/collect.py</code> <pre><code>def collect_ssh(\n    *,\n    task_group_id: int,\n    task_group_activity_id: int,\n    resource: Resource,\n    profile: Profile,\n    wheel_file: FractalUploadedFile | None = None,\n) -&gt; None:\n    \"\"\"\n    Collect a task package over SSH\n\n    This function runs as a background task, therefore exceptions must be\n    handled.\n\n    NOTE: since this function is sync, it runs within a thread - due to\n    starlette/fastapi handling of background tasks (see\n    https://github.com/encode/starlette/blob/master/starlette/background.py).\n\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        ssh_config:\n        resource:\n        wheel_file:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    # Work within a temporary folder, where also logs will be placed\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = Path(tmpdir) / \"log\"\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.info(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (remote) task_group path does not exist\n                    # NOTE: this is not part of the try/except below, in order\n                    # to avoid removing the existing folder (as part of the\n                    # exception-handling).\n                    if fractal_ssh.remote_exists(task_group.path):\n                        error_msg = f\"{task_group.path} already exists.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileExistsError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    # Create remote `task_group.path` and `script_dir_remote`\n                    # folders\n                    script_dir_remote = (\n                        Path(task_group.path) / SCRIPTS_SUBFOLDER\n                    ).as_posix()\n                    fractal_ssh.mkdir(folder=task_group.path, parents=True)\n                    fractal_ssh.mkdir(folder=script_dir_remote, parents=True)\n\n                    # Write wheel file locally and send it to remote path,\n                    # and set task_group.archive_path\n                    if wheel_file is not None:\n                        wheel_filename = wheel_file.filename\n                        archive_path = (\n                            Path(task_group.path) / wheel_filename\n                        ).as_posix()\n                        tmp_archive_path = (\n                            Path(tmpdir) / wheel_filename\n                        ).as_posix()\n                        logger.info(\n                            f\"Write wheel file into {tmp_archive_path}\"\n                        )\n                        with open(tmp_archive_path, \"wb\") as f:\n                            f.write(wheel_file.contents)\n                        fractal_ssh.send_file(\n                            local=tmp_archive_path,\n                            remote=archive_path,\n                        )\n                        task_group.archive_path = archive_path\n                        task_group = add_commit_refresh(obj=task_group, db=db)\n\n                    python_bin = get_python_interpreter(\n                        python_version=task_group.python_version,\n                        resource=resource,\n                    )\n                    replacements = get_collection_replacements(\n                        task_group=task_group,\n                        python_bin=python_bin,\n                        resource=resource,\n                    )\n\n                    # Prepare common arguments for _customize_and_run_template\n                    common_args = dict(\n                        replacements=replacements,\n                        script_dir_local=Path(\n                            tmpdir, SCRIPTS_SUBFOLDER\n                        ).as_posix(),\n                        script_dir_remote=script_dir_remote,\n                        prefix=(\n                            f\"{int(time.time())}_\"\n                            f\"{TaskGroupActivityActionV2.COLLECT}\"\n                        ),\n                        fractal_ssh=fractal_ssh,\n                        logger_name=LOGGER_NAME,\n                    )\n\n                    logger.info(\"installing - START\")\n\n                    # Set status to ONGOING and refresh logs\n                    activity.status = TaskGroupActivityStatusV2.ONGOING\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run script 1\n                    stdout = _customize_and_run_template(\n                        template_filename=\"1_create_venv.sh\",\n                        **common_args,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run script 2\n                    stdout = _customize_and_run_template(\n                        template_filename=\"2_pip_install.sh\",\n                        **common_args,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run script 3\n                    pip_freeze_stdout = _customize_and_run_template(\n                        template_filename=\"3_pip_freeze.sh\",\n                        **common_args,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run script 4\n                    stdout = _customize_and_run_template(\n                        template_filename=\"4_pip_show.sh\",\n                        **common_args,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run script 5\n                    venv_info = _customize_and_run_template(\n                        template_filename=\"5_get_venv_size_and_file_number.sh\",\n                        **common_args,\n                    )\n                    venv_size, venv_file_number = venv_info.split()\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    pkg_attrs = parse_script_pip_show_stdout(stdout)\n\n                    for key, value in pkg_attrs.items():\n                        logger.debug(f\"parsed from pip-show: {key}={value}\")\n                    # Check package_name match between pip show and task-group\n                    package_name_pip_show = pkg_attrs.get(\"package_name\")\n                    package_name_task_group = task_group.pkg_name\n                    compare_package_names(\n                        pkg_name_pip_show=package_name_pip_show,\n                        pkg_name_task_group=package_name_task_group,\n                        logger_name=LOGGER_NAME,\n                    )\n                    # Extract/drop parsed attributes\n                    package_name = package_name_task_group\n                    python_bin = pkg_attrs.pop(\"python_bin\")\n                    package_root_parent_remote = pkg_attrs.pop(\n                        \"package_root_parent\"\n                    )\n                    manifest_path_remote = pkg_attrs.pop(\"manifest_path\")\n\n                    # TODO SSH: Use more robust logic to determine\n                    # `package_root`. Examples: use `importlib.util.find_spec`\n                    # or parse the output of `pip show --files {package_name}`.\n                    package_name_underscore = package_name.replace(\"-\", \"_\")\n                    package_root_remote = (\n                        Path(package_root_parent_remote)\n                        / package_name_underscore\n                    ).as_posix()\n\n                    # Read and validate remote manifest file\n                    pkg_manifest_dict = fractal_ssh.read_remote_json_file(\n                        manifest_path_remote\n                    )\n                    logger.info(f\"Loaded {manifest_path_remote=}\")\n                    pkg_manifest = ManifestV2(**pkg_manifest_dict)\n                    logger.info(\"Manifest is a valid ManifestV2\")\n\n                    logger.info(\"_prepare_tasks_metadata - start\")\n                    task_list = prepare_tasks_metadata(\n                        package_manifest=pkg_manifest,\n                        package_version=task_group.version,\n                        package_root=Path(package_root_remote),\n                        python_bin=Path(python_bin),\n                    )\n                    logger.info(\"_prepare_tasks_metadata - end\")\n\n                    logger.info(\n                        \"create_db_tasks_and_update_task_group - \" \"start\"\n                    )\n                    create_db_tasks_and_update_task_group_sync(\n                        task_list=task_list,\n                        task_group_id=task_group.id,\n                        db=db,\n                    )\n                    logger.info(\"create_db_tasks_and_update_task_group - end\")\n\n                    # Update task_group data\n                    logger.info(\n                        \"Add env_info, venv_size and venv_file_number \"\n                        \"to TaskGroupV2 - start\"\n                    )\n                    task_group.env_info = pip_freeze_stdout\n                    task_group.venv_size_in_kB = int(venv_size)\n                    task_group.venv_file_number = int(venv_file_number)\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n                    logger.info(\n                        \"Add env_info, venv_size and venv_file_number \"\n                        \"to TaskGroupV2 - end\"\n                    )\n\n                    # Finalize (write metadata to DB)\n                    logger.info(\"finalising - START\")\n                    activity.status = TaskGroupActivityStatusV2.OK\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n                    logger.info(\"finalising - END\")\n                    logger.info(\"END\")\n                    reset_logger_handlers(logger)\n\n                except Exception as collection_e:\n                    # Delete corrupted package dir\n                    try:\n                        logger.info(\n                            f\"Now delete remote folder {task_group.path}\"\n                        )\n                        fractal_ssh.remove_folder(\n                            folder=task_group.path,\n                            safe_root=profile.tasks_remote_dir,\n                        )\n                        logger.info(\n                            f\"Deleted remoted folder {task_group.path}\"\n                        )\n                    except Exception as e_rm:\n                        logger.error(\n                            \"Removing folder failed. \"\n                            f\"Original error: {str(e_rm)}\"\n                        )\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        log_file_path=log_file_path,\n                        logger_name=LOGGER_NAME,\n                        exception=collection_e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/collect_pixi/","title":"collect_pixi","text":""},{"location":"reference/fractal_server/tasks/v2/ssh/collect_pixi/#fractal_server.tasks.v2.ssh.collect_pixi.collect_ssh_pixi","title":"<code>collect_ssh_pixi(*, task_group_id, task_group_activity_id, tar_gz_file, resource, profile)</code>","text":"<p>Collect a task package over SSH</p> <p>This function runs as a background task, therefore exceptions must be handled.</p> <p>NOTE: since this function is sync, it runs within a thread - due to starlette/fastapi handling of background tasks (see https://github.com/encode/starlette/blob/master/starlette/background.py).</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required <code>ssh_config</code> required <code>tar_gz_file</code> <code>FractalUploadedFile</code> required Source code in <code>fractal_server/tasks/v2/ssh/collect_pixi.py</code> <pre><code>def collect_ssh_pixi(\n    *,\n    task_group_id: int,\n    task_group_activity_id: int,\n    tar_gz_file: FractalUploadedFile,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Collect a task package over SSH\n\n    This function runs as a background task, therefore exceptions must be\n    handled.\n\n    NOTE: since this function is sync, it runs within a thread - due to\n    starlette/fastapi handling of background tasks (see\n    https://github.com/encode/starlette/blob/master/starlette/background.py).\n\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        ssh_config:\n        tar_gz_file:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    # Work within a temporary folder, where also logs will be placed\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = Path(tmpdir) / \"log\"\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.info(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (remote) task_group path does not exist\n                    if fractal_ssh.remote_exists(task_group.path):\n                        error_msg = f\"{task_group.path} already exists.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileExistsError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    # Create remote `task_group.path` and `script_dir_remote`\n                    # folders\n                    script_dir_remote = Path(\n                        task_group.path, SCRIPTS_SUBFOLDER\n                    ).as_posix()\n                    fractal_ssh.mkdir(folder=task_group.path, parents=True)\n                    fractal_ssh.mkdir(folder=script_dir_remote, parents=True)\n\n                    # Write tar.gz file locally and send it to remote path,\n                    # and set task_group.archive_path\n                    tar_gz_filename = tar_gz_file.filename\n                    archive_path = (\n                        Path(task_group.path) / tar_gz_filename\n                    ).as_posix()\n                    tmp_archive_path = Path(tmpdir, tar_gz_filename).as_posix()\n                    logger.info(f\"Write tar.gz file into {tmp_archive_path}\")\n                    with open(tmp_archive_path, \"wb\") as f:\n                        f.write(tar_gz_file.contents)\n                    fractal_ssh.send_file(\n                        local=tmp_archive_path,\n                        remote=archive_path,\n                    )\n                    task_group.archive_path = archive_path\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n\n                    replacements = {\n                        (\n                            \"__PIXI_HOME__\",\n                            resource.tasks_pixi_config[\"versions\"][\n                                task_group.pixi_version\n                            ],\n                        ),\n                        (\"__PACKAGE_DIR__\", task_group.path),\n                        (\"__TAR_GZ_PATH__\", task_group.archive_path),\n                        (\n                            \"__IMPORT_PACKAGE_NAME__\",\n                            task_group.pkg_name.replace(\"-\", \"_\"),\n                        ),\n                        (\"__SOURCE_DIR_NAME__\", SOURCE_DIR_NAME),\n                        (\"__FROZEN_OPTION__\", \"\"),\n                        (\n                            \"__TOKIO_WORKER_THREADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"TOKIO_WORKER_THREADS\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_SOLVES__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_SOLVES\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_DOWNLOADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_DOWNLOADS\"\n                                ]\n                            ),\n                        ),\n                    }\n\n                    logger.info(\"installing - START\")\n\n                    # Set status to ONGOING and refresh logs\n                    activity.status = TaskGroupActivityStatusV2.ONGOING\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    common_args = dict(\n                        script_dir_local=(\n                            Path(tmpdir, SCRIPTS_SUBFOLDER)\n                        ).as_posix(),\n                        script_dir_remote=script_dir_remote,\n                        prefix=(\n                            f\"{int(time.time())}_\"\n                            f\"{TaskGroupActivityActionV2.COLLECT}\"\n                        ),\n                        logger_name=LOGGER_NAME,\n                        fractal_ssh=fractal_ssh,\n                    )\n\n                    # Run the three pixi-related scripts\n                    stdout = _customize_and_run_template(\n                        template_filename=\"pixi_1_extract.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    logger.debug(f\"STDOUT: {stdout}\")\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Simplify `pyproject.toml`\n                    source_dir = Path(\n                        task_group.path, SOURCE_DIR_NAME\n                    ).as_posix()\n                    pyproject_toml_path = Path(source_dir, \"pyproject.toml\")\n                    edit_pyproject_toml_in_place_ssh(\n                        fractal_ssh=fractal_ssh,\n                        pyproject_toml_path=pyproject_toml_path,\n                        resource=resource,\n                    )\n\n                    # Prepare scripts 2 and 3\n                    remote_script2_path = _customize_and_send_template(\n                        template_filename=\"pixi_2_install.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    remote_script3_path = _customize_and_send_template(\n                        template_filename=\"pixi_3_post_install.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    logger.debug(\n                        \"Post-installation script written to \"\n                        f\"{remote_script3_path=}.\"\n                    )\n                    logger.debug(\n                        \"Installation script written to \"\n                        f\"{remote_script2_path=}.\"\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run scripts 2 and 3\n                    stdout = run_script_on_remote_slurm(\n                        script_paths=[\n                            remote_script2_path,\n                            remote_script3_path,\n                            f\"chmod -R 755 {source_dir}\",\n                        ],\n                        slurm_config=resource.tasks_pixi_config[\n                            \"SLURM_CONFIG\"\n                        ],\n                        fractal_ssh=fractal_ssh,\n                        logger_name=LOGGER_NAME,\n                        prefix=common_args[\"prefix\"],\n                        db=db,\n                        activity=activity,\n                        log_file_path=log_file_path,\n                        poll_interval=resource.jobs_poll_interval,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Parse stdout\n                    parsed_output = parse_collect_stdout(stdout)\n                    package_root_remote = parsed_output[\"package_root\"]\n                    venv_size = parsed_output[\"venv_size\"]\n                    venv_file_number = parsed_output[\"venv_file_number\"]\n                    project_python_wrapper = parsed_output[\n                        \"project_python_wrapper\"\n                    ]\n\n                    # Read and validate remote manifest file\n                    manifest_path_remote = (\n                        f\"{package_root_remote}/__FRACTAL_MANIFEST__.json\"\n                    )\n                    pkg_manifest_dict = fractal_ssh.read_remote_json_file(\n                        manifest_path_remote\n                    )\n                    logger.info(f\"Loaded {manifest_path_remote=}\")\n                    pkg_manifest = ManifestV2(**pkg_manifest_dict)\n                    logger.info(\"Manifest is a valid ManifestV2\")\n\n                    logger.info(\"_prepare_tasks_metadata - start\")\n                    task_list = prepare_tasks_metadata(\n                        package_manifest=pkg_manifest,\n                        package_version=task_group.version,\n                        package_root=Path(package_root_remote),\n                        project_python_wrapper=Path(project_python_wrapper),\n                    )\n                    logger.info(\"_prepare_tasks_metadata - end\")\n\n                    logger.info(\n                        \"create_db_tasks_and_update_task_group - \" \"start\"\n                    )\n                    create_db_tasks_and_update_task_group_sync(\n                        task_list=task_list,\n                        task_group_id=task_group.id,\n                        db=db,\n                    )\n                    logger.info(\"create_db_tasks_and_update_task_group - end\")\n\n                    # NOTE: see issue 2626 about whether to keep `pixi.lock`\n                    # files in the database\n                    remote_pixi_lock_file = Path(\n                        task_group.path,\n                        SOURCE_DIR_NAME,\n                        \"pixi.lock\",\n                    ).as_posix()\n                    pixi_lock_contents = fractal_ssh.read_remote_text_file(\n                        remote_pixi_lock_file\n                    )\n\n                    # Update task_group data\n                    logger.info(\n                        \"Add env_info, venv_size and venv_file_number \"\n                        \"to TaskGroupV2 - start\"\n                    )\n                    task_group.env_info = pixi_lock_contents\n                    task_group.venv_size_in_kB = int(venv_size)\n                    task_group.venv_file_number = int(venv_file_number)\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n                    logger.info(\n                        \"Add env_info, venv_size and venv_file_number \"\n                        \"to TaskGroupV2 - end\"\n                    )\n\n                    # Finalize (write metadata to DB)\n                    logger.info(\"finalising - START\")\n                    activity.status = TaskGroupActivityStatusV2.OK\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n                    logger.info(\"finalising - END\")\n                    logger.info(\"END\")\n                    reset_logger_handlers(logger)\n\n                except Exception as collection_e:\n                    # Delete corrupted package dir\n                    try:\n                        logger.info(\n                            f\"Now delete remote folder {task_group.path}\"\n                        )\n                        fractal_ssh.remove_folder(\n                            folder=task_group.path,\n                            safe_root=profile.tasks_remote_dir,\n                        )\n                        logger.info(\n                            f\"Deleted remoted folder {task_group.path}\"\n                        )\n                    except Exception as e_rm:\n                        logger.error(\n                            \"Removing folder failed. \"\n                            f\"Original error: {str(e_rm)}\"\n                        )\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        log_file_path=log_file_path,\n                        logger_name=LOGGER_NAME,\n                        exception=collection_e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/deactivate/","title":"deactivate","text":""},{"location":"reference/fractal_server/tasks/v2/ssh/deactivate/#fractal_server.tasks.v2.ssh.deactivate.deactivate_ssh","title":"<code>deactivate_ssh(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Deactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required <code>ssh_config</code> required Source code in <code>fractal_server/tasks/v2/ssh/deactivate.py</code> <pre><code>def deactivate_ssh(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Deactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        ssh_config:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    user=profile.username,\n                    host=resource.host,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (local) task_group venv_path does exist\n                    if not fractal_ssh.remote_exists(task_group.venv_path):\n                        error_msg = f\"{task_group.venv_path} does not exist.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileNotFoundError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    activity.status = TaskGroupActivityStatusV2.ONGOING\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    if task_group.env_info is None:\n                        logger.warning(\n                            \"Recreate pip-freeze information, since \"\n                            f\"{task_group.env_info=}. NOTE: this should \"\n                            \"only happen for task groups created before 2.9.0.\"\n                        )\n\n                        # Prepare replacements for templates\n                        replacements = get_collection_replacements(\n                            task_group=task_group,\n                            python_bin=\"/not/applicable\",\n                            resource=resource,\n                        )\n\n                        # Define script_dir_remote and create it if missing\n                        script_dir_remote = (\n                            Path(task_group.path) / SCRIPTS_SUBFOLDER\n                        ).as_posix()\n                        fractal_ssh.mkdir(\n                            folder=script_dir_remote, parents=True\n                        )\n\n                        # Prepare arguments for `_customize_and_run_template`\n                        common_args = dict(\n                            replacements=replacements,\n                            script_dir_local=(\n                                Path(tmpdir) / SCRIPTS_SUBFOLDER\n                            ).as_posix(),\n                            script_dir_remote=script_dir_remote,\n                            prefix=(\n                                f\"{int(time.time())}_\"\n                                f\"{TaskGroupActivityActionV2.DEACTIVATE}\"\n                            ),\n                            fractal_ssh=fractal_ssh,\n                            logger_name=LOGGER_NAME,\n                        )\n\n                        # Run `pip freeze`\n                        pip_freeze_stdout = _customize_and_run_template(\n                            template_filename=\"3_pip_freeze.sh\",\n                            **common_args,\n                        )\n\n                        # Update pip-freeze data\n                        logger.info(\n                            \"Add pip freeze stdout to TaskGroupV2 - start\"\n                        )\n                        activity.log = get_current_log(log_file_path)\n                        activity = add_commit_refresh(obj=activity, db=db)\n                        task_group.env_info = pip_freeze_stdout\n                        task_group = add_commit_refresh(obj=task_group, db=db)\n                        logger.info(\n                            \"Add pip freeze stdout to TaskGroupV2 - end\"\n                        )\n\n                    # Handle some specific cases for wheel-file case\n                    if task_group.origin == TaskGroupV2OriginEnum.WHEELFILE:\n                        logger.info(\n                            f\"Handle specific cases for {task_group.origin=}.\"\n                        )\n\n                        # Blocking situation: `archive_path` is not set or\n                        # points to a missing path\n                        if (\n                            task_group.archive_path is None\n                            or not fractal_ssh.remote_exists(\n                                task_group.archive_path\n                            )\n                        ):\n                            error_msg = (\n                                \"Invalid wheel path for task group with \"\n                                f\"{task_group_id=}. \"\n                                f\"{task_group.archive_path=}  is unset or \"\n                                \"does not exist.\"\n                            )\n                            logger.error(error_msg)\n                            fail_and_cleanup(\n                                task_group=task_group,\n                                task_group_activity=activity,\n                                logger_name=LOGGER_NAME,\n                                log_file_path=log_file_path,\n                                exception=FileNotFoundError(error_msg),\n                                db=db,\n                            )\n                            return\n\n                        # Recoverable situation: `archive_path` was not yet\n                        # copied over to the correct server-side folder\n                        archive_path_parent_dir = Path(\n                            task_group.archive_path\n                        ).parent\n                        if archive_path_parent_dir != Path(task_group.path):\n                            logger.warning(\n                                f\"{archive_path_parent_dir.as_posix()} \"\n                                f\"differs from {task_group.path}. \"\n                                \"NOTE: this should only happen for task \"\n                                \"groups created before 2.9.0.\"\n                            )\n\n                            if (\n                                task_group.archive_path\n                                not in task_group.env_info\n                            ):\n                                raise ValueError(\n                                    f\"Cannot find {task_group.archive_path=} \"\n                                    \"in pip-freeze data. Exit.\"\n                                )\n\n                            logger.info(\n                                f\"Now copy wheel file into {task_group.path}.\"\n                            )\n                            new_archive_path = _copy_wheel_file_ssh(\n                                task_group=task_group,\n                                fractal_ssh=fractal_ssh,\n                                logger_name=LOGGER_NAME,\n                            )\n                            logger.info(\n                                f\"Copied wheel file to {new_archive_path}.\"\n                            )\n\n                            task_group.archive_path = new_archive_path\n                            new_pip_freeze = task_group.env_info.replace(\n                                task_group.archive_path,\n                                new_archive_path,\n                            )\n                            task_group.env_info = new_pip_freeze\n                            task_group = add_commit_refresh(\n                                obj=task_group, db=db\n                            )\n                            logger.info(\n                                \"Updated `archive_path` and `env_info` \"\n                                \"task-group attributes.\"\n                            )\n\n                    # Fail if `env_info` includes \"github\", see\n                    # https://github.com/fractal-analytics-platform/fractal-server/issues/2142\n                    for forbidden_string in FORBIDDEN_DEPENDENCY_STRINGS:\n                        if forbidden_string in task_group.env_info:\n                            raise ValueError(\n                                \"Deactivation and reactivation of task \"\n                                f\"packages with direct {forbidden_string} \"\n                                \"dependencies are not currently supported. \"\n                                \"Exit.\"\n                            )\n\n                    # We now have all required information for reactivating the\n                    # virtual environment at a later point\n\n                    # Actually mark the task group as non-active\n                    logger.info(\"Now setting `active=False`.\")\n                    task_group.active = False\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n\n                    # Proceed with deactivation\n                    logger.info(f\"Now removing {task_group.venv_path}.\")\n                    fractal_ssh.remove_folder(\n                        folder=task_group.venv_path,\n                        safe_root=profile.tasks_remote_dir,\n                    )\n                    logger.info(f\"All good, {task_group.venv_path} removed.\")\n                    activity.status = TaskGroupActivityStatusV2.OK\n                    activity.log = get_current_log(log_file_path)\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    reset_logger_handlers(logger)\n\n                except Exception as e:\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        exception=e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/deactivate_pixi/","title":"deactivate_pixi","text":""},{"location":"reference/fractal_server/tasks/v2/ssh/deactivate_pixi/#fractal_server.tasks.v2.ssh.deactivate_pixi.deactivate_ssh_pixi","title":"<code>deactivate_ssh_pixi(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Deactivate a pixi task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_activity_id</code> <code>int</code> required <code>task_group_id</code> <code>int</code> required <code>resource</code> <code>Resource</code> required <code>profile</code> <code>Profile</code> required Source code in <code>fractal_server/tasks/v2/ssh/deactivate_pixi.py</code> <pre><code>def deactivate_ssh_pixi(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Deactivate a pixi task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_activity_id:\n        task_group_id:\n        resource:\n        profile:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (remote) task_group venv_path does exist\n                    source_dir = Path(\n                        task_group.path, SOURCE_DIR_NAME\n                    ).as_posix()\n                    if not fractal_ssh.remote_exists(source_dir):\n                        error_msg = f\"{source_dir} does not exist.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileNotFoundError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    # Actually mark the task group as non-active\n                    logger.info(\"Now setting `active=False`.\")\n                    task_group.active = False\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n\n                    # Proceed with deactivation\n                    logger.info(f\"Now removing {source_dir}.\")\n                    fractal_ssh.remove_folder(\n                        folder=source_dir,\n                        safe_root=profile.tasks_remote_dir,\n                    )\n                    logger.info(f\"All good, {source_dir} removed.\")\n                    activity.status = TaskGroupActivityStatusV2.OK\n                    activity.log = get_current_log(log_file_path)\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    reset_logger_handlers(logger)\n\n                except Exception as e:\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        exception=e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/delete/","title":"delete","text":""},{"location":"reference/fractal_server/tasks/v2/ssh/delete/#fractal_server.tasks.v2.ssh.delete.delete_ssh","title":"<code>delete_ssh(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Delete a task group.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required <code>ssh_config</code> required Source code in <code>fractal_server/tasks/v2/ssh/delete.py</code> <pre><code>def delete_ssh(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Delete a task group.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        ssh_config:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    activity.status = TaskGroupActivityStatusV2.ONGOING\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    db.delete(task_group)\n                    db.commit()\n                    logger.debug(\"Task group removed from database.\")\n\n                    if task_group.origin != TaskGroupV2OriginEnum.OTHER:\n                        logger.debug(\n                            f\"Removing remote {task_group.path=} \"\n                            f\"(with {profile.tasks_remote_dir=}).\"\n                        )\n                        fractal_ssh.remove_folder(\n                            folder=task_group.path,\n                            safe_root=profile.tasks_remote_dir,\n                        )\n                        logger.debug(f\"Remote {task_group.path=} removed.\")\n\n                    activity.status = TaskGroupActivityStatusV2.OK\n                    activity.log = get_current_log(log_file_path)\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    logger.debug(\"END\")\n\n                except Exception as e:\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        exception=e,\n                        db=db,\n                    )\n\n                finally:\n                    reset_logger_handlers(logger)\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/reactivate/","title":"reactivate","text":""},{"location":"reference/fractal_server/tasks/v2/ssh/reactivate/#fractal_server.tasks.v2.ssh.reactivate.reactivate_ssh","title":"<code>reactivate_ssh(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Reactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required <code>ssh_config</code> required Source code in <code>fractal_server/tasks/v2/ssh/reactivate.py</code> <pre><code>def reactivate_ssh(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Reactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        ssh_config:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.info(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (remote) task_group venv_path does not\n                    # exist\n                    if fractal_ssh.remote_exists(task_group.venv_path):\n                        error_msg = f\"{task_group.venv_path} already exists.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileExistsError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    activity.status = TaskGroupActivityStatusV2.ONGOING\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Prepare replacements for templates\n                    python_bin = get_python_interpreter(\n                        python_version=task_group.python_version,\n                        resource=resource,\n                    )\n                    replacements = get_collection_replacements(\n                        task_group=task_group,\n                        python_bin=python_bin,\n                        resource=resource,\n                    )\n\n                    # Prepare replacements for templates\n                    pip_freeze_file_local = f\"{tmpdir}/pip_freeze.txt\"\n                    pip_freeze_file_remote = (\n                        Path(task_group.path) / \"_tmp_pip_freeze.txt\"\n                    ).as_posix()\n                    with open(pip_freeze_file_local, \"w\") as f:\n                        f.write(task_group.env_info)\n                    fractal_ssh.send_file(\n                        local=pip_freeze_file_local,\n                        remote=pip_freeze_file_remote,\n                    )\n                    replacements.append(\n                        (\"__PIP_FREEZE_FILE__\", pip_freeze_file_remote)\n                    )\n\n                    # Define script_dir_remote and create it if missing\n                    script_dir_remote = (\n                        Path(task_group.path) / SCRIPTS_SUBFOLDER\n                    ).as_posix()\n                    fractal_ssh.mkdir(folder=script_dir_remote, parents=True)\n\n                    # Prepare common arguments for _customize_and_run_template\n                    common_args = dict(\n                        replacements=replacements,\n                        script_dir_local=(\n                            Path(tmpdir) / SCRIPTS_SUBFOLDER\n                        ).as_posix(),\n                        script_dir_remote=script_dir_remote,\n                        prefix=(\n                            f\"{int(time.time())}_\"\n                            f\"{TaskGroupActivityActionV2.REACTIVATE}\"\n                        ),\n                        fractal_ssh=fractal_ssh,\n                        logger_name=LOGGER_NAME,\n                    )\n\n                    # Create remote directory for scripts\n                    fractal_ssh.mkdir(folder=script_dir_remote)\n\n                    logger.info(\"start - create venv\")\n                    _customize_and_run_template(\n                        template_filename=\"1_create_venv.sh\",\n                        **common_args,\n                    )\n                    logger.info(\"end - create venv\")\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    logger.info(\"start - install from pip freeze\")\n                    _customize_and_run_template(\n                        template_filename=\"6_pip_install_from_freeze.sh\",\n                        **common_args,\n                    )\n                    logger.info(\"end - install from pip freeze\")\n                    activity.log = get_current_log(log_file_path)\n                    activity.status = TaskGroupActivityStatusV2.OK\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n                    task_group.active = True\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n                    logger.info(\"END\")\n\n                    reset_logger_handlers(logger)\n\n                except Exception as reactivate_e:\n                    # Delete corrupted venv_path\n                    try:\n                        logger.info(\n                            f\"Now delete folder {task_group.venv_path}\"\n                        )\n                        fractal_ssh.remove_folder(\n                            folder=task_group.venv_path,\n                            safe_root=profile.tasks_remote_dir,\n                        )\n                        logger.info(f\"Deleted folder {task_group.venv_path}\")\n                    except Exception as rm_e:\n                        logger.error(\n                            \"Removing folder failed. \"\n                            f\"Original error: {str(rm_e)}\"\n                        )\n\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        exception=reactivate_e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/fractal_server/tasks/v2/ssh/reactivate_pixi/","title":"reactivate_pixi","text":""},{"location":"reference/fractal_server/tasks/v2/ssh/reactivate_pixi/#fractal_server.tasks.v2.ssh.reactivate_pixi.reactivate_ssh_pixi","title":"<code>reactivate_ssh_pixi(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Reactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> <p>Parameters:</p> Name Type Description Default <code>task_group_id</code> <code>int</code> required <code>task_group_activity_id</code> <code>int</code> required <code>resource</code> <code>Resource</code> required <code>profile</code> <code>Profile</code> required Source code in <code>fractal_server/tasks/v2/ssh/reactivate_pixi.py</code> <pre><code>def reactivate_ssh_pixi(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Reactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n        profile:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.info(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (remote) task_group source_dir does not\n                    # exist\n                    source_dir = Path(\n                        task_group.path, SOURCE_DIR_NAME\n                    ).as_posix()\n                    if fractal_ssh.remote_exists(source_dir):\n                        error_msg = f\"{source_dir} already exists.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileExistsError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    replacements = {\n                        (\n                            \"__PIXI_HOME__\",\n                            resource.tasks_pixi_config[\"versions\"][\n                                task_group.pixi_version\n                            ],\n                        ),\n                        (\"__PACKAGE_DIR__\", task_group.path),\n                        (\"__TAR_GZ_PATH__\", task_group.archive_path),\n                        (\n                            \"__IMPORT_PACKAGE_NAME__\",\n                            task_group.pkg_name.replace(\"-\", \"_\"),\n                        ),\n                        (\"__SOURCE_DIR_NAME__\", SOURCE_DIR_NAME),\n                        (\"__FROZEN_OPTION__\", \"--frozen\"),\n                        (\n                            \"__TOKIO_WORKER_THREADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"TOKIO_WORKER_THREADS\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_SOLVES__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_SOLVES\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_DOWNLOADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_DOWNLOADS\"\n                                ]\n                            ),\n                        ),\n                    }\n\n                    logger.info(\"installing - START\")\n\n                    # Set status to ONGOING and refresh logs\n                    activity.status = TaskGroupActivityStatusV2.ONGOING\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    script_dir_remote = Path(\n                        task_group.path, SCRIPTS_SUBFOLDER\n                    ).as_posix()\n                    common_args = dict(\n                        script_dir_local=(\n                            Path(tmpdir) / SCRIPTS_SUBFOLDER\n                        ).as_posix(),\n                        script_dir_remote=script_dir_remote,\n                        prefix=(\n                            f\"{int(time.time())}_\"\n                            f\"{TaskGroupActivityActionV2.REACTIVATE}\"\n                        ),\n                        logger_name=LOGGER_NAME,\n                        fractal_ssh=fractal_ssh,\n                    )\n\n                    # Run script 1 - extract tar.gz into `source_dir`\n                    stdout = _customize_and_run_template(\n                        template_filename=\"pixi_1_extract.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    logger.debug(f\"STDOUT: {stdout}\")\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Simplify `pyproject.toml`\n                    source_dir = Path(\n                        task_group.path, SOURCE_DIR_NAME\n                    ).as_posix()\n                    pyproject_toml_path = Path(source_dir, \"pyproject.toml\")\n                    edit_pyproject_toml_in_place_ssh(\n                        fractal_ssh=fractal_ssh,\n                        pyproject_toml_path=pyproject_toml_path,\n                        resource=resource,\n                    )\n                    # Write pixi.lock into `source_dir`\n                    pixi_lock_local = Path(tmpdir, \"pixi.lock\").as_posix()\n                    pixi_lock_remote = Path(\n                        task_group.path, SOURCE_DIR_NAME, \"pixi.lock\"\n                    ).as_posix()\n                    logger.info(\n                        f\"Write `env_info` contents into {pixi_lock_local}\"\n                    )\n                    with open(pixi_lock_local, \"w\") as f:\n                        f.write(task_group.env_info)\n                    fractal_ssh.send_file(\n                        local=pixi_lock_local,\n                        remote=pixi_lock_remote,\n                    )\n\n                    # Prepare scripts 2 and 3\n                    remote_script2_path = _customize_and_send_template(\n                        template_filename=\"pixi_2_install.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    remote_script3_path = _customize_and_send_template(\n                        template_filename=\"pixi_3_post_install.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    logger.debug(\n                        \"Post-installation script written to \"\n                        f\"{remote_script3_path=}.\"\n                    )\n                    logger.debug(\n                        \"Installation script written to \"\n                        f\"{remote_script2_path=}.\"\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run scripts 2 and 3\n                    stdout = run_script_on_remote_slurm(\n                        script_paths=[\n                            remote_script2_path,\n                            remote_script3_path,\n                            f\"chmod -R 755 {source_dir}\",\n                        ],\n                        slurm_config=resource.tasks_pixi_config[\n                            \"SLURM_CONFIG\"\n                        ],\n                        fractal_ssh=fractal_ssh,\n                        logger_name=LOGGER_NAME,\n                        prefix=common_args[\"prefix\"],\n                        db=db,\n                        activity=activity,\n                        log_file_path=log_file_path,\n                        poll_interval=resource.jobs_poll_interval,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Finalize (write metadata to DB)\n                    activity.status = TaskGroupActivityStatusV2.OK\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n                    task_group.active = True\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n                    logger.info(\"END\")\n\n                    reset_logger_handlers(logger)\n\n                except Exception as reactivate_e:\n                    # Delete corrupted source_dir\n                    try:\n                        logger.info(f\"Now delete folder {source_dir}\")\n                        fractal_ssh.remove_folder(\n                            folder=source_dir,\n                            safe_root=profile.tasks_remote_dir,\n                        )\n                        logger.info(f\"Deleted folder {source_dir}\")\n                    except Exception as rm_e:\n                        logger.error(\n                            \"Removing folder failed. \"\n                            f\"Original error: {str(rm_e)}\"\n                        )\n\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        exception=reactivate_e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/fractal_server/types/","title":"types","text":""},{"location":"reference/fractal_server/types/validators/","title":"validators","text":""},{"location":"reference/fractal_server/types/validators/_common_validators/","title":"_common_validators","text":""},{"location":"reference/fractal_server/types/validators/_common_validators/#fractal_server.types.validators._common_validators.val_absolute_path","title":"<code>val_absolute_path(path)</code>","text":"<p>Check that a string attribute is an absolute path</p> Source code in <code>fractal_server/types/validators/_common_validators.py</code> <pre><code>def val_absolute_path(path: str) -&gt; str:\n    \"\"\"\n    Check that a string attribute is an absolute path\n    \"\"\"\n    if not os.path.isabs(path):\n        raise ValueError(f\"String must be an absolute path (given '{path}').\")\n    return path\n</code></pre>"},{"location":"reference/fractal_server/types/validators/_common_validators/#fractal_server.types.validators._common_validators.valdict_keys","title":"<code>valdict_keys(d)</code>","text":"<p>Strip every key of the dictionary, and fail if there are identical keys</p> Source code in <code>fractal_server/types/validators/_common_validators.py</code> <pre><code>def valdict_keys(d: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Strip every key of the dictionary, and fail if there are identical keys\n    \"\"\"\n    old_keys = list(d.keys())\n    new_keys = [key.strip() for key in old_keys]\n    if any(k == \"\" for k in new_keys):\n        raise ValueError(f\"Empty string in {new_keys}.\")\n    if len(new_keys) != len(set(new_keys)):\n        raise ValueError(\n            f\"Dictionary contains multiple identical keys: '{d}'.\"\n        )\n    for old_key, new_key in zip(old_keys, new_keys):\n        if new_key != old_key:\n            d[new_key] = d.pop(old_key)\n    return d\n</code></pre>"},{"location":"reference/fractal_server/types/validators/_filter_validators/","title":"_filter_validators","text":""},{"location":"reference/fractal_server/types/validators/_workflow_task_arguments_validators/","title":"_workflow_task_arguments_validators","text":""}]}