{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Fractal Analytics Framework","text":"<p>Fractal is a framework developed at the BioVisionCenter to process bioimaging data at scale in the OME-Zarr format and prepare the images for interactive visualization.</p> <p>This is the server component of the fractal analytics platform (find more information about Fractal in general and the other repositories at the Fractal home page). The source code is available on the fractal-server GitHub repository.</p>"},{"location":"#licence-and-copyright","title":"Licence and Copyright","text":"<p>The Fractal project is developed by the BioVisionCenter at the University of Zurich, who contracts eXact lab s.r.l. for software engineering and development support.</p> <p>Unless otherwise specified, Fractal components are released under the BSD 3-Clause License, and copyright is with the BioVisionCenter at the University of Zurich.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>Note: Numbers like (#1234) point to closed Pull Requests on the fractal-server repository.</p>"},{"location":"changelog/#2200-unreleased","title":"2.20.0 (unreleased)","text":"<ul> <li>API:<ul> <li>Introduce version flexibility in workflow-import endpoint (#3154).</li> </ul> </li> <li>Database:<ul> <li>Enforce non-duplication constraints for <code>TaskGroupV2</code> and <code>TaskGroupActivityV2</code> via <code>unique</code> database checks (#3185).</li> </ul> </li> <li>Application:<ul> <li>Improve lifespan logging (#3201).</li> </ul> </li> <li>Runner:<ul> <li>Mention package name/version in job logs (#3200)</li> </ul> </li> <li>Documentation:<ul> <li>Draft Fractal-runner specification (#3196).</li> </ul> </li> </ul>"},{"location":"changelog/#2194","title":"2.19.4","text":"<ul> <li>SSH:<ul> <li>Try to refresh SSH connection upon <code>paramiko.ssh_exception.SSHException</code> (#3189).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>fastapi</code> to <code>0.129.*</code> (#3190).</li> <li>Bump <code>sqlmodel</code> to <code>0.0.33</code> (#3190).</li> </ul> </li> </ul>"},{"location":"changelog/#2193","title":"2.19.3","text":"<ul> <li>Task lifecycle:<ul> <li>Expose <code>preamble</code> for pixi/SLURM task collection (#3182).</li> </ul> </li> </ul>"},{"location":"changelog/#2192","title":"2.19.2","text":"<ul> <li>Task lifecycle:<ul> <li>Support both <code>--mem</code> and <code>--mem-per-cpu</code> for pixi/SLURM task collection (#3180).</li> </ul> </li> <li>Runner:<ul> <li>Also log workflowtask <code>alias</code> field, if present (#3179).</li> </ul> </li> <li>API:<ul> <li>Add regex validator to <code>Project</code> and <code>Dataset</code> schemas (#3175).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>gunicorn</code> to v25 (#3174).</li> <li>Bump <code>packaging</code> to v26 (#3178).</li> </ul> </li> </ul>"},{"location":"changelog/#2191","title":"2.19.1","text":"<ul> <li>Runner:<ul> <li>Expand <code>STDERR_IGNORE_PATTERNS</code> for SLURM errors with <code>srun: warning: can't run 1 processes on 8 nodes, setting nnodes to 1</code> (#3169).</li> <li>Expose SLURM <code>nodes</code> parameter (#3171).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump documentation dependencies (#3172).</li> </ul> </li> </ul>"},{"location":"changelog/#2190","title":"2.19.0","text":"<ul> <li>API:<ul> <li>Add <code>POST /admin/v2/linkuserproject/verify/</code> (#3130).</li> <li>Include new <code>is_guest</code> field in users CRUD (#3130).</li> <li>Return empty list of project-sharing invitations for <code>is_guest=True</code> users (#3130).</li> <li>Rename <code>current_user_act_ver_prof</code> into <code>get_api_guest</code>, and add a more restrictive <code>get_api_user</code> dependency for non-read-only endpoints (#3130).</li> <li>Make <code>UserManager.validate_password</code> fail if password is longer than 72 bytes (#3141).</li> <li>Prevent guest users from self update (#3142).</li> <li>Deprecate use of <code>TaskV2.source</code> in workflow imports and superuser-only task queries(#3147, #3148).</li> <li>Improve definition of latest version in task import (#3153).</li> <li>Remove <code>GET /project/{project_id}/status-legacy/</code> endpoint (#3160).</li> </ul> </li> <li>Database:<ul> <li>Drop <code>TaskV2.source</code> (#3147).</li> <li>Add <code>UserOAuth.is_guest</code> boolean column and corresponding <code>CHECK</code> constraint (#3130).</li> <li>Add <code>description</code> to workflow and <code>description</code> and <code>alias</code> to workflow task (#3156, #3164).</li> <li>Drop <code>DatasetV2.history</code> (#3160).</li> <li>Add <code>fractal_server_version</code> column to <code>JobV2</code> and <code>TaskGroupActivityV2</code> (#3161).</li> </ul> </li> <li>Internal:<ul> <li>Refactor modules for endpoints in <code>/api/</code> but not in <code>/api/v2/</code> (#3132).</li> </ul> </li> <li>Testing:<ul> <li>Run some GitHub Actions when targeting <code>ihb-develop</code> branch (#3138).</li> </ul> </li> <li>Documentation:<ul> <li>Introduce <code>CONTRIBUTING.md</code> (#3157).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>gunicorn</code> to v24 (#3158).</li> <li>Remove <code>pre-commit</code> from project dependencies (#3157).</li> </ul> </li> </ul>"},{"location":"changelog/#2186","title":"2.18.6","text":"<ul> <li>API:<ul> <li>Drop all redundant <code>db.close()</code> statements (#3118).</li> <li>Expunge items before <code>setattr</code> to prevent <code>StaleDataError</code> (#3118).</li> </ul> </li> <li>Database:<ul> <li>Use <code>autoflush=True</code> also for sync db sessions (#3119).</li> </ul> </li> <li>Dependencies:<ul> <li>Drop <code>mypy</code> dev dependency (#3123).</li> <li>Drop support for Python 3.11 (#3129).</li> </ul> </li> <li>Testing:<ul> <li>Implement some more <code>zizmor</code> recommendations (#3121, #3122, #3124, #3125).</li> <li>Remove <code>GitHubSecurityLab/actions-permissions/monitor</code> to avoid TLS issues (#3124).</li> <li>Bump <code>uv</code> version in GitHub Actions (#3124).</li> </ul> </li> </ul>"},{"location":"changelog/#2185","title":"2.18.5","text":"<ul> <li>API:<ul> <li>Skip email-sending logic when email settings are not present (#3102).</li> <li>Clarify viewer-path endpoint (#3096).</li> </ul> </li> <li>Internals:<ul> <li>Fix bug where <code>merge_type_filters</code> would mutate one of its arguments (#3116).</li> </ul> </li> <li>Documentation:<ul> <li>Fix OAuth API documentation (#3106).</li> </ul> </li> <li>Testing:<ul> <li>Add script to run OAuth tests locally (#3110).</li> <li>Fix OAuth test by including <code>fastapi-users</code> cookies (#3110).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>fastapi</code> to v0.128 (#3110).</li> <li>Bump <code>sqlmodel</code> to v0.0.31 (#3110).</li> <li>Bump <code>fastapi-users</code> to v15.0.3 (#3110).</li> <li>Bump <code>uvicorn</code> to v0.40 (#3110).</li> <li>Bump <code>pydantic-settings</code> to v2.12.0 (#3110).</li> </ul> </li> </ul>"},{"location":"changelog/#2184","title":"2.18.4","text":"<ul> <li>Include <code>LICENSE</code> file in distribution (#3099, #3100).</li> </ul>"},{"location":"changelog/#2183","title":"2.18.3","text":"<ul> <li>API:<ul> <li>Add <code>resource_id</code> query parameter for admin jobs endpoint (#3082).</li> <li>Support custom email claim in OIDC provider (#3095).</li> </ul> </li> <li>Runner:<ul> <li>Include <code>use_mem_per_cpu: bool = False</code> flag for SLURM configuration (#3078).</li> <li>Replace large traceback with placeholder (#3081).</li> </ul> </li> <li>Settings:<ul> <li>Introduce <code>OAuthSettings.OAUTH_EMAIL_CLAIM</code> (#3095).</li> </ul> </li> <li>Dependencies:<ul> <li>Move from <code>poetry</code> optional dependency groups to standard dependency groups for <code>dev</code> and <code>docs</code> (#3086).</li> </ul> </li> <li>Internal:<ul> <li>Move <code>fractal_server</code> to <code>src/fractal_server</code> (#3086). Reverted with #3092.</li> <li>Move from <code>poetry</code> to <code>uv</code> (#3086, #3088, #3089).</li> </ul> </li> <li>Testing:<ul> <li>Implement several <code>zizmor</code> recommendations (#3077).</li> <li>Add test for include/exclude options in build backend (#3089).</li> </ul> </li> </ul>"},{"location":"changelog/#2182","title":"2.18.2","text":"<ul> <li>API:<ul> <li>Review logs in <code>read_log_file</code> aux function (#3073).</li> <li>Review the job-list endpoints, so that their response is sorted and they do not check for project ownership information (#3076).</li> </ul> </li> <li>Database:<ul> <li>Drop <code>TaskGroupV2.venv_size_in_kB</code> and <code>TaskGroupV2.venv_file_number</code> (#3075).</li> </ul> </li> <li>Task-group lifecycle:<ul> <li>Stop measuring venv size and venv file number (#3075).</li> </ul> </li> </ul>"},{"location":"changelog/#2181","title":"2.18.1","text":"<ul> <li>Database:<ul> <li>Drop <code>server_default</code> for <code>is_owner</code>, <code>is_verified</code> and <code>permissions</code> in <code>LinkUserProjectV2</code> (#3058).</li> <li>Drop <code>server_default</code> for <code>UserOAuth.project_dirs</code> and drop column <code>UserOAuth.project_dir</code> (#3058).</li> </ul> </li> </ul>"},{"location":"changelog/#2180","title":"2.18.0","text":"<p>NOTE: This version requires running a data-migration script (<code>fractalctl update-db-data</code>).</p> <p>WARNING: Before upgrading to this version, make sure that no jobs are marked as submitted in the current database tables.</p> <p>The main contents of this release are the introduction of the project sharing and a review of the authorization scheme for <code>fractal-data</code>.</p> <ul> <li>API:<ul> <li>Add project-owner endpoints <code>/api/v2/project/{project_id}/guest/</code> (#2999).</li> <li>Add project-guest endpoints <code>/api/v2/project/invitation/</code>, <code>/api/v2/project/{project_id}/access/</code> and <code>/api/v2/project/{project_id}/access/accept/</code> (#2999).</li> <li>Add project-sharing admin endpoint (#2999).</li> <li>Add granular access-control rules to <code>/api/v2</code> endpoints, valid for both project owners and guests (#2999, #3029).</li> <li>Add pagination to <code>GET /admin/v2/task-group/activity/</code> and <code>GET /admin/v2/task-group/</code> (#3023).</li> <li>Do not cast endpoint return values to <code>PaginationResponse[X]</code> (#3023).</li> <li>Reduce API logging level for some endpoints (#3010).</li> <li>Modify <code>GET /auth/current-user/allowed-viewer-paths/</code> logic, with <code>include_shared_projects</code> query parameter (#3031, #3069).</li> <li>Add validator for paths to forbid parent-directory references (#3031).</li> <li>Add check to <code>PATCH /auth/users/{user_id}/</code> when patching <code>project_dirs</code> (#3043, #3069).</li> <li>Review job-submission endpoint (#3041).</li> <li>Prevent submissions if <code>Resource.prevent_new_submissions</code> is set (#3042).</li> <li>Add <code>normpath</code> to <code>AbsolutePathStr</code> type (#3050).</li> <li>Split <code>zarr_dir</code> request body argument of <code>POST /project/{project_id}/dataset/</code> into <code>project_dir</code> and <code>zarr_subfolder</code> (#3051).</li> <li>Add <code>zarr_dir</code> validity check in <code>POST /project/{project_id}/dataset/import/</code> (#3051).</li> <li>Remove <code>zarr_dir</code> request body argument from <code>PATCH /project/{project_id}/dataset/{dataset_id}/</code> (#3051).</li> <li>Validate <code>images</code> in <code>DatasetImport</code> (#3057).</li> <li>Sort response of <code>GET /admin/v2/task-group/activity/</code> starting with the most recent activities (#3066).</li> </ul> </li> <li>App:<ul> <li>Add <code>SlowResponseMiddleware</code> middleware (#3035, #3038, #3060).</li> </ul> </li> <li>Settings:<ul> <li>Add <code>Settings.FRACTAL_LONG_REQUEST_TIME</code> configuration variable (#3035).</li> </ul> </li> <li>Runner:<ul> <li>Improve logging for when dataset or workflow is not found in the database (#3046).</li> <li>Prevent job-execution from continuing if <code>Resource.prevent_new_submissions</code> is set (#3042).</li> <li>Espose <code>shebang_line</code> in resource runner configuration (#3064).</li> </ul> </li> <li>Database:<ul> <li>Add <code>Resource.prevent_new_submissions</code> boolean flag (#3042).</li> <li>Add project-sharing-related <code>LinkUserProjectV2</code> columns (#2999).</li> <li>Move <code>UserOAuth.project_dir</code> to <code>.project_dirs</code> and drop <code>UserGrop.viewer_paths</code> (#3031).</li> <li>Enforce max one submitted <code>JobV2</code> per <code>DatasetV2</code> (#3044).</li> <li>Create 2.18.0 data-migration script (#3031, #3050).</li> </ul> </li> <li>Settings:<ul> <li>Drop <code>DataSettings</code> (#3031).</li> <li>Reduce API logging level for some endpoints (#3010).</li> </ul> </li> <li>Internal:<ul> <li>Remove the \"V2\" label from names of internal schemas and API route tags (#3037).</li> </ul> </li> <li>Testing:<ul> <li>Expand SLURM-batching-heuristics test (#3011).</li> <li>Also validate resources and profiles in <code>migrations.yml</code> action (#3046).</li> <li>Replace <code>MockCurrentUser.user_kwargs</code> with explicit keyword arguments (#3061).</li> </ul> </li> <li>Dependencies:<ul> <li>Support Python 3.14 (#3015).</li> </ul> </li> </ul>"},{"location":"changelog/#2172","title":"2.17.2","text":"<p>NOTE: Starting from this version, the <code>unzip</code> command-line tool must be available (see #2978).</p> <ul> <li>API:<ul> <li>Allow reading logs from zipped job folders (#2978).</li> <li>Do not raise 422 from check-owner functions (#2988).</li> <li>Remove <code>GET /admin/v2/project/</code>, <code>GET /api/v2/dataset/</code> and <code>GET /api/v2/workflow/</code> (#2989).</li> </ul> </li> <li>Database:<ul> <li>Add indexes to <code>HistoryImageCache</code> and <code>HistoryUnit</code> foreign keys to improve <code>DatasetV2</code> deletion time (#2987).</li> <li>Drop <code>ProjectV2.user_list</code> relationship (#2991).</li> <li>Adopt <code>onclause</code> on <code>JOIN</code> statements (#2993).</li> </ul> </li> <li>Runner:<ul> <li>Handle default <code>batch_size</code> value for local runner (#2949).</li> <li>Rename <code>SudoSlurmRunner</code> into <code>SlurmSudoRunner</code> (#2980).</li> <li>Ignore some SLURM/stderr patterns when populating <code>executor_error_log</code> (#2984).</li> </ul> </li> <li>App settings:<ul> <li>Transform <code>DB_ECHO</code> configuration variable from boolean to <code>Literal['true', 'false']</code> (#2985).</li> </ul> </li> <li>Documentation:<ul> <li>Major review of documentation, including making it up-to-date with v2.17.0 and relying more on autogenerated contents (#2949, #2983).</li> </ul> </li> <li>Development:<ul> <li>Add <code>shellcheck</code> to precommit, for <code>fractal-server/</code> files (#2986).</li> <li>Replace <code>reorder_python_imports</code>, <code>flake8</code> and <code>black</code> with <code>ruff</code> in pre-commit (#2994).</li> </ul> </li> <li>Task-group lifecycle:<ul> <li>Support extras in pinned packages names (#3000).</li> </ul> </li> <li>Testing:<ul> <li>Update benchmarks (#2990).</li> <li>Drop <code>benchmarks_runner</code> (#2990).</li> </ul> </li> </ul>"},{"location":"changelog/#2171","title":"2.17.1","text":"<ul> <li>Runner:<ul> <li>Raise an error for a non-converter task running on an empty image list (#2971).</li> </ul> </li> <li>Database:<ul> <li>Apply all database-schema changes made possible by 2.17.0 data migration (#2972), namely:<ul> <li>Drop <code>user_settings</code> table and corresponding <code>UserOAuth.user_settings_id</code> foreign key.</li> <li>Make <code>resource_id</code> foreign key non nullable in <code>ProjectV2</code> and <code>TaskGroupV2</code> tables.</li> <li>Drop <code>server_default=\"/PLACEHOLDER</code> for <code>UserOAuth.project_dir</code>.</li> </ul> </li> </ul> </li> <li>App:<ul> <li>Streamline graceful-shutdown logic in lifespan (#2972).</li> </ul> </li> <li>Settings:<ul> <li>Accept float values for <code>FRACTAL_GRACEFUL_SHUTDOWN_TIME</code> (#2972).</li> </ul> </li> <li>Testing:<ul> <li>Update testing database with 2.17.0 data migration (#2974).</li> <li>Introduce <code>generic_task_converter</code> in <code>fractal-tasks-mock</code> (#2976).</li> </ul> </li> </ul>"},{"location":"changelog/#2170","title":"2.17.0","text":"<p>This version requires running a data-migration script (<code>fractalctl update-db-data</code>), see detailed instructions.</p> <p>The main content of this release is the introduction of the computational resource&amp;profile concepts, and a review of the application settings.</p> <ul> <li>API (main PRs: #2809, #2870, #2877, #2884, #2911, #2915, #2925, #2940, #2941, #2943, #2956):<ul> <li>Introduce API for <code>Resource</code> and <code>Profile</code> models.</li> <li>Drop API for user settings.</li> <li>Drop handling of deprecated <code>DatasetV2.filters</code> attribute when creating dataset dumps (#2917).</li> <li>Enable querying users by <code>resource_id</code> (#2877).</li> <li>Check matching-<code>resource_id</code> upon job submission (#2896).</li> <li>Treat <code>TaskGroupV2.resource_id</code> as not nullable (#2896).</li> <li>Split <code>/api/settings/</code> into smaller-scope endpoints.</li> <li>Update rules for read access to task groups with resource information (#2941).</li> <li>Only show tasks and task groups associated to current user's <code>Resource</code> (#2906, #2943).</li> <li>Add pagination to admin job and task endpoints (#2958).</li> <li>Add <code>task_type</code> query parameter to admin task endpoint (#2958).</li> </ul> </li> <li>Task-group lifecycle:<ul> <li>Rely on resource and profile rather than user settings (#2809).</li> <li>Fix postponed commit after task-group deletion (#2964).</li> <li>Add Python3.14 option for task collection (#2965).</li> </ul> </li> <li>Runner<ul> <li>Rely on resource and profile rather than user settings (#2809).</li> <li>Make <code>extra_lines</code> a non-optional list in SLURM configuration (#2893).</li> <li>Enable <code>user_local_exports</code> on SLURM-SSH runner.</li> </ul> </li> <li>Database and models (also #2931):<ul> <li>Introduce <code>Resource</code> and <code>Profile</code> models (#2809).</li> <li>Introduce <code>resource_id</code> foreign key for task-group and project models (#).</li> <li>Move <code>project_dir</code> and <code>slurm_accounts</code> from <code>UserSettings</code> to <code>UserOAuth</code>.</li> <li>Make <code>project_dir</code> required.</li> <li>Discontinue usage of <code>UserSettings</code> table.</li> <li>Add data-migration script for version 2.17.0 (#2933).</li> </ul> </li> <li>Authentication API:<ul> <li>Drop OAuth-based self registration (#2890).</li> </ul> </li> <li>App settings (#2874, #2882, #2895, #2898, #2916, #2922, #2968):<ul> <li>Remove all configuration variables that are now part of <code>Resource</code>s.</li> <li>Split main <code>Settings</code> model into smaller-scope models.</li> <li>Remove email-password encryption.</li> <li>Introduce <code>init-db-data</code> command.</li> <li>Set default <code>FRACTAL_API_MAX_JOB_LIST_LENGTH</code> to 25 (#2928).</li> <li>Introduce <code>FRACTAL_DEFAULT_GROUP_NAME</code>, set to either <code>\"All\"</code> or <code>None</code> (#2939).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>fastapi</code> to v0.120 (#2921).</li> <li>Bump <code>uvicorn</code> to v0.38 (#2921).</li> <li>Bump <code>fastapi-users</code> to v15 (#2907).</li> </ul> </li> <li>Testing and GitHub actions:<ul> <li>Simplify Python environment in documentation GitHub action (#2919).</li> <li>Simplify PyPI-publish GitHub action (#2927).</li> <li>Drop explicit dependency on `python-dotenv (#2921).</li> </ul> </li> <li>Testing:<ul> <li>Introduce <code>pytest-env</code> dependency.</li> <li>Update testing database to version 2.16.6 (#2909).</li> <li>Test OAuth flow with <code>pytest</code> and remove OAuth GHA (#2929).</li> </ul> </li> </ul>"},{"location":"changelog/#2166","title":"2.16.6","text":"<ul> <li>API:<ul> <li>Fix bug in import-workflow endpoint, leading to the wrong task-group being selected (#2863).</li> </ul> </li> <li>Models:<ul> <li>Fix use of custom <code>AttributeFilters</code> type in SQLModel model (#2830).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>pydantic</code> to 2.12.0 (#2830).</li> <li>Bump <code>poetry</code> to 2.2.1 in GitHub actions (#2830).</li> </ul> </li> <li>Testing:<ul> <li>Add pre-commit rule to prevent custom types is models (#2830).</li> </ul> </li> </ul>"},{"location":"changelog/#2165","title":"2.16.5","text":"<ul> <li>Dependencies:<ul> <li>Bump <code>fastapi</code>, <code>sqlmodel</code>, <code>uvicorn</code>, <code>pydantic-settings</code> versions (#2827).</li> </ul> </li> </ul>"},{"location":"changelog/#2164","title":"2.16.4","text":"<ul> <li>Task life cycle:<ul> <li>Switch to PyPI Index API for finding latest package versions (#2790).</li> </ul> </li> <li>SSH:<ul> <li>Bump default lock-acquisition timeout from 250 to 500 seconds (#2826).</li> <li>Introduce structured logs for SSH-lock dynamics (#2826).</li> </ul> </li> <li>API:<ul> <li>Replace <code>HTTP_422_UNPROCESSABLE_CONTENT</code> with <code>HTTP_422_UNPROCESSABLE_CONTENT</code> (#2790).</li> </ul> </li> <li>Internal:<ul> <li>Move <code>app.runner</code> into <code>runner</code> (#2814).</li> </ul> </li> <li>Testing:<ul> <li>Use function-scoped base folder for backend (#2793).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>fastapi</code> version (#2790).</li> <li>Bump <code>uvicorn-worker</code> and <code>sqlmodel</code> versions (#2792).</li> </ul> </li> </ul>"},{"location":"changelog/#2163","title":"2.16.3","text":"<ul> <li>Task life cycle:<ul> <li>Move post-pixi-installation logic into the same SLURM job as <code>pixi install</code>, for SSH/SLURM deployment (#2787).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>cryptography</code> (#2788).</li> </ul> </li> </ul>"},{"location":"changelog/#2162","title":"2.16.2","text":"<ul> <li>Task life cycle:<ul> <li>Move <code>pixi install</code> execution to SLURM jobs, for SSH/SLURM deployment (#2784).</li> <li>Add <code>SLURM_CONFIG</code> to <code>PixiSettings</code> (#2784).</li> </ul> </li> </ul>"},{"location":"changelog/#2161","title":"2.16.1","text":"<ul> <li>Runner:<ul> <li>Drop top-level executor-error-related logs (#2783).</li> <li>Drop obsolete <code>FRACTAL_SLURM_SBATCH_SLEEP</code> configuration variable (#2785).</li> </ul> </li> </ul>"},{"location":"changelog/#2160","title":"2.16.0","text":"<ul> <li>Runner:<ul> <li>Record SLURM-job stderr in <code>JobV2.executor_error_log</code> (#2750, #2771, #2773).</li> </ul> </li> <li>Task life cycle:<ul> <li>Support both pre-pinning and post-pinning of dependencies (#2761).</li> <li>Drop <code>FRACTAL_MAX_PIP_VERSION</code> configuration variable (#2766).</li> <li>Make TaskGroup deletion a lifecycle operation (#2759).</li> </ul> </li> <li>Testing:<ul> <li>Add <code>out_of_memory</code> mock task (#2770).</li> <li>Make fixtures sync when possible (#2776).</li> <li>Bump pytest-related dependencies (#2776).</li> </ul> </li> </ul>"},{"location":"changelog/#2159","title":"2.15.9","text":"<ul> <li>API:<ul> <li>In <code>POST /api/v2/project/{project_id}/status/images/</code>, include all available types&amp;attributes (#2762).</li> </ul> </li> <li>Internal:<ul> <li>Optimize <code>fractal_server.images.tools.aggregate_attributes</code> (#2762).</li> </ul> </li> </ul>"},{"location":"changelog/#2158","title":"2.15.8","text":"<ul> <li>Runner:<ul> <li>Split <code>SlurmJob</code> submission into three steps, reducing SSH connections (#2749).</li> <li>Deprecate experimental <code>pre_submission_commands</code> feature (#2749).</li> </ul> </li> <li>Documentation:<ul> <li>Fix <code>Fractal Users</code> documentation page (#2738).</li> <li>Improve documentation for Pixi task collection (#2742).</li> </ul> </li> <li>Task life cycle:<ul> <li>Add test to lock <code>FRACTAL_MAX_PIP_VERSION</code> with latest version on PyPI (#2752).</li> <li>Use pixi v0.54.1 in tests (#2758).</li> </ul> </li> <li>Internal<ul> <li>Improve type hints (#2739).</li> </ul> </li> <li>SSH:<ul> <li>Add a decorator to open a new connection (socket) if the decorated function hits a <code>NoValidConnectionError</code> or a <code>OSError</code> (#2747).</li> <li>Remove multiple-attempts logic (#2747).</li> </ul> </li> </ul>"},{"location":"changelog/#2157","title":"2.15.7","text":"<ul> <li>API:<ul> <li>Capture SSH related failure in submit-job endpoint (#2734).</li> </ul> </li> <li>Task life cycle:<ul> <li>Also edit <code>tool.pixi.project.platforms</code> section of <code>pyproject.toml</code>, for <code>pixi</code>-based task collection (#2711).</li> </ul> </li> <li>Internal:<ul> <li>Extend use of <code>UnreachableBranchError</code> in API and runner (#2726).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>fastapi</code> to version <code>0.116.*</code> (#2718).</li> <li>Bump all <code>docs</code> optional dependencies (#2722).</li> <li>Add support for Python 3.13 (#2716).</li> </ul> </li> <li>Documentation:<ul> <li>Remove/fix several obsolete docs pages (#2722).</li> </ul> </li> <li>Testing:<ul> <li>Improve/optimize several tests (#2727).</li> <li>Use <code>poetry</code> 2.1.3 in all GitHub Actions and always install it using <code>pipx</code> (#2731).</li> <li>Move all GitHub Action runners to <code>ubuntu-24.04</code> (#2732).</li> <li>Avoid using <code>threading</code> private method (#2733).</li> </ul> </li> </ul>"},{"location":"changelog/#2156","title":"2.15.6","text":"<ul> <li>Runner:<ul> <li>Remove obsolete <code>JobExecutionError</code> attributes and <code>TaskExecutionError</code> handling (#2708).</li> <li>Always interpret <code>needs_gpu</code> string as a boolean (#2706).</li> </ul> </li> <li>Task life cycle:<ul> <li>Use <code>bash --login</code> for <code>pixi install</code> execution over SSH (#2709).</li> </ul> </li> <li>Development:<ul> <li>Move <code>example</code> folder to root directory (#2720).</li> </ul> </li> </ul>"},{"location":"changelog/#2155","title":"2.15.5","text":"<ul> <li>API:<ul> <li>Update <code>HistoryRun</code> and <code>HistoryUnit</code>s statuses when <code>Workflow</code>s are manually labeled as failed (#2705).</li> </ul> </li> </ul>"},{"location":"changelog/#2154","title":"2.15.4","text":"<ul> <li>Task lifecycle:<ul> <li>Edit <code>pyproject.toml</code> in-place before running <code>pixi install</code> (#2696).</li> </ul> </li> </ul>"},{"location":"changelog/#2153","title":"2.15.3","text":"<ul> <li>API:<ul> <li>Ongoing <code>WorkflowTask</code>s are marked as submitted during <code>Job</code> execution (#2692).</li> </ul> </li> </ul>"},{"location":"changelog/#2152","title":"2.15.2","text":"<ul> <li>API:<ul> <li>Improve logging for <code>PATCH /admin/v2/job/{job_id}/</code> (#2686).</li> <li>Prevent deletion and reordering of <code>WorkflowTask</code>s in <code>Workflow</code>s associated to submitted <code>Job</code>s (#2689).</li> </ul> </li> <li>Database:<ul> <li>Set <code>pool_pre_ping=True</code> for sync db engine (#2676).</li> </ul> </li> <li>Runner:<ul> <li>Update <code>chmod ... -R</code> to <code>chmod -R ...</code> (#2681).</li> <li>Add custom handling of some <code>slurm_load_jobs</code> socket-timeout error (#2683).</li> <li>Remove redundant <code>mkdir</code> in SLURM SSH runner (#2671).</li> <li>Do not write to SLURM stderr from remote worker (#2691).</li> <li>Fix spurious version-mismatch warning in remote worker (#2691).</li> </ul> </li> <li>SSH:<ul> <li>Always set <code>in_stream=False</code> for <code>fabric.Connection.run</code> (#2694).</li> </ul> </li> <li>Testing:<ul> <li>Fix <code>test_FractalSSH.py::test_folder_utils</code> for MacOS (#2678).</li> <li>Add pytest marker <code>fails_on_macos</code> (#2681).</li> <li>Remove patching of <code>sys.stdin</code>, thanks to updates to <code>fabric.Connection.run</code> arguments (#2694).</li> </ul> </li> </ul>"},{"location":"changelog/#2151","title":"2.15.1","text":"<p>This release fixes the reason for yanking 2.15.0.</p> <ul> <li>Database:<ul> <li>Modify <code>JSON-&gt;JSONB</code> database schema migration in-place, so that columns which represent JSON Schemas or <code>meta_*</code> fields remain in JSON form rather than JSONB (#2664, #2666).</li> </ul> </li> </ul>"},{"location":"changelog/#2150-yanked","title":"2.15.0 [yanked]","text":"<p>This release was yanked on PyPI, because its conversion of all JSON columns into JSONB changes the key order, see https://github.com/fractal-analytics-platform/fractal-server/issues/2663.</p> <ul> <li>Database:<ul> <li>Rename <code>TaskGroupV2.wheel_path</code> into <code>TaskGroupV2.archive_path</code> (#2627).</li> <li>Rename <code>TaskGroupV2.pip_freeze</code> into <code>TaskGroupV2.env_info</code> (#2627).</li> <li>Add <code>TaskGroupV2.pixi_version</code> (#2627).</li> <li>Transform every JSON column to JSONB (#2662).</li> </ul> </li> <li>API:<ul> <li>Introduce new value <code>TaskGroupV2OriginEnum.PIXI</code> (#2627).</li> <li>Exclude <code>TaskGroupV2.env_info</code> from API responses (#2627).</li> <li>Introduce <code>POST /api/v2/task/collect/pixi/</code> (#2627).</li> <li>Extend deactivation/reactivation endpoints to pixi task groups (#2627).</li> <li>Block <code>DELETE /api/v2/task-group/{task_group_id}/</code> if a task-group activity is ongoing (#2642).</li> <li>Introduce <code>_verify_non_duplication_group_path</code> auxiliary function (#2643).</li> </ul> </li> <li>Task lifecycle:<ul> <li>Introduce full support for pixi task-group lifecycle (#2627, #2651, #2652, #2654).</li> </ul> </li> <li>SSH:<ul> <li>Introduce <code>FractalSSH.read_remote_text_file</code> (#2627).</li> </ul> </li> <li>Runner:<ul> <li>Fix use of <code>worker_init/extra_lines</code> for multi-image job execution (#2660).</li> <li>Support SLURM configuration options <code>nodelist</code> and <code>exclude</code> (#2660).</li> </ul> </li> <li>App configuration:<ul> <li>Introduce new configuration variable <code>FRACTAL_PIXI_CONFIG_FILE</code> and new attribute <code>Settings.pixi</code> (#2627, #2650).</li> </ul> </li> </ul>"},{"location":"changelog/#21416","title":"2.14.16","text":"<ul> <li>Internal:<ul> <li>Refactor and optimize enrich-image functions (#2620).</li> </ul> </li> <li>Database:<ul> <li>Add indices to <code>HistoryImageCache</code> table (#2620).</li> </ul> </li> <li>SSH:<ul> <li>Increase Paramiko <code>banner_timeout</code> from the default 15 seconds to 30 seconds (#2632).</li> <li>Re-include <code>check_connection</code> upon <code>SlurmSSHRunner</code> startup (#2636).</li> </ul> </li> <li>Testing:<ul> <li>Introduce benchmarks for database operations (#2620).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>cryptography</code>, <code>packaging</code> and <code>python-dotenv</code> dependencies (#2630).</li> </ul> </li> </ul>"},{"location":"changelog/#21415","title":"2.14.15","text":"<ul> <li>API:<ul> <li>Add required <code>workflowtask_id</code> query parameter to <code>verify-unique-types</code> endpoint (#2619).</li> <li>Enrich images with status within <code>verify-unique-types</code> endpoint, when necessary (#2619).</li> </ul> </li> </ul>"},{"location":"changelog/#21414","title":"2.14.14","text":"<ul> <li>API:<ul> <li>Fix <code>GET /api/v2/task-group/</code> by adding missing sorting before <code>itertools.groupby</code> (#2614).</li> </ul> </li> <li>Internal:<ul> <li>Drop <code>execute_command_async</code> function (#2611).</li> <li>Introduce <code>TaskType</code> enum (#2612).</li> </ul> </li> </ul>"},{"location":"changelog/#21413","title":"2.14.13","text":"<ul> <li>API:<ul> <li>Group response items of <code>GET /api/v2/task-group/</code> by <code>pkg_name</code> (#2596).</li> <li>Disambiguate response items of <code>GET /api/v2/task-group/</code> (#2596).</li> </ul> </li> <li>Internal:<ul> <li>Introduce <code>UnreachableBranchError</code> (#2596).</li> </ul> </li> <li>Testing:<ul> <li>Enforce task-group non-duplication constraints in <code>task_factory_v2</code> fixture (#2596).</li> </ul> </li> </ul>"},{"location":"changelog/#21412","title":"2.14.12","text":"<ul> <li>Runner:<ul> <li>Enable status-based selection of images to process (#2588).</li> </ul> </li> <li>API:<ul> <li>Remove <code>unit_status</code> query parameter from <code>/project/{project_id}/status/images/</code> (#2588).</li> <li>Remove default type filters from <code>/project/{project_id}/status/images/</code> (#2588).</li> <li>Sort lists of existing attribute values in <code>aggregate_attributes</code> (#2588).</li> </ul> </li> <li>Task-group lifecycle:<ul> <li>Split <code>pip install</code> command into two steps (#2600).</li> </ul> </li> </ul>"},{"location":"changelog/#21411","title":"2.14.11","text":"<ul> <li>Task-group lifecycle:<ul> <li>Support version-pinning for two dependencies in task collection (#2590, #2599).</li> <li>Support version-pinning for previously-missing dependencies in task collection (#2590, #2599).</li> </ul> </li> <li>Development:<ul> <li>Improve <code>mypy</code> configuration in <code>pyproject.toml</code> (#2595).</li> </ul> </li> </ul>"},{"location":"changelog/#21410","title":"2.14.10","text":"<p>This version requires a data-migration script (<code>fractalctl update-db-data</code>).</p> <ul> <li>Database:<ul> <li>Improve data-migration script that is necessary for 2.14.8 (#2594).</li> </ul> </li> </ul>"},{"location":"changelog/#2149","title":"2.14.9","text":"<p>WARNING: Do not release this version, but go directly to 2.14.10.</p> <ul> <li>Task-group lifecycle:<ul> <li>Improve handling of SSH-related errors (#2589).</li> </ul> </li> <li>Database:<ul> <li>Rename data-migration script that is necessary for 2.14.8 (#2592).</li> </ul> </li> </ul>"},{"location":"changelog/#2148","title":"2.14.8","text":"<p>WARNING: Do not release this version, but go directly to 2.14.10.</p> <ul> <li>API:<ul> <li>Update <code>POST /project/{project_id}/workflow/{workflow_id}/wftask/replace-task/</code> so that it re-uses existing workflow task (#2565).</li> </ul> </li> <li>Database:<ul> <li>Add <code>HistoryRun.task_id</code> column (#2565).</li> </ul> </li> <li>Internal:<ul> <li>Refactor: extract <code>enrich_image_list</code> function from <code>/project/{project_id}/status/images/</code> endpoint (#2585).</li> </ul> </li> </ul>"},{"location":"changelog/#2147","title":"2.14.7","text":"<ul> <li>Runner:<ul> <li>Re-include SLURM accounts for both sudo-slurm and ssh-slurm runners (#2580)</li> <li>Re-include use of <code>worker_init</code> for both sudo-slurm and ssh-slurm runners (#2580)</li> </ul> </li> <li>Testing:<ul> <li>Use <code>Optional</code> for argument type hints in mock tasks (#2575).</li> </ul> </li> </ul>"},{"location":"changelog/#2146","title":"2.14.6","text":"<ul> <li>API:<ul> <li>Introduce <code>api/v2/project/{project.id}/workflow/{workflow.id}/version-update-candidates/</code> endpoint (#2556).</li> </ul> </li> <li>Task lifecycle:<ul> <li>Use dedicated SSH connections for lifecycle background tasks (#2569).</li> <li>Also set <code>TaskGroup.version</code> for custom task collections (#2573).</li> </ul> </li> <li>Internal:<ul> <li>Inherit from <code>StrEnum</code> rather than <code>str, Enum</code> (#2561).</li> <li>Run <code>pyupgrade</code> on codebase (#2563).</li> <li>Remove init file of obsolete folder (#2571).</li> </ul> </li> <li>Dependencies:<ul> <li>Deprecate Python 3.10 (#2561).</li> </ul> </li> </ul>"},{"location":"changelog/#2145","title":"2.14.5","text":"<p>This version introduces an important internal refactor of the runner component, with the goal of simplifying the SLURM version.</p> <ul> <li>Runner:<ul> <li>Make <code>submit/multisubmit</code> method take static arguments, rather than a callable (#2549).</li> <li>Replace input/output pickle files with JSON files (#2549).</li> <li>Drop possibility of non-<code>utf-8</code> encoding for <code>_run_command_as_user</code> function (#2549).</li> <li>Avoid local-remote-local round trip for task parameters, by writing the local file first (#2549).</li> <li>Stop relying on positive/negative return codes to produce either <code>TaskExecutionError</code>/<code>JobExecutionError</code>, in favor of only producing <code>TaskExecutionError</code> at the lowest task-execution level (#2549).</li> <li>Re-implement <code>run_single_task</code> within SLURM remote <code>worker</code> function (#2549).</li> <li>Drop <code>TaskFiles.remote_files_dict</code> (#2549).</li> <li>Drop obsolete <code>FRACTAL_SLURM_ERROR_HANDLING_INTERVAL</code> config variable (#2549).</li> <li>Drop obsolete <code>utils_executors.py</code> module (#2549).</li> </ul> </li> <li>Dependencies:<ul> <li>Drop <code>cloudpickle</code> dependency (#2549).</li> <li>Remove copyright references to <code>clusterfutures</code> (#2549).</li> </ul> </li> <li>Testing:<ul> <li>Introduce <code>slurm_alive</code> fixture that checks <code>scontrol ping</code> results (#2549).</li> </ul> </li> </ul>"},{"location":"changelog/#2144","title":"2.14.4","text":"<ul> <li>API:<ul> <li>Replace most <code>field_validator</code>s with <code>Annotated</code> types, and review <code>model_validator</code>s (#2504).</li> </ul> </li> <li>SSH:<ul> <li>Remove Python-wrapper layer for <code>tar</code> commands (#2554).</li> <li>Add <code>elapsed</code> information to SSH-lock-was-acquired log (#2558).</li> </ul> </li> </ul>"},{"location":"changelog/#2143","title":"2.14.3","text":"<ul> <li>Runner:<ul> <li>Skip creation/removal of folder copy in compress-folder module (#2553).</li> <li>Drop obsolete <code>--extra-import-paths</code> option from SLURM remote worker (#2550).</li> </ul> </li> </ul>"},{"location":"changelog/#2142","title":"2.14.2","text":"<ul> <li>API:<ul> <li>Handle inaccessible <code>python_interpreter</code> or <code>package_root</code> in custom task collection  (#2536).</li> <li>Do not raise non-processed-images warning if the previous task is a converter (#2546).</li> </ul> </li> <li>App:<ul> <li>Use <code>Enum</code> values in f-strings, for filenames and error messages (#2540).</li> </ul> </li> <li>Runner:<ul> <li>Handle exceptions in post-task-execution runner code (#2543).</li> </ul> </li> </ul>"},{"location":"changelog/#2141","title":"2.14.1","text":"<ul> <li>API:<ul> <li>Add <code>POST /project/{project_id}/dataset/{dataset_id}/images/non-processed/</code> endpoint (#2524, #2533).</li> </ul> </li> <li>Runner:<ul> <li>Do not create temporary output-pickle files (#2539).</li> <li>Set logging level to <code>DEBUG</code> within <code>compress_folder</code> and <code>extract_archive</code> modules (#2539).</li> <li>Transform job-error log into warning (#2539).</li> <li>Drop <code>FRACTAL_SLURM_INTERVAL_BEFORE_RETRIEVAL</code> (#2525, #2531).</li> <li>Increase <code>MAX_NUM_THREADS</code> from 4 to 12 (#2520).</li> <li>Support re-deriving an existing image with a non-trivial <code>origin</code> (#2527).</li> </ul> </li> <li>Testing:<ul> <li>Adopt ubuntu24 containers for CI (#2530).</li> <li>Do not run Python3.11 container CI for PRs, but only for merges (#2519).</li> <li>Add mock wheel file and update assertion for pip 25.1 (#2523).</li> <li>Optimize <code>test_reactivate_local_fail</code> (#2511).</li> <li>Replace <code>fractal-tasks-core</code> with <code>testing-tasks-mock</code> in tests (#2511).</li> <li>Improve flaky test (#2513).</li> </ul> </li> </ul>"},{"location":"changelog/#2140","title":"2.14.0","text":"<p>This release mostly concerns the new database/runner integration in view of providing more granular history/status information. This includes a full overhaul of the runner.</p> <ul> <li>API:<ul> <li>Add all new status endpoints.</li> <li>Add <code>GET /job/latest/</code> endpoint (#2389).</li> <li>Make request body required for <code>replace-task</code> endpoint (#2355).</li> <li>Introduce shared tools for pagination.</li> <li>Remove <code>valstr</code> validator and introduce <code>NonEmptyString</code> in schemas (#2352).</li> </ul> </li> <li>Database<ul> <li>New tables <code>HistoryRun</code>, <code>HistoryUnit</code> and <code>HistoryImageCache</code> tables.</li> <li>Drop attribute/type filters from dataset table.</li> <li>Add <code>type_filters</code> column to job table.</li> <li>Use <code>ondelete</code> flag in place of custom DELETE-endpoint logics.</li> </ul> </li> <li>Runner<ul> <li>Full overhaul of runners. Among the large number of changes, this includes:<ul> <li>Fully drop the <code>concurrent.futures</code> interface.</li> <li>Fully drop the multithreaded nature of SLURM runners, in favor of a more linear submission/retrieval flow.</li> <li>New <code>BaseRunner</code>, <code>LocalRunner</code>, <code>BaseSlurmRunner</code>, <code>SlurmSSHRunner</code> and <code>SlurmSudoRunner</code> objects.</li> <li>The two SLURM runners now share a large part of base logic.</li> <li>Database updates to <code>HistoryRun</code>, <code>HistoryUnit</code> and <code>HistoryImageCache</code> tables.</li> <li>We do not fill <code>Dataset.history</code> any more.</li> </ul> </li> </ul> </li> <li>Task lifecycle:<ul> <li>Drop hard-coded use of <code>--no-cache-dir</code> for <code>pip install</code> command (#2357).</li> </ul> </li> <li>App:<ul> <li>Obfuscate sensitive information from settings using <code>SecretStr</code> (#2333).</li> <li>Drop <code>FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE</code> obsolete configuration variable (#2359).</li> </ul> </li> <li>Testing:<ul> <li>Use <code>fractal-task-tools</code> to build <code>fractal-tasks-mock</code> manifest (#2374).</li> </ul> </li> <li>Development:<ul> <li>Add <code>codespell</code> to precommit (#2358).</li> <li>Drop obsolete <code>examples</code> folder (#2405).</li> </ul> </li> </ul>"},{"location":"changelog/#2131","title":"2.13.1","text":"<ul> <li>API:<ul> <li>Add <code>AccountingRecord</code> and <code>AccountingRecordSlurm</code> tables (#2267).</li> <li>Add <code>/admin/v2/impersonate</code> endpoint (#2280).</li> <li>Replace <code>_raise_if_naive_datetime</code> with <code>AwareDatetime</code> (#2283).</li> </ul> </li> <li>Database:<ul> <li>Add <code>/admin/v2/accounting/</code> and <code>/admin/v2/accounting/slurm/</code> endpoints (#2267).</li> </ul> </li> <li>Runner:<ul> <li>Populate <code>AccountingRecord</code> from runner (#2267).</li> </ul> </li> <li>App:<ul> <li>Review configuration variables for email-sending (#2269).</li> <li>Reduce error-level log records(#2282).</li> </ul> </li> <li>Testing:<ul> <li>Drop obsolete files/folders from <code>tests/data</code> (#2281).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>httpx</code> to version <code>0.28.*</code> (#2284).</li> </ul> </li> </ul>"},{"location":"changelog/#2130","title":"2.13.0","text":"<p>With this release we switch to Pydantic v2.</p> <ul> <li>Runner:<ul> <li>Deprecate <code>FRACTAL_BACKEND_RUNNER=\"local_experimental\"</code> (#2273).</li> <li>Fully replace <code>clusterfutures</code> classes with custom ones (#2272).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>pydantic</code> to v2 (#2270).</li> <li>Drop <code>clusterfutures</code> dependency (#2272).</li> <li>Drop <code>psutil</code> dependency (#2273).</li> <li>Bump <code>cryptography</code> to version <code>44.0.*</code> (#2274).</li> <li>Bump <code>sqlmodel</code> to version <code>0.0.22</code> (#2275).</li> <li>Bump <code>packaging</code> to version <code>24.*.*</code> (#2275).</li> <li>Bump <code>cloudpickle</code> to version <code>3.1.*</code> (#2275).</li> <li>Bump <code>uvicorn-workers</code> to version <code>0.3.0</code> (#2275).</li> <li>Bump <code>gunicorn</code> to version <code>23.*.*</code> (#2275).</li> <li>Bump <code>httpx</code> to version <code>0.27.*</code> (#2275).</li> </ul> </li> </ul>"},{"location":"changelog/#2121","title":"2.12.1","text":"<p>Note: this version requires a manual update of email-related configuration variables.</p> <ul> <li>API:<ul> <li>Deprecate <code>use_dataset_filters</code> query parameter for <code>/project/{project_id}/dataset/{dataset_id}/images/query/</code> (#2231).</li> </ul> </li> <li>App:<ul> <li>Add fractal-server version to logs (#2228).</li> <li>Review configuration variables for email-sending (#2241).</li> </ul> </li> <li>Database:<ul> <li>Remove <code>run_migrations_offline</code> from <code>env.py</code> and make <code>run_migrations_online</code> sync (#2239).</li> </ul> </li> <li>Task lifecycle:<ul> <li>Reset logger handlers upon success of a background lifecycle operation, to avoid open file descriptors (#2256).</li> </ul> </li> <li>Runner<ul> <li>Sudo/SLURM executor checks the fractal-server version using <code>FRACTAL_SLURM_WORKER_PYTHON</code> config variable, if set (#2240).</li> <li>Add <code>uname -n</code> to SLURM submission scripts (#2247).</li> <li>Handle <code>_COMPONENT_KEY_</code>-related errors in sudo/SLURM executor, to simplify testing (#2245).</li> <li>Drop obsolete <code>SlurmJob.workflow_task_file_prefix</code> for both SSH/sudo executors (#2245).</li> <li>Drop obsolete <code>keep_pickle_files</code> attribute from slurm executors (#2246).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump <code>uvicorn</code> version (#2242).</li> </ul> </li> <li>Testing:<ul> <li>Improve testing of sudo-Slurm executor (#2245, #2246).</li> <li>Introduce <code>container</code> pytest marker (#2249).</li> <li>Split CI GitHub Actions in three jobs: API, not API and Containers (#2249).</li> </ul> </li> </ul>"},{"location":"changelog/#2120","title":"2.12.0","text":"<p>WARNING: The database schema update introduced via this version is non-reversible.</p> <ul> <li>API:<ul> <li>Drop V1 endpoints (#2230).</li> </ul> </li> <li>Database:<ul> <li>Drop V1 tables (#2230).</li> </ul> </li> <li>Runner:<ul> <li>Drop V1 runners (#2230).</li> </ul> </li> <li>Testing:<ul> <li>Drop V1 tests (#2230).</li> <li>Update V2 tests to keep coverage stable (#2230).</li> </ul> </li> </ul>"},{"location":"changelog/#2111","title":"2.11.1","text":"<ul> <li>Database<ul> <li>Drop columns <code>DatasetV2.filters</code> and <code>WorkflowTaskV2.input_filters</code> (#2232).</li> </ul> </li> </ul>"},{"location":"changelog/#2110","title":"2.11.0","text":"<p>This version revamps the filters data structure, and it introduces complex attribute filters.</p> <p>Note: This release requires running <code>fractalctl update-db-data</code>. Some legacy columns will be removed from the database, either as part of the <code>2.11.0</code> data-migration or as part of the <code>2.11.1</code> schema migration. Please make sure you have a database dump.</p> <ul> <li>API:<ul> <li>Align API with new database schemas for filters-related columns (#2168, #2196, #2202).</li> <li>Support importing workflows or datasets with legacy (pre-<code>2.11.0</code>) filters-related fields (#2185, #2227).</li> <li>Avoid blocking operations from the download-job-logs endpoint, when the zip archive of a running job is requested (#2225).</li> <li>Update and simplify <code>/api/v2/project/{project_id}/status/</code>, dropping use of temporary job files (#2169).</li> <li>Add new (experimental) <code>/project/{project_id}/workflow/{workflow_id}/type-filters-flow/</code> endpoint (#2208).</li> </ul> </li> <li>Database:<ul> <li>Update table schemas for all filters-related columns:<ul> <li>Always handle attribute- and type-filters in different columns (#2168).</li> <li>Update attribute-filter-values type from scalar to list (#2168, #2196).</li> <li>Deprecate attribute filters for <code>WorkflowTaskV2</code> (#2168).</li> <li>Add attribute filters to <code>JobV2</code> (#2168).</li> </ul> </li> <li><code>2.11.0</code> data-migration script (#2168, #2202, #2208, #2209).</li> </ul> </li> <li>Runner:<ul> <li>Introduce database writes in runner component, to replace the use of temporary files (#2169).</li> <li>Use <code>TaskV2.input_types</code> for filtering, rather than validation (#2191, #2196).</li> <li>Make job-execution background-task function sync, to make it transparent that it runs on a thread (#2220).</li> <li>Remove all filters from <code>TaskOutput</code> (#2190).</li> </ul> </li> <li>Task Collection:<ul> <li>Improve logs handling for failed task collections (#2192)</li> </ul> </li> <li>Testing:<ul> <li>Speed up CI by splitting it into more jobs (#2210).</li> </ul> </li> </ul>"},{"location":"changelog/#2106","title":"2.10.6","text":"<ul> <li>Task lifecycle:<ul> <li>Use unique logger names for task-lifecycle operations (#2204).</li> </ul> </li> </ul>"},{"location":"changelog/#2105","title":"2.10.5","text":"<ul> <li>App:<ul> <li>Add missing space in \"To\" field for email settings (#2173).</li> </ul> </li> <li>Testing:<ul> <li>Improve configuration for coverage GitHub Action step (#2175).</li> <li>Add <code>persist-credentials: false</code> to <code>actions/checkout@v4</code> GitHub Action steps (#2176).</li> </ul> </li> <li>Dependencies:<ul> <li>Require <code>bumpver&gt;2024.0</code> (#2179).</li> </ul> </li> </ul>"},{"location":"changelog/#2104","title":"2.10.4","text":"<ul> <li>Switch to poetry v2 (#2165).</li> <li>Require Python &lt;3.13 (#2165).</li> </ul>"},{"location":"changelog/#2103","title":"2.10.3","text":"<p>Note: this version fixes a bug introduced in version 2.10.1.</p> <ul> <li>API:<ul> <li>Fix bug in <code>POST /api/v2/project/{p_id}/workflow/{w_id}/wftask/replace-task/</code> endpoint (#2163).</li> <li>Add validation for <code>.whl</code> filename (#2147).</li> <li>Trim whitespaces in <code>DatasetCreateV2.zarr_dir</code> (#2138).</li> <li>Support sending emails upon new OAuth signup (#2150).</li> </ul> </li> <li>App:<ul> <li>Introduce configuration for email settings (#2150).</li> </ul> </li> <li>Command-line interface:<ul> <li>Add <code>fractalctl email-settings</code> (#2150).</li> </ul> </li> <li>Dependencies:<ul> <li>Add direct dependency on <code>cryptography</code> (#2150).</li> </ul> </li> <li>Testing:<ul> <li>Introduce <code>mailpit</code>-based end-to-end test of email sending (#2150).</li> </ul> </li> </ul>"},{"location":"changelog/#2102","title":"2.10.2","text":"<ul> <li>App:<ul> <li>Add <code>FRACTAL_PIP_CACHE_DIR</code> configuration variable (#2141).</li> </ul> </li> <li>Tasks life cycle:<ul> <li>Prevent deactivation of task groups with <code>\"github.com\"</code> in pip-freeze information (#2144).</li> </ul> </li> <li>Runner:<ul> <li>Handle early shutdown for sudo SLURM executor (#2132).</li> <li>Fix repeated setting of <code>timestamp_ended</code> in task-group reactivation (#2140).</li> </ul> </li> </ul>"},{"location":"changelog/#2101","title":"2.10.1","text":"<ul> <li>API:<ul> <li>Add <code>POST /api/v2/project/{p_id}/workflow/{w_id}/wftask/replace-task/</code> endpoint (#2129).</li> </ul> </li> <li>Testing:<ul> <li>Use system postgresql in GitHub actions, rather than independent container (#2199).</li> </ul> </li> </ul>"},{"location":"changelog/#2100","title":"2.10.0","text":"<ul> <li>API:<ul> <li>Major update of <code>POST /api/v2/task/collect/pip/</code>, to support wheel-file upload (#2113).</li> </ul> </li> <li>Testing:<ul> <li>Add test of private task collection (#2126).</li> </ul> </li> </ul>"},{"location":"changelog/#292","title":"2.9.2","text":"<ul> <li>API<ul> <li>Remove <code>cache_dir</code> and use <code>project_dir/.fractal_cache</code> (#2121).</li> </ul> </li> <li>Docs<ul> <li>Improve docstrings and reduce mkdocs warnings (#2122).</li> </ul> </li> </ul>"},{"location":"changelog/#291","title":"2.9.1","text":"<ul> <li>Task collection:<ul> <li>Fix bug in wheel-based SSH task-collection (#2119).</li> </ul> </li> <li>Testing:<ul> <li>Re-include a specific test previously skipped for Python 3.12 (#2114).</li> <li>Add metadata to <code>fractal-tasks-mock</code> package (#2117).</li> </ul> </li> <li>Docs:<ul> <li>Add info about working versions.</li> </ul> </li> </ul>"},{"location":"changelog/#290","title":"2.9.0","text":"<p>WARNING 1: This version drops support for sqlite, and removes the configuration variables <code>DB_ENGINE</code> and <code>SQLITE_PATH</code>.</p> <p>WARNING 2: This version removes the <code>CollectionStateV2</code> database table. Make sure you have a database dump before running <code>fractalctl set-db</code>, since this operation cannot be undone.</p> <ul> <li>API<ul> <li>Remove <code>GET /api/v2/task/collect/{state_id}/</code> endpoint (#2010).</li> <li>Remove <code>active</code> property from <code>PATCH /api/v2/task-group/{task_group_id}/</code> (#2033).</li> <li>Add <code>GET /api/v2/task-group/activity/</code> endpoint (#2005, #2027).</li> <li>Add <code>GET /api/v2/task-group/activity/{task_group_activity_id}/</code> endpoint (#2005).</li> <li>Add <code>GET /admin/v2/task-group/activity/</code> endpoint (#2005, #2027).</li> <li>Add <code>POST /api/v2/task-group/{task_group_id}/{deactivate|reactivate}</code> endpoints (#2033, #2066, #2078).</li> <li>Add <code>POST /admin/v2/task-group/{task_group_id}/{deactivate|reactivate}</code> endpoints (#2062, #2078).</li> <li>Remove <code>GET /auth/current-user/viewer-paths/</code> (#2096).</li> <li>Add <code>GET /auth/current-user/allowed-viewer-paths/</code>, with logic for <code>fractal-vizarr-viewer</code> authorization (#2096).</li> <li>Add <code>category</code>, <code>modality</code> and <code>author</code> query parameters to <code>GET /admin/v2/task/</code> (#2102).</li> <li>Add <code>POST /auth/group/{group_id}/add-user/{user_id}/</code> (#2101).</li> <li>Add <code>POST /auth/group/{group_id}/remove-user/{user_id}/</code> (#2101, #2111).</li> <li>Add <code>POST /auth/users/{user_id}/set-groups/</code> (#2106).</li> <li>Remove <code>new_user_ids</code> property from <code>PATCH /auth/group/{group_id}/</code> (#2101).</li> <li>Remove <code>new_group_ids</code> property from <code>PATCH /auth/users/{user_id}/</code> (#2106).</li> <li>Internals:</li> <li>Fix bug in <code>_get_collection_task_group_activity_status_message</code> (#2047).</li> <li>Remove <code>valutc</code> validator for timestamps from API schemas, since it does not match with <code>psycopg3</code> behavior (#2064).</li> <li>Add query parameters <code>timestamp_last_used_{min|max}</code> to <code>GET /admin/v2/task-group/</code> (#2061).</li> <li>Remove <code>_convert_to_db_timestamp</code> and add <code>_raise_if_naive_datetime</code>: now API only accepts timezone-aware datetimes as query parameters (#2068).</li> <li>Remove <code>_encode_as_utc</code>: now timestamps are serialized in JSONs with their own timezone (#2081).</li> </ul> </li> <li>Database<ul> <li>Drop support for sqlite, and remove the <code>DB_ENGINE</code> and <code>SQLITE_PATH</code> configuration variables (#2052).</li> <li>Add <code>TaskGroupActivityV2</code> table (#2005).</li> <li>Drop <code>CollectionStateV2</code> table (#2010).</li> <li>Add <code>TaskGroupV2.pip_freeze</code> nullable column (#2017).</li> <li>Add  <code>venv_size_in_kB</code> and <code>venv_file_number</code> to <code>TaskGroupV2</code> (#2034).</li> <li>Add <code>TaskGroupV2.timestamp_last_used</code> column, updated on job submission (#2049, #2061, #2086).</li> </ul> </li> <li>Task-lifecycle internals:<ul> <li>Refactor task collection and database-session management in background tasks (#2030).</li> <li>Update <code>TaskGroupActivityV2</code> objects (#2005).</li> <li>Update filename and path for task-collection scripts (#2008).</li> <li>Copy wheel file into <code>task_group.path</code> and update <code>task_group.wheel_path</code>, for local task collection (#2020).</li> <li>Set <code>TaskGroupActivityV2.timestamp_ended</code> when collections terminate (#2026).</li> <li>Refactor bash templates and add <code>install_from_freeze.sh</code> (#2029).</li> <li>Introduce background operations for local reactivate/deactivate (#2033).</li> <li>Introduce background operations for SSH reactivate/deactivate (#2066).</li> <li>Fix escaping of newlines within f-strings, in logs (#2028).</li> <li>Improve handling of task groups created before 2.9.0 (#2050).</li> <li>Add <code>TaskGroupCreateV2Strict</code> for task collections (#2080).</li> <li>Always create <code>script_dir_remote</code> in SSH lifecycle background tasks (#2089).</li> <li>Postpone setting <code>active=False</code> in task-group deactivation to after all preliminary checks (#2100).</li> </ul> </li> <li>Runner:<ul> <li>Improve error handling in <code>_zip_folder_to_file_and_remove</code> (#2057).</li> <li>Improve error handling in <code>FractalSlurmSSHExecutor</code> <code>handshake</code> method (#2083).</li> <li>Use the \"spawn\" start method for the multiprocessing context, for the <code>ProcessPoolExecutor</code>-based runner (#2084).</li> <li>Extract common functionalities from SLURM/sudo and SLURM/SSH executors (#2107).</li> </ul> </li> <li>SSH internals:<ul> <li>Add <code>FractalSSH.remote_exists</code> method (#2008).</li> <li>Drop <code>FractalSSH.{_get,_put}</code> wrappers of <code>SFTPClient</code> methods (#2077).</li> <li>Try re-opening the connection in <code>FractalSSH.check_connection</code> when an error occurs (#2035).</li> <li>Move <code>NoValidConnectionError</code> exception handling into <code>FractalSSH.log_and_raise</code> method (#2070).</li> <li>Improve closed-socket testing (#2076).</li> </ul> </li> <li>App:</li> <li>Add <code>FRACTAL_VIEWER_AUTHORIZATION_SCHEME</code> and <code>FRACTAL_VIEWER_BASE_FOLDER</code> configuration variables (#2096).</li> <li>Testing:<ul> <li>Drop <code>fetch-depth</code> from <code>checkout</code> in GitHub actions (#2039).</li> </ul> </li> <li>Scripts:<ul> <li>Introduce <code>scripts/export_v1_workflows.py</code> (#2043).</li> </ul> </li> <li>Dependencies:<ul> <li>Remove <code>passlib</code> dependency (#2112).</li> <li>Bump <code>fastapi-users</code> to v14, which includes switch to <code>pwdlib</code> (#2112).</li> </ul> </li> </ul>"},{"location":"changelog/#281","title":"2.8.1","text":"<ul> <li>API:<ul> <li>Validate all user-provided strings that end up in pip-install commands (#2003).</li> </ul> </li> </ul>"},{"location":"changelog/#280","title":"2.8.0","text":"<ul> <li>Task collection<ul> <li>Now both the local and SSH versions of the task collection use the bash templates (#1980).</li> <li>Update task-collections database logs incrementally (#1980).</li> <li>Add <code>TaskGroupV2.pinned_package_versions_string</code> property (#1980).</li> <li>Support pinned-package versions for SSH task collection (#1980).</li> <li>Now <code>pip install</code> uses <code>--no-cache</code> (#1980).</li> </ul> </li> <li>API<ul> <li>Deprecate the <code>verbose</code> query parameter in <code>GET /api/v2/task/collect/{state_id}/</code> (#1980).</li> <li>Add <code>project_dir</code> attribute to <code>UserSettings</code> (#1990).</li> <li>Set a default for <code>DatasetV2.zarr_dir</code> (#1990).</li> <li>Combine the <code>args_schema_parallel</code> and <code>args_schema_non_parallel</code> query parameters in <code>GET /api/v2/task/</code> into a single parameter <code>args_schema</code> (#1998).</li> </ul> </li> </ul>"},{"location":"changelog/#271","title":"2.7.1","text":"<p>WARNING: As of this version, all extras for <code>pip install</code> are deprecated and the corresponding dependencies become required.</p> <ul> <li>Database:<ul> <li>Drop <code>TaskV2.owner</code> column (#1977).</li> <li>Make <code>TaskV2.taskgroupv2_id</code> column required (#1977).</li> </ul> </li> <li>Dependencies:<ul> <li>Make <code>psycopg[binary]</code> dependency required, and drop <code>postgres-pyscopg-binary</code> extra (#1970).</li> <li>Make <code>gunicorn</code> dependency required, and drop <code>gunicorn</code> extra (#1970).</li> </ul> </li> <li>Testing:<ul> <li>Switch from SQLite to Postgres in the OAuth Github action (#1981).</li> </ul> </li> </ul>"},{"location":"changelog/#270","title":"2.7.0","text":"<p>WARNING: This release comes with several specific notes:</p> <ol> <li>It requires running <code>fractalctl update-db-data</code> (after <code>fractalctl set-db</code>).</li> <li>When running <code>fractalctl update-db-data</code>, the environment variable    <code>FRACTAL_V27_DEFAULT_USER_EMAIL</code> must be set, e.g. as in    <code>FRACTAL_V27_DEFAULT_USER_EMAIL=admin@fractal.yx fractalctl    update-db-data</code>. This user must exist, and they will own all    previously-common tasks/task-groups.</li> <li>The pip extra <code>postgres</code> is deprecated, in favor of <code>postgres-psycopg-binary</code>.</li> <li>The configuration variable <code>DB_ENGINE=\"postgres\"</code> is deprecated, in favor of <code>DB_ENGINE=\"postgres-psycopg\"</code>.</li> <li>Python3.9 is deprecated.</li> </ol> <ul> <li>API:<ul> <li>Users and user groups:<ul> <li>Replace <code>UserRead.group_names</code> and <code>UserRead.group_ids</code> with <code>UserRead.group_ids_names</code> ordered list (#1844, #1850).</li> <li>Deprecate <code>GET /auth/group-names/</code> (#1844).</li> <li>Add <code>DELETE /auth/group/{id}/</code> endpoint (#1885).</li> <li>Add <code>PATCH auth/group/{group_id}/user-settings/</code> bulk endpoint (#1936).</li> </ul> </li> <li>Task groups:<ul> <li>Introduce <code>/api/v2/task-group/</code> routes (#1817, #1847, #1852, #1856, #1943).</li> <li>Respond with 422 error when any task-creating endpoint would break a non-duplication constraint (#1861).</li> <li>Enforce non-duplication constraints on <code>TaskGroupV2</code> (#1865).</li> <li>Fix non-duplication check in <code>PATCH /api/v2/task-group/{id}/</code> (#1911).</li> <li>Add cascade operations to <code>DELETE /api/v2/task-group/{task_group_id}/</code> and to <code>DELETE /admin/v2/task-group/{task_group_id}/</code> (#1867).</li> <li>Expand use and validators for <code>TaskGroupCreateV2</code> schema (#1861).</li> <li>Do not process task <code>source</code>s in task/task-group CRUD operations (#1861).</li> <li>Do not process task <code>owner</code>s in task/task-group CRUD operations (#1861).</li> </ul> </li> <li>Tasks:<ul> <li>Drop <code>TaskCreateV2.source</code> (#1909).</li> <li>Drop <code>TaskUpdateV2.version</code> (#1905).</li> <li>Revamp access-control for <code>/api/v2/task/</code> endpoints, based on task-group attributes (#1817).</li> <li>Update <code>/api/v2/task/</code> endpoints and schemas with new task attributes (#1856).</li> <li>Forbid changing <code>TaskV2.name</code> (#1925).</li> </ul> </li> <li>Task collection:<ul> <li>Improve preliminary checks in task-collection endpoints (#1861).</li> <li>Refactor split between task-collection endpoints and background tasks (#1861).</li> <li>Create <code>TaskGroupV2</code> object within task-collection endpoints (#1861).</li> <li>Fix response of task-collection endpoint (#1902).</li> <li>Automatically discover PyPI package version if missing or invalid (#1858, #1861, #1902).</li> <li>Use appropriate log-file path in collection-status endpoint (#1902).</li> <li>Add task <code>authors</code> to manifest schema (#1856).</li> <li>Do not use <code>source</code> for custom task collection (#1893).</li> <li>Rename custom-task-collection request-body field from <code>source</code> to <code>label</code> (#1896).</li> <li>Improve error messages from task collection (#1913).</li> <li>Forbid non-unique task names in <code>ManifestV2</code> (#1925).</li> </ul> </li> <li>Workflows and workflow tasks:<ul> <li>Introduce additional checks in POST-workflowtask endpoint, concerning non-active or non-accessible tasks (#1817).</li> <li>Introduce additional intormation in GET-workflow endpoint, concerning non-active or non-accessible tasks (#1817).</li> <li>Introduce additional intormation in PATCH-workflow endpoint, concerning non-active or non-accessible tasks (#1868, #1869).</li> <li>Stop logging warnings for non-common tasks in workflow export (#1893).</li> <li>Drop <code>WorkflowTaskCreateV2.order</code> (#1906).</li> <li>Update endpoints for workflow import/export  (#1925, #1939, #1960).</li> </ul> </li> <li>Datasets:<ul> <li>Remove <code>TaskDumpV2.owner</code> attribute (#1909).</li> </ul> </li> <li>Jobs:<ul> <li>Prevent job submission if includes non-active or non-accessible tasks (#1817).</li> <li>Remove rate limit for <code>POST /project/{project_id}/job/submit/</code> (#1944).</li> </ul> </li> <li>Admin:<ul> <li>Remove <code>owner</code> from <code>GET admin/v2/task/</code> (#1909).</li> <li>Deprecate <code>kind</code> query parameter for <code>/admin/v2/task/</code> (#1893).</li> <li>Add <code>origin</code> and <code>pkg_name</code> query parameters to <code>GET /admin/v2/task-group/</code> (#1979).</li> </ul> </li> <li>Schemas:<ul> <li>Forbid extras in <code>TaskCollectPipV2</code> (#1891).</li> <li>Forbid extras in all Create/Update/Import schemas (#1895).</li> <li>Deprecate internal <code>TaskCollectPip</code> schema in favor of <code>TaskGroupV2</code> (#1861).</li> </ul> </li> </ul> </li> <li>Database:<ul> <li>Introduce <code>TaskGroupV2</code> table (#1817, #1856).</li> <li>Add  <code>timestamp_created</code> column to <code>LinkUserGroup</code> table (#1850).</li> <li>Add <code>TaskV2</code> attributes <code>authors</code>, <code>tags</code>, <code>category</code> and <code>modality</code> (#1856).</li> <li>Add <code>update-db-data</code> script (#1820, #1888).</li> <li>Add <code>taskgroupv2_id</code> foreign key to <code>CollectionStateV2</code> (#1867).</li> <li>Make <code>TaskV2.source</code> nullable and drop its uniqueness constraint (#1861).</li> <li>Add <code>TaskGroupV2</code> columns <code>wheel_path</code>, <code>pinned_package_versions</code> (#1861).</li> <li>Clean up <code>alembic</code> migration scripts (#1894).</li> <li>Verify task-group non-duplication constraint in <code>2.7.0</code> data-migration script (#1927).</li> <li>Normalize <code>pkg_name</code> in <code>2.7.0</code> data-migration script (#1930).</li> <li>Deprecate <code>DB_ENGINE=\"postgres\"</code> configuration variable (#1946).</li> </ul> </li> <li>Runner:<ul> <li>Do not create local folders with 755 permissions unless <code>FRACTAL_BACKEND_RUNNER=\"slurm\"</code> (#1923).</li> <li>Fix bug of SSH/SFTP commands not acquiring lock (#1949).</li> <li>Fix bug of unhandled exception in SSH/SLURM executor (#1963).</li> <li>Always remove task-subfolder compressed archive (#1949).</li> </ul> </li> <li>Task collection:<ul> <li>Create base directory (in SSH mode), if missing (#1949).</li> <li>Fix bug of SSH/SFTP commands not acquiring lock (#1949).</li> </ul> </li> <li>SSH:<ul> <li>Improve logging for SSH-connection-locking flow (#1949).</li> <li>Introduce <code>FractalSSH.fetch_file</code> and <code>FractalSSH.read_remote_json_file</code> (#1949).</li> <li>Use <code>paramiko.sftp_client.SFTPClient</code> methods directly rathen than <code>fabric</code> wrappers (#1949).</li> <li>Disable prefetching for <code>SFTPClient.get</code> (#1949).</li> </ul> </li> <li>Internal:<ul> <li>Update <code>_create_first_group</code> so that it only searches for <code>UserGroups</code> with a given name (#1964).</li> </ul> </li> <li>Dependencies:<ul> <li>Bump fastapi to <code>0.115</code> (#1942).</li> <li>Remove pip extra <code>postgres</code>, corresponding to <code>psycopg2+asyncpg</code> (#1946).</li> <li>Deprecate python3.9 (#1946).</li> </ul> </li> <li>Testing:<ul> <li>Benchmark <code>GET /api/v2/task-group/</code> (#1922).</li> <li>Use new <code>ubuntu22-slurm-multipy</code> image, with Python3.12 and with Python-version specific venvs (#1946, #1969).</li> <li>Get <code>DB_ENGINE</code> variable from <code>os.environ</code> rather than from installed packages (#1968).</li> </ul> </li> </ul>"},{"location":"changelog/#264","title":"2.6.4","text":"<ul> <li>Database<ul> <li>Fix use of naming convention for database schema-migration scripts (#1819).</li> </ul> </li> <li>Testing:<ul> <li>Test <code>alembic downgrade base</code> (#1819).</li> <li>Add <code>GET /api/v2/task/</code> to benchmarks (#1825).</li> </ul> </li> </ul>"},{"location":"changelog/#263","title":"2.6.3","text":"<ul> <li>API:<ul> <li>Introduce <code>GET /auth/current-user/viewer-paths/</code> endpoint (#1816).</li> <li>Add <code>viewer_paths</code> attribute to <code>UserGroup</code> endpoints (#1816).</li> </ul> </li> <li>Database:<ul> <li>Add  <code>viewer_paths</code> column to <code>UserGroup</code> table (#1816).</li> </ul> </li> <li>Runner:<ul> <li>Anticipate <code>wait_thread.shutdown_callback</code> assignment in <code>FractalSlurmExecutor</code>, to avoid an uncaught exception (#1815).</li> </ul> </li> </ul>"},{"location":"changelog/#262","title":"2.6.2","text":"<ul> <li>Allow setting <code>UserSettings</code> attributes to <code>None</code> in standard/strict PATCH endpoints (#1814).</li> </ul>"},{"location":"changelog/#261","title":"2.6.1","text":"<ul> <li>App (internal):<ul> <li>Remove <code>FRACTAL_SLURM_SSH_HOST</code>, <code>FRACTAL_SLURM_SSH_USER</code>, <code>FRACTAL_SLURM_SSH_PRIVATE_KEY_PATH</code> and <code>FRACTAL_SLURM_SSH_WORKING_BASE_DIR</code> from <code>Settings</code>  (#1804).</li> </ul> </li> <li>Database:<ul> <li>Drop <code>slurm_user</code>, <code>slurm_accounts</code> and <code>cache_dir</code> columns from <code>UserOAuth</code> (#1804)</li> </ul> </li> </ul>"},{"location":"changelog/#260","title":"2.6.0","text":"<p>WARNING: This release requires running <code>fractalctl update-db-data</code> (after <code>fractalctl set-db</code>).</p> <ul> <li>API:<ul> <li>Introduce user-settings API, in <code>/auth/users/{user_id}/settings/</code> and <code>/auth/current-user/settings/</code> (#1778, #1807).</li> <li>Add the creation of empty settings to <code>UserManager.on_after_register</code> hook (#1778).</li> <li>Remove deprecated user's attributes (<code>slurm_user</code>, <code>cache_dir</code>, <code>slurm_accounts</code>) from API, in favor of new <code>UserSetting</code> ones (#1778).</li> <li>Validate user settings in endpoints that rely on them (#1778).</li> <li>Propagate user settings to background tasks when needed (#1778).</li> </ul> </li> <li>Database:<ul> <li>Introduce new <code>user_settings</code> table, and link it to <code>user_oauth</code> (#1778).</li> </ul> </li> <li>Internal:</li> <li>Remove redundant string validation in <code>FractalSSH.remove_folder</code> and <code>TaskCollectCustomV2</code> (#1810).</li> <li>Make <code>validate_cmd</code> more strict about non-string arguments (#1810).</li> </ul>"},{"location":"changelog/#252","title":"2.5.2","text":"<ul> <li>App:<ul> <li>Replace <code>fractal_ssh</code> attribute with <code>fractal_ssh_list</code>, in <code>app.state</code> (#1790).</li> <li>Move creation of SSH connections from app startup to endpoints (#1790).</li> </ul> </li> <li>Internal<ul> <li>Introduce <code>FractalSSHList</code>, in view of support for multiple SSH/Slurm service users (#1790).</li> <li>Make <code>FractalSSH.close()</code> more aggressively close <code>Transport</code> attribute (#1790).</li> <li>Set <code>look_for_keys=False</code> for paramiko/fabric connection (#1790).</li> </ul> </li> <li>Testing:<ul> <li>Add fixture to always test that threads do not accumulate during tests (#1790).</li> </ul> </li> </ul>"},{"location":"changelog/#251","title":"2.5.1","text":"<ul> <li>API:<ul> <li>Make <code>WorkflowTaskDumpV2</code> attributes <code>task_id</code> and <code>task</code> optional (#1784).</li> <li>Add validation for user-provided strings that execute commands with subprocess or remote-shell (#1767).</li> </ul> </li> <li>Runner and task collection:<ul> <li>Validate commands before running them via <code>subprocess</code> or <code>fabric</code> (#1767).</li> </ul> </li> </ul>"},{"location":"changelog/#250","title":"2.5.0","text":"<p>WARNING: This release has a minor API bug when displaying a V2 dataset with a history that contains legacy tasks. It's recommended to update to 2.5.1.</p> <p>This release removes support for including V1 tasks in V2 workflows. This comes with changes to the database (data and metadata), to the API, and to the V2 runner.</p> <ul> <li>Runner:<ul> <li>Deprecate running v1 tasks within v2 workflows (#1721).</li> </ul> </li> <li>Database:<ul> <li>Remove <code>Task.is_v2_compatible</code> column (#1721).</li> <li>For table <code>WorkflowTaskV2</code>, drop <code>is_legacy_task</code> and <code>task_legacy_id</code> columns, remove <code>task_legacy</code> ORM attribute, make <code>task_id</code> required, make <code>task</code> required (#1721).</li> </ul> </li> <li>API:<ul> <li>Drop v1-v2-task-compatibility admin endpoint (#1721).</li> <li>Drop <code>/task-legacy/</code> endpoint (#1721).</li> <li>Remove legacy task code branches from <code>WorkflowTaskV2</code> CRUD endpoints (#1721).</li> <li>Add OAuth accounts info to <code>UserRead</code> at <code>.oauth_accounts</code> (#1765).</li> </ul> </li> <li>Testing:<ul> <li>Improve OAuth Github Action to test OAuth account flow (#1765).</li> </ul> </li> </ul>"},{"location":"changelog/#242","title":"2.4.2","text":"<ul> <li>App:<ul> <li>Improve logging in <code>fractalctl set-db</code> (#1764).</li> </ul> </li> <li>Runner:<ul> <li>Add <code>--set-home</code> to <code>sudo -u</code> impersonation command, to fix Ubuntu18 behavior (#1762).</li> </ul> </li> <li>Testing:<ul> <li>Start tests of migrations from valid v2.4.0 database (#1764).</li> </ul> </li> </ul>"},{"location":"changelog/#241","title":"2.4.1","text":"<p>This is mainly a bugfix release, re-implementing a check that was removed in 2.4.0.</p> <ul> <li>API:<ul> <li>Re-introduce check for existing-user-email in <code>PATCH /auth/users/{id}/</code> (#1760).</li> </ul> </li> </ul>"},{"location":"changelog/#240","title":"2.4.0","text":"<p>This release introduces support for user groups, but without linking it to any access-control rules (which will be introduced later).</p> <p>NOTE: This release requires running the <code>fractalctl update-db-data</code> script.</p> <ul> <li>App:<ul> <li>Move creation of first user from application startup into <code>fractalctl set-db</code> command (#1738, #1748).</li> <li>Add creation of default user group into <code>fractalctl set-db</code> command (#1738).</li> <li>Create <code>update-db-script</code> for current version, that adds all users to default group (#1738).</li> </ul> </li> <li>API:<ul> <li>Added <code>/auth/group/</code> and <code>/auth/group-names/</code> routers (#1738, #1752).</li> <li>Implement <code>/auth/users/{id}/</code> POST/PATCH routes in <code>fractal-server</code> (#1738, #1747, #1752).</li> <li>Introduce <code>UserUpdateWithNewGroupIds</code> schema for <code>PATCH /auth/users/{id}/</code> (#1747, #1752).</li> <li>Add <code>UserManager.on_after_register</code> hook to add new users to default user group (#1738).</li> </ul> </li> <li>Database:<ul> <li>Added new <code>usergroup</code> and <code>linkusergroup</code> tables (#1738).</li> </ul> </li> <li>Internal<ul> <li>Refactored <code>fractal_server.app.auth</code> and <code>fractal_server.app.security</code> (#1738)/</li> <li>Export all relevant modules in <code>app.models</code>, since it matters e.g. for <code>autogenerate</code>-ing migration scripts (#1738).</li> </ul> </li> <li>Testing<ul> <li>Add <code>UserGroup</code> validation to <code>scripts/validate_db_data_with_read_schemas.py</code> (#1746).</li> </ul> </li> </ul>"},{"location":"changelog/#2311","title":"2.3.11","text":"<ul> <li>SSH runner:<ul> <li>Move remote-folder creation from <code>submit_workflow</code> to more specific <code>_process_workflow</code> (#1728).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add <code>GET /auth/token/login/</code> to tested endpoints (#1720).</li> </ul> </li> <li>Testing:<ul> <li>Update GitHub actions <code>upload-artifact</code> and <code>download-artifact</code> to <code>v4</code> (#1725).</li> </ul> </li> </ul>"},{"location":"changelog/#2310","title":"2.3.10","text":"<ul> <li>Fix minor bug in zipping-job logging (#1716).</li> </ul>"},{"location":"changelog/#239","title":"2.3.9","text":"<ul> <li>Add logging for zipping-job-folder operations (#1714).</li> </ul>"},{"location":"changelog/#238","title":"2.3.8","text":"<p>NOTE: <code>FRACTAL_API_V1_MODE=\"include_without_submission\"</code> is now transformed into <code>FRACTAL_API_V1_MODE=\"include_read_only\"</code>.</p> <ul> <li>API:<ul> <li>Support read-only mode for V1 (#1701).</li> <li>Improve handling of zipped job-folder in download-logs endpoints (#1702).</li> </ul> </li> <li>Runner:<ul> <li>Improve database-error handling in V2 job execution (#1702).</li> <li>Zip job folder after job execution (#1702).</li> </ul> </li> <li>App:<ul> <li><code>UvicornWorker</code> is now imported from <code>uvicorn-worker</code> (#1690).</li> </ul> </li> <li>Testing:<ul> <li>Remove <code>HAS_LOCAL_SBATCH</code> variable and related if-branches (#1699).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add <code>GET /auth/current-user/</code> to tested endpoints (#1700).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>mkdocstrings</code> to <code>^0.25.2</code> (#1707).</li> <li>Update <code>fastapi</code> to <code>^0.112.0</code> (#1705).</li> </ul> </li> </ul>"},{"location":"changelog/#237","title":"2.3.7","text":"<ul> <li>SSH SLURM executor:<ul> <li>Handle early shutdown in SSH executor (#1696).</li> </ul> </li> <li>Task collection:<ul> <li>Introduce a new configuration variable <code>FRACTAL_MAX_PIP_VERSION</code> to pin task-collection pip (#1675).</li> </ul> </li> </ul>"},{"location":"changelog/#236","title":"2.3.6","text":"<ul> <li>API:<ul> <li>When creating a WorkflowTask, do not pre-populate its top-level arguments based on JSON Schema default values (#1688).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>sqlmodel</code> to <code>^0.0.21</code> (#1674).</li> <li>Add <code>uvicorn-worker</code> (#1690).</li> </ul> </li> </ul>"},{"location":"changelog/#235","title":"2.3.5","text":"<p>WARNING: The <code>pre_submission_commands</code> SLURM configuration is included as an experimental feature, since it is still not useful for its main intended goal (calling <code>module load</code> before running <code>sbatch</code>).</p> <ul> <li>SLURM runners:<ul> <li>Expose <code>gpus</code> SLURM parameter (#1678).</li> <li>For SSH executor, add <code>pre_submission_commands</code> (#1678).</li> <li>Removed obsolete arguments from <code>get_slurm_config</code> function (#1678).</li> </ul> </li> <li>SSH features:<ul> <li>Add <code>FractalSSH.write_remote_file</code> method (#1678).</li> </ul> </li> </ul>"},{"location":"changelog/#234","title":"2.3.4","text":"<ul> <li>SSH SLURM runner:<ul> <li>Refactor <code>compress_folder</code> and <code>extract_archive</code> modules, and stop using <code>tarfile</code> library (#1641).</li> </ul> </li> <li>API:<ul> <li>Introduce <code>FRACTAL_API_V1_MODE=include_without_submission</code> to include V1 API but forbid job submission (#1664).</li> </ul> </li> <li>Testing:<ul> <li>Do not test V1 API with <code>DB_ENGINE=\"postgres-psycopg\"</code> (#1667).</li> <li>Use new Fractal SLURM containers in CI (#1663).</li> <li>Adapt tests so that they always refer to the current Python version (the one running <code>pytest</code>), when needed; this means that we don't require the presence of any additional Python version in the development environment, apart from the current one (#1633).</li> <li>Include Python3.11 in some tests (#1669).</li> <li>Simplify CI SLURM Dockerfile after base-image updates (#1670).</li> <li>Cache <code>ubuntu22-slurm-multipy</code> Docker image in CI (#1671).</li> <li>Add <code>oauth.yaml</code> GitHub action to test OIDC authentication (#1665).</li> </ul> </li> </ul>"},{"location":"changelog/#233","title":"2.3.3","text":"<p>This release fixes a SSH-task-collection bug introduced in version 2.3.1.</p> <ul> <li>API:<ul> <li>Expose new superuser-restricted endpoint <code>GET /api/settings/</code> (#1662).</li> </ul> </li> <li>SLURM runner:<ul> <li>Make <code>FRACTAL_SLURM_SBATCH_SLEEP</code> configuration variable <code>float</code> (#1658).</li> </ul> </li> <li>SSH features:<ul> <li>Fix wrong removal of task-package folder upon task-collection failure (#1649).</li> <li>Remove <code>FractalSSH.rename_folder</code> method (#1654).</li> </ul> </li> <li>Testing:<ul> <li>Refactor task-collection fixtures (#1637).</li> </ul> </li> </ul>"},{"location":"changelog/#232","title":"2.3.2","text":"<p>WARNING: The remove-remote-venv-folder in the SSH task collection is broken (see issue 1633). Do not deploy this version in an SSH-based <code>fractal-server</code> instance.</p> <ul> <li>API:<ul> <li>Fix incorrect zipping of structured job-log folders (#1648).</li> </ul> </li> </ul>"},{"location":"changelog/#231","title":"2.3.1","text":"<p>This release includes a bugfix for task names with special characters.</p> <p>WARNING: The remove-remote-venv-folder in the SSH task collection is broken (see issue 1633). Do not deploy this version in an SSH-based <code>fractal-server</code> instance.</p> <ul> <li>Runner:<ul> <li>Improve sanitization of subfolder names (commits from 3d89d6ba104d1c6f11812bc9de5cbdff25f81aa2 to 426fa3522cf2eef90d8bd2da3b2b8a5b646b9bf4).</li> </ul> </li> <li>API:<ul> <li>Improve error message when task-collection Python is not defined (#1640).</li> <li>Use a single endpoint for standard and SSH task collection (#1640).</li> </ul> </li> <li>SSH features:<ul> <li>Remove remote venv folder upon failed task collection in SSH mode (#1634, #1640).</li> <li>Refactor <code>FractalSSH</code> (#1635).</li> <li>Set <code>fabric.Connection.forward_agent=False</code> (#1639).</li> </ul> </li> <li>Testing:<ul> <li>Improved testing of SSH task-collection API (#1640).</li> <li>Improved testing of <code>FractalSSH</code> methods (#1635).</li> <li>Stop testing SQLite database for V1 in CI (#1630).</li> </ul> </li> </ul>"},{"location":"changelog/#230","title":"2.3.0","text":"<p>This release includes two important updates: 1. An Update update to task-collection configuration variables and logic. 2. The first released version of the experimental SSH features.</p> <p>Re: task-collection configuration, we now support two main use cases:</p> <ol> <li> <p>When running a production instance (including on a SLURM cluster), you    should set e.g. <code>FRACTAL_TASKS_PYTHON_DEFAULT_VERSION=3.10</code>, and make sure    that <code>FRACTAL_TASKS_PYTHON_3_10=/some/python</code> is an absolute path. Optionally,    you can define other variables like <code>FRACTAL_TASKS_PYTHON_3_9</code>,    <code>FRACTAL_TASKS_PYTHON_3_11</code> or <code>FRACTAL_TASKS_PYTHON_3_12</code>.</p> </li> <li> <p>If you leave <code>FRACTAL_TASKS_PYTHON_DEFAULT_VERSION</code> unset, then only the    Python interpreter that is currently running <code>fractal-server</code> can be used    for task collection.</p> </li> </ol> <p>WARNING: If you don't set <code>FRACTAL_TASKS_PYTHON_DEFAULT_VERSION</code>, then you will only have a single Python interpreter available for tasks (namely the one running <code>fractal-server</code>).</p> <ul> <li>API:<ul> <li>Introduce <code>api/v2/task/collect/custom/</code> endpoint (#1607, #1613, #1617, #1629).</li> </ul> </li> <li>Task collection:<ul> <li>Introduce task-collection Python-related configuration variables (#1587).</li> <li>Always set Python version for task collection, and only use <code>FRACTAL_TASKS_PYTHON_X_Y</code> variables (#1587).</li> <li>Refactor task-collection functions and schemas (#1587, #1617).</li> <li>Remove <code>TaskCollectStatusV2</code> and <code>get_collection_data</code> internal schema/function (#1598).</li> <li>Introduce <code>CollectionStatusV2</code> enum for task-collection status (#1598).</li> <li>Reject task-collection request if it includes a wheel file and a version (#1608). SSH features:</li> <li>Introduce <code>fractal_server/ssh</code> subpackage (#1545, #1599, #1611).</li> <li>Introduce SSH executor and runner (#1545).</li> <li>Introduce SSH task collection (#1545, #1599, #1626).</li> <li>Introduce SSH-related configuration variables (#1545).</li> <li>Modify app lifespan to handle SSH connection (#1545).</li> <li>Split <code>app/runner/executor/slurm</code> into <code>sudo</code> and <code>ssh</code> subfolders (#1545).</li> <li>Introduce FractalSSH object which is a wrapper class around fabric.Connection object. It provides a <code>lock</code> to avoid loss of ssh instructions and a custom timeout (#1618)</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>sqlmodel</code> to <code>^0.0.19</code> (#1584).</li> <li>Update <code>pytest-asyncio</code> to <code>^0.23</code> (#1558).</li> </ul> </li> <li>Testing:<ul> <li>Test the way <code>FractalProcessPoolExecutor</code> spawns processes and threads (#1579).</li> <li>Remove <code>event_loop</code> fixture: every test will run on its own event loop (#1558).</li> <li>Test task collection with non-canonical package name (#1602).</li> </ul> </li> </ul>"},{"location":"changelog/#220","title":"2.2.0","text":"<p>This release streamlines options for the Gunicorn startup command, and includes two new experimental features.</p> <p>NOTE 1: you can now enable custom Gunicorn worker/logger by adding the following options to the <code>gunicorn</code> startup command: - <code>--worker-class fractal_server.gunicorn_fractal.FractalWorker</code> - <code>--logger-class fractal_server.gunicorn_fractal.FractalGunicornLogger</code></p> <p>NOTE 2: A new experimental local runner is available, which uses processes instead of threads and support shutdown. You can try it out with the configuration variable <code>FRACTAL_BACKEND_RUNNER=local_experimental</code></p> <p>NOTE 3: A new PostgreSQL database adapter is available, fully based on <code>psycopg3</code> (rather than <code>pyscopg2</code>+<code>asyncpg</code>). You can try it out with the configuration variable <code>DB_ENGINE=postgres-psycopg</code> (note that this requires the <code>pip install</code> extra <code>postgres-psycopg-binary</code>).</p> <ul> <li>API:<ul> <li>Add extensive logs to <code>DELETE /api/v2/project/{project_id}</code> (#1532).</li> <li>Remove catch of <code>IntegrityError</code> in <code>POST /api/v1/project</code> (#1530).</li> </ul> </li> <li>App and deployment:<ul> <li>Move <code>FractalGunicornLogger</code> and <code>FractalWorker</code> into <code>fractal_server/gunicorn_fractal.py</code> (#1535).</li> <li>Add custom gunicorn/uvicorn worker to handle SIGABRT signal (#1526).</li> <li>Store list of submitted jobs in app state (#1538).</li> <li>Add logic for graceful shutdown for job slurm executors (#1547).</li> </ul> </li> <li>Runner:<ul> <li>Change structure of job folders, introducing per-task subfolders (#1523).</li> <li>Rename internal <code>workflow_dir</code> and <code>workflow_dir_user</code> variables to local/remote (#1534).</li> <li>Improve handling of errors in <code>submit_workflow</code> background task (#1556, #1566).</li> <li>Add new <code>local_experimental</code> runner, based on <code>ProcessPoolExecutor</code> (#1544, #1566).</li> </ul> </li> <li>Database:<ul> <li>Add new Postgres adapter <code>psycopg</code> (#1562).</li> </ul> </li> <li>Dependencies<ul> <li>Add <code>fabric</code> to <code>dev</code> dependencies (#1518).</li> <li>Add new <code>postgres-psycopg-binary</code> extra (#1562).</li> </ul> </li> <li>Testing:<ul> <li>Extract <code>pytest-docker</code> fixtures into a dedicated module (#1516).</li> <li>Rename SLURM containers in CI (#1516).</li> <li>Install and run SSH daemon in CI containers (#1518).</li> <li>Add unit test of SSH connection via fabric/paramiko (#1518).</li> <li>Remove obsolete folders from <code>tests/data</code> (#1517).</li> </ul> </li> </ul>"},{"location":"changelog/#210","title":"2.1.0","text":"<p>This release fixes a severe bug where SLURM-executor auxiliary threads are not joined when a Fractal job ends.</p> <ul> <li>App:<ul> <li>Add missing join for <code>wait_thread</code> upon <code>FractalSlurmExecutor</code> exit (#1511).</li> <li>Replace <code>startup</code>/<code>shutdown</code> events with <code>lifespan</code> event (#1501).</li> </ul> </li> <li>API:<ul> <li>Remove <code>Path.resolve</code> from the submit-job endpoints and add validator for <code>Settings.FRACTAL_RUNNER_WORKING_BASE_DIR</code> (#1497).</li> </ul> </li> <li>Testing:<ul> <li>Improve dockerfiles for SLURM (#1495, #1496).</li> <li>Set short timeout for <code>docker compose down</code> (#1500).</li> </ul> </li> </ul>"},{"location":"changelog/#206","title":"2.0.6","text":"<p>NOTE: This version changes log formats. For <code>uvicorn</code> logs, this change requires no action. For <code>gunicorn</code>, logs formats are only changed by adding the following command-line option: <code>gunicorn ... --logger-class fractal_server.logger.gunicorn_logger.FractalGunicornLogger</code>.</p> <ul> <li>API:<ul> <li>Add <code>FRACTAL_API_V1_MODE</code> environment variable to include/exclude V1 API (#1480).</li> <li>Change format of uvicorn loggers (#1491).</li> <li>Introduce <code>FractalGunicornLogger</code> class (#1491).</li> </ul> </li> <li>Runner:<ul> <li>Fix missing <code>.log</code> files in server folder for SLURM jobs (#1479).</li> </ul> </li> <li>Database:<ul> <li>Remove <code>UserOAuth.project_list</code> and <code>UserOAuth.project_list_v2</code> relationships (#1482).</li> </ul> </li> <li>Dev dependencies:<ul> <li>Bump <code>pytest</code> to <code>8.1.*</code> (#1486).</li> <li>Bump <code>coverage</code> to <code>7.5.*</code> (#1486).</li> <li>Bump <code>pytest-docker</code> to <code>3.1.*</code> (#1486).</li> <li>Bump <code>pytest-subprocess</code> to <code>^1.5</code> (#1486).</li> </ul> </li> <li>Benchmarks:<ul> <li>Move <code>populate_db</code> scripts into <code>benchmark</code> folder (#1489).</li> </ul> </li> </ul>"},{"location":"changelog/#205","title":"2.0.5","text":"<ul> <li>API:<ul> <li>Add <code>GET /admin/v2/task/</code> (#1465).</li> <li>Improve error message in DELETE-task endpoint (#1471).</li> </ul> </li> <li>Set <code>JobV2</code> folder attributes from within the submit-job endpoint (#1464).</li> <li>Tests:<ul> <li>Make SLURM CI work on MacOS (#1476).</li> </ul> </li> </ul>"},{"location":"changelog/#204","title":"2.0.4","text":"<ul> <li>Add <code>FRACTAL_SLURM_SBATCH_SLEEP</code> configuration variable (#1467).</li> </ul>"},{"location":"changelog/#203","title":"2.0.3","text":"<p>WARNING: This update requires running a fix-db script, via <code>fractalctl update-db-data</code>.</p> <ul> <li>Database:<ul> <li>Create fix-db script to remove <code>images</code> and <code>history</code> from dataset dumps in V1/V2 jobs (#1456).</li> </ul> </li> <li>Tests:<ul> <li>Split <code>test_full_workflow_v2.py</code> into local/slurm files (#1454).</li> </ul> </li> </ul>"},{"location":"changelog/#202","title":"2.0.2","text":"<p>WARNING: Running this version on a pre-existing database (where the <code>jobsv2</code> table has some entries) is broken. Running this version on a freshly-created database works as expected.</p> <ul> <li>API:<ul> <li>Fix bug in status endpoint (#1449).</li> <li>Improve handling of out-of-scope scenario in status endpoint (#1449).</li> <li>Do not include dataset <code>history</code> in <code>JobV2.dataset_dump</code> (#1445).</li> <li>Forbid extra arguments in <code>DumpV2</code> schemas (#1445).</li> </ul> </li> <li>API V1:<ul> <li>Do not include dataset <code>history</code> in <code>ApplyWorkflow.{input,output}_dataset_dump</code> (#1453).</li> </ul> </li> <li>Move settings logs to <code>check_settings</code> and use fractal-server <code>set_logger</code> (#1452).</li> <li>Benchmarks:<ul> <li>Handle some more errors in benchmark flow (#1445).</li> </ul> </li> <li>Tests:<ul> <li>Update testing database to version 2.0.1 (#1445).</li> </ul> </li> </ul>"},{"location":"changelog/#201","title":"2.0.1","text":"<ul> <li>Database/API:<ul> <li>Do not include <code>dataset_dump.images</code> in <code>JobV2</code> table (#1441).</li> </ul> </li> <li>Internal functions:<ul> <li>Introduce more robust <code>reset_logger_handlers</code> function (#1425).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add <code>POST /api/v2/project/project_id/dataset/dataset_id/images/query/</code> in bechmarks  to evaluate the impact of the number of images during the query (#1441).</li> </ul> </li> <li>Development:<ul> <li>Use <code>poetry</code> 1.8.2 in GitHub actions and documentation.</li> </ul> </li> </ul>"},{"location":"changelog/#200","title":"2.0.0","text":"<p>Major update.</p>"},{"location":"changelog/#1410","title":"1.4.10","text":"<p>WARNING: Starting from this version, the dependencies for the <code>slurm</code> extra are required; commands like <code>pip install fractal-server[slurm,postgres]</code> must be replaced by <code>pip install fractal-server[postgres]</code>.</p> <ul> <li>Dependencies:<ul> <li>Make <code>clusterfutures</code> and <code>cloudpickle</code> required dependencies (#1255).</li> <li>Remove <code>slurm</code> extra from package (#1255).</li> </ul> </li> <li>API:<ul> <li>Handle invalid history file in <code>GET /project/{project_id}/dataset/{dataset_id}/status/</code> (#1259).</li> </ul> </li> <li>Runner:<ul> <li>Add custom <code>_jobs_finished</code> function to check the job status and to avoid squeue errors (#1266)</li> </ul> </li> </ul>"},{"location":"changelog/#149","title":"1.4.9","text":"<p>This release is a follow-up of 1.4.7 and 1.4.8, to mitigate the risk of job folders becoming very large.</p> <ul> <li>Runner:<ul> <li>Exclude <code>history</code> from <code>TaskParameters</code> object for parallel tasks, so that it does not end up in input pickle files (#1247).</li> </ul> </li> </ul>"},{"location":"changelog/#148","title":"1.4.8","text":"<p>This release is a follow-up of 1.4.7, to mitigate the risk of job folders becoming very large.</p> <ul> <li>Runner:<ul> <li>Exclude <code>metadata[\"image\"]</code> from <code>TaskParameters</code> object for parallel tasks, so that it does not end up in input pickle files (#1245).</li> <li>Exclude components list from <code>workflow.log</code> logs (#1245).</li> </ul> </li> <li>Database:<ul> <li>Remove spurious logging of <code>fractal_server.app.db</code> string (#1245).</li> </ul> </li> </ul>"},{"location":"changelog/#147","title":"1.4.7","text":"<p>This release provides a bugfix (PR 1239) and a workaround (PR 1238) for the SLURM runner, which became relevant for the use case of processing a large dataset (300 wells with 25 cycles each).</p> <ul> <li>Runner:<ul> <li>Do not include <code>metadata[\"image\"]</code> in JSON file with task arguments (#1238).</li> <li>Add <code>FRACTAL_RUNNER_TASKS_INCLUDE_IMAGE</code> configuration variable, to define exceptions where tasks still require <code>metadata[\"image\"]</code> (#1238).</li> <li>Fix bug in globbing patterns, when copying files from user-side to server-side job folder in SLURM executor (#1239).</li> </ul> </li> <li>API:<ul> <li>Fix error message for rate limits in apply-workflow endpoint (#1231).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add more scenarios, as per issue #1184 (#1232).</li> </ul> </li> </ul>"},{"location":"changelog/#146","title":"1.4.6","text":"<ul> <li>API:<ul> <li>Add <code>GET /admin/job/{job_id}</code> (#1230).</li> <li>Handle <code>FileNotFound</code> in <code>GET /project/{project_id}/job/{job_id}/</code> (#1230).</li> </ul> </li> </ul>"},{"location":"changelog/#145","title":"1.4.5","text":"<ul> <li>Remove CORS middleware (#1228).</li> <li>Testing:<ul> <li>Fix <code>migrations.yml</code> GitHub action (#1225).</li> </ul> </li> </ul>"},{"location":"changelog/#144","title":"1.4.4","text":"<ul> <li>API:<ul> <li>Add rate limiting to <code>POST /{project_id}/workflow/{workflow_id}/apply/</code> (#1199).</li> <li>Allow users to read the logs of ongoing jobs with <code>GET /project/{project_id}/job/{job_id}/</code>, using <code>show_tmp_logs</code> query parameter (#1216).</li> <li>Add <code>log</code> query parameter in <code>GET {/api/v1/job/,/api/v1/{project.id}/job/,/admin/job/}</code>, to trim response body (#1218).</li> <li>Add <code>args_schema</code> query parameter in <code>GET /api/v1/task/</code> to trim response body (#1218).</li> <li>Add <code>history</code> query parameter in <code>GET {/api/v1/dataset/,/api/v1/project/{project.id}/dataset/}</code> to trim response body (#1219).</li> <li>Remove <code>task_list</code> from <code>job.workflow_dump</code> creation in <code>/api/v1/{project_id}/workflow/{workflow_id}/apply/</code>(#1219)</li> <li>Remove <code>task_list</code> from <code>WorkflowDump</code> Pydantic schema (#1219)</li> </ul> </li> <li>Dependencies:<ul> <li>Update fastapi to <code>^0.109.0</code> (#1222).</li> <li>Update gunicorn to <code>^21.2.0</code> (#1222).</li> <li>Update aiosqlite to <code>^0.19.0</code> (#1222).</li> <li>Update uvicorn to <code>^0.27.0</code> (#1222).</li> </ul> </li> </ul>"},{"location":"changelog/#143","title":"1.4.3","text":"<p>WARNING:</p> <p>This update requires running a fix-db script, via <code>fractalctl update-db-data</code>.</p> <ul> <li>API:<ul> <li>Improve validation of <code>UserCreate.slurm_accounts</code> (#1162).</li> <li>Add <code>timestamp_created</code> to <code>WorkflowRead</code>, <code>WorkflowDump</code>, <code>DatasetRead</code> and <code>DatasetDump</code> (#1152).</li> <li>Make all dumps in <code>ApplyWorkflowRead</code> non optional (#1175).</li> <li>Ensure that timestamps in <code>Read</code> schemas are timezone-aware, regardless of <code>DB_ENGINE</code> (#1186).</li> <li>Add timezone-aware timestamp query parameters to all <code>/admin</code> endpoints (#1186).</li> </ul> </li> <li>API (internal):<ul> <li>Change the class method <code>Workflow.insert_task</code> into the auxiliary function <code>_workflow_insert_task</code> (#1149).</li> </ul> </li> <li>Database:<ul> <li>Make <code>WorkflowTask.workflow_id</code> and <code>WorkflowTask.task_id</code> not nullable (#1137).</li> <li>Add <code>Workflow.timestamp_created</code> and <code>Dataset.timestamp_created</code> columns (#1152).</li> <li>Start a new <code>current.py</code> fix-db script (#1152, #1195).</li> <li>Add to <code>migrations.yml</code> a new script (<code>validate_db_data_with_read_schemas.py</code>) that validates test-DB data with Read schemas (#1187).</li> <li>Expose <code>fix-db</code> scripts via command-line option <code>fractalctl update-db-data</code> (#1197).</li> </ul> </li> <li>App (internal):<ul> <li>Check in <code>Settings</code> that <code>psycopg2</code>, <code>asyngpg</code> and <code>cfut</code>, if required, are installed (#1167).</li> <li>Split <code>DB.set_db</code> into sync/async methods (#1165).</li> <li>Rename <code>DB.get_db</code> into <code>DB.get_async_db</code> (#1183).</li> <li>Normalize names of task packages (#1188).</li> </ul> </li> <li>Testing:<ul> <li>Update <code>clean_db_fractal_1.4.1.sql</code> to <code>clean_db_fractal_1.4.2.sql</code>, and change <code>migrations.yml</code> target version (#1152).</li> <li>Reorganise the test directory into subdirectories, named according to the order in which we want the CI to execute them (#1166).</li> <li>Split the CI into two independent jobs, <code>Core</code> and <code>Runner</code>, to save time through parallelisation (#1204).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>python-dotenv</code> to version 0.21.0 (#1172).</li> </ul> </li> <li>Runner:<ul> <li>Remove <code>JobStatusType.RUNNING</code>, incorporating it into <code>JobStatusType.SUBMITTED</code> (#1179).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add <code>fractal_client.py</code> and <code>populate_script_v2.py</code> for creating different database status scenarios (#1178).</li> <li>Add a custom benchmark suite in <code>api_bench.py</code>.</li> <li>Remove locust.</li> </ul> </li> <li>Documentation:<ul> <li>Add the minimum set of environment variables required to set the database and start the server (#1198).</li> </ul> </li> </ul>"},{"location":"changelog/#142","title":"1.4.2","text":"<p>WARNINGs:</p> <ol> <li>This update requires running a fix-db script, available at https://raw.githubusercontent.com/fractal-analytics-platform/fractal-server/1.4.2/scripts/fix_db/current.py.</li> <li>Starting from this version, non-verified users have limited access to <code>/api/v1/</code> endpoints. Before the upgrade, all existing users must be manually set to verified.</li> </ol> <ul> <li>API:<ul> <li>Prevent access to <code>GET/PATCH</code> task endpoints for non-verified users (#1114).</li> <li>Prevent access to task-collection and workflow-apply endpoints for non-verified users (#1099).</li> <li>Make first-admin-user verified (#1110).</li> <li>Add the automatic setting of <code>ApplyWorkflow.end_timestamp</code> when patching <code>ApplyWorkflow.status</code> via <code>PATCH /admin/job/{job_id}</code> (#1121).</li> <li>Change <code>ProjectDump.timestamp_created</code> type from <code>datetime</code> to <code>str</code> (#1120).</li> <li>Change <code>_DatasetHistoryItem.workflowtask</code> type into <code>WorkflowTaskDump</code> (#1139).</li> <li>Change status code of stop-job endpoints to 202 (#1151).</li> </ul> </li> <li>API (internal):<ul> <li>Implement cascade operations explicitly, in <code>DELETE</code> endpoints for datasets, workflows and projects (#1130).</li> <li>Update <code>GET /project/{project_id}/workflow/{workflow_id}/job/</code> to avoid using <code>Workflow.job_list</code> (#1130).</li> <li>Remove obsolete sync-database dependency from apply-workflow endpoint (#1144).</li> </ul> </li> <li>Database:<ul> <li>Add <code>ApplyWorkflow.project_dump</code> column (#1070).</li> <li>Provide more meaningful names to fix-db scripts (#1107).</li> <li>Add <code>Project.timestamp_created</code> column, with timezone-aware default (#1102, #1131).</li> <li>Remove <code>Dataset.list_jobs_input</code> and <code>Dataset.list_jobs_output</code> relationships (#1130).</li> <li>Remove <code>Workflow.job_list</code> (#1130).</li> </ul> </li> <li>Runner:<ul> <li>In SLURM backend, use <code>slurm_account</code> (as received from apply-workflow endpoint) with top priority (#1145).</li> <li>Forbid setting of SLURM account from <code>WorkflowTask.meta</code> or as part of <code>worker_init</code> variable (#1145).</li> <li>Include more info in error message upon <code>sbatch</code> failure (#1142).</li> <li>Replace <code>sbatch</code> <code>--chdir</code> option with <code>-D</code>, to support also slurm versions before 17.11 (#1159).</li> </ul> </li> <li>Testing:<ul> <li>Extended systematic testing of database models (#1078).</li> <li>Review <code>MockCurrentUser</code> fixture, to handle different kinds of users (#1099).</li> <li>Remove <code>persist</code> from <code>MockCurrentUser</code> (#1098).</li> <li>Update <code>migrations.yml</code> GitHub Action to use up-to-date database and also test fix-db script (#1101).</li> <li>Add more schema-based validation to fix-db current script (#1107).</li> <li>Update <code>.dict()</code> to <code>.model_dump()</code> for <code>SQLModel</code> objects, to fix some <code>DeprecationWarnings</code>(##1133).</li> <li>Small improvement in schema coverage (#1125).</li> <li>Add unit test for <code>security</code> module (#1036).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>sqlmodel</code> to version 0.0.14 (#1124).</li> </ul> </li> <li>Benchmarks:<ul> <li>Add automatic benchmark system for API's performances (#1123)</li> </ul> </li> <li>App (internal):<ul> <li>Move <code>_create_first_user</code> from <code>main</code> to <code>security</code> module, and allow it to create multiple regular users (#1036).</li> </ul> </li> </ul>"},{"location":"changelog/#141","title":"1.4.1","text":"<ul> <li>API:<ul> <li>Add <code>GET /admin/job/{job_id}/stop/</code> and <code>GET /admin/job/{job_id}/download/</code> endpoints (#1059).</li> <li>Use <code>DatasetDump</code> and <code>WorkflowDump</code> models for \"dump\" attributes of <code>ApplyWorkflowRead</code> (#1049, #1082).</li> <li>Add <code>slurm_accounts</code> to <code>User</code> schemas and add <code>slurm_account</code> to <code>ApplyWorkflow</code> schemas (#1067).</li> <li>Prevent providing a <code>package_version</code> for task collection from a <code>.whl</code> local package (#1069).</li> <li>Add <code>DatasetRead.project</code> and <code>WorkflowRead.project</code> attributes (#1082).</li> </ul> </li> <li>Database:<ul> <li>Make <code>ApplyWorkflow.workflow_dump</code> column non-nullable (#1049).</li> <li>Add <code>UserOAuth.slurm_accounts</code> and <code>ApplyWorkflow.slurm_account</code> columns (#1067).</li> <li>Add script for adding <code>ApplyWorkflow.user_email</code> (#1058).</li> <li>Add <code>Dataset.project</code> and <code>Workflow.project</code> relationships (#1082).</li> <li>Avoid using <code>Project</code> relationships <code>dataset_list</code> or <code>workflow_list</code> within some <code>GET</code> endpoints (#1082).</li> <li>Fully remove <code>Project</code> relationships <code>dataset_list</code>, <code>workflow_list</code> and <code>job_list</code> (#1091).</li> </ul> </li> <li>Testing:<ul> <li>Only use ubuntu-22.04 in GitHub actions (#1061).</li> <li>Improve unit testing of database models (#1082).</li> </ul> </li> <li>Dependencies:<ul> <li>Pin <code>bcrypt</code> to 4.0.1 to avoid warning in passlib (#1060).</li> </ul> </li> <li>Runner:<ul> <li>Set SLURM-job working directory to <code>job.working_dir_user</code> through <code>--chdir</code> option (#1064).</li> </ul> </li> </ul>"},{"location":"changelog/#140","title":"1.4.0","text":"<ul> <li>API:<ul> <li>Major endpoint changes:<ul> <li>Add trailing slash to all endpoints' paths (#1003).</li> <li>Add new admin-area endpoints restricted to superusers at <code>/admin</code> (#947, #1009, #1032).</li> <li>Add new <code>GET</code> endpoints <code>api/v1/job/</code> and <code>api/v1/project/{project_id}/workflow/{workflow_id}/job/</code> (#969, #1003).</li> <li>Add new <code>GET</code> endpoints <code>api/v1/dataset/</code> and <code>api/v1/workflow/</code> (#988, #1003).</li> <li>Add new <code>GET</code> endpoint <code>api/v1/project/{project_id}/dataset/</code> (#993).</li> <li>Add <code>PATCH /admin/job/{job_id}/</code> endpoint (#1030, #1053).</li> <li>Move <code>GET /auth/whoami/</code> to <code>GET /auth/current-user/</code> (#1013).</li> <li>Move <code>PATCH /auth/users/me/</code> to <code>PATCH /auth/current-user/</code> (#1013, #1035).</li> <li>Remove <code>DELETE /auth/users/{id}/</code> endpoint (#994).</li> <li>Remove <code>GET /auth/users/me/</code> (#1013).</li> <li>Remove <code>POST</code> <code>/auth/forgot-password/</code>, <code>/auth/reset-password/</code>, <code>/auth/request-verify-token/</code>, <code>/auth/verify/</code> (#1033).</li> <li>Move <code>GET /auth/userlist/</code> to <code>GET /auth/users/</code> (#1033).</li> </ul> </li> <li>New behaviors or responses of existing endpoints:<ul> <li>Change response of <code>/api/v1/project/{project_id}/job/{job_id}/stop/</code> endpoint to 204 no-content (#967).</li> <li>Remove <code>dataset_list</code> attribute from <code>ProjectRead</code>, which affects all <code>GET</code> endpoints that return some project (#993).</li> <li>Make it possible to delete a <code>Dataset</code>, <code>Workflow</code> or <code>Project</code>, even when it is in relationship to an <code>ApplyWorkflow</code> - provided that the <code>ApplyWorkflow</code> is not pending or running (#927, #973).</li> <li>Align <code>ApplyWorkflowRead</code> with new <code>ApplyWorkflow</code>, which has optional foreign keys <code>project_id</code>, <code>workflow_id</code>, <code>input_dataset_id</code>, and <code>output_dataset_id</code> (#984).</li> <li>Define types for <code>ApplyWorkflowRead</code> \"dump\" attributes (#990). WARNING: reverted with #999.</li> </ul> </li> <li>Internal changes:<ul> <li>Move all routes definitions into <code>fractal_server/app/routes</code> (#976).</li> <li>Fix construction of <code>ApplyWorkflow.workflow_dump</code>, within apply endpoint (#968).</li> <li>Fix construction of <code>ApplyWorkflow</code> attributes <code>input_dataset_dump</code> and <code>output_dataset_dump</code>, within apply endpoint (#990).</li> <li>Remove <code>asyncio.gather</code>, in view of SQLAlchemy2 update (#1004).</li> </ul> </li> </ul> </li> <li>Database:<ul> <li>Make foreign-keys of <code>ApplyWorkflow</code> (<code>project_id</code>, <code>workflow_id</code>, <code>input_dataset_id</code>, <code>output_dataset_id</code>) optional (#927).</li> <li>Add columns <code>input_dataset_dump</code>, <code>output_dataset_dump</code> and <code>user_email</code> to <code>ApplyWorkflow</code> (#927).</li> <li>Add relations <code>Dataset.list_jobs_input</code> and <code>Dataset.list_jobs_output</code> (#927).</li> <li>Make <code>ApplyWorkflow.start_timestamp</code> non-nullable (#927).</li> <li>Remove <code>\"cascade\": \"all, delete-orphan\"</code> from <code>Project.job_list</code> (#927).</li> <li>Add <code>Workflow.job_list</code> relation (#927).</li> <li>Do not use <code>Enum</code>s as column types (e.g. for <code>ApplyWorkflow.status</code>), but only for (de-)serialization (#974).</li> <li>Set <code>pool_pre_ping</code> option to <code>True</code>, for asyncpg driver (#1037).</li> <li>Add script for updating DB from 1.4.0 to 1.4.1 (#1010)</li> <li>Fix missing try/except in sync session (#1020).</li> </ul> </li> <li>App:<ul> <li>Skip creation of first-superuser when one superuser already exists (#1006).</li> </ul> </li> <li>Dependencies:<ul> <li>Update sqlalchemy to version <code>&gt;=2.0.23,&lt;2.1</code> (#1044).</li> <li>Update sqlmodel to version 0.0.12 (#1044).</li> <li>Upgrade asyncpg to version 0.29.0 (#1036).</li> </ul> </li> <li>Runner:<ul> <li>Refresh DB objects within <code>submit_workflow</code> (#927).</li> </ul> </li> <li>Testing:<ul> <li>Add <code>await db_engine.dispose()</code> in <code>db_create_tables</code> fixture (#1047).</li> <li>Set <code>debug=False</code> in <code>event_loop</code> fixture (#1044).</li> <li>Improve <code>test_full_workflow.py</code> (#971).</li> <li>Update <code>pytest-asyncio</code> to v0.21 (#1008).</li> <li>Fix CI issue related to event loop and asyncpg (#1012).</li> <li>Add GitHub Action testing database migrations (#1010).</li> <li>Use greenlet v3 in <code>poetry.lock</code> (#1044).</li> </ul> </li> <li>Documentation:<ul> <li>Add OAuth2 example endpoints to Web API page (#1034, #1038).</li> </ul> </li> <li>Development:<ul> <li>Use poetry 1.7.1 (#1043).</li> </ul> </li> </ul>"},{"location":"changelog/#1314-do-not-use","title":"1.3.14 (do not use!)","text":"<p>WARNING: This version introduces a change that is then reverted in 1.4.0, namely it sets the <code>ApplyWorkflow.status</code> type to <code>Enum</code>, when used with PostgreSQL. It is recommended to not use it, and upgrade to 1.4.0 directly.</p> <ul> <li>Make <code>Dataset.resource_list</code> an <code>ordering_list</code>, ordered by <code>Resource.id</code> (#951).</li> <li>Expose <code>redirect_url</code> for OAuth clients (#953).</li> <li>Expose JSON Schema for the <code>ManifestV1</code> Pydantic model (#942).</li> <li>Improve delete-resource endpoint (#943).</li> <li>Dependencies:<ul> <li>Upgrade sqlmodel to 0.0.11 (#949).</li> </ul> </li> <li>Testing:<ul> <li>Fix bug in local tests with Docker/SLURM (#948).</li> </ul> </li> </ul>"},{"location":"changelog/#1313","title":"1.3.13","text":"<ul> <li>Configure sqlite WAL to avoid \"database is locked\" errors (#860).</li> <li>Dependencies:<ul> <li>Add <code>sqlalchemy[asyncio]</code> extra, and do not directly require <code>greenlet</code> (#895).</li> <li>Fix <code>cloudpickle</code>-version definition in <code>pyproject.toml</code> (#937).</li> <li>Remove obsolete <code>sqlalchemy_utils</code> dependency (#939).</li> </ul> </li> <li>Testing:<ul> <li>Use ubuntu-22 for GitHub CI (#909).</li> <li>Run GitHub CI both with SQLite and Postgres (#915).</li> <li>Disable <code>postgres</code> service in GitHub action when running tests with SQLite (#931).</li> <li>Make <code>test_commands.py</code> tests stateless, also when running with Postgres (#917).</li> </ul> </li> <li>Documentation:<ul> <li>Add information about minimal supported SQLite version (#916).</li> </ul> </li> </ul>"},{"location":"changelog/#1312","title":"1.3.12","text":"<ul> <li>Project creation:<ul> <li>Do not automatically create a dataset upon project creation (#897).</li> <li>Remove <code>ProjectCreate.default_dataset_name</code> attribute (#897).</li> </ul> </li> <li>Dataset history:<ul> <li>Create a new (non-nullable) history column in <code>Dataset</code> table (#898, #901).</li> <li>Deprecate history handling in <code>/project/{project_id}/job/{job_id}</code> endpoint (#898).</li> <li>Deprecate <code>HISTORY_LEGACY</code> (#898).</li> </ul> </li> <li>Testing:<ul> <li>Remove obsolete fixture <code>slurm_config</code> (#903).</li> </ul> </li> </ul>"},{"location":"changelog/#1311","title":"1.3.11","text":"<p>This is mainly a bugfix release for the <code>PermissionError</code> issue.</p> <ul> <li>Fix <code>PermissionError</code>s in parallel-task metadata aggregation for the SLURM backend (#893).</li> <li>Documentation:<ul> <li>Bump <code>mkdocs-render-swagger-plugin</code> to 0.1.0 (#889).</li> </ul> </li> <li>Testing:<ul> <li>Fix <code>poetry install</code> command and <code>poetry</code> version in GitHub CI (#889).</li> </ul> </li> </ul>"},{"location":"changelog/#1310","title":"1.3.10","text":"<p>Warning: updating to this version requires changes to the configuration variable</p> <ul> <li>Updates to SLURM interface:<ul> <li>Remove <code>sudo</code>-requiring <code>ls</code> calls from <code>FractalFileWaitThread.check</code> (#885);</li> <li>Change default of <code>FRACTAL_SLURM_POLL_INTERVAL</code> to 5 seconds (#885);</li> <li>Rename <code>FRACTAL_SLURM_OUTPUT_FILE_GRACE_TIME</code> configuration variables into <code>FRACTAL_SLURM_ERROR_HANDLING_INTERVAL</code> (#885);</li> <li>Remove <code>FRACTAL_SLURM_KILLWAIT_INTERVAL</code> variable and corresponding logic (#885);</li> <li>Remove <code>_multiple_paths_exist_as_user</code> helper function (#885);</li> <li>Review type hints and default values of SLURM-related configuration variables (#885).</li> </ul> </li> <li>Dependencies:<ul> <li>Update <code>fastapi</code> to version <code>^0.103.0</code> (#877);</li> <li>Update <code>fastapi-users</code> to version <code>^12.1.0</code> (#877).</li> </ul> </li> </ul>"},{"location":"changelog/#139","title":"1.3.9","text":"<ul> <li>Make updated-metadata collection robust for metadiff files consisting of a single <code>null</code> value (#879).</li> <li>Automate procedure for publishing package to PyPI (#881).</li> </ul>"},{"location":"changelog/#138","title":"1.3.8","text":"<ul> <li>Backend runner:<ul> <li>Add aggregation logic for parallel-task updated metadata (#852);</li> <li>Make updated-metadata collection robust for missing files (#852, #863).</li> </ul> </li> <li>Database interface:</li> <li>API:<ul> <li>Prevent user from bypassing workflow-name constraint via the PATCH endpoint (#867).</li> <li>Handle error upon task collection, when tasks exist in the database but not on-disk (#874).</li> <li>Add <code>_check_project_exists</code> helper function (#872).</li> </ul> </li> <li>Configuration variables:<ul> <li>Remove <code>DEPLOYMENT_TYPE</code> variable and update <code>alive</code> endpoint (#875);</li> <li>Introduce <code>Settings.check_db</code> method, and call it during inline/offline migrations (#855);</li> <li>Introduce <code>Settings.check_runner</code> method (#875);</li> <li>Fail if <code>FRACTAL_BACKEND_RUNNER</code> is <code>\"local\"</code> and <code>FRACTAL_LOCAL_CONFIG_FILE</code> is set but missing on-disk (#875);</li> <li>Clean up <code>Settings.check</code> method and improve its coverage (#875);</li> </ul> </li> <li>Package, repository, documentation:<ul> <li>Change <code>fractal_server.common</code> from being a git-submodule to being a regular folder (#859).</li> <li>Pin documentation dependencies (#865).</li> <li>Split <code>app/models/project.py</code> into two modules for dataset and project (#871).</li> <li>Revamp documentation on database interface and on the corresponding configuration variables (#855).</li> </ul> </li> </ul>"},{"location":"changelog/#137","title":"1.3.7","text":"<ul> <li>Oauth2-related updates (#822):<ul> <li>Update configuration of OAuth2 clients, to support OIDC/GitHub/Google;</li> <li>Merge <code>SQLModelBaseOAuthAccount</code> and <code>OAuthAccount</code> models;</li> <li>Update <code>UserOAuth.oauth_accounts</code> relationship and fix <code>list_users</code> endpoint accordingly;</li> <li>Introduce dummy <code>UserManager.on_after_login</code> method;</li> <li>Rename <code>OAuthClient</code> into <code>OAuthClientConfig</code>;</li> <li>Revamp users-related parts of documentation.</li> </ul> </li> </ul>"},{"location":"changelog/#136","title":"1.3.6","text":"<ul> <li>Update <code>output_dataset.meta</code> also when workflow execution fails (#843).</li> <li>Improve error message for unknown errors in job execution (#843).</li> <li>Fix log message incorrectly marked as \"error\" (#846).</li> </ul>"},{"location":"changelog/#135","title":"1.3.5","text":"<ul> <li>Review structure of dataset history (#803):<ul> <li>Re-define structure for <code>history</code> property of <code>Dataset.meta</code>;</li> <li>Introduce <code>\"api/v1/project/{project_id}/dataset/{dataset_id}/status/\"</code> endpoint;</li> <li>Introduce <code>\"api/v1/project/{project_id}/dataset/{dataset_id}/export_history/\"</code> endpoint;</li> <li>Move legacy history to <code>Dataset.meta[\"HISTORY_LEGACY\"]</code>.</li> </ul> </li> <li>Make <code>first_task_index</code> and <code>last_task_index</code> properties of <code>ApplyWorkflow</code> required (#803).</li> <li>Add <code>docs_info</code> and <code>docs_link</code> to Task model (#814)</li> <li>Accept <code>TaskUpdate.version=None</code> in task-patch endpoint (#818).</li> <li>Store a copy of the <code>Workflow</code> into the optional column <code>ApplyWorkflow.workflow_dump</code> at the time of submission (#804, #834).</li> <li>Prevent execution of multiple jobs with the same output dataset (#801).</li> <li>Transform non-absolute <code>FRACTAL_TASKS_DIR</code> into absolute paths, relative to the current working directory (#825).</li> <li>Error handling:<ul> <li>Raise an appropriate error if a task command is not executable (#800).</li> <li>Improve handling of errors raised in <code>get_slurm_config</code> (#800).</li> </ul> </li> <li>Documentation:<ul> <li>Clarify documentation about <code>SlurmConfig</code> (#798).</li> <li>Update documentation configuration and GitHub actions (#811).</li> </ul> </li> <li>Tests:<ul> <li>Move <code>tests/test_common.py</code> into <code>fractal-common</code> repository (#808).</li> <li>Switch to <code>docker compose</code> v2 and unpin <code>pyyaml</code> version (#816).</li> </ul> </li> </ul>"},{"location":"changelog/#134","title":"1.3.4","text":"<ul> <li>Support execution of a workflow subset (#784).</li> <li>Fix internal server error for invalid <code>task_id</code> in <code>create_workflowtask</code> endpoint (#782).</li> <li>Improve logging in background task collection (#776).</li> <li>Handle failures in <code>submit_workflow</code> without raising errors (#787).</li> <li>Simplify internal function for execution of a list of task (#780).</li> <li>Exclude <code>common/tests</code> and other git-related files from build (#795).</li> <li>Remove development dependencies <code>Pillow</code> and <code>pytest-mock</code> (#795).</li> <li>Remove obsolete folders from <code>tests/data</code> folder (#795).</li> </ul>"},{"location":"changelog/#133","title":"1.3.3","text":"<ul> <li>Pin Pydantic to v1 (#779).</li> </ul>"},{"location":"changelog/#132","title":"1.3.2","text":"<ul> <li>Add sqlalchemy naming convention for DB constraints, and add <code>render_as_batch=True</code> to <code>do_run_migrations</code> (#757).</li> <li>Fix bug in job-stop endpoint, due to missing default for <code>FractalSlurmExecutor.wait_thread.shutdown_file</code> (#768, #769).</li> <li>Fix bug upon inserting a task with <code>meta=None</code> into a Workflow (#772).</li> </ul>"},{"location":"changelog/#131","title":"1.3.1","text":"<ul> <li>Fix return value of stop-job endpoint (#764).</li> <li>Expose new GET <code>WorkflowTask</code> endpoint (#762).</li> <li>Clean up API modules (#762):<ul> <li>Split workflow/workflowtask modules;</li> <li>Split tasks/task-collection modules.</li> </ul> </li> </ul>"},{"location":"changelog/#130","title":"1.3.0","text":"<ul> <li>Refactor user model:<ul> <li>Switch from UUID4 to int for IDs (#660, #684).</li> <li>Fix many-to-many relationship between users and project (#660).</li> <li>Rename <code>Project.user_member_list</code> into <code>Project.user_list</code> (#660).</li> <li>Add <code>username</code> column (#704).</li> </ul> </li> <li>Update endpoints (see also 1.2-&gt;1.3 upgrade info in the documentation):<ul> <li>Review endpoint URLs (#669).</li> <li>Remove foreign keys from payloads (#669).</li> </ul> </li> <li>Update <code>Task</code> models, task collection and task-related endpoints:<ul> <li>Add <code>version</code> and <code>owner</code> columns to <code>Task</code> model (#704).</li> <li>Set <code>Task.version</code> during task collection (#719).</li> <li>Set <code>Task.owner</code> as part of create-task endpoint (#704).</li> <li>For custom tasks, prepend <code>owner</code> to user-provided <code>source</code> (#725).</li> <li>Remove <code>default_args</code> from <code>Tasks</code> model and from manifest tasks (#707).</li> <li>Add <code>args_schema</code> and <code>args_schema_version</code> to <code>Task</code> model (#707).</li> <li>Expose <code>args_schema</code> and <code>args_schema_version</code> in task POST/PATCH endpoints (#749).</li> <li>Make <code>Task.source</code> task-specific rather than package-specific (#719).</li> <li>Make <code>Task.source</code> unique (#725).</li> <li>Update <code>_TaskCollectPip</code> methods, attributes and properties (#719).</li> <li>Remove private/public options for task collection (#704).</li> <li>Improve error message for missing package manifest (#704).</li> <li>Improve behavior when task-collection folder already exists (#704).</li> <li>Expose <code>pinned_package_version</code> for tasks collection (#744).</li> <li>Restrict Task editing to superusers and task owners (#733).</li> <li>Implement <code>delete_task</code> endpoint (#745).</li> </ul> </li> <li>Update <code>Workflow</code> and <code>WorkflowTask</code> endpoints:<ul> <li>Always merge new <code>WorkflowTask.args</code> with defaults from <code>Task.args_schema</code>, in <code>update_workflowtask</code> endpoint (#759).</li> <li>Remove <code>WorkflowTask.overridden_meta</code> property and on-the-fly overriding of <code>meta</code> (#752).</li> <li>Add warning when exporting workflows which include custom tasks (#728).</li> <li>When importing a workflow, only use tasks' <code>source</code> values, instead of <code>(source,name)</code> pairs (#719).</li> </ul> </li> <li>Job execution:<ul> <li>Add <code>FractalSlurmExecutor.shutdown</code> and corresponding endpoint (#631, #691, #696).</li> <li>In <code>FractalSlurmExecutor</code>, make <code>working_dir*</code> attributes required (#679).</li> <li>Remove <code>ApplyWorkflow.overwrite_input</code> column (#684, #694).</li> <li>Make <code>output_dataset_id</code> a required argument of apply-workflow endpoint (#681).</li> <li>Improve error message related to out-of-space disk (#699).</li> <li>Include timestamp in job working directory, to avoid name clashes (#756).</li> </ul> </li> <li>Other updates to endpoints and database:<ul> <li>Add <code>ApplyWorkflow.end_timestamp</code> column (#687, #684).</li> <li>Prevent deletion of a <code>Workflow</code>/<code>Dataset</code> in relationship with existing <code>ApplyWorkflow</code> (#703).</li> <li>Add project-name uniqueness constraint in project-edit endpoint (#689).</li> </ul> </li> <li>Other updates to internal logic:<ul> <li>Drop <code>WorkflowTask.arguments</code> property and <code>WorkflowTask.assemble_args</code> method (#742).</li> <li>Add test for collection of tasks packages with tasks in a subpackage (#743).</li> <li>Expose <code>FRACTAL_CORS_ALLOW_ORIGIN</code> environment variable (#688).</li> <li>Expose <code>FRACTAL_DEFAULT_ADMIN_USERNAME</code> environment variable (#751).</li> </ul> </li> <li>Package and repository:<ul> <li>Remove <code>fastapi-users-db-sqlmodel</code> dependency (#660).</li> <li>Make coverage measure more accurate (#676) and improve coverage (#678).</li> <li>Require pydantic version to be <code>&gt;=1.10.8</code> (#711, #713).</li> <li>Include multiple <code>fractal-common</code> updates (#705, #719).</li> <li>Add test equivalent to <code>alembic check</code> (#722).</li> <li>Update <code>poetry.lock</code> to address security alerts (#723).</li> <li>Remove <code>sqlmodel</code> from <code>fractal-common</code>, and declare database models with multiple inheritance (#710).</li> <li>Make email generation more robust in <code>MockCurrentUser</code> (#730).</li> <li>Update <code>poetry.lock</code> to <code>cryptography=41</code>, to address security alert (#739).</li> <li>Add <code>greenlet</code> as a direct dependency (#748).</li> <li>Removed tests for <code>IntegrityError</code> (#754).</li> </ul> </li> </ul>"},{"location":"changelog/#125","title":"1.2.5","text":"<ul> <li>Fix bug in task collection when using sqlite (#664, #673).</li> <li>Fix bug in task collection from local package, where package extras were not considered (#671).</li> <li>Improve error handling in workflow-apply endpoint (#665).</li> <li>Fix a bug upon project removal in the presence of project-related jobs (#666). Note: this removes the <code>ApplyWorkflow.Project</code> attribute.</li> </ul>"},{"location":"changelog/#124","title":"1.2.4","text":"<ul> <li>Review setup for database URLs, especially to allow using UNIX-socket connections for postgresql (#657).</li> </ul>"},{"location":"changelog/#123","title":"1.2.3","text":"<ul> <li>Fix bug that was keeping multiple database conection open (#649).</li> </ul>"},{"location":"changelog/#122","title":"1.2.2","text":"<ul> <li>Fix bug related to <code>user_local_exports</code> in SLURM-backend configuration (#642).</li> </ul>"},{"location":"changelog/#121","title":"1.2.1","text":"<ul> <li>Fix bug upon creation of first user when using multiple workers (#632).</li> <li>Allow both ports 5173 and 4173 as CORS origins (#637).</li> </ul>"},{"location":"changelog/#120","title":"1.2.0","text":"<ul> <li>Drop <code>project.project_dir</code> and replace it with <code>user.cache_dir</code> (#601).</li> <li>Update SLURM backend (#582, #612, #614); this includes (1) combining several tasks in a single SLURM job, and (2) offering more granular sources for SLURM configuration options.</li> <li>Expose local user exports in SLURM configuration file (#625).</li> <li>Make local backend rely on custom <code>FractalThreadPoolExecutor</code>, where <code>parallel_tasks_per_job</code> can affect parallelism (#626).</li> <li>Review logging configuration (#619, #623).</li> <li>Update to fastapi <code>0.95</code> (#587).</li> <li>Minor improvements in dataset-edit endpoint (#593) and tests (#589).</li> <li>Include test of non-python task (#594).</li> <li>Move dummy tasks from package to tests (#601).</li> <li>Remove deprecated parsl backend (#607).</li> <li>Improve error handling in workflow-import endpoint (#595).</li> <li>Also show logs for successful workflow execution (#635).</li> </ul>"},{"location":"changelog/#111","title":"1.1.1","text":"<ul> <li>Include <code>reordered_workflowtask_ids</code> in workflow-edit endpoint payload, to reorder the task list of a workflow (#585).</li> </ul>"},{"location":"changelog/#110","title":"1.1.0","text":"<ul> <li>Align with new tasks interface in <code>fractal-tasks-core&gt;=0.8.0</code>, and remove <code>glob_pattern</code> column from <code>resource</code> database table (#544).</li> <li>Drop python 3.8 support (#527).</li> <li>Improve validation of API request payloads (#545).</li> <li>Improve request validation in project-creation endpoint (#537).</li> <li>Update the endpoint to patch a <code>Task</code> (#526).</li> <li>Add new project-update endpoint, and relax constraints on <code>project_dir</code> in new-project endpoint (#563).</li> <li>Update <code>DatasetUpdate</code> schema (#558 and #565).</li> <li>Fix redundant task-error logs in slurm backend (#552).</li> <li>Improve handling of task-collection errors (#559).</li> <li>If <code>FRACTAL_BACKEND_RUNNER=slurm</code>, include some configuration checks at server startup (#529).</li> <li>Fail if <code>FRACTAL_SLURM_WORKER_PYTHON</code> has different versions of <code>fractal-server</code> or <code>cloudpickle</code> (#533).</li> </ul>"},{"location":"changelog/#108","title":"1.0.8","text":"<ul> <li>Fix handling of parallel-tasks errors in <code>FractalSlurmExecutor</code> (#497).</li> <li>Add test for custom tasks (#500).</li> <li>Improve formatting of job logs (#503).</li> <li>Improve error handling in workflow-execution server endpoint (#515).</li> <li>Update <code>_TaskBase</code> schema from fractal-common (#517).</li> </ul>"},{"location":"changelog/#107","title":"1.0.7","text":"<ul> <li>Update endpoints to import/export a workflow (#495).</li> </ul>"},{"location":"changelog/#106","title":"1.0.6","text":"<ul> <li>Add new endpoints to import/export a workflow (#490).</li> </ul>"},{"location":"changelog/#105","title":"1.0.5","text":"<ul> <li>Separate workflow-execution folder into two (server- and user-owned) folders, to avoid permission issues (#475).</li> <li>Explicitly pin sqlalchemy to v1 (#480).</li> </ul>"},{"location":"changelog/#104","title":"1.0.4","text":"<ul> <li>Add new POST endpoint to create new Task (#486).</li> </ul>"},{"location":"changelog/#103","title":"1.0.3","text":"<p>Missing due to releasing error.</p>"},{"location":"changelog/#102","title":"1.0.2","text":"<ul> <li>Add <code>FRACTAL_RUNNER_MAX_TASKS_PER_WORKFLOW</code> configuration variable (#469).</li> </ul>"},{"location":"changelog/#101","title":"1.0.1","text":"<ul> <li>Fix bug with environment variable names (#468).</li> </ul>"},{"location":"changelog/#100","title":"1.0.0","text":"<ul> <li>First release listed in CHANGELOG.</li> </ul>"},{"location":"cli_reference/","title":"CLI Reference","text":"<p>This page shows the help screens for the <code>fractalctl</code> command and its subcommands.</p>"},{"location":"cli_reference/#fractalctl","title":"<code>fractalctl</code>","text":"<pre><code>usage: fractalctl [-h] {start,openapi,set-db,init-db-data,update-db-data} ...\n\nfractal-server commands\n\noptions:\n  -h, --help            show this help message and exit\n\nCommands:\n  {start,openapi,set-db,init-db-data,update-db-data}\n</code></pre>"},{"location":"cli_reference/#fractalctl-start","title":"<code>fractalctl start</code>","text":"<pre><code>usage: fractalctl start [-h] [--host HOST] [-p PORT] [--reload]\n\nStart the server (with uvicorn)\n\noptions:\n  -h, --help            show this help message and exit\n  --host HOST           bind socket to this host (default: 127.0.0.1)\n  -p PORT, --port PORT  bind socket to this port (default: 8000)\n  --reload              enable auto-reload\n</code></pre>"},{"location":"cli_reference/#fractalctl-openapi","title":"<code>fractalctl openapi</code>","text":"<pre><code>usage: fractalctl openapi [-h] [-f OPENAPI_FILE]\n\nSave the `openapi.json` file\n\noptions:\n  -h, --help            show this help message and exit\n  -f OPENAPI_FILE, --openapi-file OPENAPI_FILE\n                        Filename for OpenAPI schema dump\n</code></pre>"},{"location":"cli_reference/#fractalctl-set-db","title":"<code>fractalctl set-db</code>","text":"<pre><code>usage: fractalctl set-db [-h]\n\nInitialise/upgrade database schemas.\n\noptions:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"cli_reference/#fractalctl-init-db-data","title":"<code>fractalctl init-db-data</code>","text":"<pre><code>usage: fractalctl init-db-data [-h] [--resource RESOURCE] [--profile PROFILE]\n                               [--admin-email ADMIN_EMAIL]\n                               [--admin-pwd ADMIN_PWD]\n                               [--admin-project-dir ADMIN_PROJECT_DIR]\n\nPopulate database with initial data.\n\noptions:\n  -h, --help            show this help message and exit\n  --resource RESOURCE   Either `default` or path to the JSON file of the first\n                        resource.\n  --profile PROFILE     Either `default` or path to the JSON file of the first\n                        profile.\n  --admin-email ADMIN_EMAIL\n                        Email of the first admin user.\n  --admin-pwd ADMIN_PWD\n                        Password for the first admin user.\n  --admin-project-dir ADMIN_PROJECT_DIR\n                        Project_dir for the first admin user.\n</code></pre>"},{"location":"cli_reference/#fractalctl-update-db-data","title":"<code>fractalctl update-db-data</code>","text":"<pre><code>usage: fractalctl update-db-data [-h]\n\nApply data-migration script to an existing database.\n\noptions:\n  -h, --help  show this help message and exit\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#set-configuration-variables","title":"Set configuration variables","text":"<p>There are two possibilities for setting the configuration variables that determine the settings for <code>fractal-server</code>:</p> <ol> <li>Define them as environment variables, in the same environment where the <code>fractal-server</code> process will runs:     <pre><code>export VARIABLE1=value1\nexport VARIABLE2=value2\n</code></pre></li> <li>Write them inside a file named <code>.fractal_server.env</code>, in your current working directory, with contents like     <pre><code>VARIABLE1=value1\nVARIABLE2=value2\n</code></pre></li> </ol> <p>Once the variables have been defined in one of these ways, they will be read automatically by the Fractal Server during the start-up phase.</p> <p>If the same variable is defined twice, both as an environment variable and inside the <code>.fractal_server.env</code> file, the value defined in the environment takes priority.</p>"},{"location":"configuration/#get-current-configuration","title":"Get current configuration","text":"<p>Admins can retrieve the current settings through the appropriate endpoints <code>GET /api/settings/</code>.</p>"},{"location":"configuration/#configuration-variables","title":"Configuration variables","text":"<p>Here are all the configuration variables with their description (note: expand the \"Source code\" blocks to see the default values).</p>"},{"location":"configuration/#fractal_server.config._main.Settings","title":"<code>Settings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Contains the general configuration variables for Fractal Server.</p> ATTRIBUTE DESCRIPTION <code>JWT_EXPIRE_SECONDS</code> <p>JWT token lifetime, in seconds.</p> <p> TYPE: <code>int</code> </p> <code>JWT_SECRET_KEY</code> <p>JWT secret. \u26a0\ufe0f Set this variable to a secure string, and do not disclose it.</p> <p> TYPE: <code>SecretStr</code> </p> <code>COOKIE_EXPIRE_SECONDS</code> <p>Cookie token lifetime, in seconds.</p> <p> TYPE: <code>int</code> </p> <code>FRACTAL_RUNNER_BACKEND</code> <p>Select which runner backend to use.</p> <p> TYPE: <code>Literal['local', 'slurm_ssh', 'slurm_sudo']</code> </p> <code>FRACTAL_LOGGING_LEVEL</code> <p>Logging-level threshold for logging Only logs of with this level (or higher) will appear in the console logs.</p> <p> TYPE: <code>int</code> </p> <code>FRACTAL_API_MAX_JOB_LIST_LENGTH</code> <p>Number of ids that can be stored in the <code>jobs</code> attribute of <code>app.state</code>.</p> <p> TYPE: <code>int</code> </p> <code>FRACTAL_GRACEFUL_SHUTDOWN_TIME</code> <p>Waiting time for the shutdown phase of executors, in seconds.</p> <p> TYPE: <code>float</code> </p> <code>FRACTAL_HELP_URL</code> <p>The URL of an instance-specific Fractal help page.</p> <p> TYPE: <code>HttpUrl | None</code> </p> <code>FRACTAL_DEFAULT_GROUP_NAME</code> <p>Name of the default user group.</p> <p>If set to <code>\"All\"</code>, then the user group with that name is a special user group (e.g. it cannot be deleted, and new users are automatically added to it). If set to <code>None</code> (the default value), then user groups are all equivalent, independently on their name.</p> <p> TYPE: <code>Literal['All'] | None</code> </p> <code>FRACTAL_LONG_REQUEST_TIME</code> <p>Time limit beyond which the execution of an API request is considered slow and an appropriate warning is logged by the middleware.</p> <p> TYPE: <code>float</code> </p> Source code in <code>fractal_server/config/_main.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"\n    Contains the general configuration variables for Fractal Server.\n\n    Attributes:\n        JWT_EXPIRE_SECONDS:\n            JWT token lifetime, in seconds.\n        JWT_SECRET_KEY:\n            JWT secret.&lt;br&gt;\n            \u26a0\ufe0f Set this variable to a secure string, and do not disclose it.\n        COOKIE_EXPIRE_SECONDS:\n            Cookie token lifetime, in seconds.\n        FRACTAL_RUNNER_BACKEND:\n            Select which runner backend to use.\n        FRACTAL_LOGGING_LEVEL:\n            Logging-level threshold for logging\n            Only logs of with this level (or higher) will appear in the console\n            logs.\n        FRACTAL_API_MAX_JOB_LIST_LENGTH:\n            Number of ids that can be stored in the `jobs` attribute of\n            `app.state`.\n        FRACTAL_GRACEFUL_SHUTDOWN_TIME:\n            Waiting time for the shutdown phase of executors, in seconds.\n        FRACTAL_HELP_URL:\n            The URL of an instance-specific Fractal help page.\n        FRACTAL_DEFAULT_GROUP_NAME:\n            Name of the default user group.\n\n            If set to `\"All\"`, then the user group with that name is a special\n            user group (e.g. it cannot be deleted, and new users are\n            automatically added to it). If set to `None` (the default value),\n            then user groups are all equivalent, independently on their name.\n        FRACTAL_LONG_REQUEST_TIME:\n            Time limit beyond which the execution of an API request is\n            considered *slow* and an appropriate warning is logged by the\n            middleware.\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    JWT_EXPIRE_SECONDS: int = 180\n    JWT_SECRET_KEY: SecretStr\n    COOKIE_EXPIRE_SECONDS: int = 86400\n    # Note: we do not use ResourceType here to avoid circular imports\n    FRACTAL_RUNNER_BACKEND: Literal[\"local\", \"slurm_ssh\", \"slurm_sudo\"] = (\n        \"local\"\n    )\n    FRACTAL_LOGGING_LEVEL: int = logging.INFO\n    FRACTAL_API_MAX_JOB_LIST_LENGTH: int = 25\n    FRACTAL_GRACEFUL_SHUTDOWN_TIME: float = 30.0\n    FRACTAL_HELP_URL: HttpUrl | None = None\n    FRACTAL_DEFAULT_GROUP_NAME: Literal[\"All\"] | None = None\n    FRACTAL_LONG_REQUEST_TIME: float = 30.0\n</code></pre>"},{"location":"configuration/#fractal_server.config._database.DatabaseSettings","title":"<code>DatabaseSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Minimal set of configurations needed for operating on the database (e.g for schema migrations).</p> ATTRIBUTE DESCRIPTION <code>DB_ECHO</code> <p>If <code>\"true\"</code>, make database operations verbose.</p> <p> TYPE: <code>Literal['true', 'false']</code> </p> <code>POSTGRES_USER</code> <p>User to use when connecting to the PostgreSQL database.</p> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>POSTGRES_PASSWORD</code> <p>Password to use when connecting to the PostgreSQL database.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>POSTGRES_HOST</code> <p>URL to the PostgreSQL server or path to a UNIX domain socket.</p> <p> TYPE: <code>NonEmptyStr</code> </p> <code>POSTGRES_PORT</code> <p>Port number to use when connecting to the PostgreSQL server.</p> <p> TYPE: <code>NonNegativeInt</code> </p> <code>POSTGRES_DB</code> <p>Name of the PostgreSQL database to connect to.</p> <p> TYPE: <code>NonEmptyStr</code> </p> Source code in <code>fractal_server/config/_database.py</code> <pre><code>class DatabaseSettings(BaseSettings):\n    \"\"\"\n    Minimal set of configurations needed for operating on the database (e.g\n    for schema migrations).\n\n    Attributes:\n        DB_ECHO:\n            If `\"true\"`, make database operations verbose.\n        POSTGRES_USER:\n            User to use when connecting to the PostgreSQL database.\n        POSTGRES_PASSWORD:\n            Password to use when connecting to the PostgreSQL database.\n        POSTGRES_HOST:\n            URL to the PostgreSQL server or path to a UNIX domain socket.\n        POSTGRES_PORT:\n            Port number to use when connecting to the PostgreSQL server.\n        POSTGRES_DB:\n            Name of the PostgreSQL database to connect to.\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    DB_ECHO: Literal[\"true\", \"false\"] = \"false\"\n    POSTGRES_USER: NonEmptyStr | None = None\n    POSTGRES_PASSWORD: SecretStr | None = None\n    POSTGRES_HOST: NonEmptyStr = \"localhost\"\n    POSTGRES_PORT: NonNegativeInt = 5432\n    POSTGRES_DB: NonEmptyStr\n\n    @property\n    def DATABASE_URL(self) -&gt; URL:\n        if self.POSTGRES_PASSWORD is None:\n            password = None\n        else:\n            password = self.POSTGRES_PASSWORD.get_secret_value()\n\n        url = URL.create(\n            drivername=\"postgresql+psycopg\",\n            username=self.POSTGRES_USER,\n            password=password,\n            host=self.POSTGRES_HOST,\n            port=self.POSTGRES_PORT,\n            database=self.POSTGRES_DB,\n        )\n        return url\n</code></pre>"},{"location":"configuration/#fractal_server.config._email.EmailSettings","title":"<code>EmailSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Class with settings for email-sending feature.</p> ATTRIBUTE DESCRIPTION <code>FRACTAL_EMAIL_SENDER</code> <p>Address of the OAuth-signup email sender.</p> <p> TYPE: <code>EmailStr | None</code> </p> <code>FRACTAL_EMAIL_PASSWORD</code> <p>Password for the OAuth-signup email sender.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>FRACTAL_EMAIL_SMTP_SERVER</code> <p>SMTP server for the OAuth-signup emails.</p> <p> TYPE: <code>str | None</code> </p> <code>FRACTAL_EMAIL_SMTP_PORT</code> <p>SMTP server port for the OAuth-signup emails.</p> <p> TYPE: <code>int | None</code> </p> <code>FRACTAL_EMAIL_INSTANCE_NAME</code> <p>Fractal instance name, to be included in the OAuth-signup emails.</p> <p> TYPE: <code>str | None</code> </p> <code>FRACTAL_EMAIL_RECIPIENTS</code> <p>Comma-separated list of recipients of the OAuth-signup emails.</p> <p> TYPE: <code>str | None</code> </p> <code>FRACTAL_EMAIL_USE_STARTTLS</code> <p>Whether to use StartTLS when using the SMTP server.</p> <p> TYPE: <code>Literal['true', 'false']</code> </p> <code>FRACTAL_EMAIL_USE_LOGIN</code> <p>Whether to use login when using the SMTP server. If 'true', FRACTAL_EMAIL_PASSWORD  must be provided.</p> <p> TYPE: <code>Literal['true', 'false']</code> </p> Source code in <code>fractal_server/config/_email.py</code> <pre><code>class EmailSettings(BaseSettings):\n    \"\"\"\n    Class with settings for email-sending feature.\n\n    Attributes:\n        FRACTAL_EMAIL_SENDER:\n            Address of the OAuth-signup email sender.\n        FRACTAL_EMAIL_PASSWORD:\n            Password for the OAuth-signup email sender.\n        FRACTAL_EMAIL_SMTP_SERVER:\n            SMTP server for the OAuth-signup emails.\n        FRACTAL_EMAIL_SMTP_PORT:\n            SMTP server port for the OAuth-signup emails.\n        FRACTAL_EMAIL_INSTANCE_NAME:\n            Fractal instance name, to be included in the OAuth-signup emails.\n        FRACTAL_EMAIL_RECIPIENTS:\n            Comma-separated list of recipients of the OAuth-signup emails.\n        FRACTAL_EMAIL_USE_STARTTLS:\n            Whether to use StartTLS when using the SMTP server.\n        FRACTAL_EMAIL_USE_LOGIN:\n            Whether to use login when using the SMTP server.\n            If 'true', FRACTAL_EMAIL_PASSWORD  must be provided.\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    FRACTAL_EMAIL_SENDER: EmailStr | None = None\n    FRACTAL_EMAIL_PASSWORD: SecretStr | None = None\n    FRACTAL_EMAIL_SMTP_SERVER: str | None = None\n    FRACTAL_EMAIL_SMTP_PORT: int | None = None\n    FRACTAL_EMAIL_INSTANCE_NAME: str | None = None\n    FRACTAL_EMAIL_RECIPIENTS: str | None = None\n    FRACTAL_EMAIL_USE_STARTTLS: Literal[\"true\", \"false\"] = \"true\"\n    FRACTAL_EMAIL_USE_LOGIN: Literal[\"true\", \"false\"] = \"true\"\n\n    public: PublicEmailSettings | None = None\n    \"\"\"\n    The validated field which is actually used in `fractal-server`,\n    automatically populated upon creation.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def validate_email_settings(self: Self) -&gt; Self:\n        \"\"\"\n        Set `self.public`.\n        \"\"\"\n\n        email_values = [\n            self.FRACTAL_EMAIL_SENDER,\n            self.FRACTAL_EMAIL_SMTP_SERVER,\n            self.FRACTAL_EMAIL_SMTP_PORT,\n            self.FRACTAL_EMAIL_INSTANCE_NAME,\n            self.FRACTAL_EMAIL_RECIPIENTS,\n        ]\n        if len(set(email_values)) == 1:\n            # All required EMAIL attributes are None\n            pass\n        elif None in email_values:\n            # Not all required EMAIL attributes are set\n            error_msg = (\n                \"Invalid FRACTAL_EMAIL configuration. \"\n                f\"Given values: {email_values}.\"\n            )\n            raise ValueError(error_msg)\n        else:\n            use_starttls = self.FRACTAL_EMAIL_USE_STARTTLS == \"true\"\n            use_login = self.FRACTAL_EMAIL_USE_LOGIN == \"true\"\n\n            if use_login and self.FRACTAL_EMAIL_PASSWORD is None:\n                raise ValueError(\n                    \"'FRACTAL_EMAIL_USE_LOGIN' is 'true' but \"\n                    \"'FRACTAL_EMAIL_PASSWORD' is not provided.\"\n                )\n\n            self.public = PublicEmailSettings(\n                sender=self.FRACTAL_EMAIL_SENDER,\n                recipients=self.FRACTAL_EMAIL_RECIPIENTS.split(\",\"),\n                smtp_server=self.FRACTAL_EMAIL_SMTP_SERVER,\n                port=self.FRACTAL_EMAIL_SMTP_PORT,\n                password=self.FRACTAL_EMAIL_PASSWORD,\n                instance_name=self.FRACTAL_EMAIL_INSTANCE_NAME,\n                use_starttls=use_starttls,\n                use_login=use_login,\n            )\n\n        return self\n</code></pre>"},{"location":"configuration/#fractal_server.config._oauth.OAuthSettings","title":"<code>OAuthSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for integration with an OAuth identity provider.</p> ATTRIBUTE DESCRIPTION <code>OAUTH_CLIENT_NAME</code> <p>Name of the client.</p> <p> TYPE: <code>Annotated[NonEmptyStr, StringConstraints(to_lower=True)] | None</code> </p> <code>OAUTH_CLIENT_ID</code> <p>ID of client.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>OAUTH_CLIENT_SECRET</code> <p>Secret to authorise against the identity provider.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>OAUTH_OIDC_CONFIG_ENDPOINT</code> <p>OpenID Connect configuration endpoint, for autodiscovery of relevant endpoints.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>OAUTH_REDIRECT_URL</code> <p>String to be used as <code>redirect_url</code> argument in <code>fastapi_users.get_oauth_router</code>, and then in <code>httpx_oauth.integrations.fastapi.OAuth2AuthorizeCallback</code>.</p> <p> TYPE: <code>str | None</code> </p> <code>OAUTH_EMAIL_CLAIM</code> <p>Name of the OIDC claim with the user's email address. This is <code>\"email\"</code> by default, but can be customized (e.g. to <code>\"mail\"</code>) to fit with the response from the userinfo endpoint - see https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/config/_oauth.py</code> <pre><code>class OAuthSettings(BaseSettings):\n    \"\"\"\n    Settings for integration with an OAuth identity provider.\n\n    Attributes:\n        OAUTH_CLIENT_NAME: Name of the client.\n        OAUTH_CLIENT_ID: ID of client.\n        OAUTH_CLIENT_SECRET:\n            Secret to authorise against the identity provider.\n        OAUTH_OIDC_CONFIG_ENDPOINT:\n            OpenID Connect configuration endpoint, for autodiscovery of\n            relevant endpoints.\n        OAUTH_REDIRECT_URL:\n            String to be used as `redirect_url` argument in\n            `fastapi_users.get_oauth_router`, and then in\n            `httpx_oauth.integrations.fastapi.OAuth2AuthorizeCallback`.\n        OAUTH_EMAIL_CLAIM:\n            Name of the OIDC claim with the user's email address. This is\n            `\"email\"` by default, but can be customized (e.g. to `\"mail\"`) to\n            fit with the response from the userinfo endpoint - see\n            https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    OAUTH_CLIENT_NAME: (\n        Annotated[\n            NonEmptyStr,\n            StringConstraints(to_lower=True),\n        ]\n        | None\n    ) = None\n    OAUTH_CLIENT_ID: SecretStr | None = None\n    OAUTH_CLIENT_SECRET: SecretStr | None = None\n    OAUTH_OIDC_CONFIG_ENDPOINT: SecretStr | None = None\n    OAUTH_REDIRECT_URL: str | None = None\n    OAUTH_EMAIL_CLAIM: str = \"email\"\n\n    @model_validator(mode=\"after\")\n    def check_configuration(self: Self) -&gt; Self:\n        if (\n            self.OAUTH_CLIENT_NAME not in [\"google\", \"github\", None]\n            and self.OAUTH_OIDC_CONFIG_ENDPOINT is None\n        ):\n            raise ValueError(\n                f\"self.OAUTH_OIDC_CONFIG_ENDPOINT=None but \"\n                f\"{self.OAUTH_CLIENT_NAME=}\"\n            )\n        return self\n\n    @property\n    def is_set(self) -&gt; bool:\n        return None not in (\n            self.OAUTH_CLIENT_NAME,\n            self.OAUTH_CLIENT_ID,\n            self.OAUTH_CLIENT_SECRET,\n        )\n</code></pre>"},{"location":"configuration/#minimal-working-example","title":"Minimal working example","text":"<p>This is a minimal working example of a <code>.fractal_server.env</code>, with all the required configuration variables set: <pre><code>JWT_SECRET_KEY=secret-key-for-jwt-tokens\nPOSTGRES_DB=fractal-database-name\n</code></pre> These are the only required variables. All others, if not specified, will assume their default value.</p>"},{"location":"development/","title":"Development","text":"<p>The development of <code>fractal-server</code> takes place on the <code>fractal-server</code> Github repository. To ask questions or to inform us of a bug or unexpected behavior, please feel free to open an issue.</p> <p>This document describes how to contribute code to the repository. Other relevant links are:</p> <ul> <li>Main webpage of the Fractal project</li> <li>Documentation for <code>fractal-server</code></li> <li><code>fractal-server</code> project on PyPI</li> </ul>"},{"location":"development/#initial-setup","title":"Initial setup","text":""},{"location":"development/#repository","title":"Repository","text":"<p>You can clone the <code>fractal-server</code> repository via <pre><code>git clone https://github.com/fractal-analytics-platform/fractal-server.git\n</code></pre> or (see https://docs.github.com/en/get-started/git-basics/about-remote-repositories): <pre><code>git clone git@github.com:fractal-analytics-platform/fractal-server.git\n</code></pre></p>"},{"location":"development/#uv","title":"<code>uv</code>","text":"<p>We use uv to manage the development environment and the dependencies - see https://docs.astral.sh/uv/getting-started/installation/ for methods to install it. From the <code>fractal-server</code> root folder, you can get started through <pre><code># Create a new virtual environment in `.venv`\nuv venv\n\n# Install both the required dependencies and the optional dev/docs dependencies\nuv sync --frozen --group dev --group docs\n\n# Run a command from within this environment without updating the `uv.lock` file\nuv run --frozen fractalctl --help\n</code></pre></p>"},{"location":"development/#pre-commit","title":"<code>pre-commit</code>","text":"<p>We use pre-commit to run several checks on files that are being committed. To set it up locally, you should run <pre><code># Install pre-commit globally\npipx install pre-commit\n\n# Add the pre-commit hook to your local repository\npre-commit install\n</code></pre></p>"},{"location":"development/#development","title":"Development","text":""},{"location":"development/#git","title":"<code>git</code>","text":""},{"location":"development/#branches-and-tags","title":"Branches and tags","text":"<ul> <li>The default branch for this repository is <code>main</code>. This branch is meant to be ready to be released at any time (that is, it should not include code that is knowingly broken), but it is not meant to be stable (that is, it can and it will include breaking changes with respect to previous versions).</li> <li>Releases are made by pushing a tag with some specific label (e.g. <code>1.2.3</code>), and they are meant to be immutable (that is, we are not planning to ever edit on of these references).<ul> <li>The tag creation on GitHub also triggers the creation of a PyPI release - see the release section below.</li> </ul> </li> </ul>"},{"location":"development/#pull-requests","title":"Pull requests","text":"<ul> <li>Unless the contributed change has an extremely narrow scope or it was already discussed with the Fractal team, please open an issue before working on a new feature.</li> <li>Typical code contributions should take the form of Pull Requests (PR) towards the <code>main</code> branch.</li> <li>When working towards a new complex feature, we sometimes introduce a specific \"base\" branch (e.g. <code>dev-new-feature-X</code>) and we open smaller-scoped PRs towards that base branch. When the core of the new feature is ready, we open a new PR from <code>dev-new-feature-X</code> to <code>main</code>.</li> <li>Opening a PR also creates a corresponding checklist.</li> <li>Opening a PR towards <code>main</code> also triggers several automated tests.</li> </ul>"},{"location":"development/#tests","title":"Tests","text":"<p>Unit and integration testing of Fractal Server uses the pytest testing framework. Typical examples of how to run tests locally are <pre><code># Run all tests\nuv run --frozen pytest\n</code></pre> or <pre><code># Run all tests (more verbose)\nuv run --frozen pytest -v -s --log-cli-level info --full-trace\n</code></pre></p> <p>Pytest markers are used to include or exclude some specific tests, notably the ones that require access to some external resources (Docker). An example: <pre><code># Run tests the do not require any Docker container\nuv run --frozen pytest -m \"not container and not oauth\"\n</code></pre></p>"},{"location":"development/#slurm","title":"SLURM","text":"<p>To test the SLURM backend, we use a custom version of a Docker local SLURM cluster (defined in https://github.com/fractal-analytics-platform/fractal-containers). The pytest plugin pytest-docker is then used to spin up the Docker containers for the duration of the tests.</p> <p>Important: this requires docker being installed on the development system, and the current user being in the <code>docker</code> group. A simple check for this requirement is to run a command like <code>docker ps</code>, and verify that it does not raise any permission-related error. Note that also <code>docker compose</code> must be available.</p> <p>The specific tests that require a SLURM cluster through <code>pytest-docker</code> can be run via <pre><code>uv run --frozen pytest -m container\n</code></pre></p>"},{"location":"development/#oauth-and-email","title":"OAuth and email","text":"<p>For testing OAuth integration and email-sending features, we rely on containerized Dex and mailpit services. A dedicated script shows how to run these specific tests (with the appropriate containers and with all relevant configuration variables): <pre><code>./tools/run_oauth_tests.sh\n</code></pre></p>"},{"location":"development/#update-database-schema","title":"Update database schema","text":"<p>Whenever the database schemas in <code>app/models</code> are modified, you should create a migration script. To check whether this is needed, run <pre><code>uv run --frozen alembic check\n</code></pre></p> <p>If needed, the simplest procedure is to use <code>alembic --autogenerate</code> to create an incremental migration script, as in <pre><code>export POSTGRES_DB=\"autogenerate-fractal-revision\"\ndropdb --if-exist \"$POSTGRES_DB\"\ncreatedb \"$POSTGRES_DB\"\nuv run --frozen fractalctl set-db --skip-init-data\nuv run --frozen alembic revision --autogenerate -m \"Description of the current migration\"\n</code></pre> If successful, this procedure creates a new file in <code>fractal_server/migrations/versions/</code>.</p>"},{"location":"development/#release","title":"Release","text":"<ol> <li>Checkout to branch <code>main</code> (this is not strictly needed, and a common use case is to make pre-releases from a feature branch).</li> <li>Update the <code>CHANGELOG.md</code> file (e.g. remove <code>(unreleased)</code> from the upcoming version).</li> <li>If you have modified the models, then you must also create a new migration script (note: in principle the CI will fail if you forget this step).</li> <li>Use one of the following <pre><code># Bump version e.g. from 1.2.3a6 to 1.2.3a7\nuv run --frozen bumpver update --tag-num --dry\n\n# Bump version e.g. from 1.2.3 to 1.2.4\nuv run --frozen bumpver update --patch --dry\n\n# Bump version e.g. from 1.2.3 to 1.3.0\nuv run --frozen bumpver update --minor --dry\n\n# Bump version to a specific target X.Y.Z\nuv run --frozen bumpver update --set-version X.Y.Z --dry\n</code></pre> to test updating the version bump.</li> <li>If the previous step looks OK, remove <code>--dry</code> and re-run to actually bump the version, commit and push the changes.</li> <li>Approve (or have approved) the new version at Publish package to PyPI.</li> <li>After the release: If the release was a stable one (e.g. <code>X.Y.Z</code>, not <code>X.Y.Za1</code> or <code>X.Y.Zrc2</code>), move <code>fractal_server/data_migrations/X_Y_Z.py</code> to <code>fractal_server/data_migrations/old</code>.</li> </ol>"},{"location":"development/#documentation","title":"Documentation","text":"<p>Documentation for <code>fractal-server</code> is hosted at https://fractal-analytics-platform.github.io/fractal-server and built as part of a dedicated GitHub action. The code reference is build automatically, based on docstrings and type hints in the Python code.</p> <p>Docstrings should be formatted as in the Google Python Style Guide.</p> <p>For building the documentation we use mkdocs and the Material theme.</p> <p>To build the documentation and serve it at http://127.0.0.1:8000: <pre><code># Export some required environment variables\nexport POSTGRES_DB=mock_fractal\nexport JWT_SECRET_KEY=mock_fractal\n\n# Build and serve the documentation\nuv run --frozen mkdocs serve --config-file mkdocs.yml\n</code></pre></p>"},{"location":"install_and_deploy/","title":"Install and deploy","text":"<p>Fractal Server is the core ingredient of the Fractal framework, which includes several other Fractal components (e.g. a web client) and also relies on external resources being available (e.g. a PostgreSQL database and a SLURM cluster).</p> <p>This page describes the basic procedure to setup and maintain a local <code>fractal-server</code> deployment, which is useful for testing and development, but a full-fledged deployment involves many more aspects. Some examples of deployment setups are available as container-based demos at https://github.com/fractal-analytics-platform/fractal-containers/tree/main/examples.</p>"},{"location":"install_and_deploy/#prerequisites","title":"Prerequisites","text":"<p>The following will assume that:</p> <ul> <li>You are using a Python version greater or equal than 3.11</li> <li> <p>You are working within an isolated Python environment, for example a virtual environment created through <code>venv</code> as in   <pre><code>python3 -m venv venv\n./venv/bin/activate\n</code></pre></p> </li> <li> <p>You have configured the required environment variables (see configuration page).</p> </li> <li> <p>If you choose to declare the environment variables using the <code>.fractal_server.env</code> file, that file must be placed in the current working directory;</p> </li> <li> <p>You have access to a dedicated PostgreSQL database (see the database page).</p> </li> <li> <p>A few common UNIX tools are available, including <code>du</code>, <code>find</code>, <code>cut</code>, <code>cat</code>, <code>wc</code>, <code>bash</code>, <code>ls</code>, <code>tar</code>, <code>ssh</code> and <code>unzip</code>.</p> </li> </ul>"},{"location":"install_and_deploy/#install","title":"Install","text":"<p>Fractal Server is hosted on PyPI, and can be installed with <code>pip</code>: <pre><code>pip install fractal-server\n</code></pre></p> <p>Fractal Server is also available as a Conda package, but the PyPI version is the recommended one.</p> <p>For details on how to install Fractal Server in a development environment see the Development page.</p>"},{"location":"install_and_deploy/#how-to-deploy","title":"How to deploy","text":"<p>Installing <code>fractal-server</code> will automatically install <code>fractalctl</code>, its companion command-line utility that provides the basic commands for deploying Fractal Server.</p>"},{"location":"install_and_deploy/#set-up-database-schemas","title":"Set up database schemas","text":"<p>Use the command <pre><code>fractalctl set-db\n</code></pre> to apply the schema migrations to the database. This command uses the configuration variables described in DatabaseSettings - notably including the database name <code>POSTGRES_DB</code>.</p> <p>Note: the corresponding PostgreSQL database must already exist, since it won't be created by <code>fractalctl set-db</code>. You can often create it directly through <code>createdb</code>.</p>"},{"location":"install_and_deploy/#initialize-database-data","title":"Initialize database data","text":"<p>With the command <pre><code>fractalctl init-db-data\n</code></pre> you can initialize several relevant database tables. Its behaviors depends on the environment variables and command-line arguments (see the <code>fractalctl init-db-data</code> documentation), and it can optionally</p> <ul> <li>create the default user group (if <code>FRACTAL_DEFAULT_GROUP_NAME=All</code>);</li> <li>create the first admin user, by providing the <code>--admin-*</code> flags;</li> <li>create the first resource/profile pair and associate users to them, providing <code>--resource</code> and <code>--profile</code>.</li> </ul>"},{"location":"install_and_deploy/#start-fractal-server","title":"Start <code>fractal-server</code>","text":"<p>Use the command <pre><code>fractalctl start\n</code></pre> to start the server using Uvicorn.</p> <p>To verify that the server is up, you can use the <code>/api/alive/</code> endpoint - as in <pre><code>curl http://localhost:8000/api/alive/\n{\"alive\":true,\"version\":\"2.17.0\"}\n</code></pre></p>"},{"location":"install_and_deploy/#upgrade-fractal-server","title":"Upgrade <code>fractal-server</code>","text":"<p>The high-level procedure for upgrading <code>fractal-server</code> on an existing instance is as follows:</p> <ul> <li>Stop the running <code>fractal-server</code> process.</li> <li>Create a backup dump of the current database data (see database page).</li> <li>Review the CHANGELOG, and check whether this version upgrade requires any special procedure.</li> <li>Upgrade <code>fractal-server</code> (e.g. as in <code>pip install fractal-server==1.2.3</code>).</li> <li>Update the database schemas (as in <code>fractalctl set-db</code>).</li> <li>If the CHANGELOG requires it, run the data-migration command (<code>fractalctl update-db-data</code>). Depending on the specific upgrade, this may require additional actions or information.</li> <li>Restart the <code>fractal-server</code> process.</li> </ul>"},{"location":"openapi/","title":"Web API","text":""},{"location":"assets/resource_and_profile/snippet/","title":"Snippet","text":""},{"location":"assets/resource_and_profile/snippet/#resource-examples","title":"Resource examples","text":"LocalSLURM/sudoSLURM/SSH <pre><code>{\n    \"type\": \"local\",\n    \"name\": \"Local resource\",\n    \"jobs_local_dir\": \"/somewhere/jobs\",\n    \"jobs_runner_config\": {\n        \"parallel_tasks_per_job\": 1\n    },\n    \"jobs_poll_interval\": 0,\n    \"tasks_local_dir\": \"/somewhere/tasks\",\n    \"tasks_python_config\": {\n        \"default_version\": \"3.12\",\n        \"versions\": {\n            \"3.12\": \"/some-venv/bin/python\"\n        }\n    },\n    \"tasks_pixi_config\": {},\n    \"tasks_pip_cache_dir\": null\n}\n</code></pre> <pre><code>{\n    \"type\": \"slurm_sudo\",\n    \"name\": \"SLURM cluster\",\n    \"jobs_local_dir\": \"/somewhere/local-jobs\",\n    \"jobs_runner_config\": {\n        \"default_slurm_config\": {\n            \"partition\": \"partition-name\",\n            \"cpus_per_task\": 1,\n            \"mem\": \"100M\"\n        },\n        \"gpu_slurm_config\": {\n            \"partition\": \"gpu\",\n            \"extra_lines\": [\n                \"#SBATCH --gres=gpu:v100:1\"\n            ]\n        },\n        \"user_local_exports\": {\n            \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n            \"NAPARI_CONFIG\": \"napari_config.json\"\n        },\n        \"batching_config\": {\n            \"target_cpus_per_job\": 1,\n            \"max_cpus_per_job\": 1,\n            \"target_mem_per_job\": 200,\n            \"max_mem_per_job\": 500,\n            \"target_num_jobs\": 2,\n            \"max_num_jobs\": 4\n        }\n    },\n    \"jobs_slurm_python_worker\": \"/some/venv/bin/python3.12\",\n    \"jobs_poll_interval\": 10,\n    \"tasks_local_dir\": \"/somewhere/local-tasks\",\n    \"tasks_python_config\": {\n        \"default_version\": \"3.12\",\n        \"versions\": {\n            \"3.11\": \"/some/venv/bin/python3.11\",\n            \"3.12\": \"/some/venv/bin/python3.12\"\n        }\n    },\n    \"tasks_pixi_config\": {},\n    \"tasks_pip_cache_dir\": null\n}\n</code></pre> <pre><code>{\n    \"type\": \"slurm_ssh\",\n    \"name\": \"Remote SLURM cluster\",\n    \"host\": \"slurm-cluster.example.org\",\n    \"jobs_local_dir\": \"/somewhere/local-jobs\",\n    \"jobs_runner_config\": {\n        \"default_slurm_config\": {\n            \"partition\": \"partition-name\",\n            \"cpus_per_task\": 1,\n            \"mem\": \"100M\"\n        },\n        \"gpu_slurm_config\": {\n            \"partition\": \"gpu\",\n            \"extra_lines\": [\n                \"#SBATCH --gres=gpu:v100:1\"\n            ]\n        },\n        \"user_local_exports\": {\n            \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n            \"NAPARI_CONFIG\": \"napari_config.json\"\n        },\n        \"batching_config\": {\n            \"target_cpus_per_job\": 1,\n            \"max_cpus_per_job\": 1,\n            \"target_mem_per_job\": 200,\n            \"max_mem_per_job\": 500,\n            \"target_num_jobs\": 2,\n            \"max_num_jobs\": 4\n        }\n    },\n    \"jobs_slurm_python_worker\": \"/some/venv/bin/python3.12\",\n    \"jobs_poll_interval\": 10,\n    \"tasks_local_dir\": \"/somewhere/local-tasks\",\n    \"tasks_python_config\": {\n        \"default_version\": \"3.12\",\n        \"versions\": {\n            \"3.11\": \"/some/venv/bin/python3.11\",\n            \"3.12\": \"/some/venv/bin/python3.12\"\n        }\n    },\n    \"tasks_pixi_config\": {},\n    \"tasks_pip_cache_dir\": null\n}\n</code></pre>"},{"location":"assets/resource_and_profile/snippet/#profile-examples","title":"Profile examples","text":"LocalSLURM/sudoSLURM/SSH <pre><code>{\n    \"name\": \"Local profile\",\n    \"resource_type\": \"local\"\n}\n</code></pre> <pre><code>{\n    \"name\": \"SLURM/sudo profile\",\n    \"resource_type\": \"slurm_sudo\",\n    \"username\": \"slurm-username\"\n}\n</code></pre> <pre><code>{\n    \"name\": \"SLURM/SSH profile\",\n    \"resource_type\": \"slurm_ssh\",\n    \"username\": \"slurm-username\",\n    \"ssh_key_path\": \"/somewhere/private.key\",\n    \"jobs_remote_dir\": \"/somewhere/jobs\",\n    \"tasks_remote_dir\": \"/somewhere/tasks\"\n}\n</code></pre>"},{"location":"internals/","title":"Fractal Server internal components","text":"<p>This section describes some internal <code>fractal-server</code> components.</p>"},{"location":"internals/database/","title":"Database Interface","text":"<p>Fractal Server only allows PostgreSQL to be used as database; the database-related configuration variables are described below (and in the configuration page).</p> <p>To use PostgreSQL as a database, <code>fractal-server</code> depends on <code>sqlalchemy</code> and <code>psycopg[binary]</code>. As of <code>fractal-server</code> 2.17.0, several versions of PostgreSQL from v10 to v18 have been tested successfully at least once.</p>"},{"location":"internals/database/#setup","title":"Setup","text":"<p>We assume that a PostgreSQL service is active, with some host (this can be e.g. <code>localhost</code> or a UNIX socket like <code>/var/run/postgresql/</code>), a port (we use the default 5432 in the examples below) and a user (e.g. <code>postgres</code> or <code>fractal</code>).</p> <p>\u26a0\ufe0f Notes:</p> <ol> <li>The postgres user must be created from outside <code>fractal-server</code>.</li> <li>A given machine user may or may not require a password (e.g. depending on    whether the machine username matches with the PostgreSQL username, and on    whether connection happens via a UNIX socket). See documentation here:    https://www.postgresql.org/docs/current/auth-pg-hba-conf.html.</li> </ol> <p>Here we create a database called <code>fractal_db</code>, through the <code>createdb</code> command:</p> <pre><code>$ createdb \\\n    --host=localhost \\\n    --port=5432 \\\n    --username=postgres \\\n    --no-password \\\n    --owner=fractal \\\n    fractal_db\n</code></pre> <p>All options of this command (and of the ones below) should be aligned with the configuration of a specific PostgreSQL instance. Within <code>fractal-server</code>, this is done by setting the following configuration variables (before running <code>fractalctl</code> commands):</p> <ul> <li> <p>Required:</p> <pre><code>POSTGRES_DB=fractal_db\n</code></pre> </li> <li> <p>Optional:</p> <pre><code>POSTGRES_HOST=localhost             # default: localhost\nPOSTGRES_PORT=5432                  # default: 5432\nPOSTGRES_USER=fractal               # default: None\nPOSTGRES_PASSWORD=secret            # default: None\n</code></pre> </li> </ul> <p>Note that <code>POSTGRES_HOST</code> can be either a URL or the path to a UNIX domain socket (e.g. <code>/var/run/postgresql</code>).</p>"},{"location":"internals/database/#backup-and-restore","title":"Backup and restore","text":"<p>To backup and restore data, one can use the utilities <code>pg_dump</code> and <code>psql</code>.</p> <p>It is possible to dump/restore data in various formats (see documentation of <code>pg_dump</code>), but in this example we stick with the default plain-text format.</p> <pre><code>$ pg_dump \\\n    --host=localhost \\\n    --port=5432\\\n    --username=fractal \\\n    --format=plain \\\n    --file=fractal_dump.sql \\\n    fractal_db\n</code></pre> <p>In order to restore a database from a dump, we first create a new empty one (<code>new_fractal_db</code>): <pre><code>$ createdb \\\n    --host=localhost \\\n    --port=5432\\\n    --username=postgres \\\n    --no-password \\\n    --owner=fractal \\\n    new_fractal_db\n</code></pre> and then we populate it using the dumped data:</p> <pre><code>$ psql \\\n    --host=localhost \\\n    --port=5432\\\n    --username=fractal \\\n    --dbname=new_fractal_db &lt; fractal_dump.sql\n</code></pre> <p>One of the multiple ways to compress data is to use <code>gzip</code>, by adapting the commands above as in: <pre><code>$ pg_dump ... | gzip -c fractal_dump.sql.gz\n$ gzip --decompress --keep fractal_dump.sql.gz\n$ createdb ...\n$ psql ... &lt; fractal_dump.sql\n</code></pre></p>"},{"location":"internals/logging/","title":"Logging","text":"<p>Logging in <code>fractal-server</code> is based on the standard <code>logging</code> library, and its logging levels are defined here. For a more detailed view on <code>fractal-server</code> logging, see the logger module documentation.</p> <p>The logger module exposes the functions to set/get/close a logger, and it defines where the records are sent to (e.g. the <code>fractal-server</code> console or a specific file). The logging levels of a logger created with <code>set_logger</code> are defined as follows:</p> <ul> <li>The minimum logging level for logs to appear in the console is set by   <code>FRACTAL_LOGGING_LEVEL</code>;</li> <li>The <code>FileHandler</code> logger handlers are always set at the <code>DEBUG</code> level, that   is, they write all log records.</li> </ul> <p>This means that the <code>FRACTAL_LOGGING_LEVEL</code> offers a quick way to switch to very verbose console logging (setting it e.g. to <code>10</code>, that is, <code>DEBUG</code> level) and to switch back to less verbose logging (e.g. <code>FRACTAL_LOGGING_LEVEL=20</code> or <code>30</code>), without ever modifying the on-file logs. Note that the typical reason for having on-file logs in <code>fractal-server</code> is to log information about background tasks, that are not executed as part of an API endpoint.</p>"},{"location":"internals/logging/#example-use-cases","title":"Example use cases","text":"<ol> <li> <p>Module-level logs that should only appear in the <code>fractal-server</code> console <pre><code>from fractal_server.logger import set_logger\n\nmodule_logger = set_logger(__name__)\n\ndef my_function():\n    module_logger.debug(\"This is an DEBUG log, from my_function\")\n    module_logger.info(\"This is an INFO log, from my_function\")\n    module_logger.warning(\"This is a WARNING log, from my_function\")\n</code></pre> Note that only logs with level equal or higher to <code>FRACTAL_LOGGING_LEVEL</code> will be shown.</p> </li> <li> <p>Function-level logs that should only appear in the <code>fractal-server</code> console <pre><code>from fractal_server.logger import set_logger\n\ndef my_function():\n    function_logger = set_logger(\"my_function\")\n    function_logger.debug(\"This is an DEBUG log, from my_function\")\n    function_logger.info(\"This is an INFO log, from my_function\")\n    function_logger.warning(\"This is a WARNING log, from my_function\")\n</code></pre> Note that only logs with level equal or higher to <code>FRACTAL_LOGGING_LEVEL</code> will be shown.</p> </li> <li> <p>Custom logs that should appear both in the fractal-server console and in a    log file <pre><code>from fractal_server.logger import set_logger\nfrom fractal_server.logger import close_logger\n\ndef my_function():\n    this_logger = set_logger(\"this_logger\", log_file_path=\"/tmp/this.log\")\n    this_logger.debug(\"This is an DEBUG log, from my_function\")\n    this_logger.info(\"This is an INFO log, from my_function\")\n    this_logger.warning(\"This is a WARNING log, from my_function\")\n    close_logger(this_logger)\n</code></pre> Note that only logs with level equal or higher to <code>FRACTAL_LOGGING_LEVEL</code> will be shown in the console, but all logs will be written to <code>\"/tmp/this.log\"</code>.</p> </li> </ol>"},{"location":"internals/users/","title":"Users","text":"<p>Fractal Server's user model and authentication/authorization systems are powered by the FastAPI Users library, and most of the components described below can be identified in the corresponding overview.</p> <p></p>"},{"location":"internals/users/#fractal_server.app.models.security.UserOAuth","title":"<code>UserOAuth</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for the <code>user_oauth</code> database table.</p> <p>This class is a modification of <code>SQLModelBaseUserDB</code> from <code>fastapi_users_db_sqlmodel</code>. Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence.</p> <p>Note that several class attributes are the default ones from <code>fastapi-users</code> .</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int | None</code> </p> <code>email</code> <p> TYPE: <code>EmailStr</code> </p> <code>hashed_password</code> <p> TYPE: <code>str</code> </p> <code>is_active</code> <p>If this is <code>False</code>, the user has no access to the <code>/api/v2/</code> endpoints.</p> <p> TYPE: <code>bool</code> </p> <code>is_superuser</code> <p> TYPE: <code>bool</code> </p> <code>is_verified</code> <p>If this is <code>False</code>, the user has no access to the <code>/api/v2/</code> endpoints.</p> <p> TYPE: <code>bool</code> </p> <code>oauth_accounts</code> <p> TYPE: <code>list[OAuthAccount]</code> </p> <code>profile_id</code> <p>Foreign key linking the user to a <code>Profile</code>. If this is unset, the user has no access to the <code>/api/v2/</code> endpoints.</p> <p> TYPE: <code>int | None</code> </p> <code>project_dirs</code> <p>Absolute paths of the user's project directory. This is used (A) as a default base folder for the <code>zarr_dir</code> of new datasets (where the output Zarr are located), and (B) as a folder which is included by default in the paths that a user is allowed to stream (if the <code>fractal-data</code> integration is set up). two goals:</p> <p> TYPE: <code>list[str]</code> </p> <code>slurm_accounts</code> <p>List of SLURM accounts that the user can select upon running a job.</p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class UserOAuth(SQLModel, table=True):\n    \"\"\"\n    ORM model for the `user_oauth` database table.\n\n    This class is a modification of\n    [`SQLModelBaseUserDB`](https://github.com/fastapi-users/fastapi-users-db-sqlmodel/blob/83980d7f20886120f4636a102ab1822b4c366f63/fastapi_users_db_sqlmodel/__init__.py#L15-L32)\n    from `fastapi_users_db_sqlmodel`.\n    Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence.\n\n    Note that several class attributes are\n    [the default ones from `fastapi-users`\n    ](https://fastapi-users.github.io/fastapi-users/latest/configuration/schemas/).\n\n    Attributes:\n        id:\n        email:\n        hashed_password:\n        is_active:\n            If this is `False`, the user has no access to the `/api/v2/`\n            endpoints.\n        is_superuser:\n        is_verified:\n            If this is `False`, the user has no access to the `/api/v2/`\n            endpoints.\n        oauth_accounts:\n        profile_id:\n            Foreign key linking the user to a `Profile`. If this is unset,\n            the user has no access to the `/api/v2/` endpoints.\n        project_dirs:\n            Absolute paths of the user's project directory. This is used (A) as\n            a default base folder for the `zarr_dir` of new datasets (where\n            the output Zarr are located), and (B) as a folder which is included\n            by default in the paths that a user is allowed to stream (if the\n            `fractal-data` integration is set up).\n            two goals:\n        slurm_accounts:\n            List of SLURM accounts that the user can select upon running a job.\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n\n    __tablename__ = \"user_oauth\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    email: EmailStr = Field(\n        sa_column_kwargs={\"unique\": True, \"index\": True},\n        nullable=False,\n    )\n    hashed_password: str\n    is_active: bool = Field(default=True, nullable=False)\n    is_superuser: bool = Field(default=False, nullable=False)\n    is_verified: bool = Field(default=False, nullable=False)\n    is_guest: bool = Field(\n        sa_column=Column(\n            BOOLEAN,\n            server_default=\"false\",\n            nullable=False,\n        ),\n    )\n\n    oauth_accounts: list[\"OAuthAccount\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"lazy\": \"joined\", \"cascade\": \"all, delete\"},\n    )\n\n    profile_id: int | None = Field(\n        foreign_key=\"profile.id\",\n        default=None,\n        ondelete=\"RESTRICT\",\n    )\n\n    project_dirs: list[str] = Field(\n        sa_column=Column(ARRAY(String), nullable=False),\n    )\n\n    slurm_accounts: list[str] = Field(\n        sa_column=Column(ARRAY(String), server_default=\"{}\"),\n    )\n\n    __table_args__ = (\n        CheckConstraint(\n            \"NOT (is_superuser AND is_guest)\",\n            name=\"superuser_is_not_guest\",\n        ),\n    )\n</code></pre>"},{"location":"internals/users/#first-user","title":"First user","text":"<p>To manage <code>fractal-server</code> you need to create a first user with superuser privileges. This is done by means of the <code>init-db-data</code> command together with the<code>--admin-email</code>/<code>--admin-pwd</code>/<code>--admin-project-dir</code> flags, either during the startup phase or at a later stage.</p> <p>The most common use cases for <code>fractal-server</code> are:</p> <ol> <li>The server is used by a single user (e.g. on their own machine). In this case you may simply use the first (and only) user.</li> <li>The server has multiple users, and it is connected to one or more SLURM clusters. To execute jobs on a SLURM cluster, a user must be associated to that cluster and to a valid cluster-user via its [<code>Profile</code>] (more details here).</li> </ol>"},{"location":"internals/users/#authentication","title":"Authentication","text":""},{"location":"internals/users/#login","title":"Login","text":"<p>An authentication backend is composed of two parts:</p> <ul> <li>the transport, that manages how the token will be carried over the request,</li> <li>the strategy, which manages how the token is generated and secured.</li> </ul> <p>Fractal Server provides two authentication backends (Bearer and Cookie), both based the JWT strategy. Each backend produces both <code>/auth/login</code> and <code>/auth/logout</code> routes.</p> <p>FastAPI Users provides the <code>logout</code> endpoint by default, but this is not relevant in <code>fractal-server</code> since we do not store tokens in the database.</p>"},{"location":"internals/users/#bearer","title":"Bearer","text":"<p>The Bearer transport backend provides login at <code>/auth/token/login</code> <pre><code>$ curl \\\n    -X POST \\\n    -H \"Content-Type: application/x-www-form-urlencoded\" \\\n    -d \"username=admin@example.org&amp;password=1234\" \\\n    http://127.0.0.1:8000/auth/token/login/\n\n{\n    \"access_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxIiwiYXVkIjpbImZhc3RhcGktdXNlcnM6YXV0aCJdLCJleHAiOjE3NjI4NzI5MzB9.nLwZHeZCRWSUo5TzaQlho8uMBAf1Fl4XqXSA32lSPJs\",\n    \"token_type\":\"bearer\"\n}\n</code></pre></p>"},{"location":"internals/users/#cookie","title":"Cookie","text":"<p>The Cookie transport backend provides login at <code>/auth/login</code></p> <pre><code>$ curl \\\n    -X POST \\\n    -H \"Content-Type: application/x-www-form-urlencoded\" \\\n    -d \"username=admin@example.org&amp;password=1234\" \\\n    --cookie-jar - \\\n    http://127.0.0.1:8000/auth/login/\n\n\n# Netscape HTTP Cookie File\n# https://curl.se/docs/http-cookies.html\n# This file was generated by libcurl! Edit at your own risk.\n\n#HttpOnly_127.0.0.1 FALSE   /   TRUE    0   fastapiusersauth    eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxIiwiYXVkIjpbImZhc3RhcGktdXNlcnM6YXV0aCJdLCJleHAiOjE3NjI5NTg0MDl9.NeB5tie2Mey5w7NkxMhqpablOGBjiKPLncwQT8d5HF4\n</code></pre>"},{"location":"internals/users/#authenticated-calls","title":"Authenticated calls","text":"<p>Once you have the token, you can use it to identify yourself by sending it along in the header of an API request. Here is an example with an API request to <code>/auth/current-user/</code>: <pre><code>$ curl \\\n    -X GET \\\n    -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxIiwiYXVkIjpbImZhc3RhcGktdXNlcnM6YXV0aCJdLCJleHAiOjE3NjI5NTg0MDl9.NeB5tie2Mey5w7NkxMhqpablOGBjiKPLncwQT8d5HF4\" \\\n    http://127.0.0.1:8000/auth/current-user/\n\n{\n    \"id\": 1,\n    \"email\": \"admin@example.org\",\n    \"is_active\": true,\n    \"is_superuser\": true,\n    \"is_verified\": true,\n    \"group_ids_names\": null,\n    \"oauth_accounts\": [],\n    \"profile_id\": null,\n    \"project_dir\": \"/tmp/fractal\",\n    \"slurm_accounts\": []\n}\n</code></pre></p>"},{"location":"internals/users/#oauth2","title":"OAuth2","text":"<p>Fractal Server also allows a different authentication procedure, not based on the knowledge of a user's password but on external <code>OAuth2</code> authentication clients.</p> <p>Through the <code>httpx-oauth</code> library, we currently support <code>OpenID Connect</code> (aka <code>OIDC</code>), <code>GitHub</code> and <code>Google</code> (and more clients can be readily included).</p>"},{"location":"internals/users/#configuration","title":"Configuration","text":"<p>To use a certain <code>OAuth2</code> client, you must first register the <code>fractal-server</code> application (see instructions for GitHub and Google).</p> <p>During app registration, you should provide two endpoints:</p> <ul> <li>the <code>Homepage URL</code> (e.g. <code>http://127.0.0.1:8000/</code>),</li> <li>the <code>Authorization callback URL</code> (e.g. <code>http://127.0.0.1:8000/auth/github/callback/</code>, where <code>github</code> could be any client name).</li> </ul> <p>and at the end of this procedure, you will kwnow the Client ID and Client Secret for the app.</p> <p>Note: You have to enable the \"Email addresses\" permission for your GitHub registered app, at https://github.com/settings/apps/{registered-app}/permissions. A similar setting may be required for Google.</p> <p>To add an <code>OAuth2</code> client, you must provide valid <code>OAuthSettings</code> variables:</p> OIDCGitHubGoogle <pre><code>OAUTH_CLIENT_NAME=any-name-except-github-or-google\nOAUTH_CLIENT_ID=...\nOAUTH_CLIENT_SECRET=...\nOAUTH_OIDC_CONFIG_ENDPOINT=...  # e.g. https://example.org/.well-known/openid-configuration\nOAUTH_REDIRECT_URL=...          # e.g. https://fractal-web.example.org/auth/login/oauth2\n</code></pre> <pre><code>OAUTH_CLIENT_NAME=github\nOAUTH_CLIENT_ID=...\nOAUTH_CLIENT_SECRET=...\nOAUTH_REDIRECT_URL=...  # e.g. https://fractal-web.example.org/auth/login/oauth2\n</code></pre> <pre><code>OAUTH_CLIENT_NAME=google\nOAUTH_CLIENT_ID=...\nOAUTH_CLIENT_SECRET=...\nOAUTH_REDIRECT_URL=...  # e.g. https://fractal-web.example.org/auth/login/oauth2\n</code></pre> <p>When <code>fractal-server</code> starts with proper <code>OAuthSettings</code>, two new routes will be generated:</p> <ul> <li><code>/auth/{OAUTH_CLIENT_NAME}/authorize/</code> ,</li> <li><code>/auth/{OAUTH_CLIENT_NAME}/callback/</code> (the <code>Authorization callback URL</code> of the client).</li> </ul> <p>Note that the <code>OAUTH_REDIRECT_URL</code> environment variable is optional. It is not relevant for the examples described in this page, since they are all in the command-line interface. However, it is required when OAuth authentication is performed starting from a browser (e.g. through the <code>fractal-web</code> client), since the callback URL should be opened in the browser itself.</p>"},{"location":"internals/users/#authorization-code-flow","title":"Authorization Code Flow","text":"<p>Authentication via OAuth2 client is based on the Authorization Code Flow, as described in this diagram</p> <p> </p> <p>(adapted from https://auth0.com/docs/get-started/authentication-and-authorization-flow/authorization-code-flow, \u00a9 2023 Okta, Inc.)   </p> <p>We can now review how <code>fractal-server</code> handles these steps:</p> <ul> <li> <p>Steps 1 \u2192 4</p> <ul> <li>The starting point is <code>/auth/client-name/authorize/</code>;</li> <li>Here an <code>authorization_url</code> is generated and provided to the user;</li> <li>This URL will redirect the user to the Authorization Server (which is e.g. GitHub or Google, and not related to <code>fractal-server</code>), together with a <code>state</code> code for increased security;</li> <li>The user must authenticate and grant <code>fractal-server</code> the permissions it requires.</li> </ul> </li> <li> <p>Steps 5 \u2192 8</p> <ul> <li>The flow comes back to <code>fractal-server</code> at <code>/auth/client-name/callback/</code>, together with the Authorization Code.</li> <li>A FastAPI dependency of the callback endpoint, <code>oauth2_authorize_callback</code>, takes care of exchanging this code for the Access Token.</li> </ul> </li> <li> <p>Steps 9 \u2192 10</p> <ul> <li>The callback endpoint uses the Access Token to obtain the user's email address and an account identifier from the Resource Server (which, depending on the client, may coincide with the Authorization Server).</li> </ul> </li> </ul> <p>After that, the callback endpoint performs some extra operations, which are not strictly part of the <code>OAuth2</code> protocol:</p> <ul> <li>It checks that <code>state</code> is still valid;</li> <li>If the user has never authenticated with <code>OAuth2</code> before:<ul> <li>it adds to the database a new entry to the <code>oauthaccount</code> table, properly linked to the <code>user_oauth</code> table; at subsequent logins that entry will just be updated.</li> <li>it sends a notification of the login to the addresses indicated in <code>FRACTAL_EMAIL_RECIPIENTS</code>.</li> </ul> </li> <li>It prepares a JWT token for the user and serves it in the Response Cookie.</li> </ul>"},{"location":"internals/users/#full-example","title":"Full example","text":"<p>A given <code>fractal-server</code> instance is registered as a GitHub App, and <code>fractal-server</code> is configured accordingly: <pre><code>OAUTH_CLIENT_NAME=github\nOAUTH_CLIENT_ID=...\nOAUTH_CLIENT_SECRET=...\n</code></pre></p> <p>There is a single user, created with the command <pre><code>fractalctl init-db-data \\\n    --admin-email person@university.edu \\\n    --admin-pwd 1234 \\\n    --admin-project-dir=/tmp/fractal\n</code></pre></p> <p>Now the user wants to log in using her GitHub account associated to the same email.</p> <p>First, she makes a call to <code>/auth/github/authorize/</code>: <pre><code>$ curl \\\n    -X GET \\\n    http://127.0.0.1:8000/auth/github/authorize/\n\n{\n    \"authorization_url\":\"https://github.com/login/oauth/authorize/?\n        response_type=code&amp;\n        client_id=...&amp;\n        redirect_uri=...&amp;\n        state=...&amp;\n        scope=user+user%3Aemail\"\n}\n</code></pre></p> <p>Now the <code>authorization_url</code> must be visited using a browser. After logging in to GitHub, she is asked to grant the app the permissions it requires.</p> <p>After that, she is redirected back to <code>fractal-server</code> at <code>/auth/github/callback/</code>, together with two query parameters: <pre><code>http://127.0.0.1:8000/auth/github/callback/?\n    code=...&amp;\n    state=...\n</code></pre></p> <p>The callback function does not return anything, but the response cookie contains a JWT token <pre><code>\"fastapiusersauth\": {\n    \"httpOnly\": true,\n    \"path\": \"/\",\n    \"samesite\": \"None\",\n    \"secure\": true,\n    \"value\": \"ey...\"     &lt;----- This is the JWT token\n}\n</code></pre></p> <p>The response cookie can be found using the developer tools of the browser, inspecting the response on the network page.</p> <p>The user can now make authenticated calls using this token, as in <pre><code>curl \\\n    -X GET \\\n    -H \"Authorization: Bearer ey...\" \\\n    http://127.0.0.1:8000/auth/current-user/\n\n{\n  \"id\": 1,\n  \"email\": \"person@university.edu\",\n  \"is_active\": true,\n  \"is_superuser\": true,\n  \"is_verified\": true,\n  \"group_ids_names\": null,\n  \"oauth_accounts\": [\n    {\n      \"id\": 1,\n      \"account_email\": \"person@university.edu\",\n      \"oauth_name\": \"github\"\n    }\n  ],\n  \"profile_id\": null,\n  \"project_dir\": \"/tmp/fractal\",\n  \"slurm_accounts\": []\n}\n</code></pre></p>"},{"location":"internals/users/#authorization","title":"Authorization","text":"<p>On top of being authenticated, a user must be authorized in order to perform specific actions in <code>fratal-server</code>:</p> <ul> <li><code>/api/alive/</code> is public, and accessible without authentication.</li> <li><code>/auth/current-user/</code> endpoints (including <code>/auth/current-user/profile-info/</code>) require authentication and the user must be active.<ul> <li><code>GET /current-user/allowed-viewer-paths/</code> requires that the user is both active and verified.</li> </ul> </li> <li><code>/api/v2/</code> endpoints require authentication, and the user must be active and verified and they must have a non-null <code>profile_id</code>.</li> <li>Active superusers can access all <code>/admin/</code> and <code>/auth/</code> endpoints.</li> </ul>"},{"location":"internals/integrations/","title":"Computational integrations","text":"<p>On top of exposing a web API, a Fractal instance must be integrated to at least one computational resource which is used mainly for two goals:</p> <ul> <li>Setting up Python environments for task groups - see more details.</li> <li>Executing scientific tasks - see more details.</li> </ul>"},{"location":"internals/integrations/#supported-integrations","title":"Supported integrations","text":"<p>The configuration variable <code>FRACTAL_RUNNER_BACKEND</code> determines which one of the three following modality is in-place:</p> <ol> <li> <p>In a local instance, every computational operation (setting up task environments and executing tasks) is run by the machine user who is running <code>fractal-server</code>, and the instance typically only has a single user. Note: this integration is mostly supported for testing and development.</p> </li> <li> <p>A SLURM/sudo instance requires access to a SLURM cluster, with some additional assumptions - notably:</p> <ul> <li>The user that runs <code>fractal-server</code> also needs sufficient permissions to impersonate other users for running jobs (e.g. via <code>sudo -u some-user sbatch /some/submission-script.sh</code>);</li> <li>There must be a shared filesystem which both the user running <code>fractal-server</code> and other users have access to.</li> </ul> </li> <li> <p>A SLURM/SSH instance requires access to a SLURM cluster through SSH, by impersonating one or several service users.</p> </li> </ol> <p>The specific configuration for each computational resource is defined in the <code>Resource</code> database table, with the following creation schemas:</p> <ul> <li>Local resource</li> <li>SLURM/sudo resource</li> <li>SLURM/SSH resource</li> </ul> <p>For each resource, there may be one or many computational profiles, with the following creation schemas:</p> <ul> <li>Local profile</li> <li>SLURM/sudo profile</li> <li>SLURM/SSH profile</li> </ul> <p>Here are some minimal examples of how to configure resources and profiles in the three different cases:</p>"},{"location":"internals/integrations/#resource-examples","title":"Resource examples","text":"LocalSLURM/sudoSLURM/SSH <pre><code>{\n    \"type\": \"local\",\n    \"name\": \"Local resource\",\n    \"jobs_local_dir\": \"/somewhere/jobs\",\n    \"jobs_runner_config\": {\n        \"parallel_tasks_per_job\": 1\n    },\n    \"jobs_poll_interval\": 0,\n    \"tasks_local_dir\": \"/somewhere/tasks\",\n    \"tasks_python_config\": {\n        \"default_version\": \"3.12\",\n        \"versions\": {\n            \"3.12\": \"/some-venv/bin/python\"\n        }\n    },\n    \"tasks_pixi_config\": {},\n    \"tasks_pip_cache_dir\": null\n}\n</code></pre> <pre><code>{\n    \"type\": \"slurm_sudo\",\n    \"name\": \"SLURM cluster\",\n    \"jobs_local_dir\": \"/somewhere/local-jobs\",\n    \"jobs_runner_config\": {\n        \"default_slurm_config\": {\n            \"partition\": \"partition-name\",\n            \"cpus_per_task\": 1,\n            \"mem\": \"100M\"\n        },\n        \"gpu_slurm_config\": {\n            \"partition\": \"gpu\",\n            \"extra_lines\": [\n                \"#SBATCH --gres=gpu:v100:1\"\n            ]\n        },\n        \"user_local_exports\": {\n            \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n            \"NAPARI_CONFIG\": \"napari_config.json\"\n        },\n        \"batching_config\": {\n            \"target_cpus_per_job\": 1,\n            \"max_cpus_per_job\": 1,\n            \"target_mem_per_job\": 200,\n            \"max_mem_per_job\": 500,\n            \"target_num_jobs\": 2,\n            \"max_num_jobs\": 4\n        }\n    },\n    \"jobs_slurm_python_worker\": \"/some/venv/bin/python3.12\",\n    \"jobs_poll_interval\": 10,\n    \"tasks_local_dir\": \"/somewhere/local-tasks\",\n    \"tasks_python_config\": {\n        \"default_version\": \"3.12\",\n        \"versions\": {\n            \"3.11\": \"/some/venv/bin/python3.11\",\n            \"3.12\": \"/some/venv/bin/python3.12\"\n        }\n    },\n    \"tasks_pixi_config\": {},\n    \"tasks_pip_cache_dir\": null\n}\n</code></pre> <pre><code>{\n    \"type\": \"slurm_ssh\",\n    \"name\": \"Remote SLURM cluster\",\n    \"host\": \"slurm-cluster.example.org\",\n    \"jobs_local_dir\": \"/somewhere/local-jobs\",\n    \"jobs_runner_config\": {\n        \"default_slurm_config\": {\n            \"partition\": \"partition-name\",\n            \"cpus_per_task\": 1,\n            \"mem\": \"100M\"\n        },\n        \"gpu_slurm_config\": {\n            \"partition\": \"gpu\",\n            \"extra_lines\": [\n                \"#SBATCH --gres=gpu:v100:1\"\n            ]\n        },\n        \"user_local_exports\": {\n            \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n            \"NAPARI_CONFIG\": \"napari_config.json\"\n        },\n        \"batching_config\": {\n            \"target_cpus_per_job\": 1,\n            \"max_cpus_per_job\": 1,\n            \"target_mem_per_job\": 200,\n            \"max_mem_per_job\": 500,\n            \"target_num_jobs\": 2,\n            \"max_num_jobs\": 4\n        }\n    },\n    \"jobs_slurm_python_worker\": \"/some/venv/bin/python3.12\",\n    \"jobs_poll_interval\": 10,\n    \"tasks_local_dir\": \"/somewhere/local-tasks\",\n    \"tasks_python_config\": {\n        \"default_version\": \"3.12\",\n        \"versions\": {\n            \"3.11\": \"/some/venv/bin/python3.11\",\n            \"3.12\": \"/some/venv/bin/python3.12\"\n        }\n    },\n    \"tasks_pixi_config\": {},\n    \"tasks_pip_cache_dir\": null\n}\n</code></pre>"},{"location":"internals/integrations/#profile-examples","title":"Profile examples","text":"LocalSLURM/sudoSLURM/SSH <pre><code>{\n    \"name\": \"Local profile\",\n    \"resource_type\": \"local\"\n}\n</code></pre> <pre><code>{\n    \"name\": \"SLURM/sudo profile\",\n    \"resource_type\": \"slurm_sudo\",\n    \"username\": \"slurm-username\"\n}\n</code></pre> <pre><code>{\n    \"name\": \"SLURM/SSH profile\",\n    \"resource_type\": \"slurm_ssh\",\n    \"username\": \"slurm-username\",\n    \"ssh_key_path\": \"/somewhere/private.key\",\n    \"jobs_remote_dir\": \"/somewhere/jobs\",\n    \"tasks_remote_dir\": \"/somewhere/tasks\"\n}\n</code></pre>"},{"location":"internals/integrations/_advanced_slurm_config/","title":"SLURM runners","text":""},{"location":"internals/integrations/_advanced_slurm_config/#configuration","title":"Configuration","text":""},{"location":"internals/integrations/_advanced_slurm_config/#environment-variables","title":"Environment variables","text":"<p>The <code>fractal-server</code> admin may need to set some global variables that should be included in all SLURM submission scripts. This can be achieved via the <code>extra_lines</code> SLURM-runner configuration property, for instance as in <pre><code>{\n  \"default_slurm_config\": {\n    \"partition\": \"main\",\n    \"extra_lines\": [\n      \"export SOMEVARIABLE=123\",\n      \"export ANOTHERVARIABLE=ABC\"\n    ]\n  }\n}\n</code></pre></p> <p>There exists another use case where the value of a variable depends on the user who runs a certain task. A relevant example is that user A (who will run the task via SLURM) needs to define the cache-directory paths for some libraries they use (and those must be paths where user A can write). This use case is supported through the <code>user_local_exports</code> SLURM-runner configuration property. If this is set as in <pre><code>{\n  ...\n  \"user_local_exports\": {\n    \"LIBRARY_1_CACHE_DIR\": \"somewhere/library_1\",\n    \"LIBRARY_2_FILE\": \"somewhere/else/library_2.json\"\n  }\n}\n</code></pre> then the SLURM submission script will include the lines <pre><code>...\nexport LIBRARY_1_CACHE_DIR=/my/cache/somewhere/library_1\nexport LIBRARY_2_FILE=/my/cache/somewhere/else/library_2.json\n...\n</code></pre> Note that all paths in the values of <code>user_local_exports</code> are interpreted as relative to a base directory which is user-specific (e.g. the base cache directory for a user with <code>project_dir=\"/my/project_dir\"</code> is <code>/my/project_dir/.fractal_cache</code>). Also note that in this case <code>fractal-server</code> only compiles the configuration options into lines of the SLURM submission script, without performing any check on the validity of the given paths.</p>"},{"location":"internals/integrations/_advanced_slurm_config/#task-batching","title":"Task batching","text":"<p>The SLURM backend in <code>fractal-server</code> may combine multiple tasks in the same SLURM job (AKA batching), in order to reduce the total number of SLURM jobs that are submitted. This is especially relevant for SLURM clusters with constraints on the number of jobs that a user is allowed to submit over a certain timespan.</p> <p>The logic for handling the batching parameters (that is, how many tasks can be combined in the same SLURM job, and how many of them can run in parallel) is implemented in this configuration block and in this submodule.</p>"},{"location":"internals/integrations/_advanced_slurm_config/#user-impersonation","title":"User impersonation","text":""},{"location":"internals/integrations/_advanced_slurm_config/#sudo-based-impersonation","title":"<code>sudo</code>-based impersonation","text":"<p>The user who runs <code>fractal-server</code> must have sufficient privileges for running some commands via <code>sudo -u</code> to impersonate other users of the SLURM cluster without any password. The required commands include <code>sbatch</code>, <code>scancel</code>, <code>cat</code>, <code>ls</code> and <code>mkdir</code>. An example of how to achieve this is to add this block to the <code>sudoers</code> file: <pre><code>Runas_Alias FRACTAL_IMPERSONATE_USERS = fractal, user1, user2, user3\nCmnd_Alias FRACTAL_CMD = /usr/bin/sbatch, /usr/bin/scancel, /usr/bin/cat, /usr/bin/ls, /usr/bin/mkdir\nfractal ALL=(FRACTAL_IMPERSONATE_USERS) NOPASSWD:FRACTAL_CMD\n</code></pre> where <code>fractal</code> is the user running <code>fractal-server</code>, and <code>{user1,user2,user3}</code> are the users who can be impersonated. Note that one could also grant <code>fractal</code> the option of impersonating a whole UNIX group, instead of listing users one by one.</p> <p>The user's <code>Profile</code> includes the <code>username</code> to be impersonated.</p>"},{"location":"internals/integrations/_advanced_slurm_config/#ssh-based-impersonation","title":"SSH-based impersonation","text":"<p>In this scenario, one or many service users exist on the SLURM cluster and the <code>fractal-server</code> SLURM/SSH runner impersonates these service users when connecting to the cluster via SSH for handling jobs. Each Fractal user's <code>Profile</code> includes the <code>username</code> of the service user to be impersonated, and the path to the corresponding SSH private key.</p>"},{"location":"internals/integrations/_task_execution/","title":"Fractal task-execution","text":"<p>This page describes how <code>fractal-server</code> runs a sequence of Fractal tasks and processes the metadata they produce.</p> <p>NOTE: The process of defining a single full specification for this interface is still ongoing.</p> <p>The description below is based on concepts and definitions which are part of <code>fractal-server</code>. For the specific case of the Fractal image list, a more detailed description is available at https://fractal-analytics-platform.github.io/image_list. For clarifications about other terms or definitions, the starting point is the <code>execute_tasks</code> function in the <code>runner.py</code> Python module.</p> <p>Within <code>fractal-server</code>, a Fractal task is associated to a <code>TaskV2</code> object, which has either one or both non-parallel and parallel components (where \"both\" corresponds to compound tasks). The <code>command_non_parallel</code> and <code>command_parallel</code> attributes, when set, represent a command-line executables which are used to run the task. As an example, if <code>command_non_parallel = \"/path/to/python /path/to/my_task.py</code>, then the command that is executed will look like <pre><code>/path/to/python /path/to/my_task.py --args-json /path/to/args.json --out-json /path/to/out.json\n</code></pre> For Fractal tasks that are developed in Python, the <code>fractal-task-tools</code> exposes a helper tool to implement this command-line interface.</p> <p>The main entrypoint for task execution in <code>fractal-server</code> is the <code>execute_tasks</code> function, which executes a list of tasks (that is, part of a Fractal workflow). Its input arguments include:</p> <ul> <li>a Fractal dataset (which also contains an image list),</li> <li>a list of workflow tasks (each one associated to a <code>TaskV2</code> object),</li> <li>filters based on image types or attributes, set by the user upon job submission.</li> </ul> <p>In the following parts of this page we provide a high-level description of the <code>execute_tasks</code> flow. Some aspects which are not covered here are:</p> <ul> <li>Validation procedures and error handling.</li> <li>Fractal-job statuses and history tracking.</li> <li>Advanced status-based image filtering.</li> </ul>"},{"location":"internals/integrations/_task_execution/#initialization-phase","title":"Initialization phase","text":"<p>Before starting the execution of the tasks, <code>fractal-server</code> initializes some relevant variables.</p> <ul> <li>Variables that are extracted from the current dataset state:<ul> <li><code>zarr_dir</code></li> <li>The current image list</li> </ul> </li> <li>Variables that are extracted from user-provided job-submission parameters:<ul> <li>Image-type filters to apply to the image list.</li> </ul> </li> </ul> <p>After this preliminary phase the following three steps (pre-execution, execution, post-execution) are repeated for all tasks in the list.</p>"},{"location":"internals/integrations/_task_execution/#pre-task-execution-phase","title":"Pre-task-execution phase","text":"<p>If the task is a converter, it does not receive any OME-Zarr image as input. For non-converter tasks, however, <code>fractal-server</code> prepares a list of images that will be part of either <code>zarr_urls</code> (for non-parallel or compound tasks) or of the individual <code>zarr_url</code> arguments (for parallel tasks).</p> <p>The input image list is constructed by applying two sets of filters to the current dataset image list:</p> <ul> <li>Image-type filters obtained as a combination of current type filters, the task input types and the user-specified workflow-task type filters.</li> <li>Image-attribute filters specified by the user upon job submission.</li> </ul> <p>This procedure leads to a <code>filtered_images</code> list, with all OME-Zarr images that should be used as input for the task.</p>"},{"location":"internals/integrations/_task_execution/#task-execution","title":"Task execution","text":"<p>This part is covered by task-type specific code blocks like <pre><code>if task.type in [TaskType.NON_PARALLEL, TaskType.CONVERTER_NON_PARALLEL]:\n    outcomes_dict, num_tasks = run_task_non_parallel(\n        images=filtered_images,\n        zarr_dir=zarr_dir,\n        wftask=wftask,\n        task=task,\n        dataset_id=dataset.id,\n        task_type=task.type,\n        # ...\n    )\nelif task.type == TaskType.PARALLEL:\n    outcomes_dict, num_tasks = run_task_parallel(\n        # ...\n    )\nelif task.type in [TaskType.COMPOUND, TaskType.CONVERTER_COMPOUND]:\n    outcomes_dict, num_tasks = run_task_compound(\n        # ...\n    )\n</code></pre> where each value of <code>outcomes_dict</code> is a <code>SubmissionOutcome</code> object and may have a <code>task_output</code> attribute which is a <code>TaskOutput</code> object.</p> <p>The inner working of e.g. the <code>run_task_non_parallel</code> function is not described here, and it is implemented in a specific job runner.</p>"},{"location":"internals/integrations/_task_execution/#post-task-execution-phase","title":"Post-task-execution phase","text":"<ul> <li>Metadata outputs from all units are merged into a single <code>TaskOutput</code> object.</li> <li>If there are no images to be created or updated, all input images in <code>filtered_images</code> are flagged as \"to be updated\", so that they will be updated e.g. with the new types set by the task.</li> <li>For each image that should be created or updated, the image <code>attributes</code>, <code>types</code> and <code>origin</code> properties are updated as appropriate.</li> <li>All images marked as \"to be removed\" are removed from the image list.</li> <li>The current type filters are updated based on task output_types</li> <li>The existing dataset image list is replaced with the new one, in the database.</li> </ul>"},{"location":"internals/integrations/runners/","title":"Job runners","text":"<p>The runner is the <code>fractal-server</code> components that executes a job (based on a certain workflow and dataset) on a computational resource.</p>"},{"location":"internals/integrations/runners/#configuration","title":"Configuration","text":"<p>The runner configuration is defined in the <code>jobs_runner_config</code> property of a computational resource. The configuration schemas reported below apply to a <code>local</code> resource (see <code>JobRunnerConfigLocal</code>) and to a <code>slurm_sudo/slurm_ssh</code> resource (see <code>JobRunnerConfigSLURM</code>). Some more specific details of the SLURM configurations are described at  advanced SLURM configuration.</p>"},{"location":"internals/integrations/runners/#fractal_server.runner.config._local.JobRunnerConfigLocal","title":"<code>JobRunnerConfigLocal</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Runner-configuration specifications, for a <code>local</code> resource.</p> <p>The typical use case is that setting <code>parallel_tasks_per_job</code> to a small number (e.g. 1) will limit parallelism when executing tasks requiring a large amount of resources (e.g. memory) on a local machine.</p> ATTRIBUTE DESCRIPTION <code>parallel_tasks_per_job</code> <p>Maximum number of tasks to be run in parallel within a local runner. If <code>None</code>, then all tasks may start at the same time.</p> <p> TYPE: <code>int | None</code> </p> Source code in <code>fractal_server/runner/config/_local.py</code> <pre><code>class JobRunnerConfigLocal(BaseModel):\n    \"\"\"\n    Runner-configuration specifications, for a `local` resource.\n\n    The typical use case is that setting `parallel_tasks_per_job` to a\n    small number (e.g. 1) will limit parallelism when executing tasks\n    requiring a large amount of resources (e.g. memory) on a local machine.\n\n    Attributes:\n        parallel_tasks_per_job:\n            Maximum number of tasks to be run in parallel within a local\n            runner. If `None`, then all tasks may start at the same time.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    parallel_tasks_per_job: int | None = None\n\n    @property\n    def batch_size(self) -&gt; int:\n        return self.parallel_tasks_per_job or 0\n</code></pre>"},{"location":"internals/integrations/runners/#fractal_server.runner.config._slurm.JobRunnerConfigSLURM","title":"<code>JobRunnerConfigSLURM</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Runner-configuration specifications, for a <code>slurm_sudo</code> or <code>slurm_ssh</code> resource.</p> <p>Note: this is a common class, which is processed and transformed into more specific configuration objects during job execution.</p> <p>Valid JSON example <pre><code>{\n    \"default_slurm_config\": {\n        \"partition\": \"partition-name\",\n        \"cpus_per_task\": 1,\n        \"mem\": \"100M\"\n    },\n    \"gpu_slurm_config\": {\n        \"partition\": \"gpu\",\n        \"extra_lines\": [\n            \"#SBATCH --gres=gpu:v100:1\"\n        ]\n    },\n    \"user_local_exports\": {\n        \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n        \"NAPARI_CONFIG\": \"napari_config.json\"\n    },\n    \"batching_config\": {\n        \"target_cpus_per_job\": 1,\n        \"max_cpus_per_job\": 1,\n        \"target_mem_per_job\": 200,\n        \"max_mem_per_job\": 500,\n        \"target_num_jobs\": 2,\n        \"max_num_jobs\": 4\n    }\n}\n</code></pre></p> ATTRIBUTE DESCRIPTION <code>default_slurm_config</code> <p>Common default options for all tasks.</p> <p> TYPE: <code>SlurmConfigSet</code> </p> <code>gpu_slurm_config</code> <p>Default configuration for all GPU tasks.</p> <p> TYPE: <code>SlurmConfigSet | None</code> </p> <code>batching_config</code> <p>Configuration of the batching strategy.</p> <p> TYPE: <code>BatchingConfigSet</code> </p> <code>user_local_exports</code> <p>Key-value pairs to be included as <code>export</code>-ed variables in SLURM submission script, after prepending values with the user's cache directory.</p> <p> TYPE: <code>DictStrStr</code> </p> Source code in <code>fractal_server/runner/config/_slurm.py</code> <pre><code>class JobRunnerConfigSLURM(BaseModel):\n    \"\"\"\n    Runner-configuration specifications, for a `slurm_sudo` or\n    `slurm_ssh` resource.\n\n    Note: this is a common class, which is processed and transformed into more\n    specific configuration objects during job execution.\n\n    Valid JSON example\n    ```json\n    {\n        \"default_slurm_config\": {\n            \"partition\": \"partition-name\",\n            \"cpus_per_task\": 1,\n            \"mem\": \"100M\"\n        },\n        \"gpu_slurm_config\": {\n            \"partition\": \"gpu\",\n            \"extra_lines\": [\n                \"#SBATCH --gres=gpu:v100:1\"\n            ]\n        },\n        \"user_local_exports\": {\n            \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n            \"NAPARI_CONFIG\": \"napari_config.json\"\n        },\n        \"batching_config\": {\n            \"target_cpus_per_job\": 1,\n            \"max_cpus_per_job\": 1,\n            \"target_mem_per_job\": 200,\n            \"max_mem_per_job\": 500,\n            \"target_num_jobs\": 2,\n            \"max_num_jobs\": 4\n        }\n    }\n    ```\n\n    Attributes:\n        default_slurm_config:\n            Common default options for all tasks.\n        gpu_slurm_config:\n            Default configuration for all GPU tasks.\n        batching_config:\n            Configuration of the batching strategy.\n        user_local_exports:\n            Key-value pairs to be included as `export`-ed variables in SLURM\n            submission script, after prepending values with the user's cache\n            directory.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    default_slurm_config: SlurmConfigSet\n    gpu_slurm_config: SlurmConfigSet | None = None\n    batching_config: BatchingConfigSet\n    user_local_exports: DictStrStr = Field(default_factory=dict)\n</code></pre>"},{"location":"internals/integrations/runners/#runners","title":"Runners","text":"<p>The three runner implementations (for the local, SLURM/sudo and SLURM/SSH cases) are constructed based on the following class hierarchy:</p> <ul> <li><code>BaseRunner</code> is the base class for all runners, which notably includes the <code>submit</code> and <code>multisubmit</code> methods (to be overridden in child classes).<ul> <li><code>LocalRunner</code> is the runner implementation for a <code>local</code> computational resource.</li> <li><code>BaseSlurmRunner</code> inherits from <code>BaseRunner</code> and adds the common part of SLURM runners:<ul> <li><code>SlurmSudoRunner</code> is the runner implementation for a <code>slurm_sudo</code> resource.</li> <li><code>SlurmSSHRunner</code> is the runner implementation for a <code>slurm_ssh</code> resource.</li> </ul> </li> </ul> </li> </ul>"},{"location":"internals/integrations/tasks/","title":"Task collection","text":"<p>This section is not in-place yet. Meanwhile, you can look at</p> <ul> <li>https://fractal-analytics-platform.github.io/tasks_spec/</li> <li>https://fractal-analytics-platform.github.io/fractal-task-tools/usage/</li> </ul>"},{"location":"internals/version_upgrades/","title":"Version upgrades","text":"<p>Here are additional details about some specific version upgrades:</p> <ul> <li>Upgrade from fractal-server 2.16 to 2.17.0.</li> <li>Upgrade from fractal-server 1.2.5 to 1.3.0.</li> </ul>"},{"location":"internals/version_upgrades/upgrade_1_2_5_to_1_3_0/","title":"Upgrade from 1.2.5 to 1.3.0","text":"<p>A large part of endpoints were updated, mostly to move the foreign-key IDs from the Pydantic models used to validate the request payload to the endpoint path. When necessary, some of those same foreign-key IDs are now passed as query parameters (rather than body or path parameters). The rationale behind this refactor is that endpoints paths are now more consistent:</p> <ul> <li>They have a hierarchical structure, when appropriate (e.g. a project may   contain a datasets, workflows and jobs, while a workflow may contain   workflowtasks, ...).</li> <li>They are more consistently defined and more transparently related to CRUD   operations (see e.g.   here).</li> </ul> <p>The list of updated endpoints is below. Note that the IDs that are now part of the path/query parameters are not required any more as part of the body parameters.</p> <pre><code>OLD GET /api/v1/job/download/{job_id}\nNEW GET /api/v1/project/{project_id}/job/{job_id}/download/\n\nOLD GET /api/v1/job/{job_id}\nNEW GET /api/v1/project/{project_id}/job/{job_id}\n\nOLD GET /api/v1/project/{project_id}/jobs/\nNEW GET /api/v1/project/{project_id}/job/\n\nOLD POST /api/v1/project/{project_id}/{dataset_id}\nNEW POST /api/v1/project/{project_id}/dataset/{dataset_id}/resource/\n\nOLD GET /api/v1/project/{project_id}/{dataset_id}\nNEW GET /api/v1/project/{project_id}/dataset/{dataset_id}\n\nOLD GET /api/v1/project/{project_id}/{dataset_id}/resources/\nNEW GET /api/v1/project/{project_id}/dataset/{dataset_id}/resource/\n\nOLD PATCH /api/v1/project/{project_id}/{dataset_id}\nNEW PATCH /api/v1/project/{project_id}/dataset/{dataset_id}\n\nOLD PATCH /api/v1/project/{project_id}/{dataset_id}/{resource_id}\nNEW PATCH /api/v1/project/{project_id}/dataset/{dataset_id}/resource/{resource_id}\n\nOLD DELETE /api/v1/project/{project_id}/{dataset_id}\nNEW DELETE /api/v1/project/{project_id}/dataset/{dataset_id}\n\nOLD DELETE /api/v1/project/{project_id}/{dataset_id}/{resource_id}\nNEW DELETE /api/v1/project/{project_id}/dataset/{dataset_id}/resource/{resource_id}\n\nOLD PATCH /api/v1/workflow/{workflow_id}/edit-task/{workflow_task_id}\nNEW PATCH /api/v1/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}\n\nOLD DELETE /api/v1/workflow/{workflow_id}/rm-task/{workflow_task_id}\nNEW DELETE /api/v1/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}\n\nOLD POST /api/v1/workflow/{workflow_id}/add-task/\nNEW POST /api/v1/project/{project_id}/workflow/{workflow_id}/wftask/\nNEW QUERY PARAMETERS: ['task_id']\n\nOLD POST /api/v1/project/{project_id}/import-workflow/\nNEW POST /api/v1/project/{project_id}/workflow/import/\n\nOLD POST /api/v1/project/apply/\nNEW POST /api/v1/project/{project_id}/workflow/{workflow_id}/apply/\nNEW QUERY PARAMETERS: ['input_dataset_id', 'output_dataset_id']\n\nOLD POST /api/v1/workflow/\nNEW POST /api/v1/project/{project_id}/workflow/\n\nOLD GET /api/v1/project/{project_id}/workflows/\nNEW GET /api/v1/project/{project_id}/workflow/\n\nOLD GET /api/v1/workflow/{workflow_id}\nNEW GET /api/v1/project/{project_id}/workflow/{workflow_id}\n\nOLD GET /api/v1/workflow/{workflow_id}/export/\nNEW GET /api/v1/project/{project_id}/workflow/{workflow_id}/export/\n\nOLD PATCH /api/v1/workflow/{workflow_id}\nNEW PATCH /api/v1/project/{project_id}/workflow/{workflow_id}\n\nOLD DELETE /api/v1/workflow/{workflow_id}\nNEW DELETE /api/v1/project/{project_id}/workflow/{workflow_id}\n\nOLD POST /api/v1/project/{project_id}/\nNEW POST /api/v1/project/{project_id}/dataset/\n\nNEW GET /api/v1/project/{project_id}/job/{job_id}/stop/\n</code></pre>"},{"location":"internals/version_upgrades/upgrade_2_16_6_to_2_17_0/","title":"Upgrade from 2.16 to 2.17.0","text":"<p>Because of the broad scope of the 2.17.0 version, this page describes the upgrade procedure in detail. Notable changes in this version include:</p> <ul> <li>New concepts of computational resources and profiles.</li> <li>Updates of the application settings.</li> <li>Users cannot self-register through OAuth any more.</li> <li>A user must meet more conditions in order to access the API (namely being marked as both active and verified, and being associated to a computational profile).</li> </ul> <p>Note: Automatic data migration for version 2.17.0 is only supported for Fractal instances attached on a SLURM cluster, and not for <code>local</code> instances.</p>"},{"location":"internals/version_upgrades/upgrade_2_16_6_to_2_17_0/#preliminary-checks-with-fractal-server-216","title":"Preliminary checks (with fractal-server 2.16)","text":"<p>These checks should be performed on a working 2.16 Fractal instance, before starting the upgrade procedure.</p> <ol> <li>In the user list, identify all users who are actually meant to use this Fractal instance, and mark them as both \"active\" and \"verified\".<ul> <li>The automated data-migration script described below will only look for users who are both active and verified.</li> <li>Settings for other users won't be modified, and they won't be able to use Fractal without a manual admin intervention.</li> </ul> </li> <li>For all active&amp;verified users, make sure that their <code>project_dir</code> is set. The data-migration script will fail if it is not set for some active&amp;verified user - because <code>project_dir</code> becomes a required user property.</li> </ol>"},{"location":"internals/version_upgrades/upgrade_2_16_6_to_2_17_0/#upgrade-procedure","title":"Upgrade procedure","text":"<ol> <li>Make a copy of the current <code>.fractal-server.env</code> file and name it <code>.fractal-server.env.old</code>.</li> <li>Make sure that <code>.fractal_server.env.old</code> includes the <code>FRACTAL_SLURM_WORKER_PYTHON</code> variable. If this variable is not set, add it and set it to the absolute path of the Python interpreter which runs <code>fractal-server</code>.</li> <li>Make a backup of the current database with <code>pg_dump</code> (see example).</li> <li>Stop the fractal-server running process (e.g. via <code>systemctl stop fractal-server</code>).</li> <li>Edit <code>.fractal_server.env</code> to align with the new version. List of changes:<ul> <li>Edit the <code>FRACTAL_RUNNER_BACKEND</code> value so that it is one of <code>slurm_sudo</code> or <code>slurm_ssh</code>.<ul> <li>NOTE: This must be changed in the <code>fractal-web</code> configuration as well.</li> </ul> </li> <li>Rename <code>FRACTAL_VIEWER_AUTHORIZATION_SCHEME</code> into <code>FRACTAL_DATA_AUTH_SCHEME</code> - if present.</li> <li>Rename <code>FRACTAL_VIEWER_BASE_FOLDER</code> into <code>FRACTAL_DATA_BASE_FOLDER</code> - if present.</li> <li>Add <code>FRACTAL_DEFAULT_GROUP_NAME=All</code> (note: the same variable must be set also in the <code>fractal-web</code> environment file).</li> <li>Update OAuth-related variables to comply with the new expected ones.<ul> <li>Add the <code>OAUTH_CLIENT_NAME</code> variable.</li> <li>Remove the client name from the names of all other variables, e.g. as in <code>OAUTH_XXX_CLIENT_ID --&gt; OAUTH_CLIENT_ID</code> (if <code>OAUTH_CLIENT_NAME=\"XXX\"</code>).</li> </ul> </li> <li>If <code>FRACTAL_EMAIL_PASSWORD</code> is set, replace its value with the non-encrypted password.</li> <li>Drop all following variables (if set):<ul> <li><code>FRACTAL_DEFAULT_ADMIN_EMAIL</code></li> <li><code>FRACTAL_DEFAULT_ADMIN_PASSWORD</code></li> <li><code>FRACTAL_DEFAULT_ADMIN_USERNAME</code></li> <li><code>FRACTAL_TASKS_DIR</code></li> <li><code>FRACTAL_RUNNER_WORKING_BASE_DIR</code></li> <li><code>FRACTAL_LOCAL_CONFIG_FILE</code></li> <li><code>FRACTAL_SLURM_CONFIG_FILE</code></li> <li><code>FRACTAL_SLURM_WORKER_PYTHON</code>.</li> <li><code>FRACTAL_TASKS_PYTHON_DEFAULT_VERSION</code></li> <li>All <code>FRACTAL_TASKS_PYTHON_3_*</code> variables</li> <li><code>FRACTAL_PIXI_CONFIG_FILE</code>.</li> <li><code>FRACTAL_SLURM_POLL_INTERVAL</code>.</li> <li><code>FRACTAL_PIP_CACHE_DIR</code></li> <li><code>FRACTAL_EMAIL_PASSWORD_KEY</code></li> </ul> </li> </ul> </li> <li>Verify that the following files are available in the current directory:</li> <li><code>.fractal_server.env.old</code></li> <li><code>.fractal_server.env</code></li> <li>The JSON file with the SLURM configuration (as defined in the <code>FRACTAL_SLURM_CONFIG_FILE</code> variable of <code>.fractal_server.env.old</code>).</li> <li>The JSON file with the pixi configuration, if defined in the <code>FRACTAL_PIXI_CONFIG_FILE</code> variable of <code>.fractal_server.env.old</code>.</li> <li>Replace the current <code>fractal-server</code> version with 2.17.0 (e.g. via <code>pip install fractal-server==2.17.0</code> - within the appropriate Python environment).</li> <li>Run the database-schema-migration command <code>fractalctl set-db</code>.</li> <li>Run the database-data-migration command <code>fractalctl update-db-data</code>.</li> <li>It is recommended to also upgrade to versions 2.17.1, right after the previous steps, because this patch release completes the migration process (by applying some schema migrations which could not be included in 2.17.0). This version update is much simpler than the previous one, since it only involves two steps:<ul> <li><code>pip install fractal-server==2.17.1</code></li> <li><code>fractalctl set-db</code></li> </ul> </li> <li>The same applies to <code>2.17.2</code>, which is already available with several small fixes or updates.</li> <li>Restart the fractal-server process (e.g. via <code>systemctl start fractal-server</code>).</li> </ol>"},{"location":"internals/version_upgrades/upgrade_2_16_6_to_2_17_0/#post-upgrade-cleanup","title":"Post-upgrade cleanup","text":"<ul> <li>Upgrade <code>fractal-web</code> to version 1.21.1.</li> <li>Edit <code>fractal-web</code> configuration as follows:<ul> <li>Add <code>FRACTAL_DEFAULT_GROUP_NAME=All</code> to the environment file</li> <li>Edit the <code>FRACTAL_RUNNER_BACKEND</code> value so that it is one of <code>slurm_sudo</code> or <code>slurm_ssh</code>.</li> </ul> </li> <li>Restart the <code>fractal-web</code> process.</li> <li>Verify that log-in still works (including via OAuth).</li> <li>Review the names of resources/profiles.</li> <li>Review the association between users and profiles.</li> <li>Verify that job execution works as expected.</li> <li>Verify that task collection works as expected.</li> <li>Verify that the OME-Zarr viewer works as expected (if configured).</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>app<ul> <li>db</li> <li>models<ul> <li>linkusergroup</li> <li>linkuserproject</li> <li>security</li> <li>v2<ul> <li>accounting</li> <li>dataset</li> <li>history</li> <li>job</li> <li>profile</li> <li>project</li> <li>resource</li> <li>task</li> <li>task_group</li> <li>workflow</li> <li>workflowtask</li> </ul> </li> </ul> </li> <li>routes<ul> <li>admin<ul> <li>v2<ul> <li>_aux_functions</li> <li>accounting</li> <li>impersonate</li> <li>job</li> <li>profile</li> <li>resource</li> <li>sharing</li> <li>task</li> <li>task_group</li> <li>task_group_lifecycle</li> </ul> </li> </ul> </li> <li>api<ul> <li>alive</li> <li>settings</li> <li>v2<ul> <li>_aux_functions</li> <li>_aux_functions_history</li> <li>_aux_functions_sharing</li> <li>_aux_functions_task_lifecycle</li> <li>_aux_functions_task_version_update</li> <li>_aux_functions_tasks</li> <li>_aux_task_group_disambiguation</li> <li>dataset</li> <li>history</li> <li>images</li> <li>job</li> <li>pre_submission_checks</li> <li>project</li> <li>sharing</li> <li>submit</li> <li>task</li> <li>task_collection</li> <li>task_collection_custom</li> <li>task_collection_pixi</li> <li>task_group</li> <li>task_group_lifecycle</li> <li>task_version_update</li> <li>workflow</li> <li>workflow_import</li> <li>workflowtask</li> </ul> </li> </ul> </li> <li>auth<ul> <li>_aux_auth</li> <li>current_user</li> <li>group</li> <li>login</li> <li>oauth</li> <li>register</li> <li>router</li> <li>users</li> <li>viewer_paths</li> </ul> </li> <li>aux<ul> <li>_job</li> <li>_runner</li> <li>_versions</li> <li>validate_user_profile</li> </ul> </li> <li>pagination</li> </ul> </li> <li>schemas<ul> <li>user</li> <li>user_group</li> <li>v2<ul> <li>accounting</li> <li>dataset</li> <li>dumps</li> <li>history</li> <li>job</li> <li>manifest</li> <li>profile</li> <li>project</li> <li>resource</li> <li>sharing</li> <li>status_legacy</li> <li>task</li> <li>task_collection</li> <li>task_group</li> <li>workflow</li> <li>workflowtask</li> </ul> </li> </ul> </li> <li>security<ul> <li>signup_email</li> </ul> </li> <li>shutdown</li> </ul> </li> <li>config<ul> <li>_data</li> <li>_database</li> <li>_email</li> <li>_main</li> <li>_oauth</li> <li>_settings_config</li> </ul> </li> <li>exceptions</li> <li>gunicorn_fractal</li> <li>images<ul> <li>models</li> <li>status_tools</li> <li>tools</li> </ul> </li> <li>logger</li> <li>main</li> <li>runner<ul> <li>components</li> <li>config<ul> <li>_local</li> <li>_slurm</li> <li>slurm_mem_to_MB</li> </ul> </li> <li>exceptions</li> <li>executors<ul> <li>base_runner</li> <li>call_command_wrapper</li> <li>local<ul> <li>get_local_config</li> <li>runner</li> </ul> </li> <li>slurm_common<ul> <li>_batching</li> <li>_job_states</li> <li>base_slurm_runner</li> <li>get_slurm_config</li> <li>remote</li> <li>slurm_config</li> <li>slurm_job_task_models</li> </ul> </li> <li>slurm_ssh<ul> <li>run_subprocess</li> <li>runner</li> <li>tar_commands</li> </ul> </li> <li>slurm_sudo<ul> <li>_subprocess_run_as_user</li> <li>runner</li> </ul> </li> </ul> </li> <li>filenames</li> <li>set_start_and_last_task_index</li> <li>task_files</li> <li>v2<ul> <li>_local</li> <li>_slurm_ssh</li> <li>_slurm_sudo</li> <li>db_tools</li> <li>deduplicate_list</li> <li>merge_outputs</li> <li>runner</li> <li>runner_functions</li> <li>submit_workflow</li> <li>task_interface</li> </ul> </li> <li>versions</li> </ul> </li> <li>ssh<ul> <li>_fabric</li> </ul> </li> <li>string_tools</li> <li>syringe</li> <li>tasks<ul> <li>config<ul> <li>_pixi</li> <li>_python</li> </ul> </li> <li>utils</li> <li>v2<ul> <li>local<ul> <li>_utils</li> <li>collect</li> <li>collect_pixi</li> <li>deactivate</li> <li>deactivate_pixi</li> <li>delete</li> <li>reactivate</li> <li>reactivate_pixi</li> </ul> </li> <li>ssh<ul> <li>_pixi_slurm_ssh</li> <li>_utils</li> <li>collect</li> <li>collect_pixi</li> <li>deactivate</li> <li>deactivate_pixi</li> <li>delete</li> <li>reactivate</li> <li>reactivate_pixi</li> </ul> </li> <li>utils_background</li> <li>utils_database</li> <li>utils_package_names</li> <li>utils_pixi</li> <li>utils_python_interpreter</li> <li>utils_templates</li> </ul> </li> </ul> </li> <li>types<ul> <li>validators<ul> <li>_common_validators</li> <li>_filter_validators</li> <li>_workflow_task_arguments_validators</li> </ul> </li> </ul> </li> <li>urls</li> <li>utils</li> <li>zip_tools</li> </ul>"},{"location":"reference/exceptions/","title":"exceptions","text":""},{"location":"reference/exceptions/#fractal_server.exceptions.UnreachableBranchError","title":"<code>UnreachableBranchError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Exception marking a code branch that should have not been reached.</p> Source code in <code>fractal_server/exceptions.py</code> <pre><code>class UnreachableBranchError(RuntimeError):\n    \"\"\"\n    Exception marking a code branch that should have not been reached.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/gunicorn_fractal/","title":"gunicorn_fractal","text":""},{"location":"reference/gunicorn_fractal/#fractal_server.gunicorn_fractal.FractalWorker","title":"<code>FractalWorker</code>","text":"<p>               Bases: <code>UvicornWorker</code></p> <p>Subclass of uvicorn workers, which also captures SIGABRT and handles it within the <code>custom_handle_abort</code> method.</p> Source code in <code>fractal_server/gunicorn_fractal.py</code> <pre><code>class FractalWorker(UvicornWorker):\n    \"\"\"\n    Subclass of uvicorn workers, which also captures SIGABRT and handles\n    it within the `custom_handle_abort` method.\n    \"\"\"\n\n    def init_signals(self) -&gt; None:\n        super().init_signals()\n        signal.signal(signal.SIGABRT, self.custom_handle_abort)\n        logger.info(\n            f\"[FractalWorker.init_signals - pid={self.pid}] \"\n            \"Set `custom_handle_abort` for SIGABRT\"\n        )\n\n    def custom_handle_abort(self, sig, frame):\n        \"\"\"\n        Custom version of `gunicorn.workers.base.Worker.handle_abort`,\n        transforming SIGABRT into SIGTERM.\n        \"\"\"\n        self.alive = False\n        logger.info(\n            f\"[FractalWorker.custom_handle_abort - pid={self.pid}] \"\n            \"Now send SIGTERM to process.\"\n        )\n        os.kill(self.pid, signal.SIGTERM)\n</code></pre>"},{"location":"reference/gunicorn_fractal/#fractal_server.gunicorn_fractal.FractalWorker.custom_handle_abort","title":"<code>custom_handle_abort(sig, frame)</code>","text":"<p>Custom version of <code>gunicorn.workers.base.Worker.handle_abort</code>, transforming SIGABRT into SIGTERM.</p> Source code in <code>fractal_server/gunicorn_fractal.py</code> <pre><code>def custom_handle_abort(self, sig, frame):\n    \"\"\"\n    Custom version of `gunicorn.workers.base.Worker.handle_abort`,\n    transforming SIGABRT into SIGTERM.\n    \"\"\"\n    self.alive = False\n    logger.info(\n        f\"[FractalWorker.custom_handle_abort - pid={self.pid}] \"\n        \"Now send SIGTERM to process.\"\n    )\n    os.kill(self.pid, signal.SIGTERM)\n</code></pre>"},{"location":"reference/logger/","title":"logger","text":"<p>This module provides logging utilities</p>"},{"location":"reference/logger/#fractal_server.logger.close_logger","title":"<code>close_logger(logger)</code>","text":"<p>Close all handlers associated to a <code>logging.Logger</code> object</p> PARAMETER DESCRIPTION <code>logger</code> <p>The actual logger</p> <p> TYPE: <code>Logger</code> </p> Source code in <code>fractal_server/logger.py</code> <pre><code>def close_logger(logger: logging.Logger) -&gt; None:\n    \"\"\"\n    Close all handlers associated to a `logging.Logger` object\n\n    Args:\n        logger: The actual logger\n    \"\"\"\n    for handle in logger.handlers:\n        handle.close()\n</code></pre>"},{"location":"reference/logger/#fractal_server.logger.config_uvicorn_loggers","title":"<code>config_uvicorn_loggers()</code>","text":"<p>Change the formatter for the uvicorn access/error loggers.</p> <p>This is similar to https://stackoverflow.com/a/68864979/19085332. See also https://github.com/tiangolo/fastapi/issues/1508.</p> <p>This function is meant to work in two scenarios:</p> <ol> <li>The most relevant case is for a <code>gunicorn</code> startup command, with    <code>--access-logfile</code> and <code>--error-logfile</code> options set.</li> <li>The case of <code>fractalctl start</code> (directly calling <code>uvicorn</code>).</li> </ol> <p>Because of the second use case, we need to check whether uvicorn loggers already have a handler. If not, we skip the formatting.</p> Source code in <code>fractal_server/logger.py</code> <pre><code>def config_uvicorn_loggers():\n    \"\"\"\n    Change the formatter for the uvicorn access/error loggers.\n\n    This is similar to https://stackoverflow.com/a/68864979/19085332. See also\n    https://github.com/tiangolo/fastapi/issues/1508.\n\n    This function is meant to work in two scenarios:\n\n    1. The most relevant case is for a `gunicorn` startup command, with\n       `--access-logfile` and `--error-logfile` options set.\n    2. The case of `fractalctl start` (directly calling `uvicorn`).\n\n    Because of the second use case, we need to check whether uvicorn loggers\n    already have a handler. If not, we skip the formatting.\n    \"\"\"\n\n    access_logger = logging.getLogger(\"uvicorn.access\")\n    if len(access_logger.handlers) &gt; 0:\n        access_logger.handlers[0].setFormatter(LOG_FORMATTER)\n\n    error_logger = logging.getLogger(\"uvicorn.error\")\n    if len(error_logger.handlers) &gt; 0:\n        error_logger.handlers[0].setFormatter(LOG_FORMATTER)\n</code></pre>"},{"location":"reference/logger/#fractal_server.logger.get_logger","title":"<code>get_logger(logger_name=None)</code>","text":"<p>Wrap the <code>logging.getLogger</code> function.</p> <p>The typical use case for this function is to retrieve a logger that was already defined, as in the following example: <pre><code>def function1(logger_name):\n    logger = get_logger(logger_name)\n    logger.info(\"Info from function1\")\n\ndef funtion2():\n    logger_name = \"my_logger\"\n    logger = set_logger(logger_name)\n    logger.info(\"Info from function2\")\n    function1(logger_name)\n    close_logger(logger)\n</code></pre></p> PARAMETER DESCRIPTION <code>logger_name</code> <p>Name of logger</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <p>Returns:     Logger with name <code>logger_name</code></p> Source code in <code>fractal_server/logger.py</code> <pre><code>def get_logger(logger_name: str | None = None) -&gt; logging.Logger:\n    \"\"\"\n    Wrap the\n    [`logging.getLogger`](https://docs.python.org/3/library/logging.html#logging.getLogger)\n    function.\n\n    The typical use case for this function is to retrieve a logger that was\n    already defined, as in the following example:\n    ```python\n    def function1(logger_name):\n        logger = get_logger(logger_name)\n        logger.info(\"Info from function1\")\n\n    def funtion2():\n        logger_name = \"my_logger\"\n        logger = set_logger(logger_name)\n        logger.info(\"Info from function2\")\n        function1(logger_name)\n        close_logger(logger)\n    ```\n\n    Args:\n        logger_name: Name of logger\n    Returns:\n        Logger with name `logger_name`\n    \"\"\"\n    return logging.getLogger(logger_name)\n</code></pre>"},{"location":"reference/logger/#fractal_server.logger.reset_logger_handlers","title":"<code>reset_logger_handlers(logger)</code>","text":"<p>Close and remove all handlers associated to a <code>logging.Logger</code> object</p> PARAMETER DESCRIPTION <code>logger</code> <p>The actual logger</p> <p> TYPE: <code>Logger</code> </p> Source code in <code>fractal_server/logger.py</code> <pre><code>def reset_logger_handlers(logger: logging.Logger) -&gt; None:\n    \"\"\"\n    Close and remove all handlers associated to a `logging.Logger` object\n\n    Args:\n        logger: The actual logger\n    \"\"\"\n    close_logger(logger)\n    logger.handlers.clear()\n</code></pre>"},{"location":"reference/logger/#fractal_server.logger.set_logger","title":"<code>set_logger(logger_name, *, log_file_path=None, default_logging_level=None)</code>","text":"<p>Set up a <code>fractal-server</code> logger</p> <p>The logger (a <code>logging.Logger</code> object) will have the following properties:</p> <ul> <li>The attribute <code>Logger.propagate</code> set to <code>False</code>;</li> <li>One and only one <code>logging.StreamHandler</code> handler, with severity level set to <code>FRACTAL_LOGGING_LEVEL</code> (or <code>default_logging_level</code>, if set), and formatter set as in the <code>logger.LOG_FORMAT</code> variable from the current module;</li> <li>One or many <code>logging.FileHandler</code> handlers, including one pointint to <code>log_file_path</code> (if set); all these handlers have severity level set to <code>logging.DEBUG</code>.</li> </ul> PARAMETER DESCRIPTION <code>logger_name</code> <p>The identifier of the logger.</p> <p> TYPE: <code>str</code> </p> <code>log_file_path</code> <p>Path to the log file.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>default_logging_level</code> <p>Override for <code>settings.FRACTAL_LOGGING_LEVEL</code></p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>logger</code> <p>The logger, as configured by the arguments.</p> <p> TYPE: <code>Logger</code> </p> Source code in <code>fractal_server/logger.py</code> <pre><code>def set_logger(\n    logger_name: str,\n    *,\n    log_file_path: str | Path | None = None,\n    default_logging_level: int | None = None,\n) -&gt; logging.Logger:\n    \"\"\"\n    Set up a `fractal-server` logger\n\n    The logger (a `logging.Logger` object) will have the following properties:\n\n    * The attribute `Logger.propagate` set to `False`;\n    * One and only one `logging.StreamHandler` handler, with severity level set\n    to `FRACTAL_LOGGING_LEVEL` (or `default_logging_level`, if set), and\n    formatter set as in the `logger.LOG_FORMAT`\n    variable from the current module;\n    * One or many `logging.FileHandler` handlers, including one pointint to\n    `log_file_path` (if set); all these handlers have severity level set to\n    `logging.DEBUG`.\n\n    Args:\n        logger_name: The identifier of the logger.\n        log_file_path: Path to the log file.\n        default_logging_level: Override for `settings.FRACTAL_LOGGING_LEVEL`\n\n    Returns:\n        logger: The logger, as configured by the arguments.\n    \"\"\"\n\n    logger = logging.getLogger(logger_name)\n    logger.propagate = False\n    logger.setLevel(logging.DEBUG)\n\n    current_stream_handlers = [\n        handler\n        for handler in logger.handlers\n        if isinstance(handler, logging.StreamHandler)\n    ]\n\n    if not current_stream_handlers:\n        stream_handler = logging.StreamHandler()\n        if default_logging_level is None:\n            settings = Inject(get_settings)\n            default_logging_level = settings.FRACTAL_LOGGING_LEVEL\n        stream_handler.setLevel(default_logging_level)\n        stream_handler.setFormatter(LOG_FORMATTER)\n        logger.addHandler(stream_handler)\n\n    if log_file_path is not None:\n        file_handler = logging.FileHandler(log_file_path, mode=\"a\")\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(LOG_FORMATTER)\n        file_handler.setFormatter(LOG_FORMATTER)\n        logger.addHandler(file_handler)\n        current_file_handlers = [\n            handler\n            for handler in logger.handlers\n            if isinstance(handler, logging.FileHandler)\n        ]\n        if len(current_file_handlers) &gt; 1:\n            logger.warning(\n                f\"Logger {logger_name} has multiple file handlers: \"\n                f\"{current_file_handlers}\"\n            )\n\n    return logger\n</code></pre>"},{"location":"reference/main/","title":"main","text":""},{"location":"reference/main/#fractal_server.main.check_settings","title":"<code>check_settings(logger_name)</code>","text":"<p>Check and register the settings</p> <p>Verify the consistency of the settings, in particular that required variables are set.</p> RAISES DESCRIPTION <code>ValidationError</code> <p>If the configuration is invalid.</p> Source code in <code>fractal_server/main.py</code> <pre><code>def check_settings(logger_name: str) -&gt; None:\n    \"\"\"\n    Check and register the settings\n\n    Verify the consistency of the settings, in particular that required\n    variables are set.\n\n    Raises:\n        ValidationError: If the configuration is invalid.\n    \"\"\"\n    settings = Inject(get_settings)\n    db_settings = Inject(get_db_settings)\n    email_settings = Inject(get_email_settings)\n    logger = get_logger(logger_name)\n    logger.debug(\"Fractal Settings:\")\n    for key, value in chain(\n        db_settings.model_dump().items(),\n        settings.model_dump().items(),\n        email_settings.model_dump().items(),\n    ):\n        if any(s in key.upper() for s in [\"PASSWORD\", \"SECRET\", \"KEY\"]):\n            value = \"*****\"\n        logger.debug(f\"  {key}: {value}\")\n</code></pre>"},{"location":"reference/main/#fractal_server.main.collect_routers","title":"<code>collect_routers(app)</code>","text":"<p>Register the routers to the application</p> PARAMETER DESCRIPTION <code>app</code> <p>The application to register the routers to.</p> <p> TYPE: <code>FastAPI</code> </p> Source code in <code>fractal_server/main.py</code> <pre><code>def collect_routers(app: FastAPI) -&gt; None:\n    \"\"\"\n    Register the routers to the application\n\n    Args:\n        app:\n            The application to register the routers to.\n    \"\"\"\n    from .app.routes.admin.v2 import router_admin\n    from .app.routes.api import router_api\n    from .app.routes.api.v2 import router_api as router_api_v2\n    from .app.routes.auth.router import router_auth\n\n    app.include_router(router_api, prefix=\"/api\")\n    app.include_router(router_api_v2, prefix=\"/api/v2\")\n    app.include_router(router_admin, prefix=\"/admin/v2\", tags=[\"Admin area\"])\n    app.include_router(router_auth, prefix=\"/auth\", tags=[\"Authentication\"])\n</code></pre>"},{"location":"reference/main/#fractal_server.main.start_application","title":"<code>start_application()</code>","text":"<p>Create the application, initialise it and collect all available routers.</p> RETURNS DESCRIPTION <code>app</code> <p>The fully initialised application.</p> <p> TYPE: <code>FastAPI</code> </p> Source code in <code>fractal_server/main.py</code> <pre><code>def start_application() -&gt; FastAPI:\n    \"\"\"\n    Create the application, initialise it and collect all available routers.\n\n    Returns:\n        app:\n            The fully initialised application.\n    \"\"\"\n    app = FastAPI(lifespan=lifespan)\n\n    settings = Inject(get_settings)\n    app.add_middleware(\n        SlowResponseMiddleware,\n        time_threshold=settings.FRACTAL_LONG_REQUEST_TIME,\n    )\n    app.add_exception_handler(\n        HTTPExceptionWithData,\n        handler=data_exception_handler,\n    )\n\n    collect_routers(app)\n    return app\n</code></pre>"},{"location":"reference/string_tools/","title":"string_tools","text":""},{"location":"reference/string_tools/#fractal_server.string_tools.interpret_as_bool","title":"<code>interpret_as_bool(value)</code>","text":"<p>Interpret a boolean or a string representation of a boolean as a boolean value.</p> <p>Accepts either a boolean (<code>True</code> or <code>False</code>) or a case-insensitive string representation of a boolean (\"true\" or \"false\"). Returns the corresponding boolean value.</p> Source code in <code>fractal_server/string_tools.py</code> <pre><code>def interpret_as_bool(value: bool | str) -&gt; bool:\n    \"\"\"\n    Interpret a boolean or a string representation of a boolean as a boolean\n    value.\n\n    Accepts either a boolean (`True` or `False`) or a case-insensitive string\n    representation of a boolean (\"true\" or \"false\").\n    Returns the corresponding boolean value.\n    \"\"\"\n    if isinstance(value, bool):\n        return value\n    elif isinstance(value, str):\n        value_lower = value.lower()\n        if value_lower == \"true\":\n            return True\n        elif value_lower == \"false\":\n            return False\n        else:\n            raise ValueError(\"String must be 'true' or 'false'.\")\n    else:\n        raise TypeError(f\"Expected bool or str, got {type(value)}: '{value}'.\")\n</code></pre>"},{"location":"reference/string_tools/#fractal_server.string_tools.sanitize_string","title":"<code>sanitize_string(value)</code>","text":"<p>Make string safe to be used in file/folder names and subprocess commands.</p> <p>Make the string lower-case, and replace any special character with an underscore, where special characters are:</p> <pre><code>&gt;&gt;&gt; string.punctuation\n'!\"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~'\n&gt;&gt;&gt; string.whitespace\n' \\t\\n\\r\\x0b\\x0c'\n</code></pre> PARAMETER DESCRIPTION <code>value</code> <p>Input string</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Sanitized value</p> Source code in <code>fractal_server/string_tools.py</code> <pre><code>def sanitize_string(value: str) -&gt; str:\n    \"\"\"\n    Make string safe to be used in file/folder names and subprocess commands.\n\n    Make the string lower-case, and replace any special character with an\n    underscore, where special characters are:\n\n\n        &gt;&gt;&gt; string.punctuation\n        '!\"#$%&amp;\\'()*+,-./:;&lt;=&gt;?@[\\\\\\\\]^_`{|}~'\n        &gt;&gt;&gt; string.whitespace\n        ' \\\\t\\\\n\\\\r\\\\x0b\\\\x0c'\n\n    Args:\n        value: Input string\n\n    Returns:\n        Sanitized value\n    \"\"\"\n    new_value = value.lower()\n    for character in __SPECIAL_CHARACTERS__:\n        new_value = new_value.replace(character, \"_\")\n    return new_value\n</code></pre>"},{"location":"reference/string_tools/#fractal_server.string_tools.validate_cmd","title":"<code>validate_cmd(command, *, allow_char=None, attribute_name='Command')</code>","text":"<p>Assert that the provided <code>command</code> does not contain any of the forbidden characters for commands (fractal_server.string_tools.NOT_ALLOWED_FOR_COMMANDS)</p> PARAMETER DESCRIPTION <code>command</code> <p>command to validate.</p> <p> TYPE: <code>str</code> </p> <code>allow_char</code> <p>chars to accept among the forbidden ones</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>attribute_name</code> <p>Name of the attribute, to be used in error message.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'Command'</code> </p> Source code in <code>fractal_server/string_tools.py</code> <pre><code>def validate_cmd(\n    command: str,\n    *,\n    allow_char: str | None = None,\n    attribute_name: str = \"Command\",\n):\n    \"\"\"\n    Assert that the provided `command` does not contain any of the forbidden\n    characters for commands\n    (fractal_server.string_tools.__NOT_ALLOWED_FOR_COMMANDS__)\n\n    Args:\n        command: command to validate.\n        allow_char: chars to accept among the forbidden ones\n        attribute_name: Name of the attribute, to be used in error message.\n    \"\"\"\n    if not isinstance(command, str):\n        raise ValueError(f\"{command=} is not a string.\")\n    forbidden = set(__NOT_ALLOWED_FOR_COMMANDS__)\n    if allow_char is not None:\n        forbidden = forbidden - set(allow_char)\n    if not forbidden.isdisjoint(set(command)):\n        raise ValueError(\n            f\"{attribute_name} must not contain any of this characters: \"\n            f\"'{forbidden}'\\n\"\n            f\"Provided {attribute_name.lower()}: '{command}'.\"\n        )\n</code></pre>"},{"location":"reference/syringe/","title":"syringe","text":"<p>This module provides an extremely simple utility for dependency injection.</p> <p>It's made up of a single singleton class that provides a directory for the dependencies. The dependencies are stored in a dictionary and can be overridden or popped from the directory.</p>"},{"location":"reference/syringe/#fractal_server.syringe--usage","title":"Usage:","text":"<pre><code>&gt;&gt;&gt; from syringe import Inject\n&gt;&gt;&gt; def foo():\n&gt;&gt;&gt;     return 42\n&gt;&gt;&gt;\n&gt;&gt;&gt; def oof():\n&gt;&gt;&gt;     return 24\n&gt;&gt;&gt;\n&gt;&gt;&gt; def bar():\n&gt;&gt;&gt;     return Inject(foo)\n&gt;&gt;&gt;\n&gt;&gt;&gt; bar()\n42\n&gt;&gt;&gt; Inject.override(foo, oof)\n&gt;&gt;&gt; bar()\n24\n&gt;&gt;&gt; Inject.pop(foo)\n&gt;&gt;&gt; bar()\n42\n</code></pre>"},{"location":"reference/syringe/#fractal_server.syringe.Inject","title":"<code>Inject = _Inject()</code>  <code>module-attribute</code>","text":"<p>The singleton instance of <code>_Inject</code>, the only public member of this module.</p>"},{"location":"reference/syringe/#fractal_server.syringe._Inject","title":"<code>_Inject</code>","text":"<p>Injection class</p> <p>This is a private class that is never directly instantiated.</p> ATTRIBUTE DESCRIPTION <code>_dependencies</code> <p>The dependency directory</p> <p> TYPE: <code>dict[Any, Any]</code> </p> Source code in <code>fractal_server/syringe.py</code> <pre><code>class _Inject:\n    \"\"\"\n    Injection class\n\n    This is a private class that is never directly instantiated.\n\n    Attributes:\n        _dependencies:\n            The dependency directory\n    \"\"\"\n\n    _dependencies: dict[Any, Any] = {}\n\n    def __init__(self):\n        global _instance_count\n        if _instance_count == 1:\n            raise RuntimeError(\"You must only instance this class once\")\n        _instance_count += 1\n\n    @classmethod\n    def __call__(cls, _callable: Callable[..., T]) -&gt; T:\n        \"\"\"\n        Call the dependency\n\n        Args:\n            _callable:\n                Callable dependency object\n\n        Returns:\n            The output of calling `_callalbe` or its dependency override.\n        \"\"\"\n        try:\n            return cls._dependencies[_callable]()\n        except KeyError:\n            return _callable()\n\n    @classmethod\n    def pop(cls, _callable: Callable[..., T]) -&gt; T:\n        \"\"\"\n        Remove the dependency from the directory\n\n        Args:\n            _callable:\n                Callable dependency object\n        \"\"\"\n        try:\n            return cls._dependencies.pop(_callable)\n        except KeyError:\n            raise RuntimeError(f\"No dependency override for {_callable}\")\n\n    @classmethod\n    def override(\n        cls, _callable: Callable[..., T], value: Callable[..., T]\n    ) -&gt; None:\n        \"\"\"\n        Override dependency\n\n        Substitute a dependency with a different arbitrary callable.\n\n        Args:\n            _callable:\n                Callable dependency object\n            value:\n                Callable override\n        \"\"\"\n        cls._dependencies[_callable] = value\n</code></pre>"},{"location":"reference/syringe/#fractal_server.syringe._Inject.__call__","title":"<code>__call__(_callable)</code>  <code>classmethod</code>","text":"<p>Call the dependency</p> PARAMETER DESCRIPTION <code>_callable</code> <p>Callable dependency object</p> <p> TYPE: <code>Callable[..., T]</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The output of calling <code>_callalbe</code> or its dependency override.</p> Source code in <code>fractal_server/syringe.py</code> <pre><code>@classmethod\ndef __call__(cls, _callable: Callable[..., T]) -&gt; T:\n    \"\"\"\n    Call the dependency\n\n    Args:\n        _callable:\n            Callable dependency object\n\n    Returns:\n        The output of calling `_callalbe` or its dependency override.\n    \"\"\"\n    try:\n        return cls._dependencies[_callable]()\n    except KeyError:\n        return _callable()\n</code></pre>"},{"location":"reference/syringe/#fractal_server.syringe._Inject.override","title":"<code>override(_callable, value)</code>  <code>classmethod</code>","text":"<p>Override dependency</p> <p>Substitute a dependency with a different arbitrary callable.</p> PARAMETER DESCRIPTION <code>_callable</code> <p>Callable dependency object</p> <p> TYPE: <code>Callable[..., T]</code> </p> <code>value</code> <p>Callable override</p> <p> TYPE: <code>Callable[..., T]</code> </p> Source code in <code>fractal_server/syringe.py</code> <pre><code>@classmethod\ndef override(\n    cls, _callable: Callable[..., T], value: Callable[..., T]\n) -&gt; None:\n    \"\"\"\n    Override dependency\n\n    Substitute a dependency with a different arbitrary callable.\n\n    Args:\n        _callable:\n            Callable dependency object\n        value:\n            Callable override\n    \"\"\"\n    cls._dependencies[_callable] = value\n</code></pre>"},{"location":"reference/syringe/#fractal_server.syringe._Inject.pop","title":"<code>pop(_callable)</code>  <code>classmethod</code>","text":"<p>Remove the dependency from the directory</p> PARAMETER DESCRIPTION <code>_callable</code> <p>Callable dependency object</p> <p> TYPE: <code>Callable[..., T]</code> </p> Source code in <code>fractal_server/syringe.py</code> <pre><code>@classmethod\ndef pop(cls, _callable: Callable[..., T]) -&gt; T:\n    \"\"\"\n    Remove the dependency from the directory\n\n    Args:\n        _callable:\n            Callable dependency object\n    \"\"\"\n    try:\n        return cls._dependencies.pop(_callable)\n    except KeyError:\n        raise RuntimeError(f\"No dependency override for {_callable}\")\n</code></pre>"},{"location":"reference/urls/","title":"urls","text":""},{"location":"reference/utils/","title":"utils","text":"<p>This module provides general purpose utilities that are not specific to any subsystem.</p>"},{"location":"reference/utils/#fractal_server.utils.execute_command_sync","title":"<code>execute_command_sync(*, command, logger_name=None, allow_char=None)</code>","text":"<p>Execute arbitrary command</p> <p>If the command returns a return code different from zero, a <code>RuntimeError</code> is raised.</p> PARAMETER DESCRIPTION <code>command</code> <p>Command to be executed.</p> <p> TYPE: <code>str</code> </p> <code>logger_name</code> <p>Name of the logger.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>allow_char</code> <p>Argument propagated to <code>validate_cmd</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>fractal_server/utils.py</code> <pre><code>def execute_command_sync(\n    *,\n    command: str,\n    logger_name: str | None = None,\n    allow_char: str | None = None,\n) -&gt; str:\n    \"\"\"\n    Execute arbitrary command\n\n    If the command returns a return code different from zero, a `RuntimeError`\n    is raised.\n\n    Args:\n        command: Command to be executed.\n        logger_name: Name of the logger.\n        allow_char: Argument propagated to `validate_cmd`.\n    \"\"\"\n    logger = get_logger(logger_name)\n    logger.debug(f\"START subprocess call to '{command}'\")\n    validate_cmd(command=command, allow_char=allow_char)\n    res = subprocess.run(  # nosec\n        shlex.split(command),\n        capture_output=True,\n        encoding=\"utf-8\",\n    )\n    returncode = res.returncode\n    stdout = res.stdout\n    stderr = res.stderr\n    if res.returncode != 0:\n        logger.debug(f\"ERROR in subprocess call to '{command}'\")\n        raise RuntimeError(\n            f\"Command {command} failed.\\n\"\n            f\"returncode={res.returncode}\\n\"\n            \"STDOUT:\\n\"\n            f\"{stdout}\\n\"\n            \"STDERR:\\n\"\n            f\"{stderr}\\n\"\n        )\n    logger.debug(f\"{returncode=}\")\n    logger.debug(\"STDOUT:\")\n    logger.debug(stdout)\n    logger.debug(\"STDERR:\")\n    logger.debug(stderr)\n    logger.debug(f\"END   subprocess call to '{command}'\")\n    return stdout\n</code></pre>"},{"location":"reference/utils/#fractal_server.utils.get_timestamp","title":"<code>get_timestamp()</code>","text":"<p>Get timezone aware timestamp.</p> Source code in <code>fractal_server/utils.py</code> <pre><code>def get_timestamp() -&gt; datetime:\n    \"\"\"\n    Get timezone aware timestamp.\n    \"\"\"\n    return datetime.now(tz=timezone.utc)\n</code></pre>"},{"location":"reference/zip_tools/","title":"zip_tools","text":""},{"location":"reference/zip_tools/#fractal_server.zip_tools._create_zip","title":"<code>_create_zip(folder, output)</code>","text":"<p>Zip a folder into a zip-file or into a BytesIO.</p> PARAMETER DESCRIPTION <code>folder</code> <p>Folder to be zipped.</p> <p> TYPE: <code>str</code> </p> <code>output</code> <p>Either a string with the path of the zip file, or a BytesIO object.</p> <p> TYPE: <code>T</code> </p> RETURNS DESCRIPTION <code>T</code> <p>Either the zip-file path string, or the modified BytesIO object.</p> Source code in <code>fractal_server/zip_tools.py</code> <pre><code>def _create_zip(folder: str, output: T) -&gt; T:\n    \"\"\"\n    Zip a folder into a zip-file or into a BytesIO.\n\n    Args:\n        folder: Folder to be zipped.\n        output: Either a string with the path of the zip file, or a BytesIO\n            object.\n\n    Returns:\n        Either the zip-file path string, or the modified BytesIO object.\n    \"\"\"\n    if isinstance(output, str) and os.path.exists(output):\n        raise FileExistsError(f\"Zip file '{output}' already exists\")\n    if isinstance(output, BytesIO) and output.getbuffer().nbytes &gt; 0:\n        raise ValueError(\"BytesIO is not empty\")\n\n    with ZipFile(output, mode=\"w\", compression=ZIP_DEFLATED) as zipfile:\n        for root, dirs, files in os.walk(folder):\n            for file in files:\n                file_path = os.path.join(root, file)\n                archive_path = os.path.relpath(file_path, folder)\n                zipfile.write(file_path, archive_path)\n    return output\n</code></pre>"},{"location":"reference/zip_tools/#fractal_server.zip_tools._folder_can_be_deleted","title":"<code>_folder_can_be_deleted(folder)</code>","text":"<p>Given the path of a folder as string, returns <code>False</code> if either: - the related zip file <code>{folder}.zip</code> does already exists, - the folder and the zip file have a different number of internal files, - the zip file has a very small size. Otherwise returns <code>True</code>.</p> Source code in <code>fractal_server/zip_tools.py</code> <pre><code>def _folder_can_be_deleted(folder: str) -&gt; bool:\n    \"\"\"\n    Given the path of a folder as string, returns `False` if either:\n    - the related zip file `{folder}.zip` does already exists,\n    - the folder and the zip file have a different number of internal files,\n    - the zip file has a very small size.\n    Otherwise returns `True`.\n    \"\"\"\n    # CHECK 1: zip file exists\n    zip_file = f\"{folder}.zip\"\n    if not os.path.exists(zip_file):\n        logger.info(\n            f\"Folder '{folder}' won't be deleted because file '{zip_file}' \"\n            \"does not exist.\"\n        )\n        return False\n\n    # CHECK 2: folder and zip file have the same number of files\n    folder_files_count = sum(1 for f in Path(folder).rglob(\"*\") if f.is_file())\n    with ZipFile(zip_file, \"r\") as zip_ref:\n        zip_files_count = len(zip_ref.namelist())\n    if folder_files_count != zip_files_count:\n        logger.info(\n            f\"Folder '{folder}' won't be deleted because it contains \"\n            f\"{folder_files_count} files while '{zip_file}' contains \"\n            f\"{zip_files_count}.\"\n        )\n        return False\n\n    # CHECK 3: zip file size is &gt;= than `THRESHOLD_ZIP_FILE_SIZE_MB`\n    zip_size = os.path.getsize(zip_file)\n    if zip_size &lt; THRESHOLD_ZIP_FILE_SIZE_MB * (1024**2):\n        logger.info(\n            f\"Folder '{folder}' won't be deleted because '{zip_file}' is too \"\n            f\"small ({zip_size / (1024**2):.5f} MB, whereas the minimum limit \"\n            f\"for deletion is {THRESHOLD_ZIP_FILE_SIZE_MB}).\"\n        )\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/zip_tools/#fractal_server.zip_tools._read_single_file_from_zip","title":"<code>_read_single_file_from_zip(*, file_path, archive_path)</code>","text":"<p>Reads and returns the contents of a single file from a ZIP archive using <code>unzip -p</code>.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>relative to the archive</p> <p> TYPE: <code>str</code> </p> <code>archive_path</code> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The file content</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>if the file is not inside the archive</p> Source code in <code>fractal_server/zip_tools.py</code> <pre><code>def _read_single_file_from_zip(*, file_path: str, archive_path: str) -&gt; str:\n    \"\"\"\n    Reads and returns the contents of a single file from a ZIP archive using\n    `unzip -p`.\n\n    Args:\n        file_path:\n            relative to the archive\n        archive_path:\n\n    Returns:\n        The file content\n\n    Raises:\n        FileNotFoundError:\n            if the file is not inside the archive\n    \"\"\"\n    result = subprocess.run(  # nosec\n        [\"unzip\", \"-p\", archive_path, file_path],\n        capture_output=True,\n        encoding=\"utf-8\",\n        check=False,\n    )\n\n    if result.returncode != 0:\n        # The caller function should handle this error\n        raise FileNotFoundError(\n            f\"File '{file_path}' not found inside archive '{archive_path}'.\"\n        )\n\n    return result.stdout\n</code></pre>"},{"location":"reference/zip_tools/#fractal_server.zip_tools._zip_folder_to_byte_stream_iterator","title":"<code>_zip_folder_to_byte_stream_iterator(folder)</code>","text":"<p>Returns byte stream with the zipped log folder of a job.</p> PARAMETER DESCRIPTION <code>folder</code> <p>the folder to zip</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/zip_tools.py</code> <pre><code>def _zip_folder_to_byte_stream_iterator(folder: str) -&gt; Iterator:\n    \"\"\"\n    Returns byte stream with the zipped log folder of a job.\n\n    Args:\n        folder: the folder to zip\n    \"\"\"\n    zip_file = Path(f\"{folder}.zip\")\n\n    if os.path.exists(zip_file):\n\n        def iterfile():\n            \"\"\"\n            https://fastapi.tiangolo.com/advanced/custom-response/#using-streamingresponse-with-file-like-objects\n            \"\"\"\n            with open(zip_file, mode=\"rb\") as file_like:\n                yield from file_like\n\n        return iterfile()\n\n    else:\n        byte_stream = _create_zip(folder, output=BytesIO())\n        return iter([byte_stream.getvalue()])\n</code></pre>"},{"location":"reference/zip_tools/#fractal_server.zip_tools._zip_folder_to_file_and_remove","title":"<code>_zip_folder_to_file_and_remove(folder)</code>","text":"<p>Creates a ZIP archive of the specified folder and removes the original folder (if it can be deleted).</p> <p>This function performs the following steps: 1. Creates a ZIP archive of the <code>folder</code> and names it with a temporary    suffix <code>_tmp.zip</code>. 2. Renames the ZIP removing the suffix (this would possibly overwrite a     file with the same name already present). 3. Checks if the folder can be safely deleted using the     <code>_folder_can_be_deleted</code> function. If so, deletes the original folder.</p> Source code in <code>fractal_server/zip_tools.py</code> <pre><code>def _zip_folder_to_file_and_remove(folder: str) -&gt; None:\n    \"\"\"\n    Creates a ZIP archive of the specified folder and removes the original\n    folder (if it can be deleted).\n\n    This function performs the following steps:\n    1. Creates a ZIP archive of the `folder` and names it with a temporary\n       suffix `_tmp.zip`.\n    2. Renames the ZIP removing the suffix (this would possibly overwrite a\n        file with the same name already present).\n    3. Checks if the folder can be safely deleted using the\n        `_folder_can_be_deleted` function. If so, deletes the original folder.\n    \"\"\"\n\n    tmp_zipfile = f\"{folder}_tmp.zip\"\n    zipfile = f\"{folder}.zip\"\n\n    try:\n        logger.info(f\"Start creating temporary zip file at '{tmp_zipfile}'.\")\n        _create_zip(folder, tmp_zipfile)\n        logger.info(\"Zip file created.\")\n    except Exception as e:\n        logger.error(\n            f\"Error while creating temporary zip file. Original error: '{e}'.\"\n        )\n        Path(tmp_zipfile).unlink(missing_ok=True)\n        return\n\n    logger.info(f\"Moving temporary zip file to {zipfile}.\")\n    shutil.move(tmp_zipfile, zipfile)\n    logger.info(\"Zip file moved.\")\n\n    if _folder_can_be_deleted(folder):\n        logger.info(f\"Removing folder '{folder}'.\")\n        shutil.rmtree(folder)\n        logger.info(\"Folder removed.\")\n</code></pre>"},{"location":"reference/app/","title":"app","text":""},{"location":"reference/app/shutdown/","title":"shutdown","text":""},{"location":"reference/app/db/","title":"db","text":"<p><code>db</code> module, loosely adapted from https://testdriven.io/blog/fastapi-sqlmodel/#async-sqlmodel</p>"},{"location":"reference/app/db/#fractal_server.app.db.DB","title":"<code>DB</code>","text":"<p>DB class</p> Source code in <code>fractal_server/app/db/__init__.py</code> <pre><code>class DB:\n    \"\"\"\n    DB class\n    \"\"\"\n\n    @classmethod\n    def engine_async(cls):\n        try:\n            return cls._engine_async\n        except AttributeError:\n            cls.set_async_db()\n            return cls._engine_async\n\n    @classmethod\n    def engine_sync(cls):\n        try:\n            return cls._engine_sync\n        except AttributeError:\n            cls.set_sync_db()\n            return cls._engine_sync\n\n    @classmethod\n    def set_async_db(cls):\n        db_settings = Inject(get_db_settings)\n\n        cls._engine_async = create_async_engine(\n            db_settings.DATABASE_URL,\n            echo=(db_settings.DB_ECHO == \"true\"),\n            future=True,\n            pool_pre_ping=True,\n        )\n        cls._async_session_maker = sessionmaker(\n            cls._engine_async,\n            class_=AsyncSession,\n            expire_on_commit=False,\n        )\n\n    @classmethod\n    def set_sync_db(cls):\n        db_settings = Inject(get_db_settings)\n\n        cls._engine_sync = create_engine(\n            db_settings.DATABASE_URL,\n            echo=(db_settings.DB_ECHO == \"true\"),\n            future=True,\n            pool_pre_ping=True,\n        )\n\n        cls._sync_session_maker = sessionmaker(cls._engine_sync)\n\n    @classmethod\n    async def get_async_db(cls) -&gt; AsyncGenerator[AsyncSession, None]:\n        \"\"\"\n        Get async database session\n        \"\"\"\n        try:\n            session_maker = cls._async_session_maker()\n        except AttributeError:\n            cls.set_async_db()\n            session_maker = cls._async_session_maker()\n        async with session_maker as async_session:\n            yield async_session\n\n    @classmethod\n    def get_sync_db(cls) -&gt; Generator[DBSyncSession, None, None]:\n        \"\"\"\n        Get sync database session\n        \"\"\"\n        try:\n            session_maker = cls._sync_session_maker()\n        except AttributeError:\n            cls.set_sync_db()\n            session_maker = cls._sync_session_maker()\n        with session_maker as sync_session:\n            yield sync_session\n</code></pre>"},{"location":"reference/app/db/#fractal_server.app.db.DB.get_async_db","title":"<code>get_async_db()</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get async database session</p> Source code in <code>fractal_server/app/db/__init__.py</code> <pre><code>@classmethod\nasync def get_async_db(cls) -&gt; AsyncGenerator[AsyncSession, None]:\n    \"\"\"\n    Get async database session\n    \"\"\"\n    try:\n        session_maker = cls._async_session_maker()\n    except AttributeError:\n        cls.set_async_db()\n        session_maker = cls._async_session_maker()\n    async with session_maker as async_session:\n        yield async_session\n</code></pre>"},{"location":"reference/app/db/#fractal_server.app.db.DB.get_sync_db","title":"<code>get_sync_db()</code>  <code>classmethod</code>","text":"<p>Get sync database session</p> Source code in <code>fractal_server/app/db/__init__.py</code> <pre><code>@classmethod\ndef get_sync_db(cls) -&gt; Generator[DBSyncSession, None, None]:\n    \"\"\"\n    Get sync database session\n    \"\"\"\n    try:\n        session_maker = cls._sync_session_maker()\n    except AttributeError:\n        cls.set_sync_db()\n        session_maker = cls._sync_session_maker()\n    with session_maker as sync_session:\n        yield sync_session\n</code></pre>"},{"location":"reference/app/models/","title":"models","text":"<p>Note that this module is imported from <code>fractal_server/migrations/env.py</code>, thus we should always export all relevant database models from here or they will not be picked up by alembic.</p>"},{"location":"reference/app/models/#fractal_server.app.models.AccountingRecord","title":"<code>AccountingRecord</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>AccountingRecord table.</p> Source code in <code>fractal_server/app/models/v2/accounting.py</code> <pre><code>class AccountingRecord(SQLModel, table=True):\n    \"\"\"\n    AccountingRecord table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    num_tasks: int\n    num_new_images: int\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.AccountingRecordSlurm","title":"<code>AccountingRecordSlurm</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>AccountingRecordSlurm table.</p> Source code in <code>fractal_server/app/models/v2/accounting.py</code> <pre><code>class AccountingRecordSlurm(SQLModel, table=True):\n    \"\"\"\n    AccountingRecordSlurm table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    slurm_job_ids: list[int] = Field(\n        default_factory=list,\n        sa_column=Column(ARRAY(Integer)),\n    )\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.DatasetV2","title":"<code>DatasetV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Dataset table.</p> Source code in <code>fractal_server/app/models/v2/dataset.py</code> <pre><code>class DatasetV2(SQLModel, table=True):\n    \"\"\"\n    Dataset table.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n    project_id: int = Field(foreign_key=\"projectv2.id\", ondelete=\"CASCADE\")\n    project: \"ProjectV2\" = Relationship(  # noqa: F821\n        sa_relationship_kwargs=dict(lazy=\"selectin\"),\n    )\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n\n    zarr_dir: str\n    images: list[dict[str, Any]] = Field(\n        sa_column=Column(JSONB, server_default=\"[]\", nullable=False)\n    )\n\n    @property\n    def image_zarr_urls(self) -&gt; list[str]:\n        return [image[\"zarr_url\"] for image in self.images]\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.HistoryImageCache","title":"<code>HistoryImageCache</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>HistoryImageCache table.</p> Source code in <code>fractal_server/app/models/v2/history.py</code> <pre><code>class HistoryImageCache(SQLModel, table=True):\n    \"\"\"\n    HistoryImageCache table.\n    \"\"\"\n\n    zarr_url: str = Field(primary_key=True)\n    dataset_id: int = Field(\n        primary_key=True,\n        foreign_key=\"datasetv2.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n    workflowtask_id: int = Field(\n        primary_key=True,\n        foreign_key=\"workflowtaskv2.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n\n    latest_history_unit_id: int = Field(\n        foreign_key=\"historyunit.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.HistoryRun","title":"<code>HistoryRun</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>HistoryRun table.</p> Source code in <code>fractal_server/app/models/v2/history.py</code> <pre><code>class HistoryRun(SQLModel, table=True):\n    \"\"\"\n    HistoryRun table.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: int | None = Field(default=None, primary_key=True)\n    dataset_id: int = Field(\n        foreign_key=\"datasetv2.id\",\n        ondelete=\"CASCADE\",\n    )\n    workflowtask_id: int | None = Field(\n        foreign_key=\"workflowtaskv2.id\",\n        default=None,\n        ondelete=\"SET NULL\",\n    )\n    job_id: int = Field(foreign_key=\"jobv2.id\")\n    task_id: int | None = Field(foreign_key=\"taskv2.id\", ondelete=\"SET NULL\")\n\n    workflowtask_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False),\n    )\n    task_group_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False),\n    )\n\n    timestamp_started: datetime = Field(\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n        default_factory=get_timestamp,\n    )\n    status: str\n    num_available_images: int\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.HistoryUnit","title":"<code>HistoryUnit</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>HistoryUnit table.</p> Source code in <code>fractal_server/app/models/v2/history.py</code> <pre><code>class HistoryUnit(SQLModel, table=True):\n    \"\"\"\n    HistoryUnit table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    history_run_id: int = Field(\n        foreign_key=\"historyrun.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n\n    logfile: str\n    status: str\n    zarr_urls: list[str] = Field(\n        sa_column=Column(ARRAY(String)),\n        default_factory=list,\n    )\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.JobV2","title":"<code>JobV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Job table.</p> Source code in <code>fractal_server/app/models/v2/job.py</code> <pre><code>class JobV2(SQLModel, table=True):\n    \"\"\"\n    Job table.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: int | None = Field(default=None, primary_key=True)\n    project_id: int | None = Field(\n        foreign_key=\"projectv2.id\", default=None, ondelete=\"SET NULL\"\n    )\n    workflow_id: int | None = Field(\n        foreign_key=\"workflowv2.id\", default=None, ondelete=\"SET NULL\"\n    )\n    dataset_id: int | None = Field(\n        foreign_key=\"datasetv2.id\", default=None, ondelete=\"SET NULL\"\n    )\n\n    user_email: str = Field(nullable=False)\n    slurm_account: str | None = None\n\n    dataset_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False)\n    )\n    workflow_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False)\n    )\n    project_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False)\n    )\n    fractal_server_version: str = Field(\n        sa_column=Column(String, server_default=\"pre-2.19.0\", nullable=False)\n    )\n\n    worker_init: str | None = None\n    working_dir: str | None = None\n    working_dir_user: str | None = None\n    first_task_index: int\n    last_task_index: int\n\n    start_timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    end_timestamp: datetime | None = Field(\n        default=None, sa_column=Column(DateTime(timezone=True))\n    )\n    status: str = JobStatusType.SUBMITTED\n    log: str | None = None\n    executor_error_log: str | None = None\n\n    attribute_filters: dict[str, list[int | float | str | bool]] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    type_filters: dict[str, bool] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n\n    __table_args__ = (\n        Index(\n            \"ix_jobv2_one_submitted_job_per_dataset\",\n            \"dataset_id\",\n            unique=True,\n            postgresql_where=text(f\"status = '{JobStatusType.SUBMITTED}'\"),\n        ),\n    )\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.OAuthAccount","title":"<code>OAuthAccount</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for OAuth accounts (<code>oauthaccount</code> database table).</p> <p>This class is based on fastapi_users_db_sqlmodel::SQLModelBaseOAuthAccount. Original Copyright: 2021 Fran\u00e7ois Voron, released under MIT licence.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int | None</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>user</code> <p> TYPE: <code>Optional[UserOAuth]</code> </p> <code>oauth_name</code> <p> TYPE: <code>str</code> </p> <code>access_token</code> <p> TYPE: <code>str</code> </p> <code>expires_at</code> <p> TYPE: <code>int | None</code> </p> <code>refresh_token</code> <p> TYPE: <code>str | None</code> </p> <code>account_id</code> <p> TYPE: <code>str</code> </p> <code>account_email</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class OAuthAccount(SQLModel, table=True):\n    \"\"\"\n    ORM model for OAuth accounts (`oauthaccount` database table).\n\n    This class is based on fastapi_users_db_sqlmodel::SQLModelBaseOAuthAccount.\n    Original Copyright: 2021 Fran\u00e7ois Voron, released under MIT licence.\n\n    Attributes:\n        id:\n        user_id:\n        user:\n        oauth_name:\n        access_token:\n        expires_at:\n        refresh_token:\n        account_id:\n        account_email:\n    \"\"\"\n\n    __tablename__ = \"oauthaccount\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    user: Optional[\"UserOAuth\"] = Relationship(back_populates=\"oauth_accounts\")\n    oauth_name: str = Field(index=True, nullable=False)\n    access_token: str = Field(nullable=False)\n    expires_at: int | None = Field(nullable=True, default=None)\n    refresh_token: str | None = Field(nullable=True, default=None)\n    account_id: str = Field(index=True, nullable=False)\n    account_email: str = Field(nullable=False)\n    model_config = ConfigDict(from_attributes=True)\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.Profile","title":"<code>Profile</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Profile table.</p> Source code in <code>fractal_server/app/models/v2/profile.py</code> <pre><code>class Profile(SQLModel, table=True):\n    \"\"\"\n    Profile table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    resource_id: int = Field(foreign_key=\"resource.id\", ondelete=\"RESTRICT\")\n\n    resource_type: str\n    \"\"\"\n    Type of resource (either `local`, `slurm_sudo` or `slurm_ssh`).\n    \"\"\"\n\n    name: str = Field(unique=True)\n    \"\"\"\n    Profile name.\n    \"\"\"\n\n    username: str | None = None\n    \"\"\"\n    Username to be impersonated, either via `sudo -u` or via `ssh`.\n    \"\"\"\n\n    ssh_key_path: str | None = None\n    \"\"\"\n    Path to the private SSH key of user `username` (only relevant if\n    `resource_type=\"slurm_ssh\"`).\n    \"\"\"\n\n    jobs_remote_dir: str | None = None\n    \"\"\"\n    Remote path of the job folder (only relevant if\n    `resource_type=\"slurm_ssh\"`).\n    \"\"\"\n\n    tasks_remote_dir: str | None = None\n    \"\"\"\n    Remote path of the task folder (only relevant if\n    `resource_type=\"slurm_ssh\"`).\n    \"\"\"\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.Profile.jobs_remote_dir","title":"<code>jobs_remote_dir = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remote path of the job folder (only relevant if <code>resource_type=\"slurm_ssh\"</code>).</p>"},{"location":"reference/app/models/#fractal_server.app.models.Profile.name","title":"<code>name = Field(unique=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Profile name.</p>"},{"location":"reference/app/models/#fractal_server.app.models.Profile.resource_type","title":"<code>resource_type</code>  <code>instance-attribute</code>","text":"<p>Type of resource (either <code>local</code>, <code>slurm_sudo</code> or <code>slurm_ssh</code>).</p>"},{"location":"reference/app/models/#fractal_server.app.models.Profile.ssh_key_path","title":"<code>ssh_key_path = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the private SSH key of user <code>username</code> (only relevant if <code>resource_type=\"slurm_ssh\"</code>).</p>"},{"location":"reference/app/models/#fractal_server.app.models.Profile.tasks_remote_dir","title":"<code>tasks_remote_dir = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remote path of the task folder (only relevant if <code>resource_type=\"slurm_ssh\"</code>).</p>"},{"location":"reference/app/models/#fractal_server.app.models.Profile.username","title":"<code>username = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Username to be impersonated, either via <code>sudo -u</code> or via <code>ssh</code>.</p>"},{"location":"reference/app/models/#fractal_server.app.models.ProjectV2","title":"<code>ProjectV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Project table.</p> Source code in <code>fractal_server/app/models/v2/project.py</code> <pre><code>class ProjectV2(SQLModel, table=True):\n    \"\"\"\n    Project table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n    resource_id: int = Field(foreign_key=\"resource.id\", ondelete=\"RESTRICT\")\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Resource table.</p> Source code in <code>fractal_server/app/models/v2/resource.py</code> <pre><code>class Resource(SQLModel, table=True):\n    \"\"\"\n    Resource table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    type: str\n    \"\"\"\n    One of `local`, `slurm_sudo` or `slurm_ssh` - matching with\n    `settings.FRACTAL_RUNNER_BACKEND`.\n    \"\"\"\n\n    name: str = Field(unique=True)\n    \"\"\"\n    Resource name.\n    \"\"\"\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    \"\"\"\n    Creation timestamp (autogenerated).\n    \"\"\"\n\n    host: str | None = None\n    \"\"\"\n    Address for ssh connections, when `type=\"slurm_ssh\"`.\n    \"\"\"\n\n    prevent_new_submissions: bool = Field(\n        sa_column=Column(\n            BOOLEAN,\n            server_default=\"false\",\n            nullable=False,\n        ),\n    )\n    \"\"\"\n    When set to true: Prevent new job submissions and stop execution of\n    ongoing jobs as soon as the current task is complete.\n    \"\"\"\n\n    jobs_local_dir: str\n    \"\"\"\n    Base local folder for job subfolders (containing artifacts and logs).\n    \"\"\"\n\n    jobs_runner_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Runner configuration, matching one of `JobRunnerConfigLocal` or\n    `JobRunnerConfigSLURM` schemas.\n    \"\"\"\n\n    jobs_slurm_python_worker: str | None = None\n    \"\"\"\n    On SLURM deloyments, this is the Python interpreter that runs the\n    `fractal-server` worker from within the SLURM jobs.\n    \"\"\"\n\n    jobs_poll_interval: int\n    \"\"\"\n    On SLURM resources: the interval to wait before new `squeue` calls.\n    On local resources: ignored.\n    \"\"\"\n\n    # task_settings\n    tasks_local_dir: str\n    \"\"\"\n    Base local folder for task-package subfolders.\n    \"\"\"\n\n    tasks_python_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Python configuration for task collection. Example:\n    ```json\n    {\n      \"default_version\": \"3.10\",\n      \"versions:{\n        \"3.10\": \"/xxx/venv-3.10/bin/python\",\n        \"3.11\": \"/xxx/venv-3.11/bin/python\",\n        \"3.12\": \"/xxx/venv-3.12/bin/python\"\n       }\n    }\n    ```\n    \"\"\"\n\n    tasks_pixi_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Pixi configuration for task collection. Basic example:\n    ```json\n    {\n        \"default_version\": \"0.41.0\",\n        \"versions\": {\n            \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n            \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n        },\n    }\n    ```\n    \"\"\"\n\n    @property\n    def pip_cache_dir_arg(self: Self) -&gt; str:\n        \"\"\"\n        If `pip_cache_dir` is set (in `self.tasks_python_config`), then\n        return `--cache_dir /something`; else return `--no-cache-dir`.\n        \"\"\"\n        _pip_cache_dir = self.tasks_python_config.get(\"pip_cache_dir\", None)\n        if _pip_cache_dir is not None:\n            return f\"--cache-dir {_pip_cache_dir}\"\n        else:\n            return \"--no-cache-dir\"\n\n    # Check constraints\n    __table_args__ = (\n        # `type` column must be one of \"local\", \"slurm_sudo\" or \"slurm_ssh\"\n        CheckConstraint(\n            \"type IN ('local', 'slurm_sudo', 'slurm_ssh')\",\n            name=\"correct_type\",\n        ),\n        # If `type` is not \"local\", `jobs_slurm_python_worker` must be set\n        CheckConstraint(\n            \"(type = 'local') OR (jobs_slurm_python_worker IS NOT NULL)\",\n            name=\"jobs_slurm_python_worker_set\",\n        ),\n    )\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.host","title":"<code>host = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address for ssh connections, when <code>type=\"slurm_ssh\"</code>.</p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.jobs_local_dir","title":"<code>jobs_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for job subfolders (containing artifacts and logs).</p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.jobs_poll_interval","title":"<code>jobs_poll_interval</code>  <code>instance-attribute</code>","text":"<p>On SLURM resources: the interval to wait before new <code>squeue</code> calls. On local resources: ignored.</p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.jobs_runner_config","title":"<code>jobs_runner_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Runner configuration, matching one of <code>JobRunnerConfigLocal</code> or <code>JobRunnerConfigSLURM</code> schemas.</p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.jobs_slurm_python_worker","title":"<code>jobs_slurm_python_worker = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>On SLURM deloyments, this is the Python interpreter that runs the <code>fractal-server</code> worker from within the SLURM jobs.</p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.name","title":"<code>name = Field(unique=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resource name.</p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.pip_cache_dir_arg","title":"<code>pip_cache_dir_arg</code>  <code>property</code>","text":"<p>If <code>pip_cache_dir</code> is set (in <code>self.tasks_python_config</code>), then return <code>--cache_dir /something</code>; else return <code>--no-cache-dir</code>.</p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.prevent_new_submissions","title":"<code>prevent_new_submissions = Field(sa_column=(Column(BOOLEAN, server_default='false', nullable=False)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When set to true: Prevent new job submissions and stop execution of ongoing jobs as soon as the current task is complete.</p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.tasks_local_dir","title":"<code>tasks_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for task-package subfolders.</p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.tasks_pixi_config","title":"<code>tasks_pixi_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Pixi configuration for task collection. Basic example: <pre><code>{\n    \"default_version\": \"0.41.0\",\n    \"versions\": {\n        \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n        \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n    },\n}\n</code></pre></p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.tasks_python_config","title":"<code>tasks_python_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Python configuration for task collection. Example: <pre><code>{\n  \"default_version\": \"3.10\",\n  \"versions:{\n    \"3.10\": \"/xxx/venv-3.10/bin/python\",\n    \"3.11\": \"/xxx/venv-3.11/bin/python\",\n    \"3.12\": \"/xxx/venv-3.12/bin/python\"\n   }\n}\n</code></pre></p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.timestamp_created","title":"<code>timestamp_created = Field(default_factory=get_timestamp, sa_column=(Column(DateTime(timezone=True), nullable=False)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Creation timestamp (autogenerated).</p>"},{"location":"reference/app/models/#fractal_server.app.models.Resource.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>One of <code>local</code>, <code>slurm_sudo</code> or <code>slurm_ssh</code> - matching with <code>settings.FRACTAL_RUNNER_BACKEND</code>.</p>"},{"location":"reference/app/models/#fractal_server.app.models.TaskGroupV2","title":"<code>TaskGroupV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/task_group.py</code> <pre><code>class TaskGroupV2(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    task_list: list[TaskV2] = Relationship(\n        sa_relationship_kwargs=dict(\n            lazy=\"selectin\", cascade=\"all, delete-orphan\"\n        ),\n    )\n\n    user_id: int = Field(foreign_key=\"user_oauth.id\")\n    user_group_id: int | None = Field(\n        foreign_key=\"usergroup.id\", default=None, ondelete=\"SET NULL\"\n    )\n    resource_id: int = Field(foreign_key=\"resource.id\", ondelete=\"RESTRICT\")\n\n    origin: str\n    pkg_name: str\n    version: str | None = None\n    python_version: str | None = None\n    pixi_version: str | None = None\n    path: str | None = None\n    archive_path: str | None = None\n    pip_extras: str | None = None\n    pinned_package_versions_pre: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    pinned_package_versions_post: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    env_info: str | None = None\n    venv_path: str | None = None\n\n    active: bool = True\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    timestamp_last_used: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(\n            DateTime(timezone=True),\n            nullable=False,\n            server_default=(\n                datetime(2024, 11, 20, tzinfo=timezone.utc).isoformat()\n            ),\n        ),\n    )\n\n    __table_args__ = (\n        Index(\n            \"ix_taskgroupv2_user_unique_constraint\",\n            \"user_id\",\n            \"pkg_name\",\n            \"version\",\n            \"resource_id\",\n            unique=True,\n            postgresql_nulls_not_distinct=True,\n        ),\n        Index(\n            \"ix_taskgroupv2_usergroup_unique_constraint\",\n            \"user_group_id\",\n            \"pkg_name\",\n            \"version\",\n            unique=True,\n            postgresql_nulls_not_distinct=True,\n            postgresql_where=column(\"user_group_id\").is_not(None),\n        ),\n        Index(\n            \"ix_taskgroupv2_path_unique_constraint\",\n            \"path\",\n            \"resource_id\",\n            unique=True,\n        ),\n    )\n\n    @property\n    def pip_install_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        extras = f\"[{self.pip_extras}]\" if self.pip_extras is not None else \"\"\n\n        if self.archive_path is not None:\n            return f\"{self.archive_path}{extras}\"\n        else:\n            if self.version is None:\n                raise ValueError(\n                    \"Cannot run `pip_install_string` with \"\n                    f\"{self.pkg_name=}, {self.archive_path=}, {self.version=}.\"\n                )\n            return f\"{self.pkg_name}{extras}=={self.version}\"\n\n    @property\n    def pinned_package_versions_pre_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_pre is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_pre)\n        return output\n\n    @property\n    def pinned_package_versions_post_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_post is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_post)\n        return output\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.TaskGroupV2.pinned_package_versions_post_string","title":"<code>pinned_package_versions_post_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/app/models/#fractal_server.app.models.TaskGroupV2.pinned_package_versions_pre_string","title":"<code>pinned_package_versions_pre_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/app/models/#fractal_server.app.models.TaskGroupV2.pip_install_string","title":"<code>pip_install_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/app/models/#fractal_server.app.models.TaskV2","title":"<code>TaskV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Model for the <code>taskv2</code> database table.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int | None</code> </p> <code>name</code> <p> TYPE: <code>str</code> </p> <code>type</code> <p> TYPE: <code>str</code> </p> <code>version</code> <p> TYPE: <code>str | None</code> </p> <code>command_non_parallel</code> <p> TYPE: <code>str | None</code> </p> <code>command_parallel</code> <p> TYPE: <code>str | None</code> </p> <code>meta_non_parallel</code> <p> TYPE: <code>dict[str, Any]</code> </p> <code>meta_parallel</code> <p> TYPE: <code>dict[str, Any]</code> </p> <code>input_types</code> <p> TYPE: <code>dict[str, bool]</code> </p> <code>output_types</code> <p> TYPE: <code>dict[str, bool]</code> </p> <code>taskgroupv2_id</code> <p> TYPE: <code>int</code> </p> <code>args_schema_version</code> <p> TYPE: <code>str | None</code> </p> <code>args_schema_non_parallel</code> <p> TYPE: <code>dict[str, Any] | None</code> </p> <code>args_schema_parallel</code> <p> TYPE: <code>dict[str, Any] | None</code> </p> <code>docs_info</code> <p> TYPE: <code>str | None</code> </p> <code>docs_link</code> <p> TYPE: <code>str | None</code> </p> <code>category</code> <p> TYPE: <code>str | None</code> </p> <code>modality</code> <p> TYPE: <code>str | None</code> </p> <code>authors</code> <p> TYPE: <code>str | None</code> </p> <code>tags</code> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/app/models/v2/task.py</code> <pre><code>class TaskV2(SQLModel, table=True):\n    \"\"\"\n    Model for the `taskv2` database table.\n\n    Attributes:\n        id:\n        name:\n        type:\n        version:\n        command_non_parallel:\n        command_parallel:\n        meta_non_parallel:\n        meta_parallel:\n        input_types:\n        output_types:\n        taskgroupv2_id:\n        args_schema_version:\n        args_schema_non_parallel:\n        args_schema_parallel:\n        docs_info:\n        docs_link:\n        category:\n        modality:\n        authors:\n        tags:\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n    type: str\n    command_non_parallel: str | None = None\n    command_parallel: str | None = None\n\n    meta_non_parallel: dict[str, Any] = Field(\n        sa_column=Column(JSON, server_default=\"{}\", default={}, nullable=False)\n    )\n    meta_parallel: dict[str, Any] = Field(\n        sa_column=Column(JSON, server_default=\"{}\", default={}, nullable=False)\n    )\n\n    version: str | None = None\n    args_schema_non_parallel: dict[str, Any] | None = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_parallel: dict[str, Any] | None = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_version: str | None = None\n    docs_info: str | None = None\n    docs_link: str | None = None\n\n    input_types: dict[str, bool] = Field(sa_column=Column(JSONB), default={})\n    output_types: dict[str, bool] = Field(sa_column=Column(JSONB), default={})\n\n    taskgroupv2_id: int = Field(foreign_key=\"taskgroupv2.id\")\n\n    category: str | None = None\n    modality: str | None = None\n    authors: str | None = None\n    tags: list[str] = Field(\n        sa_column=Column(JSONB, server_default=\"[]\", nullable=False)\n    )\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.UserGroup","title":"<code>UserGroup</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for the <code>usergroup</code> database table.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>ID of the group</p> <p> TYPE: <code>int | None</code> </p> <code>name</code> <p>Name of the group</p> <p> TYPE: <code>str</code> </p> <code>timestamp_created</code> <p>Time of creation</p> <p> TYPE: <code>datetime</code> </p> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class UserGroup(SQLModel, table=True):\n    \"\"\"\n    ORM model for the `usergroup` database table.\n\n    Attributes:\n        id: ID of the group\n        name: Name of the group\n        timestamp_created: Time of creation\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str = Field(unique=True)\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.UserOAuth","title":"<code>UserOAuth</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for the <code>user_oauth</code> database table.</p> <p>This class is a modification of <code>SQLModelBaseUserDB</code> from <code>fastapi_users_db_sqlmodel</code>. Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence.</p> <p>Note that several class attributes are the default ones from <code>fastapi-users</code> .</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int | None</code> </p> <code>email</code> <p> TYPE: <code>EmailStr</code> </p> <code>hashed_password</code> <p> TYPE: <code>str</code> </p> <code>is_active</code> <p>If this is <code>False</code>, the user has no access to the <code>/api/v2/</code> endpoints.</p> <p> TYPE: <code>bool</code> </p> <code>is_superuser</code> <p> TYPE: <code>bool</code> </p> <code>is_verified</code> <p>If this is <code>False</code>, the user has no access to the <code>/api/v2/</code> endpoints.</p> <p> TYPE: <code>bool</code> </p> <code>oauth_accounts</code> <p> TYPE: <code>list[OAuthAccount]</code> </p> <code>profile_id</code> <p>Foreign key linking the user to a <code>Profile</code>. If this is unset, the user has no access to the <code>/api/v2/</code> endpoints.</p> <p> TYPE: <code>int | None</code> </p> <code>project_dirs</code> <p>Absolute paths of the user's project directory. This is used (A) as a default base folder for the <code>zarr_dir</code> of new datasets (where the output Zarr are located), and (B) as a folder which is included by default in the paths that a user is allowed to stream (if the <code>fractal-data</code> integration is set up). two goals:</p> <p> TYPE: <code>list[str]</code> </p> <code>slurm_accounts</code> <p>List of SLURM accounts that the user can select upon running a job.</p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class UserOAuth(SQLModel, table=True):\n    \"\"\"\n    ORM model for the `user_oauth` database table.\n\n    This class is a modification of\n    [`SQLModelBaseUserDB`](https://github.com/fastapi-users/fastapi-users-db-sqlmodel/blob/83980d7f20886120f4636a102ab1822b4c366f63/fastapi_users_db_sqlmodel/__init__.py#L15-L32)\n    from `fastapi_users_db_sqlmodel`.\n    Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence.\n\n    Note that several class attributes are\n    [the default ones from `fastapi-users`\n    ](https://fastapi-users.github.io/fastapi-users/latest/configuration/schemas/).\n\n    Attributes:\n        id:\n        email:\n        hashed_password:\n        is_active:\n            If this is `False`, the user has no access to the `/api/v2/`\n            endpoints.\n        is_superuser:\n        is_verified:\n            If this is `False`, the user has no access to the `/api/v2/`\n            endpoints.\n        oauth_accounts:\n        profile_id:\n            Foreign key linking the user to a `Profile`. If this is unset,\n            the user has no access to the `/api/v2/` endpoints.\n        project_dirs:\n            Absolute paths of the user's project directory. This is used (A) as\n            a default base folder for the `zarr_dir` of new datasets (where\n            the output Zarr are located), and (B) as a folder which is included\n            by default in the paths that a user is allowed to stream (if the\n            `fractal-data` integration is set up).\n            two goals:\n        slurm_accounts:\n            List of SLURM accounts that the user can select upon running a job.\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n\n    __tablename__ = \"user_oauth\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    email: EmailStr = Field(\n        sa_column_kwargs={\"unique\": True, \"index\": True},\n        nullable=False,\n    )\n    hashed_password: str\n    is_active: bool = Field(default=True, nullable=False)\n    is_superuser: bool = Field(default=False, nullable=False)\n    is_verified: bool = Field(default=False, nullable=False)\n    is_guest: bool = Field(\n        sa_column=Column(\n            BOOLEAN,\n            server_default=\"false\",\n            nullable=False,\n        ),\n    )\n\n    oauth_accounts: list[\"OAuthAccount\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"lazy\": \"joined\", \"cascade\": \"all, delete\"},\n    )\n\n    profile_id: int | None = Field(\n        foreign_key=\"profile.id\",\n        default=None,\n        ondelete=\"RESTRICT\",\n    )\n\n    project_dirs: list[str] = Field(\n        sa_column=Column(ARRAY(String), nullable=False),\n    )\n\n    slurm_accounts: list[str] = Field(\n        sa_column=Column(ARRAY(String), server_default=\"{}\"),\n    )\n\n    __table_args__ = (\n        CheckConstraint(\n            \"NOT (is_superuser AND is_guest)\",\n            name=\"superuser_is_not_guest\",\n        ),\n    )\n</code></pre>"},{"location":"reference/app/models/#fractal_server.app.models.get_timestamp","title":"<code>get_timestamp()</code>","text":"<p>Get timezone aware timestamp.</p> Source code in <code>fractal_server/utils.py</code> <pre><code>def get_timestamp() -&gt; datetime:\n    \"\"\"\n    Get timezone aware timestamp.\n    \"\"\"\n    return datetime.now(tz=timezone.utc)\n</code></pre>"},{"location":"reference/app/models/linkusergroup/","title":"linkusergroup","text":""},{"location":"reference/app/models/linkusergroup/#fractal_server.app.models.linkusergroup.LinkUserGroup","title":"<code>LinkUserGroup</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Crossing table between User and UserGroup</p> Source code in <code>fractal_server/app/models/linkusergroup.py</code> <pre><code>class LinkUserGroup(SQLModel, table=True):\n    \"\"\"\n    Crossing table between User and UserGroup\n    \"\"\"\n\n    group_id: int = Field(\n        foreign_key=\"usergroup.id\", primary_key=True, ondelete=\"CASCADE\"\n    )\n    user_id: int = Field(\n        foreign_key=\"user_oauth.id\", primary_key=True, ondelete=\"CASCADE\"\n    )\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n</code></pre>"},{"location":"reference/app/models/linkuserproject/","title":"linkuserproject","text":""},{"location":"reference/app/models/linkuserproject/#fractal_server.app.models.linkuserproject.LinkUserProjectV2","title":"<code>LinkUserProjectV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Crossing table between User and ProjectV2</p> Source code in <code>fractal_server/app/models/linkuserproject.py</code> <pre><code>class LinkUserProjectV2(SQLModel, table=True):\n    \"\"\"\n    Crossing table between User and ProjectV2\n    \"\"\"\n\n    project_id: int = Field(\n        foreign_key=\"projectv2.id\", primary_key=True, ondelete=\"CASCADE\"\n    )\n    user_id: int = Field(foreign_key=\"user_oauth.id\", primary_key=True)\n\n    is_owner: bool\n    is_verified: bool\n    permissions: str\n\n    __table_args__ = (\n        Index(\n            \"ix_linkuserprojectv2_one_owner_per_project\",\n            \"project_id\",\n            unique=True,\n            postgresql_where=column(\"is_owner\").is_(True),\n        ),\n        CheckConstraint(\n            \"NOT (is_owner AND NOT is_verified)\",\n            name=\"owner_is_verified\",\n        ),\n        CheckConstraint(\n            \"NOT (is_owner AND permissions &lt;&gt; 'rwx')\",\n            name=\"owner_full_permissions\",\n        ),\n        CheckConstraint(\n            \"permissions IN ('r', 'rw', 'rwx')\",\n            name=\"valid_permissions\",\n        ),\n    )\n</code></pre>"},{"location":"reference/app/models/security/","title":"security","text":""},{"location":"reference/app/models/security/#fractal_server.app.models.security.OAuthAccount","title":"<code>OAuthAccount</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for OAuth accounts (<code>oauthaccount</code> database table).</p> <p>This class is based on fastapi_users_db_sqlmodel::SQLModelBaseOAuthAccount. Original Copyright: 2021 Fran\u00e7ois Voron, released under MIT licence.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int | None</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>user</code> <p> TYPE: <code>Optional[UserOAuth]</code> </p> <code>oauth_name</code> <p> TYPE: <code>str</code> </p> <code>access_token</code> <p> TYPE: <code>str</code> </p> <code>expires_at</code> <p> TYPE: <code>int | None</code> </p> <code>refresh_token</code> <p> TYPE: <code>str | None</code> </p> <code>account_id</code> <p> TYPE: <code>str</code> </p> <code>account_email</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class OAuthAccount(SQLModel, table=True):\n    \"\"\"\n    ORM model for OAuth accounts (`oauthaccount` database table).\n\n    This class is based on fastapi_users_db_sqlmodel::SQLModelBaseOAuthAccount.\n    Original Copyright: 2021 Fran\u00e7ois Voron, released under MIT licence.\n\n    Attributes:\n        id:\n        user_id:\n        user:\n        oauth_name:\n        access_token:\n        expires_at:\n        refresh_token:\n        account_id:\n        account_email:\n    \"\"\"\n\n    __tablename__ = \"oauthaccount\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    user: Optional[\"UserOAuth\"] = Relationship(back_populates=\"oauth_accounts\")\n    oauth_name: str = Field(index=True, nullable=False)\n    access_token: str = Field(nullable=False)\n    expires_at: int | None = Field(nullable=True, default=None)\n    refresh_token: str | None = Field(nullable=True, default=None)\n    account_id: str = Field(index=True, nullable=False)\n    account_email: str = Field(nullable=False)\n    model_config = ConfigDict(from_attributes=True)\n</code></pre>"},{"location":"reference/app/models/security/#fractal_server.app.models.security.UserGroup","title":"<code>UserGroup</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for the <code>usergroup</code> database table.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>ID of the group</p> <p> TYPE: <code>int | None</code> </p> <code>name</code> <p>Name of the group</p> <p> TYPE: <code>str</code> </p> <code>timestamp_created</code> <p>Time of creation</p> <p> TYPE: <code>datetime</code> </p> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class UserGroup(SQLModel, table=True):\n    \"\"\"\n    ORM model for the `usergroup` database table.\n\n    Attributes:\n        id: ID of the group\n        name: Name of the group\n        timestamp_created: Time of creation\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str = Field(unique=True)\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n</code></pre>"},{"location":"reference/app/models/security/#fractal_server.app.models.security.UserOAuth","title":"<code>UserOAuth</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>ORM model for the <code>user_oauth</code> database table.</p> <p>This class is a modification of <code>SQLModelBaseUserDB</code> from <code>fastapi_users_db_sqlmodel</code>. Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence.</p> <p>Note that several class attributes are the default ones from <code>fastapi-users</code> .</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int | None</code> </p> <code>email</code> <p> TYPE: <code>EmailStr</code> </p> <code>hashed_password</code> <p> TYPE: <code>str</code> </p> <code>is_active</code> <p>If this is <code>False</code>, the user has no access to the <code>/api/v2/</code> endpoints.</p> <p> TYPE: <code>bool</code> </p> <code>is_superuser</code> <p> TYPE: <code>bool</code> </p> <code>is_verified</code> <p>If this is <code>False</code>, the user has no access to the <code>/api/v2/</code> endpoints.</p> <p> TYPE: <code>bool</code> </p> <code>oauth_accounts</code> <p> TYPE: <code>list[OAuthAccount]</code> </p> <code>profile_id</code> <p>Foreign key linking the user to a <code>Profile</code>. If this is unset, the user has no access to the <code>/api/v2/</code> endpoints.</p> <p> TYPE: <code>int | None</code> </p> <code>project_dirs</code> <p>Absolute paths of the user's project directory. This is used (A) as a default base folder for the <code>zarr_dir</code> of new datasets (where the output Zarr are located), and (B) as a folder which is included by default in the paths that a user is allowed to stream (if the <code>fractal-data</code> integration is set up). two goals:</p> <p> TYPE: <code>list[str]</code> </p> <code>slurm_accounts</code> <p>List of SLURM accounts that the user can select upon running a job.</p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/app/models/security.py</code> <pre><code>class UserOAuth(SQLModel, table=True):\n    \"\"\"\n    ORM model for the `user_oauth` database table.\n\n    This class is a modification of\n    [`SQLModelBaseUserDB`](https://github.com/fastapi-users/fastapi-users-db-sqlmodel/blob/83980d7f20886120f4636a102ab1822b4c366f63/fastapi_users_db_sqlmodel/__init__.py#L15-L32)\n    from `fastapi_users_db_sqlmodel`.\n    Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence.\n\n    Note that several class attributes are\n    [the default ones from `fastapi-users`\n    ](https://fastapi-users.github.io/fastapi-users/latest/configuration/schemas/).\n\n    Attributes:\n        id:\n        email:\n        hashed_password:\n        is_active:\n            If this is `False`, the user has no access to the `/api/v2/`\n            endpoints.\n        is_superuser:\n        is_verified:\n            If this is `False`, the user has no access to the `/api/v2/`\n            endpoints.\n        oauth_accounts:\n        profile_id:\n            Foreign key linking the user to a `Profile`. If this is unset,\n            the user has no access to the `/api/v2/` endpoints.\n        project_dirs:\n            Absolute paths of the user's project directory. This is used (A) as\n            a default base folder for the `zarr_dir` of new datasets (where\n            the output Zarr are located), and (B) as a folder which is included\n            by default in the paths that a user is allowed to stream (if the\n            `fractal-data` integration is set up).\n            two goals:\n        slurm_accounts:\n            List of SLURM accounts that the user can select upon running a job.\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n\n    __tablename__ = \"user_oauth\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    email: EmailStr = Field(\n        sa_column_kwargs={\"unique\": True, \"index\": True},\n        nullable=False,\n    )\n    hashed_password: str\n    is_active: bool = Field(default=True, nullable=False)\n    is_superuser: bool = Field(default=False, nullable=False)\n    is_verified: bool = Field(default=False, nullable=False)\n    is_guest: bool = Field(\n        sa_column=Column(\n            BOOLEAN,\n            server_default=\"false\",\n            nullable=False,\n        ),\n    )\n\n    oauth_accounts: list[\"OAuthAccount\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"lazy\": \"joined\", \"cascade\": \"all, delete\"},\n    )\n\n    profile_id: int | None = Field(\n        foreign_key=\"profile.id\",\n        default=None,\n        ondelete=\"RESTRICT\",\n    )\n\n    project_dirs: list[str] = Field(\n        sa_column=Column(ARRAY(String), nullable=False),\n    )\n\n    slurm_accounts: list[str] = Field(\n        sa_column=Column(ARRAY(String), server_default=\"{}\"),\n    )\n\n    __table_args__ = (\n        CheckConstraint(\n            \"NOT (is_superuser AND is_guest)\",\n            name=\"superuser_is_not_guest\",\n        ),\n    )\n</code></pre>"},{"location":"reference/app/models/v2/","title":"v2","text":"<p>v2 <code>models</code> module</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.AccountingRecord","title":"<code>AccountingRecord</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>AccountingRecord table.</p> Source code in <code>fractal_server/app/models/v2/accounting.py</code> <pre><code>class AccountingRecord(SQLModel, table=True):\n    \"\"\"\n    AccountingRecord table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    num_tasks: int\n    num_new_images: int\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.AccountingRecordSlurm","title":"<code>AccountingRecordSlurm</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>AccountingRecordSlurm table.</p> Source code in <code>fractal_server/app/models/v2/accounting.py</code> <pre><code>class AccountingRecordSlurm(SQLModel, table=True):\n    \"\"\"\n    AccountingRecordSlurm table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    slurm_job_ids: list[int] = Field(\n        default_factory=list,\n        sa_column=Column(ARRAY(Integer)),\n    )\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.DatasetV2","title":"<code>DatasetV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Dataset table.</p> Source code in <code>fractal_server/app/models/v2/dataset.py</code> <pre><code>class DatasetV2(SQLModel, table=True):\n    \"\"\"\n    Dataset table.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n    project_id: int = Field(foreign_key=\"projectv2.id\", ondelete=\"CASCADE\")\n    project: \"ProjectV2\" = Relationship(  # noqa: F821\n        sa_relationship_kwargs=dict(lazy=\"selectin\"),\n    )\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n\n    zarr_dir: str\n    images: list[dict[str, Any]] = Field(\n        sa_column=Column(JSONB, server_default=\"[]\", nullable=False)\n    )\n\n    @property\n    def image_zarr_urls(self) -&gt; list[str]:\n        return [image[\"zarr_url\"] for image in self.images]\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.HistoryImageCache","title":"<code>HistoryImageCache</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>HistoryImageCache table.</p> Source code in <code>fractal_server/app/models/v2/history.py</code> <pre><code>class HistoryImageCache(SQLModel, table=True):\n    \"\"\"\n    HistoryImageCache table.\n    \"\"\"\n\n    zarr_url: str = Field(primary_key=True)\n    dataset_id: int = Field(\n        primary_key=True,\n        foreign_key=\"datasetv2.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n    workflowtask_id: int = Field(\n        primary_key=True,\n        foreign_key=\"workflowtaskv2.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n\n    latest_history_unit_id: int = Field(\n        foreign_key=\"historyunit.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.HistoryRun","title":"<code>HistoryRun</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>HistoryRun table.</p> Source code in <code>fractal_server/app/models/v2/history.py</code> <pre><code>class HistoryRun(SQLModel, table=True):\n    \"\"\"\n    HistoryRun table.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: int | None = Field(default=None, primary_key=True)\n    dataset_id: int = Field(\n        foreign_key=\"datasetv2.id\",\n        ondelete=\"CASCADE\",\n    )\n    workflowtask_id: int | None = Field(\n        foreign_key=\"workflowtaskv2.id\",\n        default=None,\n        ondelete=\"SET NULL\",\n    )\n    job_id: int = Field(foreign_key=\"jobv2.id\")\n    task_id: int | None = Field(foreign_key=\"taskv2.id\", ondelete=\"SET NULL\")\n\n    workflowtask_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False),\n    )\n    task_group_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False),\n    )\n\n    timestamp_started: datetime = Field(\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n        default_factory=get_timestamp,\n    )\n    status: str\n    num_available_images: int\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.HistoryUnit","title":"<code>HistoryUnit</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>HistoryUnit table.</p> Source code in <code>fractal_server/app/models/v2/history.py</code> <pre><code>class HistoryUnit(SQLModel, table=True):\n    \"\"\"\n    HistoryUnit table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    history_run_id: int = Field(\n        foreign_key=\"historyrun.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n\n    logfile: str\n    status: str\n    zarr_urls: list[str] = Field(\n        sa_column=Column(ARRAY(String)),\n        default_factory=list,\n    )\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.JobV2","title":"<code>JobV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Job table.</p> Source code in <code>fractal_server/app/models/v2/job.py</code> <pre><code>class JobV2(SQLModel, table=True):\n    \"\"\"\n    Job table.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: int | None = Field(default=None, primary_key=True)\n    project_id: int | None = Field(\n        foreign_key=\"projectv2.id\", default=None, ondelete=\"SET NULL\"\n    )\n    workflow_id: int | None = Field(\n        foreign_key=\"workflowv2.id\", default=None, ondelete=\"SET NULL\"\n    )\n    dataset_id: int | None = Field(\n        foreign_key=\"datasetv2.id\", default=None, ondelete=\"SET NULL\"\n    )\n\n    user_email: str = Field(nullable=False)\n    slurm_account: str | None = None\n\n    dataset_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False)\n    )\n    workflow_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False)\n    )\n    project_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False)\n    )\n    fractal_server_version: str = Field(\n        sa_column=Column(String, server_default=\"pre-2.19.0\", nullable=False)\n    )\n\n    worker_init: str | None = None\n    working_dir: str | None = None\n    working_dir_user: str | None = None\n    first_task_index: int\n    last_task_index: int\n\n    start_timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    end_timestamp: datetime | None = Field(\n        default=None, sa_column=Column(DateTime(timezone=True))\n    )\n    status: str = JobStatusType.SUBMITTED\n    log: str | None = None\n    executor_error_log: str | None = None\n\n    attribute_filters: dict[str, list[int | float | str | bool]] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    type_filters: dict[str, bool] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n\n    __table_args__ = (\n        Index(\n            \"ix_jobv2_one_submitted_job_per_dataset\",\n            \"dataset_id\",\n            unique=True,\n            postgresql_where=text(f\"status = '{JobStatusType.SUBMITTED}'\"),\n        ),\n    )\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.LinkUserProjectV2","title":"<code>LinkUserProjectV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Crossing table between User and ProjectV2</p> Source code in <code>fractal_server/app/models/linkuserproject.py</code> <pre><code>class LinkUserProjectV2(SQLModel, table=True):\n    \"\"\"\n    Crossing table between User and ProjectV2\n    \"\"\"\n\n    project_id: int = Field(\n        foreign_key=\"projectv2.id\", primary_key=True, ondelete=\"CASCADE\"\n    )\n    user_id: int = Field(foreign_key=\"user_oauth.id\", primary_key=True)\n\n    is_owner: bool\n    is_verified: bool\n    permissions: str\n\n    __table_args__ = (\n        Index(\n            \"ix_linkuserprojectv2_one_owner_per_project\",\n            \"project_id\",\n            unique=True,\n            postgresql_where=column(\"is_owner\").is_(True),\n        ),\n        CheckConstraint(\n            \"NOT (is_owner AND NOT is_verified)\",\n            name=\"owner_is_verified\",\n        ),\n        CheckConstraint(\n            \"NOT (is_owner AND permissions &lt;&gt; 'rwx')\",\n            name=\"owner_full_permissions\",\n        ),\n        CheckConstraint(\n            \"permissions IN ('r', 'rw', 'rwx')\",\n            name=\"valid_permissions\",\n        ),\n    )\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Profile","title":"<code>Profile</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Profile table.</p> Source code in <code>fractal_server/app/models/v2/profile.py</code> <pre><code>class Profile(SQLModel, table=True):\n    \"\"\"\n    Profile table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    resource_id: int = Field(foreign_key=\"resource.id\", ondelete=\"RESTRICT\")\n\n    resource_type: str\n    \"\"\"\n    Type of resource (either `local`, `slurm_sudo` or `slurm_ssh`).\n    \"\"\"\n\n    name: str = Field(unique=True)\n    \"\"\"\n    Profile name.\n    \"\"\"\n\n    username: str | None = None\n    \"\"\"\n    Username to be impersonated, either via `sudo -u` or via `ssh`.\n    \"\"\"\n\n    ssh_key_path: str | None = None\n    \"\"\"\n    Path to the private SSH key of user `username` (only relevant if\n    `resource_type=\"slurm_ssh\"`).\n    \"\"\"\n\n    jobs_remote_dir: str | None = None\n    \"\"\"\n    Remote path of the job folder (only relevant if\n    `resource_type=\"slurm_ssh\"`).\n    \"\"\"\n\n    tasks_remote_dir: str | None = None\n    \"\"\"\n    Remote path of the task folder (only relevant if\n    `resource_type=\"slurm_ssh\"`).\n    \"\"\"\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Profile.jobs_remote_dir","title":"<code>jobs_remote_dir = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remote path of the job folder (only relevant if <code>resource_type=\"slurm_ssh\"</code>).</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Profile.name","title":"<code>name = Field(unique=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Profile name.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Profile.resource_type","title":"<code>resource_type</code>  <code>instance-attribute</code>","text":"<p>Type of resource (either <code>local</code>, <code>slurm_sudo</code> or <code>slurm_ssh</code>).</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Profile.ssh_key_path","title":"<code>ssh_key_path = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the private SSH key of user <code>username</code> (only relevant if <code>resource_type=\"slurm_ssh\"</code>).</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Profile.tasks_remote_dir","title":"<code>tasks_remote_dir = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remote path of the task folder (only relevant if <code>resource_type=\"slurm_ssh\"</code>).</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Profile.username","title":"<code>username = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Username to be impersonated, either via <code>sudo -u</code> or via <code>ssh</code>.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.ProjectV2","title":"<code>ProjectV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Project table.</p> Source code in <code>fractal_server/app/models/v2/project.py</code> <pre><code>class ProjectV2(SQLModel, table=True):\n    \"\"\"\n    Project table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n    resource_id: int = Field(foreign_key=\"resource.id\", ondelete=\"RESTRICT\")\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Resource table.</p> Source code in <code>fractal_server/app/models/v2/resource.py</code> <pre><code>class Resource(SQLModel, table=True):\n    \"\"\"\n    Resource table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    type: str\n    \"\"\"\n    One of `local`, `slurm_sudo` or `slurm_ssh` - matching with\n    `settings.FRACTAL_RUNNER_BACKEND`.\n    \"\"\"\n\n    name: str = Field(unique=True)\n    \"\"\"\n    Resource name.\n    \"\"\"\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    \"\"\"\n    Creation timestamp (autogenerated).\n    \"\"\"\n\n    host: str | None = None\n    \"\"\"\n    Address for ssh connections, when `type=\"slurm_ssh\"`.\n    \"\"\"\n\n    prevent_new_submissions: bool = Field(\n        sa_column=Column(\n            BOOLEAN,\n            server_default=\"false\",\n            nullable=False,\n        ),\n    )\n    \"\"\"\n    When set to true: Prevent new job submissions and stop execution of\n    ongoing jobs as soon as the current task is complete.\n    \"\"\"\n\n    jobs_local_dir: str\n    \"\"\"\n    Base local folder for job subfolders (containing artifacts and logs).\n    \"\"\"\n\n    jobs_runner_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Runner configuration, matching one of `JobRunnerConfigLocal` or\n    `JobRunnerConfigSLURM` schemas.\n    \"\"\"\n\n    jobs_slurm_python_worker: str | None = None\n    \"\"\"\n    On SLURM deloyments, this is the Python interpreter that runs the\n    `fractal-server` worker from within the SLURM jobs.\n    \"\"\"\n\n    jobs_poll_interval: int\n    \"\"\"\n    On SLURM resources: the interval to wait before new `squeue` calls.\n    On local resources: ignored.\n    \"\"\"\n\n    # task_settings\n    tasks_local_dir: str\n    \"\"\"\n    Base local folder for task-package subfolders.\n    \"\"\"\n\n    tasks_python_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Python configuration for task collection. Example:\n    ```json\n    {\n      \"default_version\": \"3.10\",\n      \"versions:{\n        \"3.10\": \"/xxx/venv-3.10/bin/python\",\n        \"3.11\": \"/xxx/venv-3.11/bin/python\",\n        \"3.12\": \"/xxx/venv-3.12/bin/python\"\n       }\n    }\n    ```\n    \"\"\"\n\n    tasks_pixi_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Pixi configuration for task collection. Basic example:\n    ```json\n    {\n        \"default_version\": \"0.41.0\",\n        \"versions\": {\n            \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n            \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n        },\n    }\n    ```\n    \"\"\"\n\n    @property\n    def pip_cache_dir_arg(self: Self) -&gt; str:\n        \"\"\"\n        If `pip_cache_dir` is set (in `self.tasks_python_config`), then\n        return `--cache_dir /something`; else return `--no-cache-dir`.\n        \"\"\"\n        _pip_cache_dir = self.tasks_python_config.get(\"pip_cache_dir\", None)\n        if _pip_cache_dir is not None:\n            return f\"--cache-dir {_pip_cache_dir}\"\n        else:\n            return \"--no-cache-dir\"\n\n    # Check constraints\n    __table_args__ = (\n        # `type` column must be one of \"local\", \"slurm_sudo\" or \"slurm_ssh\"\n        CheckConstraint(\n            \"type IN ('local', 'slurm_sudo', 'slurm_ssh')\",\n            name=\"correct_type\",\n        ),\n        # If `type` is not \"local\", `jobs_slurm_python_worker` must be set\n        CheckConstraint(\n            \"(type = 'local') OR (jobs_slurm_python_worker IS NOT NULL)\",\n            name=\"jobs_slurm_python_worker_set\",\n        ),\n    )\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.host","title":"<code>host = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address for ssh connections, when <code>type=\"slurm_ssh\"</code>.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.jobs_local_dir","title":"<code>jobs_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for job subfolders (containing artifacts and logs).</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.jobs_poll_interval","title":"<code>jobs_poll_interval</code>  <code>instance-attribute</code>","text":"<p>On SLURM resources: the interval to wait before new <code>squeue</code> calls. On local resources: ignored.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.jobs_runner_config","title":"<code>jobs_runner_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Runner configuration, matching one of <code>JobRunnerConfigLocal</code> or <code>JobRunnerConfigSLURM</code> schemas.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.jobs_slurm_python_worker","title":"<code>jobs_slurm_python_worker = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>On SLURM deloyments, this is the Python interpreter that runs the <code>fractal-server</code> worker from within the SLURM jobs.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.name","title":"<code>name = Field(unique=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resource name.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.pip_cache_dir_arg","title":"<code>pip_cache_dir_arg</code>  <code>property</code>","text":"<p>If <code>pip_cache_dir</code> is set (in <code>self.tasks_python_config</code>), then return <code>--cache_dir /something</code>; else return <code>--no-cache-dir</code>.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.prevent_new_submissions","title":"<code>prevent_new_submissions = Field(sa_column=(Column(BOOLEAN, server_default='false', nullable=False)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When set to true: Prevent new job submissions and stop execution of ongoing jobs as soon as the current task is complete.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.tasks_local_dir","title":"<code>tasks_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for task-package subfolders.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.tasks_pixi_config","title":"<code>tasks_pixi_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Pixi configuration for task collection. Basic example: <pre><code>{\n    \"default_version\": \"0.41.0\",\n    \"versions\": {\n        \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n        \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n    },\n}\n</code></pre></p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.tasks_python_config","title":"<code>tasks_python_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Python configuration for task collection. Example: <pre><code>{\n  \"default_version\": \"3.10\",\n  \"versions:{\n    \"3.10\": \"/xxx/venv-3.10/bin/python\",\n    \"3.11\": \"/xxx/venv-3.11/bin/python\",\n    \"3.12\": \"/xxx/venv-3.12/bin/python\"\n   }\n}\n</code></pre></p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.timestamp_created","title":"<code>timestamp_created = Field(default_factory=get_timestamp, sa_column=(Column(DateTime(timezone=True), nullable=False)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Creation timestamp (autogenerated).</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.Resource.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>One of <code>local</code>, <code>slurm_sudo</code> or <code>slurm_ssh</code> - matching with <code>settings.FRACTAL_RUNNER_BACKEND</code>.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.TaskGroupV2","title":"<code>TaskGroupV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/task_group.py</code> <pre><code>class TaskGroupV2(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    task_list: list[TaskV2] = Relationship(\n        sa_relationship_kwargs=dict(\n            lazy=\"selectin\", cascade=\"all, delete-orphan\"\n        ),\n    )\n\n    user_id: int = Field(foreign_key=\"user_oauth.id\")\n    user_group_id: int | None = Field(\n        foreign_key=\"usergroup.id\", default=None, ondelete=\"SET NULL\"\n    )\n    resource_id: int = Field(foreign_key=\"resource.id\", ondelete=\"RESTRICT\")\n\n    origin: str\n    pkg_name: str\n    version: str | None = None\n    python_version: str | None = None\n    pixi_version: str | None = None\n    path: str | None = None\n    archive_path: str | None = None\n    pip_extras: str | None = None\n    pinned_package_versions_pre: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    pinned_package_versions_post: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    env_info: str | None = None\n    venv_path: str | None = None\n\n    active: bool = True\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    timestamp_last_used: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(\n            DateTime(timezone=True),\n            nullable=False,\n            server_default=(\n                datetime(2024, 11, 20, tzinfo=timezone.utc).isoformat()\n            ),\n        ),\n    )\n\n    __table_args__ = (\n        Index(\n            \"ix_taskgroupv2_user_unique_constraint\",\n            \"user_id\",\n            \"pkg_name\",\n            \"version\",\n            \"resource_id\",\n            unique=True,\n            postgresql_nulls_not_distinct=True,\n        ),\n        Index(\n            \"ix_taskgroupv2_usergroup_unique_constraint\",\n            \"user_group_id\",\n            \"pkg_name\",\n            \"version\",\n            unique=True,\n            postgresql_nulls_not_distinct=True,\n            postgresql_where=column(\"user_group_id\").is_not(None),\n        ),\n        Index(\n            \"ix_taskgroupv2_path_unique_constraint\",\n            \"path\",\n            \"resource_id\",\n            unique=True,\n        ),\n    )\n\n    @property\n    def pip_install_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        extras = f\"[{self.pip_extras}]\" if self.pip_extras is not None else \"\"\n\n        if self.archive_path is not None:\n            return f\"{self.archive_path}{extras}\"\n        else:\n            if self.version is None:\n                raise ValueError(\n                    \"Cannot run `pip_install_string` with \"\n                    f\"{self.pkg_name=}, {self.archive_path=}, {self.version=}.\"\n                )\n            return f\"{self.pkg_name}{extras}=={self.version}\"\n\n    @property\n    def pinned_package_versions_pre_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_pre is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_pre)\n        return output\n\n    @property\n    def pinned_package_versions_post_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_post is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_post)\n        return output\n</code></pre>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.TaskGroupV2.pinned_package_versions_post_string","title":"<code>pinned_package_versions_post_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.TaskGroupV2.pinned_package_versions_pre_string","title":"<code>pinned_package_versions_pre_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.TaskGroupV2.pip_install_string","title":"<code>pip_install_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/app/models/v2/#fractal_server.app.models.v2.TaskV2","title":"<code>TaskV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Model for the <code>taskv2</code> database table.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int | None</code> </p> <code>name</code> <p> TYPE: <code>str</code> </p> <code>type</code> <p> TYPE: <code>str</code> </p> <code>version</code> <p> TYPE: <code>str | None</code> </p> <code>command_non_parallel</code> <p> TYPE: <code>str | None</code> </p> <code>command_parallel</code> <p> TYPE: <code>str | None</code> </p> <code>meta_non_parallel</code> <p> TYPE: <code>dict[str, Any]</code> </p> <code>meta_parallel</code> <p> TYPE: <code>dict[str, Any]</code> </p> <code>input_types</code> <p> TYPE: <code>dict[str, bool]</code> </p> <code>output_types</code> <p> TYPE: <code>dict[str, bool]</code> </p> <code>taskgroupv2_id</code> <p> TYPE: <code>int</code> </p> <code>args_schema_version</code> <p> TYPE: <code>str | None</code> </p> <code>args_schema_non_parallel</code> <p> TYPE: <code>dict[str, Any] | None</code> </p> <code>args_schema_parallel</code> <p> TYPE: <code>dict[str, Any] | None</code> </p> <code>docs_info</code> <p> TYPE: <code>str | None</code> </p> <code>docs_link</code> <p> TYPE: <code>str | None</code> </p> <code>category</code> <p> TYPE: <code>str | None</code> </p> <code>modality</code> <p> TYPE: <code>str | None</code> </p> <code>authors</code> <p> TYPE: <code>str | None</code> </p> <code>tags</code> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/app/models/v2/task.py</code> <pre><code>class TaskV2(SQLModel, table=True):\n    \"\"\"\n    Model for the `taskv2` database table.\n\n    Attributes:\n        id:\n        name:\n        type:\n        version:\n        command_non_parallel:\n        command_parallel:\n        meta_non_parallel:\n        meta_parallel:\n        input_types:\n        output_types:\n        taskgroupv2_id:\n        args_schema_version:\n        args_schema_non_parallel:\n        args_schema_parallel:\n        docs_info:\n        docs_link:\n        category:\n        modality:\n        authors:\n        tags:\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n    type: str\n    command_non_parallel: str | None = None\n    command_parallel: str | None = None\n\n    meta_non_parallel: dict[str, Any] = Field(\n        sa_column=Column(JSON, server_default=\"{}\", default={}, nullable=False)\n    )\n    meta_parallel: dict[str, Any] = Field(\n        sa_column=Column(JSON, server_default=\"{}\", default={}, nullable=False)\n    )\n\n    version: str | None = None\n    args_schema_non_parallel: dict[str, Any] | None = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_parallel: dict[str, Any] | None = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_version: str | None = None\n    docs_info: str | None = None\n    docs_link: str | None = None\n\n    input_types: dict[str, bool] = Field(sa_column=Column(JSONB), default={})\n    output_types: dict[str, bool] = Field(sa_column=Column(JSONB), default={})\n\n    taskgroupv2_id: int = Field(foreign_key=\"taskgroupv2.id\")\n\n    category: str | None = None\n    modality: str | None = None\n    authors: str | None = None\n    tags: list[str] = Field(\n        sa_column=Column(JSONB, server_default=\"[]\", nullable=False)\n    )\n</code></pre>"},{"location":"reference/app/models/v2/accounting/","title":"accounting","text":""},{"location":"reference/app/models/v2/accounting/#fractal_server.app.models.v2.accounting.AccountingRecord","title":"<code>AccountingRecord</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>AccountingRecord table.</p> Source code in <code>fractal_server/app/models/v2/accounting.py</code> <pre><code>class AccountingRecord(SQLModel, table=True):\n    \"\"\"\n    AccountingRecord table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    num_tasks: int\n    num_new_images: int\n</code></pre>"},{"location":"reference/app/models/v2/accounting/#fractal_server.app.models.v2.accounting.AccountingRecordSlurm","title":"<code>AccountingRecordSlurm</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>AccountingRecordSlurm table.</p> Source code in <code>fractal_server/app/models/v2/accounting.py</code> <pre><code>class AccountingRecordSlurm(SQLModel, table=True):\n    \"\"\"\n    AccountingRecordSlurm table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: int = Field(foreign_key=\"user_oauth.id\", nullable=False)\n    timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    slurm_job_ids: list[int] = Field(\n        default_factory=list,\n        sa_column=Column(ARRAY(Integer)),\n    )\n</code></pre>"},{"location":"reference/app/models/v2/dataset/","title":"dataset","text":""},{"location":"reference/app/models/v2/dataset/#fractal_server.app.models.v2.dataset.DatasetV2","title":"<code>DatasetV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Dataset table.</p> Source code in <code>fractal_server/app/models/v2/dataset.py</code> <pre><code>class DatasetV2(SQLModel, table=True):\n    \"\"\"\n    Dataset table.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n    project_id: int = Field(foreign_key=\"projectv2.id\", ondelete=\"CASCADE\")\n    project: \"ProjectV2\" = Relationship(  # noqa: F821\n        sa_relationship_kwargs=dict(lazy=\"selectin\"),\n    )\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n\n    zarr_dir: str\n    images: list[dict[str, Any]] = Field(\n        sa_column=Column(JSONB, server_default=\"[]\", nullable=False)\n    )\n\n    @property\n    def image_zarr_urls(self) -&gt; list[str]:\n        return [image[\"zarr_url\"] for image in self.images]\n</code></pre>"},{"location":"reference/app/models/v2/history/","title":"history","text":""},{"location":"reference/app/models/v2/history/#fractal_server.app.models.v2.history.HistoryImageCache","title":"<code>HistoryImageCache</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>HistoryImageCache table.</p> Source code in <code>fractal_server/app/models/v2/history.py</code> <pre><code>class HistoryImageCache(SQLModel, table=True):\n    \"\"\"\n    HistoryImageCache table.\n    \"\"\"\n\n    zarr_url: str = Field(primary_key=True)\n    dataset_id: int = Field(\n        primary_key=True,\n        foreign_key=\"datasetv2.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n    workflowtask_id: int = Field(\n        primary_key=True,\n        foreign_key=\"workflowtaskv2.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n\n    latest_history_unit_id: int = Field(\n        foreign_key=\"historyunit.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n</code></pre>"},{"location":"reference/app/models/v2/history/#fractal_server.app.models.v2.history.HistoryRun","title":"<code>HistoryRun</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>HistoryRun table.</p> Source code in <code>fractal_server/app/models/v2/history.py</code> <pre><code>class HistoryRun(SQLModel, table=True):\n    \"\"\"\n    HistoryRun table.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: int | None = Field(default=None, primary_key=True)\n    dataset_id: int = Field(\n        foreign_key=\"datasetv2.id\",\n        ondelete=\"CASCADE\",\n    )\n    workflowtask_id: int | None = Field(\n        foreign_key=\"workflowtaskv2.id\",\n        default=None,\n        ondelete=\"SET NULL\",\n    )\n    job_id: int = Field(foreign_key=\"jobv2.id\")\n    task_id: int | None = Field(foreign_key=\"taskv2.id\", ondelete=\"SET NULL\")\n\n    workflowtask_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False),\n    )\n    task_group_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False),\n    )\n\n    timestamp_started: datetime = Field(\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n        default_factory=get_timestamp,\n    )\n    status: str\n    num_available_images: int\n</code></pre>"},{"location":"reference/app/models/v2/history/#fractal_server.app.models.v2.history.HistoryUnit","title":"<code>HistoryUnit</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>HistoryUnit table.</p> Source code in <code>fractal_server/app/models/v2/history.py</code> <pre><code>class HistoryUnit(SQLModel, table=True):\n    \"\"\"\n    HistoryUnit table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    history_run_id: int = Field(\n        foreign_key=\"historyrun.id\",\n        ondelete=\"CASCADE\",\n        index=True,\n    )\n\n    logfile: str\n    status: str\n    zarr_urls: list[str] = Field(\n        sa_column=Column(ARRAY(String)),\n        default_factory=list,\n    )\n</code></pre>"},{"location":"reference/app/models/v2/job/","title":"job","text":""},{"location":"reference/app/models/v2/job/#fractal_server.app.models.v2.job.JobV2","title":"<code>JobV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Job table.</p> Source code in <code>fractal_server/app/models/v2/job.py</code> <pre><code>class JobV2(SQLModel, table=True):\n    \"\"\"\n    Job table.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    id: int | None = Field(default=None, primary_key=True)\n    project_id: int | None = Field(\n        foreign_key=\"projectv2.id\", default=None, ondelete=\"SET NULL\"\n    )\n    workflow_id: int | None = Field(\n        foreign_key=\"workflowv2.id\", default=None, ondelete=\"SET NULL\"\n    )\n    dataset_id: int | None = Field(\n        foreign_key=\"datasetv2.id\", default=None, ondelete=\"SET NULL\"\n    )\n\n    user_email: str = Field(nullable=False)\n    slurm_account: str | None = None\n\n    dataset_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False)\n    )\n    workflow_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False)\n    )\n    project_dump: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False)\n    )\n    fractal_server_version: str = Field(\n        sa_column=Column(String, server_default=\"pre-2.19.0\", nullable=False)\n    )\n\n    worker_init: str | None = None\n    working_dir: str | None = None\n    working_dir_user: str | None = None\n    first_task_index: int\n    last_task_index: int\n\n    start_timestamp: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    end_timestamp: datetime | None = Field(\n        default=None, sa_column=Column(DateTime(timezone=True))\n    )\n    status: str = JobStatusType.SUBMITTED\n    log: str | None = None\n    executor_error_log: str | None = None\n\n    attribute_filters: dict[str, list[int | float | str | bool]] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    type_filters: dict[str, bool] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n\n    __table_args__ = (\n        Index(\n            \"ix_jobv2_one_submitted_job_per_dataset\",\n            \"dataset_id\",\n            unique=True,\n            postgresql_where=text(f\"status = '{JobStatusType.SUBMITTED}'\"),\n        ),\n    )\n</code></pre>"},{"location":"reference/app/models/v2/profile/","title":"profile","text":""},{"location":"reference/app/models/v2/profile/#fractal_server.app.models.v2.profile.Profile","title":"<code>Profile</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Profile table.</p> Source code in <code>fractal_server/app/models/v2/profile.py</code> <pre><code>class Profile(SQLModel, table=True):\n    \"\"\"\n    Profile table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    resource_id: int = Field(foreign_key=\"resource.id\", ondelete=\"RESTRICT\")\n\n    resource_type: str\n    \"\"\"\n    Type of resource (either `local`, `slurm_sudo` or `slurm_ssh`).\n    \"\"\"\n\n    name: str = Field(unique=True)\n    \"\"\"\n    Profile name.\n    \"\"\"\n\n    username: str | None = None\n    \"\"\"\n    Username to be impersonated, either via `sudo -u` or via `ssh`.\n    \"\"\"\n\n    ssh_key_path: str | None = None\n    \"\"\"\n    Path to the private SSH key of user `username` (only relevant if\n    `resource_type=\"slurm_ssh\"`).\n    \"\"\"\n\n    jobs_remote_dir: str | None = None\n    \"\"\"\n    Remote path of the job folder (only relevant if\n    `resource_type=\"slurm_ssh\"`).\n    \"\"\"\n\n    tasks_remote_dir: str | None = None\n    \"\"\"\n    Remote path of the task folder (only relevant if\n    `resource_type=\"slurm_ssh\"`).\n    \"\"\"\n</code></pre>"},{"location":"reference/app/models/v2/profile/#fractal_server.app.models.v2.profile.Profile.jobs_remote_dir","title":"<code>jobs_remote_dir = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remote path of the job folder (only relevant if <code>resource_type=\"slurm_ssh\"</code>).</p>"},{"location":"reference/app/models/v2/profile/#fractal_server.app.models.v2.profile.Profile.name","title":"<code>name = Field(unique=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Profile name.</p>"},{"location":"reference/app/models/v2/profile/#fractal_server.app.models.v2.profile.Profile.resource_type","title":"<code>resource_type</code>  <code>instance-attribute</code>","text":"<p>Type of resource (either <code>local</code>, <code>slurm_sudo</code> or <code>slurm_ssh</code>).</p>"},{"location":"reference/app/models/v2/profile/#fractal_server.app.models.v2.profile.Profile.ssh_key_path","title":"<code>ssh_key_path = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the private SSH key of user <code>username</code> (only relevant if <code>resource_type=\"slurm_ssh\"</code>).</p>"},{"location":"reference/app/models/v2/profile/#fractal_server.app.models.v2.profile.Profile.tasks_remote_dir","title":"<code>tasks_remote_dir = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remote path of the task folder (only relevant if <code>resource_type=\"slurm_ssh\"</code>).</p>"},{"location":"reference/app/models/v2/profile/#fractal_server.app.models.v2.profile.Profile.username","title":"<code>username = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Username to be impersonated, either via <code>sudo -u</code> or via <code>ssh</code>.</p>"},{"location":"reference/app/models/v2/project/","title":"project","text":""},{"location":"reference/app/models/v2/project/#fractal_server.app.models.v2.project.ProjectV2","title":"<code>ProjectV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Project table.</p> Source code in <code>fractal_server/app/models/v2/project.py</code> <pre><code>class ProjectV2(SQLModel, table=True):\n    \"\"\"\n    Project table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n    resource_id: int = Field(foreign_key=\"resource.id\", ondelete=\"RESTRICT\")\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n</code></pre>"},{"location":"reference/app/models/v2/resource/","title":"resource","text":""},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Resource table.</p> Source code in <code>fractal_server/app/models/v2/resource.py</code> <pre><code>class Resource(SQLModel, table=True):\n    \"\"\"\n    Resource table.\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n\n    type: str\n    \"\"\"\n    One of `local`, `slurm_sudo` or `slurm_ssh` - matching with\n    `settings.FRACTAL_RUNNER_BACKEND`.\n    \"\"\"\n\n    name: str = Field(unique=True)\n    \"\"\"\n    Resource name.\n    \"\"\"\n\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    \"\"\"\n    Creation timestamp (autogenerated).\n    \"\"\"\n\n    host: str | None = None\n    \"\"\"\n    Address for ssh connections, when `type=\"slurm_ssh\"`.\n    \"\"\"\n\n    prevent_new_submissions: bool = Field(\n        sa_column=Column(\n            BOOLEAN,\n            server_default=\"false\",\n            nullable=False,\n        ),\n    )\n    \"\"\"\n    When set to true: Prevent new job submissions and stop execution of\n    ongoing jobs as soon as the current task is complete.\n    \"\"\"\n\n    jobs_local_dir: str\n    \"\"\"\n    Base local folder for job subfolders (containing artifacts and logs).\n    \"\"\"\n\n    jobs_runner_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Runner configuration, matching one of `JobRunnerConfigLocal` or\n    `JobRunnerConfigSLURM` schemas.\n    \"\"\"\n\n    jobs_slurm_python_worker: str | None = None\n    \"\"\"\n    On SLURM deloyments, this is the Python interpreter that runs the\n    `fractal-server` worker from within the SLURM jobs.\n    \"\"\"\n\n    jobs_poll_interval: int\n    \"\"\"\n    On SLURM resources: the interval to wait before new `squeue` calls.\n    On local resources: ignored.\n    \"\"\"\n\n    # task_settings\n    tasks_local_dir: str\n    \"\"\"\n    Base local folder for task-package subfolders.\n    \"\"\"\n\n    tasks_python_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Python configuration for task collection. Example:\n    ```json\n    {\n      \"default_version\": \"3.10\",\n      \"versions:{\n        \"3.10\": \"/xxx/venv-3.10/bin/python\",\n        \"3.11\": \"/xxx/venv-3.11/bin/python\",\n        \"3.12\": \"/xxx/venv-3.12/bin/python\"\n       }\n    }\n    ```\n    \"\"\"\n\n    tasks_pixi_config: dict[str, Any] = Field(\n        sa_column=Column(JSONB, nullable=False, server_default=\"{}\")\n    )\n    \"\"\"\n    Pixi configuration for task collection. Basic example:\n    ```json\n    {\n        \"default_version\": \"0.41.0\",\n        \"versions\": {\n            \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n            \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n        },\n    }\n    ```\n    \"\"\"\n\n    @property\n    def pip_cache_dir_arg(self: Self) -&gt; str:\n        \"\"\"\n        If `pip_cache_dir` is set (in `self.tasks_python_config`), then\n        return `--cache_dir /something`; else return `--no-cache-dir`.\n        \"\"\"\n        _pip_cache_dir = self.tasks_python_config.get(\"pip_cache_dir\", None)\n        if _pip_cache_dir is not None:\n            return f\"--cache-dir {_pip_cache_dir}\"\n        else:\n            return \"--no-cache-dir\"\n\n    # Check constraints\n    __table_args__ = (\n        # `type` column must be one of \"local\", \"slurm_sudo\" or \"slurm_ssh\"\n        CheckConstraint(\n            \"type IN ('local', 'slurm_sudo', 'slurm_ssh')\",\n            name=\"correct_type\",\n        ),\n        # If `type` is not \"local\", `jobs_slurm_python_worker` must be set\n        CheckConstraint(\n            \"(type = 'local') OR (jobs_slurm_python_worker IS NOT NULL)\",\n            name=\"jobs_slurm_python_worker_set\",\n        ),\n    )\n</code></pre>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.host","title":"<code>host = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Address for ssh connections, when <code>type=\"slurm_ssh\"</code>.</p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.jobs_local_dir","title":"<code>jobs_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for job subfolders (containing artifacts and logs).</p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.jobs_poll_interval","title":"<code>jobs_poll_interval</code>  <code>instance-attribute</code>","text":"<p>On SLURM resources: the interval to wait before new <code>squeue</code> calls. On local resources: ignored.</p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.jobs_runner_config","title":"<code>jobs_runner_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Runner configuration, matching one of <code>JobRunnerConfigLocal</code> or <code>JobRunnerConfigSLURM</code> schemas.</p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.jobs_slurm_python_worker","title":"<code>jobs_slurm_python_worker = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>On SLURM deloyments, this is the Python interpreter that runs the <code>fractal-server</code> worker from within the SLURM jobs.</p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.name","title":"<code>name = Field(unique=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Resource name.</p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.pip_cache_dir_arg","title":"<code>pip_cache_dir_arg</code>  <code>property</code>","text":"<p>If <code>pip_cache_dir</code> is set (in <code>self.tasks_python_config</code>), then return <code>--cache_dir /something</code>; else return <code>--no-cache-dir</code>.</p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.prevent_new_submissions","title":"<code>prevent_new_submissions = Field(sa_column=(Column(BOOLEAN, server_default='false', nullable=False)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When set to true: Prevent new job submissions and stop execution of ongoing jobs as soon as the current task is complete.</p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.tasks_local_dir","title":"<code>tasks_local_dir</code>  <code>instance-attribute</code>","text":"<p>Base local folder for task-package subfolders.</p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.tasks_pixi_config","title":"<code>tasks_pixi_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Pixi configuration for task collection. Basic example: <pre><code>{\n    \"default_version\": \"0.41.0\",\n    \"versions\": {\n        \"0.40.0\": \"/xxx/pixi/0.40.0/\",\n        \"0.41.0\": \"/xxx/pixi/0.41.0/\"\n    },\n}\n</code></pre></p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.tasks_python_config","title":"<code>tasks_python_config = Field(sa_column=(Column(JSONB, nullable=False, server_default='{}')))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Python configuration for task collection. Example: <pre><code>{\n  \"default_version\": \"3.10\",\n  \"versions:{\n    \"3.10\": \"/xxx/venv-3.10/bin/python\",\n    \"3.11\": \"/xxx/venv-3.11/bin/python\",\n    \"3.12\": \"/xxx/venv-3.12/bin/python\"\n   }\n}\n</code></pre></p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.timestamp_created","title":"<code>timestamp_created = Field(default_factory=get_timestamp, sa_column=(Column(DateTime(timezone=True), nullable=False)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Creation timestamp (autogenerated).</p>"},{"location":"reference/app/models/v2/resource/#fractal_server.app.models.v2.resource.Resource.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>One of <code>local</code>, <code>slurm_sudo</code> or <code>slurm_ssh</code> - matching with <code>settings.FRACTAL_RUNNER_BACKEND</code>.</p>"},{"location":"reference/app/models/v2/task/","title":"task","text":""},{"location":"reference/app/models/v2/task/#fractal_server.app.models.v2.task.TaskV2","title":"<code>TaskV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> <p>Model for the <code>taskv2</code> database table.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int | None</code> </p> <code>name</code> <p> TYPE: <code>str</code> </p> <code>type</code> <p> TYPE: <code>str</code> </p> <code>version</code> <p> TYPE: <code>str | None</code> </p> <code>command_non_parallel</code> <p> TYPE: <code>str | None</code> </p> <code>command_parallel</code> <p> TYPE: <code>str | None</code> </p> <code>meta_non_parallel</code> <p> TYPE: <code>dict[str, Any]</code> </p> <code>meta_parallel</code> <p> TYPE: <code>dict[str, Any]</code> </p> <code>input_types</code> <p> TYPE: <code>dict[str, bool]</code> </p> <code>output_types</code> <p> TYPE: <code>dict[str, bool]</code> </p> <code>taskgroupv2_id</code> <p> TYPE: <code>int</code> </p> <code>args_schema_version</code> <p> TYPE: <code>str | None</code> </p> <code>args_schema_non_parallel</code> <p> TYPE: <code>dict[str, Any] | None</code> </p> <code>args_schema_parallel</code> <p> TYPE: <code>dict[str, Any] | None</code> </p> <code>docs_info</code> <p> TYPE: <code>str | None</code> </p> <code>docs_link</code> <p> TYPE: <code>str | None</code> </p> <code>category</code> <p> TYPE: <code>str | None</code> </p> <code>modality</code> <p> TYPE: <code>str | None</code> </p> <code>authors</code> <p> TYPE: <code>str | None</code> </p> <code>tags</code> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/app/models/v2/task.py</code> <pre><code>class TaskV2(SQLModel, table=True):\n    \"\"\"\n    Model for the `taskv2` database table.\n\n    Attributes:\n        id:\n        name:\n        type:\n        version:\n        command_non_parallel:\n        command_parallel:\n        meta_non_parallel:\n        meta_parallel:\n        input_types:\n        output_types:\n        taskgroupv2_id:\n        args_schema_version:\n        args_schema_non_parallel:\n        args_schema_parallel:\n        docs_info:\n        docs_link:\n        category:\n        modality:\n        authors:\n        tags:\n    \"\"\"\n\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n\n    type: str\n    command_non_parallel: str | None = None\n    command_parallel: str | None = None\n\n    meta_non_parallel: dict[str, Any] = Field(\n        sa_column=Column(JSON, server_default=\"{}\", default={}, nullable=False)\n    )\n    meta_parallel: dict[str, Any] = Field(\n        sa_column=Column(JSON, server_default=\"{}\", default={}, nullable=False)\n    )\n\n    version: str | None = None\n    args_schema_non_parallel: dict[str, Any] | None = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_parallel: dict[str, Any] | None = Field(\n        sa_column=Column(JSON), default=None\n    )\n    args_schema_version: str | None = None\n    docs_info: str | None = None\n    docs_link: str | None = None\n\n    input_types: dict[str, bool] = Field(sa_column=Column(JSONB), default={})\n    output_types: dict[str, bool] = Field(sa_column=Column(JSONB), default={})\n\n    taskgroupv2_id: int = Field(foreign_key=\"taskgroupv2.id\")\n\n    category: str | None = None\n    modality: str | None = None\n    authors: str | None = None\n    tags: list[str] = Field(\n        sa_column=Column(JSONB, server_default=\"[]\", nullable=False)\n    )\n</code></pre>"},{"location":"reference/app/models/v2/task_group/","title":"task_group","text":""},{"location":"reference/app/models/v2/task_group/#fractal_server.app.models.v2.task_group.TaskGroupV2","title":"<code>TaskGroupV2</code>","text":"<p>               Bases: <code>SQLModel</code></p> Source code in <code>fractal_server/app/models/v2/task_group.py</code> <pre><code>class TaskGroupV2(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    task_list: list[TaskV2] = Relationship(\n        sa_relationship_kwargs=dict(\n            lazy=\"selectin\", cascade=\"all, delete-orphan\"\n        ),\n    )\n\n    user_id: int = Field(foreign_key=\"user_oauth.id\")\n    user_group_id: int | None = Field(\n        foreign_key=\"usergroup.id\", default=None, ondelete=\"SET NULL\"\n    )\n    resource_id: int = Field(foreign_key=\"resource.id\", ondelete=\"RESTRICT\")\n\n    origin: str\n    pkg_name: str\n    version: str | None = None\n    python_version: str | None = None\n    pixi_version: str | None = None\n    path: str | None = None\n    archive_path: str | None = None\n    pip_extras: str | None = None\n    pinned_package_versions_pre: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    pinned_package_versions_post: dict[str, str] = Field(\n        sa_column=Column(\n            JSONB,\n            server_default=\"{}\",\n            default={},\n            nullable=True,\n        ),\n    )\n    env_info: str | None = None\n    venv_path: str | None = None\n\n    active: bool = True\n    timestamp_created: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(DateTime(timezone=True), nullable=False),\n    )\n    timestamp_last_used: datetime = Field(\n        default_factory=get_timestamp,\n        sa_column=Column(\n            DateTime(timezone=True),\n            nullable=False,\n            server_default=(\n                datetime(2024, 11, 20, tzinfo=timezone.utc).isoformat()\n            ),\n        ),\n    )\n\n    __table_args__ = (\n        Index(\n            \"ix_taskgroupv2_user_unique_constraint\",\n            \"user_id\",\n            \"pkg_name\",\n            \"version\",\n            \"resource_id\",\n            unique=True,\n            postgresql_nulls_not_distinct=True,\n        ),\n        Index(\n            \"ix_taskgroupv2_usergroup_unique_constraint\",\n            \"user_group_id\",\n            \"pkg_name\",\n            \"version\",\n            unique=True,\n            postgresql_nulls_not_distinct=True,\n            postgresql_where=column(\"user_group_id\").is_not(None),\n        ),\n        Index(\n            \"ix_taskgroupv2_path_unique_constraint\",\n            \"path\",\n            \"resource_id\",\n            unique=True,\n        ),\n    )\n\n    @property\n    def pip_install_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        extras = f\"[{self.pip_extras}]\" if self.pip_extras is not None else \"\"\n\n        if self.archive_path is not None:\n            return f\"{self.archive_path}{extras}\"\n        else:\n            if self.version is None:\n                raise ValueError(\n                    \"Cannot run `pip_install_string` with \"\n                    f\"{self.pkg_name=}, {self.archive_path=}, {self.version=}.\"\n                )\n            return f\"{self.pkg_name}{extras}=={self.version}\"\n\n    @property\n    def pinned_package_versions_pre_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_pre is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_pre)\n        return output\n\n    @property\n    def pinned_package_versions_post_string(self) -&gt; str:\n        \"\"\"\n        Prepare string to be used in `python -m pip install`.\n        \"\"\"\n        _check_origin_not_pixi(self.origin)\n\n        if self.pinned_package_versions_post is None:\n            return \"\"\n        output = _create_dependency_string(self.pinned_package_versions_post)\n        return output\n</code></pre>"},{"location":"reference/app/models/v2/task_group/#fractal_server.app.models.v2.task_group.TaskGroupV2.pinned_package_versions_post_string","title":"<code>pinned_package_versions_post_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/app/models/v2/task_group/#fractal_server.app.models.v2.task_group.TaskGroupV2.pinned_package_versions_pre_string","title":"<code>pinned_package_versions_pre_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/app/models/v2/task_group/#fractal_server.app.models.v2.task_group.TaskGroupV2.pip_install_string","title":"<code>pip_install_string</code>  <code>property</code>","text":"<p>Prepare string to be used in <code>python -m pip install</code>.</p>"},{"location":"reference/app/models/v2/task_group/#fractal_server.app.models.v2.task_group._check_origin_not_pixi","title":"<code>_check_origin_not_pixi(origin)</code>","text":"<p>Raise <code>ValueError</code> if <code>origin==\"pixi\"</code></p> Source code in <code>fractal_server/app/models/v2/task_group.py</code> <pre><code>def _check_origin_not_pixi(origin: str):\n    \"\"\"\n    Raise `ValueError` if `origin==\"pixi\"`\n    \"\"\"\n    if origin == \"pixi\":\n        raise ValueError(f\"Cannot call 'pip_install_string' if {origin=}.\")\n</code></pre>"},{"location":"reference/app/models/v2/task_group/#fractal_server.app.models.v2.task_group._create_dependency_string","title":"<code>_create_dependency_string(pinned_versions)</code>","text":"<p>Expand e.g. <code>{\"a\": \"1.2\", \"b\": \"3\"}</code> into <code>'\"a==1.2\" \"b==3\"'</code>.</p> Source code in <code>fractal_server/app/models/v2/task_group.py</code> <pre><code>def _create_dependency_string(pinned_versions: dict[str, str]) -&gt; str:\n    \"\"\"\n    Expand e.g. `{\"a\": \"1.2\", \"b\": \"3\"}` into `'\"a==1.2\" \"b==3\"'`.\n    \"\"\"\n    output = \" \".join(\n        [f'\"{key}=={value}\"' for key, value in pinned_versions.items()]\n    )\n    return output\n</code></pre>"},{"location":"reference/app/models/v2/workflow/","title":"workflow","text":""},{"location":"reference/app/models/v2/workflowtask/","title":"workflowtask","text":""},{"location":"reference/app/routes/","title":"routes","text":""},{"location":"reference/app/routes/pagination/","title":"pagination","text":""},{"location":"reference/app/routes/admin/","title":"admin","text":""},{"location":"reference/app/routes/admin/v2/","title":"v2","text":"<p><code>admin/v2</code> module</p>"},{"location":"reference/app/routes/admin/v2/_aux_functions/","title":"_aux_functions","text":""},{"location":"reference/app/routes/admin/v2/accounting/","title":"accounting","text":""},{"location":"reference/app/routes/admin/v2/impersonate/","title":"impersonate","text":""},{"location":"reference/app/routes/admin/v2/job/","title":"job","text":""},{"location":"reference/app/routes/admin/v2/job/#fractal_server.app.routes.admin.v2.job.download_job_logs","title":"<code>download_job_logs(job_id, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Download job folder</p> Source code in <code>fractal_server/app/routes/admin/v2/job.py</code> <pre><code>@router.get(\"/{job_id}/download/\", response_class=StreamingResponse)\nasync def download_job_logs(\n    job_id: int,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StreamingResponse:\n    \"\"\"\n    Download job folder\n    \"\"\"\n    # Get job from DB\n    job = await db.get(JobV2, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n    # Create and return byte stream for zipped log folder\n    PREFIX_ZIP = Path(job.working_dir).name\n    zip_filename = f\"{PREFIX_ZIP}_archive.zip\"\n    return StreamingResponse(\n        _zip_folder_to_byte_stream_iterator(folder=job.working_dir),\n        media_type=\"application/x-zip-compressed\",\n        headers={\"Content-Disposition\": f\"attachment;filename={zip_filename}\"},\n    )\n</code></pre>"},{"location":"reference/app/routes/admin/v2/job/#fractal_server.app.routes.admin.v2.job.stop_job","title":"<code>stop_job(job_id, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Stop execution of a workflow job.</p> Source code in <code>fractal_server/app/routes/admin/v2/job.py</code> <pre><code>@router.get(\"/{job_id}/stop/\", status_code=202)\nasync def stop_job(\n    job_id: int,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Stop execution of a workflow job.\n    \"\"\"\n\n    _check_shutdown_is_supported()\n\n    job = await db.get(JobV2, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n\n    _write_shutdown_file(job=job)\n\n    return Response(status_code=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"reference/app/routes/admin/v2/job/#fractal_server.app.routes.admin.v2.job.update_job","title":"<code>update_job(job_update, job_id, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Change the status of an existing job.</p> <p>This endpoint is only open to superusers, and it does not apply project-based access-control to jobs.</p> Source code in <code>fractal_server/app/routes/admin/v2/job.py</code> <pre><code>@router.patch(\"/{job_id}/\", response_model=JobRead)\nasync def update_job(\n    job_update: JobUpdate,\n    job_id: int,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; JobRead | None:\n    \"\"\"\n    Change the status of an existing job.\n\n    This endpoint is only open to superusers, and it does not apply\n    project-based access-control to jobs.\n    \"\"\"\n    job = await db.get(JobV2, job_id)\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Job {job_id} not found\",\n        )\n    if job.status != JobStatusType.SUBMITTED:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Job {job_id} has status {job.status=} != 'submitted'.\",\n        )\n\n    if job_update.status != JobStatusType.FAILED:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Cannot set job status to {job_update.status}\",\n        )\n\n    timestamp = get_timestamp()\n    setattr(job, \"status\", job_update.status)\n    setattr(job, \"end_timestamp\", timestamp)\n    setattr(\n        job,\n        \"log\",\n        f\"{job.log or ''}\\nThis job was manually marked as \"\n        f\"'{JobStatusType.FAILED}' by an admin ({timestamp.isoformat()}).\",\n    )\n\n    res = await db.execute(\n        select(HistoryRun)\n        .where(HistoryRun.job_id == job_id)\n        .order_by(HistoryRun.timestamp_started.desc())\n        .limit(1)\n    )\n    latest_run = res.scalar_one_or_none()\n    if latest_run is not None:\n        setattr(latest_run, \"status\", HistoryUnitStatus.FAILED)\n        res = await db.execute(\n            select(HistoryUnit).where(\n                HistoryUnit.history_run_id == latest_run.id\n            )\n        )\n        history_units = res.scalars().all()\n        for history_unit in history_units:\n            setattr(history_unit, \"status\", HistoryUnitStatus.FAILED)\n\n    await db.commit()\n    await db.refresh(job)\n    return job\n</code></pre>"},{"location":"reference/app/routes/admin/v2/job/#fractal_server.app.routes.admin.v2.job.view_job","title":"<code>view_job(id=None, resource_id=None, user_id=None, project_id=None, dataset_id=None, workflow_id=None, status=None, start_timestamp_min=None, start_timestamp_max=None, end_timestamp_min=None, end_timestamp_max=None, log=True, pagination=Depends(get_pagination_params), user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>JobV2</code> table.</p> PARAMETER DESCRIPTION <code>id</code> <p>If not <code>None</code>, select a given <code>applyworkflow.id</code>.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>user_id</code> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>project_id</code> <p>If not <code>None</code>, select a given <code>applyworkflow.project_id</code>.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>dataset_id</code> <p>If not <code>None</code>, select a given <code>applyworkflow.input_dataset_id</code>.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>workflow_id</code> <p>If not <code>None</code>, select a given <code>applyworkflow.workflow_id</code>.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>If not <code>None</code>, select a given <code>applyworkflow.status</code>.</p> <p> TYPE: <code>JobStatusType | None</code> DEFAULT: <code>None</code> </p> <code>start_timestamp_min</code> <p>If not <code>None</code>, select a rows with <code>start_timestamp</code> after <code>start_timestamp_min</code>.</p> <p> TYPE: <code>AwareDatetime | None</code> DEFAULT: <code>None</code> </p> <code>start_timestamp_max</code> <p>If not <code>None</code>, select a rows with <code>start_timestamp</code> before <code>start_timestamp_min</code>.</p> <p> TYPE: <code>AwareDatetime | None</code> DEFAULT: <code>None</code> </p> <code>end_timestamp_min</code> <p>If not <code>None</code>, select a rows with <code>end_timestamp</code> after <code>end_timestamp_min</code>.</p> <p> TYPE: <code>AwareDatetime | None</code> DEFAULT: <code>None</code> </p> <code>end_timestamp_max</code> <p>If not <code>None</code>, select a rows with <code>end_timestamp</code> before <code>end_timestamp_min</code>.</p> <p> TYPE: <code>AwareDatetime | None</code> DEFAULT: <code>None</code> </p> <code>log</code> <p>If <code>True</code>, include <code>job.log</code>, if <code>False</code> <code>job.log</code> is set to <code>None</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>fractal_server/app/routes/admin/v2/job.py</code> <pre><code>@router.get(\"/\", response_model=PaginationResponse[JobRead])\nasync def view_job(\n    id: int | None = None,\n    resource_id: int | None = None,\n    user_id: int | None = None,\n    project_id: int | None = None,\n    dataset_id: int | None = None,\n    workflow_id: int | None = None,\n    status: JobStatusType | None = None,\n    start_timestamp_min: AwareDatetime | None = None,\n    start_timestamp_max: AwareDatetime | None = None,\n    end_timestamp_min: AwareDatetime | None = None,\n    end_timestamp_max: AwareDatetime | None = None,\n    log: bool = True,\n    pagination: PaginationRequest = Depends(get_pagination_params),\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; PaginationResponse[JobRead]:\n    \"\"\"\n    Query `JobV2` table.\n\n    Args:\n        id: If not `None`, select a given `applyworkflow.id`.\n        user_id:\n        project_id: If not `None`, select a given `applyworkflow.project_id`.\n        dataset_id: If not `None`, select a given\n            `applyworkflow.input_dataset_id`.\n        workflow_id: If not `None`, select a given `applyworkflow.workflow_id`.\n        status: If not `None`, select a given `applyworkflow.status`.\n        start_timestamp_min: If not `None`, select a rows with\n            `start_timestamp` after `start_timestamp_min`.\n        start_timestamp_max: If not `None`, select a rows with\n            `start_timestamp` before `start_timestamp_min`.\n        end_timestamp_min: If not `None`, select a rows with `end_timestamp`\n            after `end_timestamp_min`.\n        end_timestamp_max: If not `None`, select a rows with `end_timestamp`\n            before `end_timestamp_min`.\n        log: If `True`, include `job.log`, if `False`\n            `job.log` is set to `None`.\n    \"\"\"\n\n    # Assign pagination parameters\n    page = pagination.page\n    page_size = pagination.page_size\n\n    # Prepare statements\n    stm = select(JobV2).order_by(JobV2.start_timestamp.desc())\n    stm_count = select(func.count(JobV2.id))\n    if id is not None:\n        stm = stm.where(JobV2.id == id)\n        stm_count = stm_count.where(JobV2.id == id)\n    if resource_id is not None:\n        stm = stm.join(ProjectV2, ProjectV2.id == JobV2.project_id).where(\n            ProjectV2.resource_id == resource_id\n        )\n        stm_count = stm_count.join(\n            ProjectV2, ProjectV2.id == JobV2.project_id\n        ).where(ProjectV2.resource_id == resource_id)\n    if user_id is not None:\n        stm = (\n            stm.join(\n                LinkUserProjectV2,\n                LinkUserProjectV2.project_id == JobV2.project_id,\n            )\n            .where(LinkUserProjectV2.user_id == user_id)\n            .where(LinkUserProjectV2.is_owner.is_(True))\n        )\n        stm_count = (\n            stm_count.join(\n                LinkUserProjectV2,\n                LinkUserProjectV2.project_id == JobV2.project_id,\n            )\n            .where(LinkUserProjectV2.user_id == user_id)\n            .where(LinkUserProjectV2.is_owner.is_(True))\n        )\n    if project_id is not None:\n        stm = stm.where(JobV2.project_id == project_id)\n        stm_count = stm_count.where(JobV2.project_id == project_id)\n    if dataset_id is not None:\n        stm = stm.where(JobV2.dataset_id == dataset_id)\n        stm_count = stm_count.where(JobV2.dataset_id == dataset_id)\n    if workflow_id is not None:\n        stm = stm.where(JobV2.workflow_id == workflow_id)\n        stm_count = stm_count.where(JobV2.workflow_id == workflow_id)\n    if status is not None:\n        stm = stm.where(JobV2.status == status)\n        stm_count = stm_count.where(JobV2.status == status)\n    if start_timestamp_min is not None:\n        stm = stm.where(JobV2.start_timestamp &gt;= start_timestamp_min)\n        stm_count = stm_count.where(\n            JobV2.start_timestamp &gt;= start_timestamp_min\n        )\n    if start_timestamp_max is not None:\n        stm = stm.where(JobV2.start_timestamp &lt;= start_timestamp_max)\n        stm_count = stm_count.where(\n            JobV2.start_timestamp &lt;= start_timestamp_max\n        )\n    if end_timestamp_min is not None:\n        stm = stm.where(JobV2.end_timestamp &gt;= end_timestamp_min)\n        stm_count = stm_count.where(JobV2.end_timestamp &gt;= end_timestamp_min)\n    if end_timestamp_max is not None:\n        stm = stm.where(JobV2.end_timestamp &lt;= end_timestamp_max)\n        stm_count = stm_count.where(JobV2.end_timestamp &lt;= end_timestamp_max)\n\n    # Find total number of elements\n    res_total_count = await db.execute(stm_count)\n    total_count = res_total_count.scalar()\n    if page_size is None:\n        page_size = total_count\n    else:\n        stm = stm.offset((page - 1) * page_size).limit(page_size)\n\n    # Get `page_size` rows\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return dict(\n        total_count=total_count,\n        page_size=page_size,\n        current_page=page,\n        items=job_list,\n    )\n</code></pre>"},{"location":"reference/app/routes/admin/v2/profile/","title":"profile","text":""},{"location":"reference/app/routes/admin/v2/profile/#fractal_server.app.routes.admin.v2.profile.delete_profile","title":"<code>delete_profile(profile_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete single <code>Profile</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/profile.py</code> <pre><code>@router.delete(\"/{profile_id}/\", status_code=204)\nasync def delete_profile(\n    profile_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Delete single `Profile`.\n    \"\"\"\n    profile = await _get_profile_or_404(profile_id=profile_id, db=db)\n\n    # Fail if at least one UserOAuth is associated with the Profile.\n    res = await db.execute(\n        select(func.count(UserOAuth.id)).where(\n            UserOAuth.profile_id == profile.id\n        )\n    )\n    associated_users_count = res.scalar()\n    if associated_users_count &gt; 0:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot delete Profile {profile_id} because it's associated\"\n                f\" with {associated_users_count} UserOAuths.\"\n            ),\n        )\n\n    # Delete\n    await db.delete(profile)\n    await db.commit()\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/app/routes/admin/v2/profile/#fractal_server.app.routes.admin.v2.profile.get_profile_list","title":"<code>get_profile_list(superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query single <code>Profile</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/profile.py</code> <pre><code>@router.get(\"/\", response_model=list[ProfileRead], status_code=200)\nasync def get_profile_list(\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProfileRead:\n    \"\"\"\n    Query single `Profile`.\n    \"\"\"\n    res = await db.execute(select(Profile).order_by(Profile.id))\n    profiles = res.scalars().all()\n    return profiles\n</code></pre>"},{"location":"reference/app/routes/admin/v2/profile/#fractal_server.app.routes.admin.v2.profile.get_single_profile","title":"<code>get_single_profile(profile_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query single <code>Profile</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/profile.py</code> <pre><code>@router.get(\"/{profile_id}/\", response_model=ProfileRead, status_code=200)\nasync def get_single_profile(\n    profile_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProfileRead:\n    \"\"\"\n    Query single `Profile`.\n    \"\"\"\n    profile = await _get_profile_or_404(profile_id=profile_id, db=db)\n    return profile\n</code></pre>"},{"location":"reference/app/routes/admin/v2/profile/#fractal_server.app.routes.admin.v2.profile.put_profile","title":"<code>put_profile(profile_id, profile_update, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Override single <code>Profile</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/profile.py</code> <pre><code>@router.put(\"/{profile_id}/\", response_model=ProfileRead, status_code=200)\nasync def put_profile(\n    profile_id: int,\n    profile_update: ProfileCreate,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProfileRead:\n    \"\"\"\n    Override single `Profile`.\n    \"\"\"\n    profile = await _get_profile_or_404(profile_id=profile_id, db=db)\n\n    if profile_update.name and profile_update.name != profile.name:\n        await _check_profile_name(name=profile_update.name, db=db)\n\n    for key, value in profile_update.model_dump().items():\n        setattr(profile, key, value)\n    await db.commit()\n    await db.refresh(profile)\n    return profile\n</code></pre>"},{"location":"reference/app/routes/admin/v2/resource/","title":"resource","text":""},{"location":"reference/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource._check_type_match_or_422","title":"<code>_check_type_match_or_422(new_resource)</code>","text":"<p>Handle case where <code>resource.type != FRACTAL_RUNNER_BACKEND</code></p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>def _check_type_match_or_422(new_resource: ResourceCreate) -&gt; None:\n    \"\"\"\n    Handle case where `resource.type != FRACTAL_RUNNER_BACKEND`\n    \"\"\"\n    settings = Inject(get_settings)\n    if settings.FRACTAL_RUNNER_BACKEND != new_resource.type:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"{settings.FRACTAL_RUNNER_BACKEND=} != {new_resource.type=}\"\n            ),\n        )\n</code></pre>"},{"location":"reference/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.delete_resource","title":"<code>delete_resource(resource_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete single <code>Resource</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.delete(\"/{resource_id}/\", status_code=204)\nasync def delete_resource(\n    resource_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Delete single `Resource`.\n    \"\"\"\n    resource = await _get_resource_or_404(resource_id=resource_id, db=db)\n    try:\n        await db.delete(resource)\n        await db.commit()\n        return Response(status_code=status.HTTP_204_NO_CONTENT)\n    except IntegrityError as e:\n        await db.rollback()\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"IntegrityError for resource deletion. \"\n                f\"Original error:\\n{str(e)}\"\n            ),\n        )\n</code></pre>"},{"location":"reference/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.get_resource","title":"<code>get_resource(resource_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query single <code>Resource</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.get(\"/{resource_id}/\", response_model=ResourceRead, status_code=200)\nasync def get_resource(\n    resource_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ResourceRead:\n    \"\"\"\n    Query single `Resource`.\n    \"\"\"\n    resource = await _get_resource_or_404(resource_id=resource_id, db=db)\n\n    return resource\n</code></pre>"},{"location":"reference/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.get_resource_list","title":"<code>get_resource_list(superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>Resource</code> table.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.get(\"/\", response_model=list[ResourceRead], status_code=200)\nasync def get_resource_list(\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ResourceRead]:\n    \"\"\"\n    Query `Resource` table.\n    \"\"\"\n\n    stm = select(Resource).order_by(Resource.id)\n    res = await db.execute(stm)\n    resource_list = res.scalars().all()\n\n    return resource_list\n</code></pre>"},{"location":"reference/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.get_resource_profiles","title":"<code>get_resource_profiles(resource_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>Profile</code>s for single <code>Resource</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.get(\n    \"/{resource_id}/profile/\",\n    response_model=list[ProfileRead],\n    status_code=200,\n)\nasync def get_resource_profiles(\n    resource_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ProfileRead]:\n    \"\"\"\n    Query `Profile`s for single `Resource`.\n    \"\"\"\n    await _get_resource_or_404(resource_id=resource_id, db=db)\n\n    res = await db.execute(\n        select(Profile)\n        .where(Profile.resource_id == resource_id)\n        .order_by(Profile.id)\n    )\n    profiles = res.scalars().all()\n\n    return profiles\n</code></pre>"},{"location":"reference/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.post_profile","title":"<code>post_profile(resource_id, profile_create, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create new <code>Profile</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.post(\n    \"/{resource_id}/profile/\",\n    response_model=ProfileRead,\n    status_code=201,\n)\nasync def post_profile(\n    resource_id: int,\n    profile_create: ProfileCreate,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProfileRead:\n    \"\"\"\n    Create new `Profile`.\n    \"\"\"\n    resource = await _get_resource_or_404(resource_id=resource_id, db=db)\n\n    _check_resource_type_match_or_422(\n        resource=resource,\n        new_profile=profile_create,\n    )\n    await _check_profile_name(name=profile_create.name, db=db)\n\n    profile = Profile(\n        resource_id=resource_id,\n        **profile_create.model_dump(),\n    )\n\n    db.add(profile)\n    await db.commit()\n    await db.refresh(profile)\n    return profile\n</code></pre>"},{"location":"reference/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.post_resource","title":"<code>post_resource(resource_create, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create new <code>Resource</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.post(\"/\", response_model=ResourceRead, status_code=201)\nasync def post_resource(\n    resource_create: ResourceCreate,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ResourceRead:\n    \"\"\"\n    Create new `Resource`.\n    \"\"\"\n\n    # Handle case where type!=FRACTAL_RUNNER_BACKEND\n    _check_type_match_or_422(resource_create)\n\n    await _check_resource_name(name=resource_create.name, db=db)\n\n    resource = Resource(**resource_create.model_dump())\n    db.add(resource)\n    await db.commit()\n    await db.refresh(resource)\n\n    return resource\n</code></pre>"},{"location":"reference/app/routes/admin/v2/resource/#fractal_server.app.routes.admin.v2.resource.put_resource","title":"<code>put_resource(resource_id, resource_update, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Overwrite a single <code>Resource</code>.</p> Source code in <code>fractal_server/app/routes/admin/v2/resource.py</code> <pre><code>@router.put(\n    \"/{resource_id}/\",\n    response_model=ResourceRead,\n    status_code=200,\n)\nasync def put_resource(\n    resource_id: int,\n    resource_update: ResourceCreate,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ResourceRead:\n    \"\"\"\n    Overwrite a single `Resource`.\n    \"\"\"\n\n    # Handle case where type!=FRACTAL_RUNNER_BACKEND\n    _check_type_match_or_422(resource_update)\n\n    resource = await _get_resource_or_404(resource_id=resource_id, db=db)\n\n    # Handle non-unique resource names\n    if resource_update.name and resource_update.name != resource.name:\n        await _check_resource_name(name=resource_update.name, db=db)\n\n    # Prepare new db object\n    for key, value in resource_update.model_dump().items():\n        setattr(resource, key, value)\n\n    await db.commit()\n    await db.refresh(resource)\n    return resource\n</code></pre>"},{"location":"reference/app/routes/admin/v2/sharing/","title":"sharing","text":""},{"location":"reference/app/routes/admin/v2/sharing/#fractal_server.app.routes.admin.v2.sharing.verify_invitation_for_guest","title":"<code>verify_invitation_for_guest(guest_user_id, project_id, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Verify the invitation to join a project for a guest user</p> <p>Note that a guest user would not be allowed to do this themselves.</p> Source code in <code>fractal_server/app/routes/admin/v2/sharing.py</code> <pre><code>@router.post(\"/verify/\", status_code=200)\nasync def verify_invitation_for_guest(\n    guest_user_id: int,\n    project_id: int,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; None:\n    \"\"\"\n    Verify the invitation to join a project for a guest user\n\n    Note that a guest user would not be allowed to do this themselves.\n    \"\"\"\n    # Get user and verify that they actually are a guest\n    guest_user = await _user_or_404(guest_user_id, db)\n    if not guest_user.is_guest:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Cannot accept invitations for non-guest users.\",\n        )\n\n    # Find verification and check that permissions are set to R\n    link = await get_pending_invitation_or_404(\n        user_id=guest_user_id, project_id=project_id, db=db\n    )\n    if link.permissions != ProjectPermissions.READ:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Guest users can only have 'r' permission \"\n                f\"(given: '{link.permissions}')\"\n            ),\n        )\n    # Mark the invitation as verified\n    link.is_verified = True\n    await db.commit()\n\n    return Response(status_code=status.HTTP_200_OK)\n</code></pre>"},{"location":"reference/app/routes/admin/v2/task/","title":"task","text":""},{"location":"reference/app/routes/admin/v2/task/#fractal_server.app.routes.admin.v2.task.query_tasks","title":"<code>query_tasks(id=None, version=None, name=None, task_type=None, category=None, modality=None, author=None, resource_id=None, pagination=Depends(get_pagination_params), user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Query <code>TaskV2</code> and get information about related workflows and projects.</p> Source code in <code>fractal_server/app/routes/admin/v2/task.py</code> <pre><code>@router.get(\"/\", response_model=PaginationResponse[TaskInfo])\nasync def query_tasks(\n    id: int | None = None,\n    version: str | None = None,\n    name: str | None = None,\n    task_type: TaskType | None = None,\n    category: str | None = None,\n    modality: str | None = None,\n    author: str | None = None,\n    resource_id: int | None = None,\n    pagination: PaginationRequest = Depends(get_pagination_params),\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; PaginationResponse[TaskInfo]:\n    \"\"\"\n    Query `TaskV2` and get information about related workflows and projects.\n    \"\"\"\n\n    # Assign pagination parameters\n    page = pagination.page\n    page_size = pagination.page_size\n\n    # Prepare statements\n    stm = select(TaskV2).order_by(TaskV2.id)\n    stm_count = select(func.count(TaskV2.id))\n    if id is not None:\n        stm = stm.where(TaskV2.id == id)\n        stm_count = stm_count.where(TaskV2.id == id)\n    if version is not None:\n        stm = stm.where(TaskV2.version == version)\n        stm_count = stm_count.where(TaskV2.version == version)\n    if name is not None:\n        stm = stm.where(TaskV2.name.icontains(name))\n        stm_count = stm_count.where(TaskV2.name.icontains(name))\n    if task_type is not None:\n        stm = stm.where(TaskV2.type == task_type)\n        stm_count = stm_count.where(TaskV2.type == task_type)\n    if category is not None:\n        stm = stm.where(func.lower(TaskV2.category) == category.lower())\n        stm_count = stm_count.where(\n            func.lower(TaskV2.category) == category.lower()\n        )\n    if modality is not None:\n        stm = stm.where(func.lower(TaskV2.modality) == modality.lower())\n        stm_count = stm_count.where(\n            func.lower(TaskV2.modality) == modality.lower()\n        )\n    if author is not None:\n        stm = stm.where(TaskV2.authors.icontains(author))\n        stm_count = stm_count.where(TaskV2.authors.icontains(author))\n    if resource_id is not None:\n        stm = stm.join(\n            TaskGroupV2, TaskGroupV2.id == TaskV2.taskgroupv2_id\n        ).where(TaskGroupV2.resource_id == resource_id)\n        stm_count = stm_count.join(\n            TaskGroupV2, TaskGroupV2.id == TaskV2.taskgroupv2_id\n        ).where(TaskGroupV2.resource_id == resource_id)\n\n    # Find total number of elements\n    res_total_count = await db.execute(stm_count)\n    total_count = res_total_count.scalar()\n    if page_size is None:\n        page_size = total_count\n    else:\n        stm = stm.offset((page - 1) * page_size).limit(page_size)\n\n    # Get `page_size` rows\n    res = await db.execute(stm)\n    task_list = res.scalars().all()\n\n    task_info_list = []\n    for task in task_list:\n        stm = (\n            select(WorkflowV2)\n            .join(\n                WorkflowTaskV2,\n                WorkflowTaskV2.workflow_id == WorkflowV2.id,\n            )\n            .where(WorkflowTaskV2.task_id == task.id)\n        )\n        res = await db.execute(stm)\n        wf_list = res.scalars().all()\n\n        project_users = {}\n        for project_id in set([workflow.project_id for workflow in wf_list]):\n            res = await db.execute(\n                select(UserOAuth.id, UserOAuth.email)\n                .join(\n                    LinkUserProjectV2,\n                    LinkUserProjectV2.user_id == UserOAuth.id,\n                )\n                .where(LinkUserProjectV2.project_id == project_id)\n                .where(LinkUserProjectV2.is_owner.is_(True))\n            )\n            project_users[project_id] = [\n                ProjectUser(id=p_user[0], email=p_user[1])\n                for p_user in res.all()\n            ]\n\n        task_info_list.append(\n            dict(\n                task=task.model_dump(),\n                relationships=[\n                    dict(\n                        workflow_id=workflow.id,\n                        workflow_name=workflow.name,\n                        project_id=workflow.project.id,\n                        project_name=workflow.project.name,\n                        project_users=project_users[workflow.project_id],\n                    )\n                    for workflow in wf_list\n                ],\n            )\n        )\n    return dict(\n        total_count=total_count,\n        page_size=page_size,\n        current_page=page,\n        items=task_info_list,\n    )\n</code></pre>"},{"location":"reference/app/routes/admin/v2/task_group/","title":"task_group","text":""},{"location":"reference/app/routes/admin/v2/task_group_lifecycle/","title":"task_group_lifecycle","text":""},{"location":"reference/app/routes/admin/v2/task_group_lifecycle/#fractal_server.app.routes.admin.v2.task_group_lifecycle.deactivate_task_group","title":"<code>deactivate_task_group(task_group_id, background_tasks, response, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Deactivate task-group venv</p> Source code in <code>fractal_server/app/routes/admin/v2/task_group_lifecycle.py</code> <pre><code>@router.post(\n    \"/{task_group_id}/deactivate/\",\n    response_model=TaskGroupActivityRead,\n)\nasync def deactivate_task_group(\n    task_group_id: int,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupActivityRead:\n    \"\"\"\n    Deactivate task-group venv\n    \"\"\"\n    task_group = await _get_task_group_or_404(\n        task_group_id=task_group_id, db=db\n    )\n\n    # Check that task-group is active\n    if not task_group.active:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot deactivate a task group with {task_group.active=}.\"\n            ),\n        )\n\n    # Check no other activity is ongoing\n    await check_no_ongoing_activity(task_group_id=task_group_id, db=db)\n\n    # Check no submitted jobs use tasks from this task group\n    await check_no_submitted_job(task_group_id=task_group.id, db=db)\n\n    # Shortcut for task-group with origin=\"other\"\n    if task_group.origin == TaskGroupOriginEnum.OTHER:\n        task_group.active = False\n        task_group_activity = TaskGroupActivityV2(\n            user_id=task_group.user_id,\n            taskgroupv2_id=task_group.id,\n            status=TaskGroupActivityStatus.OK,\n            action=TaskGroupActivityAction.DEACTIVATE,\n            pkg_name=task_group.pkg_name,\n            version=(task_group.version or \"N/A\"),\n            log=(\n                f\"Task group has {task_group.origin=}, set \"\n                \"task_group.active to False and exit.\"\n            ),\n            timestamp_started=get_timestamp(),\n            timestamp_ended=get_timestamp(),\n            fractal_server_version=__VERSION__,\n        )\n        db.add(task_group)\n        db.add(task_group_activity)\n        await db.commit()\n        response.status_code = status.HTTP_202_ACCEPTED\n        return task_group_activity\n\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatus.PENDING,\n        action=TaskGroupActivityAction.DEACTIVATE,\n        pkg_name=task_group.pkg_name,\n        version=task_group.version,\n        timestamp_started=get_timestamp(),\n        fractal_server_version=__VERSION__,\n    )\n    db.add(task_group_activity)\n    await db.commit()\n\n    user = await db.get(UserOAuth, task_group.user_id)\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(user=user, db=db)\n\n    # Submit background task\n    if resource.type == ResourceType.SLURM_SSH:\n        deactivate_function = deactivate_ssh\n    else:\n        deactivate_function = deactivate_local\n\n    background_tasks.add_task(\n        deactivate_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        resource=resource,\n        profile=profile,\n    )\n\n    logger.debug(\n        \"Admin task group deactivation endpoint: start deactivate \"\n        \"and return task_group_activity\"\n    )\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/app/routes/admin/v2/task_group_lifecycle/#fractal_server.app.routes.admin.v2.task_group_lifecycle.reactivate_task_group","title":"<code>reactivate_task_group(task_group_id, background_tasks, response, superuser=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Deactivate task-group venv</p> Source code in <code>fractal_server/app/routes/admin/v2/task_group_lifecycle.py</code> <pre><code>@router.post(\n    \"/{task_group_id}/reactivate/\",\n    response_model=TaskGroupActivityRead,\n)\nasync def reactivate_task_group(\n    task_group_id: int,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    superuser: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupActivityRead:\n    \"\"\"\n    Deactivate task-group venv\n    \"\"\"\n\n    task_group = await _get_task_group_or_404(\n        task_group_id=task_group_id, db=db\n    )\n\n    # Check that task-group is not active\n    if task_group.active:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot reactivate a task group with {task_group.active=}.\"\n            ),\n        )\n\n    # Check no other activity is ongoing\n    await check_no_ongoing_activity(task_group_id=task_group_id, db=db)\n\n    # Check no submitted jobs use tasks from this task group\n    await check_no_submitted_job(task_group_id=task_group.id, db=db)\n\n    # Shortcut for task-group with origin=\"other\"\n    if task_group.origin == TaskGroupOriginEnum.OTHER:\n        task_group.active = True\n        task_group_activity = TaskGroupActivityV2(\n            user_id=task_group.user_id,\n            taskgroupv2_id=task_group.id,\n            status=TaskGroupActivityStatus.OK,\n            action=TaskGroupActivityAction.REACTIVATE,\n            pkg_name=task_group.pkg_name,\n            version=(task_group.version or \"N/A\"),\n            log=(\n                f\"Task group has {task_group.origin=}, set \"\n                \"task_group.active to True and exit.\"\n            ),\n            timestamp_started=get_timestamp(),\n            timestamp_ended=get_timestamp(),\n            fractal_server_version=__VERSION__,\n        )\n        db.add(task_group)\n        db.add(task_group_activity)\n        await db.commit()\n        response.status_code = status.HTTP_202_ACCEPTED\n        return task_group_activity\n\n    if task_group.env_info is None:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot reactivate a task group with {task_group.env_info=}.\"\n            ),\n        )\n\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatus.PENDING,\n        action=TaskGroupActivityAction.REACTIVATE,\n        pkg_name=task_group.pkg_name,\n        version=task_group.version,\n        timestamp_started=get_timestamp(),\n        fractal_server_version=__VERSION__,\n    )\n    db.add(task_group_activity)\n    await db.commit()\n\n    # Get validated resource and profile\n    user = await db.get(UserOAuth, task_group.user_id)\n    resource, profile = await validate_user_profile(user=user, db=db)\n\n    # Submit background task\n    if resource.type == ResourceType.SLURM_SSH:\n        reactivate_function = reactivate_ssh\n    else:\n        reactivate_function = reactivate_local\n\n    background_tasks.add_task(\n        reactivate_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        resource=resource,\n        profile=profile,\n    )\n\n    logger.debug(\n        \"Admin task group reactivation endpoint: start reactivate \"\n        \"and return task_group_activity\"\n    )\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/app/routes/api/","title":"api","text":""},{"location":"reference/app/routes/api/alive/","title":"alive","text":""},{"location":"reference/app/routes/api/settings/","title":"settings","text":""},{"location":"reference/app/routes/api/v2/","title":"v2","text":"<p><code>api/v2</code> module</p>"},{"location":"reference/app/routes/api/v2/_aux_functions/","title":"_aux_functions","text":"<p>Auxiliary functions to get object from the database or perform simple checks</p>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._check_project_exists","title":"<code>_check_project_exists(*, project_name, user_id, db)</code>  <code>async</code>","text":"<p>Check that no other project with this name exists for this user.</p> PARAMETER DESCRIPTION <code>project_name</code> <p>Project name</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>User ID</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> RAISES DESCRIPTION <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If such a project already exists</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _check_project_exists(\n    *,\n    project_name: str,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Check that no other project with this name exists for this user.\n\n    Args:\n        project_name: Project name\n        user_id: User ID\n        db:\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If such a project already exists\n    \"\"\"\n    stm = (\n        select(ProjectV2)\n        .join(LinkUserProjectV2, LinkUserProjectV2.project_id == ProjectV2.id)\n        .where(ProjectV2.name == project_name)\n        .where(LinkUserProjectV2.user_id == user_id)\n        .where(LinkUserProjectV2.is_owner.is_(True))\n    )\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Project name ({project_name}) already in use\",\n        )\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._check_workflow_exists","title":"<code>_check_workflow_exists(*, name, project_id, db)</code>  <code>async</code>","text":"<p>Check that no other workflow of this project has the same name.</p> PARAMETER DESCRIPTION <code>name</code> <p>Workflow name</p> <p> TYPE: <code>str</code> </p> <code>project_id</code> <p>Project ID</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> RAISES DESCRIPTION <code>HTTPException(status_code=422_UNPROCESSABLE_ENTITY)</code> <p>If such a workflow already exists</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _check_workflow_exists(\n    *,\n    name: str,\n    project_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Check that no other workflow of this project has the same name.\n\n    Args:\n        name: Workflow name\n        project_id: Project ID\n        db:\n\n    Raises:\n        HTTPException(status_code=422_UNPROCESSABLE_ENTITY):\n            If such a workflow already exists\n    \"\"\"\n    stm = (\n        select(WorkflowV2)\n        .where(WorkflowV2.name == name)\n        .where(WorkflowV2.project_id == project_id)\n    )\n    res = await db.execute(stm)\n    if res.scalars().all():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Workflow with {name=} and {project_id=} already exists.\",\n        )\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_dataset_check_access","title":"<code>_get_dataset_check_access(*, project_id, dataset_id, user_id, required_permissions, db)</code>  <code>async</code>","text":"<p>Get a dataset and a project, after access control on the project</p> PARAMETER DESCRIPTION <code>project_id</code> <p> TYPE: <code>int</code> </p> <code>dataset_id</code> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>DatasetOrProject</code> <p>Dictionary with the dataset and project objects (keys: <code>dataset</code>, <code>project</code>).</p> RAISES DESCRIPTION <code>HTTPException(status_code=404_UNPROCESSABLE_ENTITY)</code> <p>If the project or the dataset do not exist or if they are not associated</p> <code>HTTPException(status_code=403_FORBIDDEN)</code> <p>If the user is not a member of the project</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_dataset_check_access(\n    *,\n    project_id: int,\n    dataset_id: int,\n    user_id: int,\n    required_permissions: ProjectPermissions,\n    db: AsyncSession,\n) -&gt; DatasetOrProject:\n    \"\"\"\n    Get a dataset and a project, after access control on the project\n\n    Args:\n        project_id:\n        dataset_id:\n        user_id:\n        db:\n\n    Returns:\n        Dictionary with the dataset and project objects (keys: `dataset`,\n            `project`).\n\n    Raises:\n        HTTPException(status_code=404_UNPROCESSABLE_ENTITY):\n            If the project or the dataset do not exist or if they are not\n            associated\n        HTTPException(status_code=403_FORBIDDEN):\n            If the user is not a member of the project\n    \"\"\"\n    # Access control for project\n    project = await _get_project_check_access(\n        project_id=project_id,\n        user_id=user_id,\n        required_permissions=required_permissions,\n        db=db,\n    )\n\n    res = await db.execute(\n        select(DatasetV2)\n        .where(DatasetV2.id == dataset_id)\n        .where(DatasetV2.project_id == project_id)\n        .execution_options(populate_existing=True)  # See issue 1087\n    )\n    dataset = res.scalars().one_or_none()\n\n    if dataset is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Dataset not found\"\n        )\n\n    return dict(dataset=dataset, project=project)\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_dataset_or_404","title":"<code>_get_dataset_or_404(*, dataset_id, db)</code>  <code>async</code>","text":"<p>Get a dataset or raise 404.</p> PARAMETER DESCRIPTION <code>dataset_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_dataset_or_404(\n    *,\n    dataset_id: int,\n    db: AsyncSession,\n) -&gt; DatasetV2:\n    \"\"\"\n    Get a dataset or raise 404.\n\n    Args:\n        dataset_id:\n        db:\n    \"\"\"\n    ds = await db.get(DatasetV2, dataset_id)\n    if ds is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Dataset {dataset_id} not found.\",\n        )\n    else:\n        return ds\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_job_check_access","title":"<code>_get_job_check_access(*, project_id, job_id, user_id, required_permissions, db)</code>  <code>async</code>","text":"<p>Get a job and a project, after access control on the project</p> PARAMETER DESCRIPTION <code>project_id</code> <p> TYPE: <code>int</code> </p> <code>job_id</code> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>JobAndProject</code> <p>Dictionary with the job and project objects (keys: <code>job</code>, <code>project</code>).</p> RAISES DESCRIPTION <code>HTTPException(status_code=404_UNPROCESSABLE_ENTITY)</code> <p>If the project or the job do not exist or if they are not associated</p> <code>HTTPException(status_code=403_FORBIDDEN)</code> <p>If the user is not a member of the project</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_job_check_access(\n    *,\n    project_id: int,\n    job_id: int,\n    user_id: int,\n    required_permissions: ProjectPermissions,\n    db: AsyncSession,\n) -&gt; JobAndProject:\n    \"\"\"\n    Get a job and a project, after access control on the project\n\n    Args:\n        project_id:\n        job_id:\n        user_id:\n        db:\n\n    Returns:\n        Dictionary with the job and project objects (keys: `job`,\n            `project`).\n\n    Raises:\n        HTTPException(status_code=404_UNPROCESSABLE_ENTITY):\n            If the project or the job do not exist or if they are not\n            associated\n        HTTPException(status_code=403_FORBIDDEN):\n            If the user is not a member of the project\n    \"\"\"\n    # Access control for project\n    project = await _get_project_check_access(\n        project_id=project_id,\n        user_id=user_id,\n        required_permissions=required_permissions,\n        db=db,\n    )\n\n    res = await db.execute(\n        select(JobV2)\n        .where(JobV2.id == job_id)\n        .where(JobV2.project_id == project_id)\n    )\n    job = res.scalars().one_or_none()\n\n    if job is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Job not found\"\n        )\n\n    return dict(job=job, project=project)\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_project_check_access","title":"<code>_get_project_check_access(*, project_id, user_id, required_permissions, db)</code>  <code>async</code>","text":"<p>Check that user is a member of project and return the project.</p> PARAMETER DESCRIPTION <code>project_id</code> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>required_permissions</code> <p> TYPE: <code>ProjectPermissions</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>ProjectV2</code> <p>The project object</p> RAISES DESCRIPTION <code>HTTPException(status_code=403_FORBIDDEN)</code> <ul> <li>If the user is not a member of the project;</li> <li>If the user has not accepted the invitation yet;</li> <li>If the user has not the target permissions.</li> </ul> <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the project does not exist</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_project_check_access(\n    *,\n    project_id: int,\n    user_id: int,\n    required_permissions: ProjectPermissions,\n    db: AsyncSession,\n) -&gt; ProjectV2:\n    \"\"\"\n    Check that user is a member of project and return the project.\n\n    Args:\n        project_id:\n        user_id:\n        required_permissions:\n        db:\n\n    Returns:\n        The project object\n\n    Raises:\n        HTTPException(status_code=403_FORBIDDEN):\n            - If the user is not a member of the project;\n            - If the user has not accepted the invitation yet;\n            - If the user has not the target permissions.\n        HTTPException(status_code=404_NOT_FOUND):\n            If the project does not exist\n    \"\"\"\n    project = await db.get(ProjectV2, project_id)\n    if project is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Project not found\"\n        )\n\n    link_user_project = await db.get(LinkUserProjectV2, (project_id, user_id))\n    if (\n        link_user_project is None\n        or not link_user_project.is_verified\n        or required_permissions not in link_user_project.permissions\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=(\n                \"You are not authorized to perform this action. \"\n                \"If you think this is by mistake, \"\n                \"please contact the project owner.\"\n            ),\n        )\n\n    return project\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_submitted_jobs_statement","title":"<code>_get_submitted_jobs_statement()</code>","text":"RETURNS DESCRIPTION <code>SelectOfScalar</code> <p>A sqlmodel statement that selects all <code>Job</code>s with</p> <code>SelectOfScalar</code> <p><code>Job.status</code> equal to <code>submitted</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>def _get_submitted_jobs_statement() -&gt; SelectOfScalar:\n    \"\"\"\n    Returns:\n        A sqlmodel statement that selects all `Job`s with\n        `Job.status` equal to `submitted`.\n    \"\"\"\n    stm = select(JobV2).where(JobV2.status == JobStatusType.SUBMITTED)\n    return stm\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_workflow_check_access","title":"<code>_get_workflow_check_access(*, workflow_id, project_id, user_id, required_permissions, db)</code>  <code>async</code>","text":"<p>Get a workflow and a project, after access control on the project.</p> PARAMETER DESCRIPTION <code>workflow_id</code> <p> TYPE: <code>int</code> </p> <code>project_id</code> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>WorkflowV2</code> <p>The workflow object.</p> RAISES DESCRIPTION <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the project or the workflow do not exist or if they are not associated</p> <code>HTTPException(status_code=403_FORBIDDEN)</code> <p>If the user is not a member of the project</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_workflow_check_access(\n    *,\n    workflow_id: int,\n    project_id: int,\n    user_id: int,\n    required_permissions: ProjectPermissions,\n    db: AsyncSession,\n) -&gt; WorkflowV2:\n    \"\"\"\n    Get a workflow and a project, after access control on the project.\n\n    Args:\n        workflow_id:\n        project_id:\n        user_id:\n        db:\n\n    Returns:\n        The workflow object.\n\n    Raises:\n        HTTPException(status_code=404_NOT_FOUND):\n            If the project or the workflow do not exist or if they are not\n            associated\n        HTTPException(status_code=403_FORBIDDEN):\n            If the user is not a member of the project\n    \"\"\"\n\n    # Access control for project\n    await _get_project_check_access(\n        project_id=project_id,\n        user_id=user_id,\n        required_permissions=required_permissions,\n        db=db,\n    )\n\n    res = await db.execute(\n        select(WorkflowV2)\n        .where(WorkflowV2.id == workflow_id)\n        .where(WorkflowV2.project_id == project_id)\n        .execution_options(populate_existing=True)  # See issue 1087\n    )\n    workflow = res.scalars().one_or_none()\n\n    if not workflow:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Workflow not found\"\n        )\n\n    return workflow\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_workflow_or_404","title":"<code>_get_workflow_or_404(*, workflow_id, db)</code>  <code>async</code>","text":"<p>Get a workflow or raise 404.</p> PARAMETER DESCRIPTION <code>workflow_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_workflow_or_404(\n    *,\n    workflow_id: int,\n    db: AsyncSession,\n) -&gt; WorkflowV2:\n    \"\"\"\n    Get a workflow or raise 404.\n\n    Args:\n        workflow_id:\n        db:\n    \"\"\"\n    wf = await db.get(WorkflowV2, workflow_id)\n    if wf is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Workflow {workflow_id} not found.\",\n        )\n    else:\n        return wf\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_workflow_task_check_access","title":"<code>_get_workflow_task_check_access(*, project_id, workflow_id, workflow_task_id, user_id, required_permissions, db)</code>  <code>async</code>","text":"<p>Check that user has access to Workflow and WorkflowTask.</p> PARAMETER DESCRIPTION <code>project_id</code> <p> TYPE: <code>int</code> </p> <code>workflow_id</code> <p> TYPE: <code>int</code> </p> <code>workflow_task_id</code> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>tuple[WorkflowTaskV2, WorkflowV2]</code> <p>Tuple of WorkflowTask and Workflow objects.</p> RAISES DESCRIPTION <code>HTTPException(status_code=404_NOT_FOUND)</code> <p>If the project, the workflow or the workflowtask do not exist or if they are not associated</p> <code>HTTPException(status_code=403_FORBIDDEN)</code> <p>If the user is not a member of the project</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_workflow_task_check_access(\n    *,\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    user_id: int,\n    required_permissions: ProjectPermissions,\n    db: AsyncSession,\n) -&gt; tuple[WorkflowTaskV2, WorkflowV2]:\n    \"\"\"\n    Check that user has access to Workflow and WorkflowTask.\n\n    Args:\n        project_id:\n        workflow_id:\n        workflow_task_id:\n        user_id:\n        db:\n\n    Returns:\n        Tuple of WorkflowTask and Workflow objects.\n\n    Raises:\n        HTTPException(status_code=404_NOT_FOUND):\n            If the project, the workflow or the workflowtask do not exist or\n            if they are not associated\n        HTTPException(status_code=403_FORBIDDEN):\n            If the user is not a member of the project\n    \"\"\"\n\n    # Access control for workflow\n    workflow = await _get_workflow_check_access(\n        workflow_id=workflow_id,\n        project_id=project_id,\n        user_id=user_id,\n        required_permissions=required_permissions,\n        db=db,\n    )\n\n    res = await db.execute(\n        select(WorkflowTaskV2)\n        .where(WorkflowTaskV2.id == workflow_task_id)\n        .where(WorkflowTaskV2.workflow_id == workflow_id)\n    )\n    workflow_task = res.scalars().one_or_none()\n\n    if workflow_task is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=\"WorkflowTask not found\",\n        )\n\n    return workflow_task, workflow\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._get_workflowtask_or_404","title":"<code>_get_workflowtask_or_404(*, workflowtask_id, db)</code>  <code>async</code>","text":"<p>Get a workflow task or raise 404.</p> PARAMETER DESCRIPTION <code>workflowtask_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _get_workflowtask_or_404(\n    *,\n    workflowtask_id: int,\n    db: AsyncSession,\n) -&gt; WorkflowTaskV2:\n    \"\"\"\n    Get a workflow task or raise 404.\n\n    Args:\n        workflowtask_id:\n        db:\n    \"\"\"\n    wftask = await db.get(WorkflowTaskV2, workflowtask_id)\n    if wftask is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"WorkflowTask {workflowtask_id} not found.\",\n        )\n    else:\n        return wftask\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions._workflow_insert_task","title":"<code>_workflow_insert_task(*, workflow_id, task_id, meta_parallel=None, meta_non_parallel=None, args_non_parallel=None, args_parallel=None, type_filters=None, description=None, alias=None, db)</code>  <code>async</code>","text":"<p>Insert a new WorkflowTask into Workflow.task_list</p> PARAMETER DESCRIPTION <code>workflow_id</code> <p> TYPE: <code>int</code> </p> <code>task_id</code> <p> TYPE: <code>int</code> </p> <code>meta_parallel</code> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>meta_non_parallel</code> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>args_non_parallel</code> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>args_parallel</code> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>type_filters</code> <p> TYPE: <code>dict[str, bool] | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>alias</code> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def _workflow_insert_task(\n    *,\n    workflow_id: int,\n    task_id: int,\n    meta_parallel: dict[str, Any] | None = None,\n    meta_non_parallel: dict[str, Any] | None = None,\n    args_non_parallel: dict[str, Any] | None = None,\n    args_parallel: dict[str, Any] | None = None,\n    type_filters: dict[str, bool] | None = None,\n    description: str | None = None,\n    alias: str | None = None,\n    db: AsyncSession,\n) -&gt; WorkflowTaskV2:\n    \"\"\"\n    Insert a new WorkflowTask into Workflow.task_list\n\n    Args:\n        workflow_id:\n        task_id:\n\n        meta_parallel:\n        meta_non_parallel:\n        args_non_parallel:\n        args_parallel:\n        type_filters:\n        description:\n        alias:\n        db:\n    \"\"\"\n    db_workflow = await db.get(WorkflowV2, workflow_id)\n    if db_workflow is None:\n        raise ValueError(f\"Workflow {workflow_id} does not exist\")\n\n    # Get task from db\n    db_task = await db.get(TaskV2, task_id)\n    if db_task is None:\n        raise ValueError(f\"TaskV2 {task_id} not found.\")\n    task_type = db_task.type\n\n    # Combine meta_parallel (higher priority)\n    # and db_task.meta_parallel (lower priority)\n    final_meta_parallel = (db_task.meta_parallel or {}).copy()\n    final_meta_parallel.update(meta_parallel or {})\n    if final_meta_parallel == {}:\n        final_meta_parallel = None\n    # Combine meta_non_parallel (higher priority)\n    # and db_task.meta_non_parallel (lower priority)\n    final_meta_non_parallel = (db_task.meta_non_parallel or {}).copy()\n    final_meta_non_parallel.update(meta_non_parallel or {})\n    if final_meta_non_parallel == {}:\n        final_meta_non_parallel = None\n\n    # Create DB entry\n    wf_task = WorkflowTaskV2(\n        task_type=task_type,\n        task_id=task_id,\n        args_non_parallel=args_non_parallel,\n        args_parallel=args_parallel,\n        meta_parallel=final_meta_parallel,\n        meta_non_parallel=final_meta_non_parallel,\n        type_filters=(type_filters or dict()),\n        description=description,\n        alias=alias,\n    )\n    db_workflow.task_list.append(wf_task)\n    flag_modified(db_workflow, \"task_list\")\n    await db.commit()\n\n    wf_task = await db.get(\n        WorkflowTaskV2,\n        wf_task.id,\n        populate_existing=True,  # See issue 1087\n    )\n\n    return wf_task\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions/#fractal_server.app.routes.api.v2._aux_functions.clean_app_job_list","title":"<code>clean_app_job_list(db, jobs_list)</code>  <code>async</code>","text":"<p>Remove from a job list all jobs with status different from submitted.</p> PARAMETER DESCRIPTION <code>db</code> <p>Async database session</p> <p> TYPE: <code>AsyncSession</code> </p> <code>jobs_list</code> <p>List of job IDs currently associated to the app.</p> <p> TYPE: <code>list[int]</code> </p> Return <p>List of IDs for submitted jobs.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions.py</code> <pre><code>async def clean_app_job_list(\n    db: AsyncSession,\n    jobs_list: list[int],\n) -&gt; list[int]:\n    \"\"\"\n    Remove from a job list all jobs with status different from submitted.\n\n    Args:\n        db: Async database session\n        jobs_list: List of job IDs currently associated to the app.\n\n    Return:\n        List of IDs for submitted jobs.\n    \"\"\"\n    logger.info(f\"[clean_app_job_list] START - {jobs_list=}.\")\n    stmt = select(JobV2).where(JobV2.id.in_(jobs_list))\n    result = await db.execute(stmt)\n    db_jobs_list = result.scalars().all()\n    submitted_job_ids = [\n        job.id for job in db_jobs_list if job.status == JobStatusType.SUBMITTED\n    ]\n    logger.info(f\"[clean_app_job_list] END - {submitted_job_ids=}.\")\n    return submitted_job_ids\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_history/","title":"_aux_functions_history","text":""},{"location":"reference/app/routes/api/v2/_aux_functions_history/#fractal_server.app.routes.api.v2._aux_functions_history._verify_workflow_and_dataset_access","title":"<code>_verify_workflow_and_dataset_access(*, project_id, workflow_id, dataset_id, user_id, required_permissions, db)</code>  <code>async</code>","text":"<p>Verify user access to a dataset/workflow pair.</p> PARAMETER DESCRIPTION <code>project_id</code> <p> TYPE: <code>int</code> </p> <code>workflow_id</code> <p> TYPE: <code>int</code> </p> <code>dataset_id</code> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_history.py</code> <pre><code>async def _verify_workflow_and_dataset_access(\n    *,\n    project_id: int,\n    workflow_id: int,\n    dataset_id: int,\n    user_id: int,\n    required_permissions: ProjectPermissions,\n    db: AsyncSession,\n) -&gt; dict[Literal[\"dataset\", \"workflow\"], DatasetV2 | WorkflowV2]:\n    \"\"\"\n    Verify user access to a dataset/workflow pair.\n\n    Args:\n        project_id:\n        workflow_id:\n        dataset_id:\n        user_id:\n        db:\n    \"\"\"\n    await _get_project_check_access(\n        project_id=project_id,\n        user_id=user_id,\n        required_permissions=required_permissions,\n        db=db,\n    )\n    workflow = await _get_workflow_or_404(\n        workflow_id=workflow_id,\n        db=db,\n    )\n    if workflow.project_id != project_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Workflow does not belong to expected project.\",\n        )\n    dataset = await _get_dataset_or_404(\n        dataset_id=dataset_id,\n        db=db,\n    )\n    if dataset.project_id != project_id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Dataset does not belong to expected project.\",\n        )\n\n    return dict(dataset=dataset, workflow=workflow)\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_history/#fractal_server.app.routes.api.v2._aux_functions_history.get_history_run_or_404","title":"<code>get_history_run_or_404(*, history_run_id, db)</code>  <code>async</code>","text":"<p>Get an existing HistoryRun  or raise a 404.</p> PARAMETER DESCRIPTION <code>history_run_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_history.py</code> <pre><code>async def get_history_run_or_404(\n    *, history_run_id: int, db: AsyncSession\n) -&gt; HistoryRun:\n    \"\"\"\n    Get an existing HistoryRun  or raise a 404.\n\n    Args:\n        history_run_id:\n        db:\n    \"\"\"\n    history_run = await db.get(HistoryRun, history_run_id)\n    if history_run is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"HistoryRun {history_run_id} not found\",\n        )\n    return history_run\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_history/#fractal_server.app.routes.api.v2._aux_functions_history.get_history_unit_or_404","title":"<code>get_history_unit_or_404(*, history_unit_id, db)</code>  <code>async</code>","text":"<p>Get an existing HistoryUnit  or raise a 404.</p> PARAMETER DESCRIPTION <code>history_unit_id</code> <p>The <code>HistoryUnit</code> id</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>An asynchronous db session</p> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_history.py</code> <pre><code>async def get_history_unit_or_404(\n    *, history_unit_id: int, db: AsyncSession\n) -&gt; HistoryUnit:\n    \"\"\"\n    Get an existing HistoryUnit  or raise a 404.\n\n    Args:\n        history_unit_id: The `HistoryUnit` id\n        db: An asynchronous db session\n    \"\"\"\n    history_unit = await db.get(HistoryUnit, history_unit_id)\n    if history_unit is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"HistoryUnit {history_unit_id} not found\",\n        )\n    return history_unit\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_history/#fractal_server.app.routes.api.v2._aux_functions_history.get_wftask_check_access","title":"<code>get_wftask_check_access(*, project_id, dataset_id, workflowtask_id, user_id, required_permissions, db)</code>  <code>async</code>","text":"<p>Verify user access for the history of this dataset and workflowtask.</p> PARAMETER DESCRIPTION <code>project_id</code> <p> TYPE: <code>int</code> </p> <code>dataset_id</code> <p> TYPE: <code>int</code> </p> <code>workflowtask_id</code> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_history.py</code> <pre><code>async def get_wftask_check_access(\n    *,\n    project_id: int,\n    dataset_id: int,\n    workflowtask_id: int,\n    user_id: int,\n    required_permissions: ProjectPermissions,\n    db: AsyncSession,\n) -&gt; WorkflowTaskV2:\n    \"\"\"\n    Verify user access for the history of this dataset and workflowtask.\n\n    Args:\n        project_id:\n        dataset_id:\n        workflowtask_id:\n        user_id:\n        db:\n    \"\"\"\n    wftask = await _get_workflowtask_or_404(\n        workflowtask_id=workflowtask_id,\n        db=db,\n    )\n    await _verify_workflow_and_dataset_access(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        workflow_id=wftask.workflow_id,\n        required_permissions=required_permissions,\n        user_id=user_id,\n        db=db,\n    )\n    return wftask\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_history/#fractal_server.app.routes.api.v2._aux_functions_history.read_log_file","title":"<code>read_log_file(*, task_name, dataset_id, logfile, job_working_dir, job_status)</code>","text":"<p>Returns the contents of a Job's log file, either directly from the working directory or from the corresponding ZIP archive.</p> <p>The function first checks if <code>logfile</code> exists on disk.</p> <p>If not, it checks if the Job working directory has been zipped and tries to read <code>logfile</code> from within the archive. (Note: it is assumed that <code>logfile</code> is relative to <code>job_working_dir</code>)</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_history.py</code> <pre><code>def read_log_file(\n    *,\n    task_name: str,\n    dataset_id: int,\n    logfile: str,\n    job_working_dir: str,\n    job_status: JobStatusType,\n) -&gt; str:\n    \"\"\"\n    Returns the contents of a Job's log file, either directly from the working\n    directory or from the corresponding ZIP archive.\n\n    The function first checks if `logfile` exists on disk.\n\n    If not, it checks if the Job working directory has been zipped and tries to\n    read `logfile` from within the archive.\n    (Note: it is assumed that `logfile` is relative to `job_working_dir`)\n    \"\"\"\n    archive_path = os.path.normpath(job_working_dir) + \".zip\"\n    try:\n        if Path(logfile).exists():\n            with open(logfile) as f:\n                return f.read()\n        elif Path(archive_path).exists():\n            relative_logfile = (\n                Path(logfile).relative_to(job_working_dir).as_posix()\n            )\n            return _read_single_file_from_zip(\n                file_path=relative_logfile, archive_path=archive_path\n            )\n        else:\n            match job_status:\n                case JobStatusType.SUBMITTED:\n                    logger.info(\n                        f\"Neither {logfile=} nor {archive_path=} exist \"\n                        \"(for submitted job).\"\n                    )\n                case _:\n                    logger.warning(\n                        f\"Error while retrieving logs for {logfile=} and \"\n                        f\"{archive_path=}.\"\n                    )\n            return (\n                f\"Logs for task '{task_name}' in dataset \"\n                f\"{dataset_id} are not available.\"\n            )\n    except Exception as e:\n        logger.error(\n            f\"Error while retrieving logs for {logfile=} and {archive_path=}. \"\n            f\"Original error: {str(e)}\"\n        )\n        return (\n            f\"Error while retrieving logs for task '{task_name}' \"\n            f\"in dataset {dataset_id}.\"\n        )\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_sharing/","title":"_aux_functions_sharing","text":""},{"location":"reference/app/routes/api/v2/_aux_functions_sharing/#fractal_server.app.routes.api.v2._aux_functions_sharing.get_link_or_404","title":"<code>get_link_or_404(*, user_id, project_id, db)</code>  <code>async</code>","text":"<p>Raises 404 if User[<code>user_id</code>] is not linked to Project[<code>project_id</code>], regardless of whether the User or Project exists.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_sharing.py</code> <pre><code>async def get_link_or_404(\n    *, user_id: int, project_id: int, db: AsyncSession\n) -&gt; LinkUserProjectV2:\n    \"\"\"\n    Raises 404 if User[`user_id`] is not linked to Project[`project_id`],\n    regardless of whether the User or Project exists.\n    \"\"\"\n    link = await db.get(LinkUserProjectV2, (project_id, user_id))\n    if link is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=\"User is not linked to project.\",\n        )\n    return link\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_sharing/#fractal_server.app.routes.api.v2._aux_functions_sharing.get_pending_invitation_or_404","title":"<code>get_pending_invitation_or_404(*, user_id, project_id, db)</code>  <code>async</code>","text":"<p>Raises 404 if User[<code>user_id</code>] has not a pending invitation to Project[<code>project_id</code>], regardless of whether the User or Project exists.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_sharing.py</code> <pre><code>async def get_pending_invitation_or_404(\n    *, user_id: int, project_id: int, db: AsyncSession\n) -&gt; LinkUserProjectV2:\n    \"\"\"\n    Raises 404 if User[`user_id`] has not a pending invitation to\n    Project[`project_id`], regardless of whether the User or Project exists.\n    \"\"\"\n    link = await get_link_or_404(user_id=user_id, project_id=project_id, db=db)\n    if link.is_verified:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=\"No pending invitation for user on this project.\",\n        )\n    return link\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_sharing/#fractal_server.app.routes.api.v2._aux_functions_sharing.get_user_id_from_email_or_404","title":"<code>get_user_id_from_email_or_404(*, user_email, db)</code>  <code>async</code>","text":"<p>Raises 404 if there is no User with email <code>user_email</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_sharing.py</code> <pre><code>async def get_user_id_from_email_or_404(\n    *, user_email: str, db: AsyncSession\n) -&gt; int:\n    \"\"\"\n    Raises 404 if there is no User with email `user_email`.\n    \"\"\"\n    res = await db.execute(\n        select(UserOAuth.id).where(UserOAuth.email == user_email)\n    )\n    user_id = res.scalar_one_or_none()\n    if user_id is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"User not found.\"\n        )\n    return user_id\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_sharing/#fractal_server.app.routes.api.v2._aux_functions_sharing.raise_403_if_not_owner","title":"<code>raise_403_if_not_owner(*, user_id, project_id, db)</code>  <code>async</code>","text":"<p>Raises 403 if User[<code>user_id</code>] is not owner of Project[<code>project_id</code>], regardless of whether the User or Project exists.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_sharing.py</code> <pre><code>async def raise_403_if_not_owner(\n    *,\n    user_id: int,\n    project_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Raises 403 if User[`user_id`] is not owner of Project[`project_id`],\n    regardless of whether the User or Project exists.\n    \"\"\"\n    res = await db.execute(\n        select(LinkUserProjectV2)\n        .where(LinkUserProjectV2.project_id == project_id)\n        .where(LinkUserProjectV2.user_id == user_id)\n        .where(LinkUserProjectV2.is_owner.is_(True))\n    )\n    link = res.scalars().one_or_none()\n    if link is None:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Current user is not the project owner.\",\n        )\n    return link\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_sharing/#fractal_server.app.routes.api.v2._aux_functions_sharing.raise_422_if_link_exists","title":"<code>raise_422_if_link_exists(*, user_id, project_id, db)</code>  <code>async</code>","text":"<p>Raises 422 if User[<code>user_id</code>] is linked Project[<code>project_id</code>], regardless of whether the User or Project exists.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_sharing.py</code> <pre><code>async def raise_422_if_link_exists(\n    *, user_id: int, project_id: int, db: AsyncSession\n) -&gt; None:\n    \"\"\"\n    Raises 422 if User[`user_id`] is linked Project[`project_id`], regardless\n    of whether the User or Project exists.\n    \"\"\"\n    link = await db.get(LinkUserProjectV2, (project_id, user_id))\n    if link is not None:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"User is already associated to project.\",\n        )\n    return\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_task_lifecycle/","title":"_aux_functions_task_lifecycle","text":""},{"location":"reference/app/routes/api/v2/_aux_functions_task_lifecycle/#fractal_server.app.routes.api.v2._aux_functions_task_lifecycle.check_no_ongoing_activity","title":"<code>check_no_ongoing_activity(*, task_group_id, db)</code>  <code>async</code>","text":"<p>Find ongoing activities for the same task group.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle.py</code> <pre><code>async def check_no_ongoing_activity(\n    *,\n    task_group_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Find ongoing activities for the same task group.\n\n    Args:\n        task_group_id:\n        db:\n    \"\"\"\n    # DB query\n    stm = (\n        select(TaskGroupActivityV2)\n        .where(TaskGroupActivityV2.taskgroupv2_id == task_group_id)\n        .where(TaskGroupActivityV2.status == TaskGroupActivityStatus.ONGOING)\n    )\n    res = await db.execute(stm)\n    ongoing_activities = res.scalars().all()\n\n    if ongoing_activities == []:\n        # All good, exit\n        return\n\n    msg = \"Found ongoing activities for the same task-group:\"\n    for ind, activity in enumerate(ongoing_activities):\n        msg = (\n            f\"{msg}\\n{ind + 1}) \"\n            f\"Action={activity.action}, \"\n            f\"status={activity.status}, \"\n            f\"timestamp_started={activity.timestamp_started}.\"\n        )\n    raise HTTPException(\n        status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n        detail=msg,\n    )\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_task_lifecycle/#fractal_server.app.routes.api.v2._aux_functions_task_lifecycle.check_no_related_workflowtask","title":"<code>check_no_related_workflowtask(*, task_group, db)</code>  <code>async</code>","text":"<p>Raises an HTTPException if any of the tasks in the TaskGroup are referenced by an existing WorkflowTask.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle.py</code> <pre><code>async def check_no_related_workflowtask(\n    *,\n    task_group: TaskGroupV2,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Raises an HTTPException if any of the tasks in the TaskGroup are referenced\n    by an existing WorkflowTask.\n    \"\"\"\n    stm = select(WorkflowTaskV2).where(\n        WorkflowTaskV2.task_id.in_([task.id for task in task_group.task_list])\n    )\n    res = await db.execute(stm)\n    bad_wftask = res.scalars().first()\n    if bad_wftask is not None:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"TaskV2 {bad_wftask.task_id} is still in use\",\n        )\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_task_lifecycle/#fractal_server.app.routes.api.v2._aux_functions_task_lifecycle.check_no_submitted_job","title":"<code>check_no_submitted_job(*, task_group_id, db)</code>  <code>async</code>","text":"<p>Find submitted jobs which include tasks from a given task group.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p>ID of the <code>TaskGroupV2</code> object.</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>Asynchronous database session.</p> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle.py</code> <pre><code>async def check_no_submitted_job(\n    *,\n    task_group_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Find submitted jobs which include tasks from a given task group.\n\n    Args:\n        task_group_id: ID of the `TaskGroupV2` object.\n        db: Asynchronous database session.\n    \"\"\"\n    stm = (\n        select(func.count(JobV2.id))\n        .join(WorkflowV2, JobV2.workflow_id == WorkflowV2.id)\n        .join(WorkflowTaskV2, WorkflowTaskV2.workflow_id == WorkflowV2.id)\n        .join(TaskV2, WorkflowTaskV2.task_id == TaskV2.id)\n        .where(WorkflowTaskV2.order &gt;= JobV2.first_task_index)\n        .where(WorkflowTaskV2.order &lt;= JobV2.last_task_index)\n        .where(JobV2.status == JobStatusType.SUBMITTED)\n        .where(TaskV2.taskgroupv2_id == task_group_id)\n    )\n    res = await db.execute(stm)\n    num_submitted_jobs = res.scalar()\n    if num_submitted_jobs &gt; 0:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot act on task group because {num_submitted_jobs} \"\n                \"submitted jobs use its tasks.\"\n            ),\n        )\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_task_lifecycle/#fractal_server.app.routes.api.v2._aux_functions_task_lifecycle.get_package_version_from_pypi","title":"<code>get_package_version_from_pypi(name, version=None)</code>  <code>async</code>","text":"<p>Make a GET call to PyPI JSON API and get latest compatible version.</p> <p>There are three cases:</p> <ol> <li><code>version</code> is set and it is found on PyPI as-is.</li> <li><code>version</code> is set but it is not found on PyPI as-is.</li> <li><code>version</code> is unset, and we query <code>PyPI</code> for latest.</li> </ol> <p>Ref https://warehouse.pypa.io/api-reference/json.html.</p> PARAMETER DESCRIPTION <code>name</code> <p>Package name.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Could be a correct version (<code>1.3.0</code>), an incomplete one (<code>1.3</code>) or <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_lifecycle.py</code> <pre><code>async def get_package_version_from_pypi(\n    name: str,\n    version: str | None = None,\n) -&gt; str:\n    \"\"\"\n    Make a GET call to PyPI JSON API and get latest *compatible* version.\n\n    There are three cases:\n\n    1. `version` is set and it is found on PyPI as-is.\n    2. `version` is set but it is not found on PyPI as-is.\n    3. `version` is unset, and we query `PyPI` for latest.\n\n    Ref https://warehouse.pypa.io/api-reference/json.html.\n\n    Args:\n        name: Package name.\n        version:\n            Could be a correct version (`1.3.0`), an incomplete one\n            (`1.3`) or `None`.\n    \"\"\"\n    normalized_name = normalize_package_name(name)\n    url = f\"https://pypi.org/simple/{normalized_name}/\"\n    hint = f\"Hint: specify the required version for '{name}'.\"\n\n    # Make request to PyPI\n    try:\n        async with AsyncClient(timeout=5.0) as client:\n            res = await client.get(url, headers=PYPI_JSON_HEADERS)\n    except TimeoutException as e:\n        error_msg = (\n            f\"A TimeoutException occurred while getting {url}.\\n\"\n            f\"Original error: {str(e)}.\"\n        )\n        logger.warning(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=error_msg,\n        )\n    except BaseException as e:\n        error_msg = (\n            f\"An unknown error occurred while getting {url}. \"\n            f\"Original error: {str(e)}.\"\n        )\n        logger.warning(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=error_msg,\n        )\n\n    # Parse response\n    if res.status_code != 200:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Could not get {url} (status_code {res.status_code}).\\n{hint}\"\n            ),\n        )\n    try:\n        response_data = res.json()\n        available_releases = response_data[\"versions\"]\n        latest_version = _find_latest_version_or_422(available_releases)\n    except KeyError as e:\n        logger.warning(\n            f\"A KeyError occurred while getting {url}. \"\n            f\"Original error: {str(e)}.\"\n        )\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"A KeyError error occurred while getting {url}.\\n{hint}\",\n        )\n\n    logger.info(\n        f\"Obtained data from {url}: \"\n        f\"{len(available_releases)} releases, \"\n        f\"latest={latest_version}.\"\n    )\n\n    if version is not None:\n        if version in available_releases:\n            logger.info(f\"Requested {version=} available on PyPI.\")\n            # Case 1: `version` is set and it is found on PyPI as-is\n            return version\n        else:\n            # Case 2: `version` is set but it is not found on PyPI as-is\n            # Filter using `version` as prefix, and sort\n            matching_versions = [\n                v for v in available_releases if v.startswith(version)\n            ]\n            logger.info(\n                f\"Requested {version=} not available on PyPI, \"\n                f\"found {len(matching_versions)} versions matching \"\n                f\"`{version}*`.\"\n            )\n            if len(matching_versions) == 0:\n                logger.info(f\"No version starting with {version} found.\")\n                raise HTTPException(\n                    status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                    detail=(\n                        f\"No version starting with {version} found.\\n{hint}\"\n                    ),\n                )\n            else:\n                latest_matching_version = sorted(matching_versions)[-1]\n                return latest_matching_version\n    else:\n        # Case 3: `version` is unset and we use latest\n        logger.info(f\"No version requested, returning {latest_version=}.\")\n        return latest_version\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_task_version_update/","title":"_aux_functions_task_version_update","text":""},{"location":"reference/app/routes/api/v2/_aux_functions_task_version_update/#fractal_server.app.routes.api.v2._aux_functions_task_version_update.get_new_workflow_task_meta","title":"<code>get_new_workflow_task_meta(*, old_workflow_task_meta, old_task_meta, new_task_meta)</code>","text":"<p>Prepare new meta field based on old/new tasks and old workflow task.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_task_version_update.py</code> <pre><code>def get_new_workflow_task_meta(\n    *,\n    old_workflow_task_meta: dict | None,\n    old_task_meta: dict | None,\n    new_task_meta: dict | None,\n) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Prepare new meta field based on old/new tasks and old workflow task.\n    \"\"\"\n\n    # When the whole `old_workflow_task_meta` is user-provided, use it\n    # as the outcome\n    if old_task_meta is None:\n        return old_workflow_task_meta\n\n    # When `old_workflow_task_meta` is unset, use the new-task meta as default.\n    if old_workflow_task_meta is None:\n        return new_task_meta\n\n    if new_task_meta is None:\n        new_task_meta = {}\n\n    # Find properties that were added to the old defaults\n    additions = {\n        k: v\n        for k, v in old_workflow_task_meta.items()\n        if v != old_task_meta.get(k)\n    }\n    # Find properties that were removed from the old defaults\n    removals = old_task_meta.keys() - old_workflow_task_meta.keys()\n\n    # Add `additions` and remove `removals`.\n    new_workflowtask_meta = {\n        k: v\n        for k, v in (new_task_meta | additions).items()\n        if k not in removals\n    }\n\n    return new_workflowtask_meta\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_tasks/","title":"_aux_functions_tasks","text":"<p>Auxiliary functions to get task and task-group object from the database or perform simple checks</p>"},{"location":"reference/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._check_type_filters_compatibility","title":"<code>_check_type_filters_compatibility(*, task_input_types, wftask_type_filters)</code>","text":"<p>Wrap <code>merge_type_filters</code> and raise <code>HTTPException</code> if needed.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>def _check_type_filters_compatibility(\n    *,\n    task_input_types: dict[str, bool],\n    wftask_type_filters: dict[str, bool],\n) -&gt; None:\n    \"\"\"\n    Wrap `merge_type_filters` and raise `HTTPException` if needed.\n    \"\"\"\n    try:\n        merge_type_filters(\n            task_input_types=task_input_types,\n            wftask_type_filters=wftask_type_filters,\n        )\n    except ValueError as e:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Incompatible type filters.\\nOriginal error: {str(e)}\",\n        )\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_full_access","title":"<code>_get_task_full_access(*, task_id, user_id, db)</code>  <code>async</code>","text":"<p>Get an existing task or raise a 404.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>ID of the required task.</p> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p>ID of the current user.</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>An asynchronous db session.</p> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_full_access(\n    *,\n    task_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; TaskV2:\n    \"\"\"\n    Get an existing task or raise a 404.\n\n    Args:\n        task_id: ID of the required task.\n        user_id: ID of the current user.\n        db: An asynchronous db session.\n    \"\"\"\n    task = await _get_task_or_404(task_id=task_id, db=db)\n    task_group = await _get_task_group_full_access(\n        task_group_id=task.taskgroupv2_id, user_id=user_id, db=db\n    )\n\n    resource_id = await _get_user_resource_id(user_id=user_id, db=db)\n    if resource_id is None or resource_id != task_group.resource_id:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=f\"User {user_id} has no access to TaskGroup's Resource.\",\n        )\n\n    return task\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_group_full_access","title":"<code>_get_task_group_full_access(*, task_group_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a task group or raise a 403 if user has no full access.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p>ID of the required task group.</p> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p>ID of the current user.</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>An asynchronous db session</p> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_group_full_access(\n    *,\n    task_group_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Get a task group or raise a 403 if user has no full access.\n\n    Args:\n        task_group_id: ID of the required task group.\n        user_id: ID of the current user.\n        db: An asynchronous db session\n    \"\"\"\n    task_group = await _get_task_group_or_404(\n        task_group_id=task_group_id, db=db\n    )\n\n    if task_group.user_id == user_id:\n        return task_group\n    else:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=(\n                \"Current user has no full access to \"\n                f\"TaskGroupV2 {task_group_id}.\",\n            ),\n        )\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_group_or_404","title":"<code>_get_task_group_or_404(*, task_group_id, db)</code>  <code>async</code>","text":"<p>Get an existing task group or raise a 404.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p>The TaskGroupV2 id</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>An asynchronous db session</p> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_group_or_404(\n    *, task_group_id: int, db: AsyncSession\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Get an existing task group or raise a 404.\n\n    Args:\n        task_group_id: The TaskGroupV2 id\n        db: An asynchronous db session\n    \"\"\"\n    task_group = await db.get(TaskGroupV2, task_group_id)\n    if task_group is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"TaskGroupV2 {task_group_id} not found\",\n        )\n    return task_group\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_group_read_access","title":"<code>_get_task_group_read_access(*, task_group_id, user_id, db)</code>  <code>async</code>","text":"<p>Get a task group or raise a 403 if user has no read access.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p>ID of the required task group.</p> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p>ID of the current user.</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>An asynchronous db session.</p> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_group_read_access(\n    *,\n    task_group_id: int,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Get a task group or raise a 403 if user has no read access.\n\n    Args:\n        task_group_id: ID of the required task group.\n        user_id: ID of the current user.\n        db: An asynchronous db session.\n    \"\"\"\n    task_group = await _get_task_group_or_404(\n        task_group_id=task_group_id, db=db\n    )\n\n    # Prepare exception to be used below\n    forbidden_exception = HTTPException(\n        status_code=status.HTTP_403_FORBIDDEN,\n        detail=(\n            f\"Current user has no read access to TaskGroupV2 {task_group_id}.\"\n        ),\n    )\n\n    if task_group.user_id == user_id:\n        return task_group\n    elif task_group.user_group_id is None:\n        raise forbidden_exception\n    else:\n        stm = (\n            select(LinkUserGroup)\n            .join(UserOAuth, UserOAuth.id == LinkUserGroup.user_id)\n            .join(Profile, Profile.id == UserOAuth.profile_id)\n            .where(LinkUserGroup.group_id == task_group.user_group_id)\n            .where(UserOAuth.id == user_id)\n            .where(Profile.resource_id == task_group.resource_id)\n        )\n        res = await db.execute(stm)\n        link = res.unique().scalars().one_or_none()\n        if link is None:\n            raise forbidden_exception\n        else:\n            return task_group\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_or_404","title":"<code>_get_task_or_404(*, task_id, db)</code>  <code>async</code>","text":"<p>Get an existing task or raise a 404.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>ID of the required task.</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>An asynchronous db session</p> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_or_404(*, task_id: int, db: AsyncSession) -&gt; TaskV2:\n    \"\"\"\n    Get an existing task or raise a 404.\n\n    Args:\n        task_id: ID of the required task.\n        db: An asynchronous db session\n    \"\"\"\n    task = await db.get(TaskV2, task_id)\n    if task is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"TaskV2 {task_id} not found\",\n        )\n    return task\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_task_read_access","title":"<code>_get_task_read_access(*, task_id, user_id, db, require_active=False)</code>  <code>async</code>","text":"<p>Get an existing task or raise a 404.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>ID of the required task.</p> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p>ID of the current user.</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>An asynchronous db session.</p> <p> TYPE: <code>AsyncSession</code> </p> <code>require_active</code> <p>If set, fail when the task group is not <code>active</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_task_read_access(\n    *,\n    task_id: int,\n    user_id: int,\n    db: AsyncSession,\n    require_active: bool = False,\n) -&gt; TaskV2:\n    \"\"\"\n    Get an existing task or raise a 404.\n\n    Args:\n        task_id: ID of the required task.\n        user_id: ID of the current user.\n        db: An asynchronous db session.\n        require_active: If set, fail when the task group is not `active`\n    \"\"\"\n    task = await _get_task_or_404(task_id=task_id, db=db)\n    task_group = await _get_task_group_read_access(\n        task_group_id=task.taskgroupv2_id, user_id=user_id, db=db\n    )\n\n    resource_id = await _get_user_resource_id(user_id=user_id, db=db)\n    if resource_id is None or resource_id != task_group.resource_id:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=f\"User {user_id} has no access to TaskGroup's Resource.\",\n        )\n\n    if require_active and not task_group.active:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Error: task {task_id} ({task.name}) is not active.\",\n        )\n\n    return task\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks._get_valid_user_group_id","title":"<code>_get_valid_user_group_id(*, user_group_id=None, private, user_id, db)</code>  <code>async</code>","text":"<p>Validate query parameters for endpoints that create some task(s).</p> PARAMETER DESCRIPTION <code>user_group_id</code> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>private</code> <p> TYPE: <code>bool</code> </p> <code>user_id</code> <p>ID of the current user</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>An asynchronous db session.</p> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>async def _get_valid_user_group_id(\n    *,\n    user_group_id: int | None = None,\n    private: bool,\n    user_id: int,\n    db: AsyncSession,\n) -&gt; int | None:\n    \"\"\"\n    Validate query parameters for endpoints that create some task(s).\n\n    Args:\n        user_group_id:\n        private:\n        user_id: ID of the current user\n        db: An asynchronous db session.\n    \"\"\"\n    if (user_group_id is not None) and (private is True):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Cannot set both {user_group_id=} and {private=}\",\n        )\n    elif private is True:\n        user_group_id = None\n    elif user_group_id is None:\n        user_group_id = await _get_default_usergroup_id_or_none(db=db)\n    else:\n        await _verify_user_belongs_to_group(\n            user_id=user_id, user_group_id=user_group_id, db=db\n        )\n    return user_group_id\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_functions_tasks/#fractal_server.app.routes.api.v2._aux_functions_tasks.integrity_error_to_422","title":"<code>integrity_error_to_422(db)</code>  <code>async</code>","text":"<p>If an IntegrityError occurs inside the context, rolls back the current transaction and raises an HTTPException with status code 422.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_functions_tasks.py</code> <pre><code>@asynccontextmanager\nasync def integrity_error_to_422(db):\n    \"\"\"\n    If an IntegrityError occurs inside the context, rolls back the current\n    transaction and raises an HTTPException with status code 422.\n    \"\"\"\n    try:\n        yield\n    except IntegrityError as e:\n        logger.warning(\n            \"Unexpected IntegrityError caught in `integrity_error_to_422`. \"\n            f\"Original error: {str(e)}\"\n        )\n        await db.rollback()\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=str(e.orig or e),\n        )\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_task_group_disambiguation/","title":"_aux_task_group_disambiguation","text":""},{"location":"reference/app/routes/api/v2/_aux_task_group_disambiguation/#fractal_server.app.routes.api.v2._aux_task_group_disambiguation._disambiguate_task_groups","title":"<code>_disambiguate_task_groups(*, matching_task_groups, user_id, default_group_id, db)</code>  <code>async</code>","text":"<p>Find ownership-based top-priority task group, if any.</p> PARAMETER DESCRIPTION <code>matching_task_groups</code> <p> TYPE: <code>list[TaskGroupV2]</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>default_group_id</code> <p> TYPE: <code>int | None</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>TaskGroupV2 | None</code> <p>The task group or <code>None</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_task_group_disambiguation.py</code> <pre><code>async def _disambiguate_task_groups(\n    *,\n    matching_task_groups: list[TaskGroupV2],\n    user_id: int,\n    default_group_id: int | None,\n    db: AsyncSession,\n) -&gt; TaskGroupV2 | None:\n    \"\"\"\n    Find ownership-based top-priority task group, if any.\n\n    Args:\n        matching_task_groups:\n        user_id:\n        default_group_id:\n        db:\n\n    Returns:\n        The task group or `None`.\n    \"\"\"\n\n    # Highest priority: task groups created by user\n    list_user_ids = [tg.user_id for tg in matching_task_groups]\n    try:\n        ind_user_id = list_user_ids.index(user_id)\n        task_group = matching_task_groups[ind_user_id]\n        logger.debug(\n            \"[_disambiguate_task_groups] \"\n            f\"Found task group {task_group.id} with {user_id=}, return.\"\n        )\n        return task_group\n    except ValueError:\n        logger.debug(\n            \"[_disambiguate_task_groups] \"\n            f\"No task group with {user_id=}, continue.\"\n        )\n\n    # Medium priority: task groups owned by default user group\n    settings = Inject(get_settings)\n    list_user_group_ids = [tg.user_group_id for tg in matching_task_groups]\n    if settings.FRACTAL_DEFAULT_GROUP_NAME is not None:\n        try:\n            ind_user_group_id = list_user_group_ids.index(default_group_id)\n            task_group = matching_task_groups[ind_user_group_id]\n            logger.debug(\n                \"[_disambiguate_task_groups] \"\n                f\"Found task group {task_group.id} with {user_id=}, return.\"\n            )\n            return task_group\n        except ValueError:\n            logger.debug(\n                \"[_disambiguate_task_groups] \"\n                \"No task group with user_group_id=\"\n                f\"{default_group_id}, continue.\"\n            )\n\n    # Lowest priority: task groups owned by other groups, sorted\n    # according to age of the user/usergroup link\n    logger.debug(\n        \"[_disambiguate_task_groups] \"\n        \"Sort remaining task groups by oldest-user-link.\"\n    )\n    stm = (\n        select(LinkUserGroup.group_id)\n        .where(LinkUserGroup.user_id == user_id)\n        .where(LinkUserGroup.group_id.in_(list_user_group_ids))\n        .order_by(LinkUserGroup.timestamp_created.asc())\n    )\n    res = await db.execute(stm)\n    oldest_user_group_id = res.scalars().first()\n    logger.debug(\n        f\"[_disambiguate_task_groups] Result: {oldest_user_group_id=}.\"\n    )\n    task_group = next(\n        iter(\n            task_group\n            for task_group in matching_task_groups\n            if task_group.user_group_id == oldest_user_group_id\n        ),\n        None,\n    )\n    return task_group\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_task_group_disambiguation/#fractal_server.app.routes.api.v2._aux_task_group_disambiguation._disambiguate_task_groups_not_none","title":"<code>_disambiguate_task_groups_not_none(*, matching_task_groups, user_id, default_group_id, db)</code>  <code>async</code>","text":"<p>Find ownership-based top-priority task group, and fail otherwise.</p> PARAMETER DESCRIPTION <code>matching_task_groups</code> <p> TYPE: <code>list[TaskGroupV2]</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>default_group_id</code> <p> TYPE: <code>int | None</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>TaskGroupV2</code> <p>The top-priority task group.</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_task_group_disambiguation.py</code> <pre><code>async def _disambiguate_task_groups_not_none(\n    *,\n    matching_task_groups: list[TaskGroupV2],\n    user_id: int,\n    default_group_id: int | None,\n    db: AsyncSession,\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Find ownership-based top-priority task group, and fail otherwise.\n\n    Args:\n        matching_task_groups:\n        user_id:\n        default_group_id:\n        db:\n\n    Returns:\n        The top-priority task group.\n    \"\"\"\n    task_group = await _disambiguate_task_groups(\n        matching_task_groups=matching_task_groups,\n        user_id=user_id,\n        default_group_id=default_group_id,\n        db=db,\n    )\n    if task_group is None:\n        error_msg = (\n            \"[_disambiguate_task_groups_not_none] Could not find a task \"\n            f\"group ({user_id=}, {default_group_id=}).\"\n        )\n        logger.error(f\"UnreachableBranchError {error_msg}\")\n        raise UnreachableBranchError(error_msg)\n    else:\n        return task_group\n</code></pre>"},{"location":"reference/app/routes/api/v2/_aux_task_group_disambiguation/#fractal_server.app.routes.api.v2._aux_task_group_disambiguation.remove_duplicate_task_groups","title":"<code>remove_duplicate_task_groups(*, task_groups, user_id, default_group_id, db)</code>  <code>async</code>","text":"<p>Extract an item for each <code>version</code> from a sorted task-group list.</p> PARAMETER DESCRIPTION <code>task_groups</code> <p>A list of task groups with identical <code>pkg_name</code></p> <p> TYPE: <code>list[TaskGroupV2]</code> </p> <code>user_id</code> <p>User ID</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>list[TaskGroupV2]</code> <p>New list of task groups with no duplicate <code>(pkg_name,version)</code> entries</p> Source code in <code>fractal_server/app/routes/api/v2/_aux_task_group_disambiguation.py</code> <pre><code>async def remove_duplicate_task_groups(\n    *,\n    task_groups: list[TaskGroupV2],\n    user_id: int,\n    default_group_id: int | None,\n    db: AsyncSession,\n) -&gt; list[TaskGroupV2]:\n    \"\"\"\n    Extract an item for each `version` from a *sorted* task-group list.\n\n    Args:\n        task_groups: A list of task groups with identical `pkg_name`\n        user_id: User ID\n\n    Returns:\n        New list of task groups with no duplicate `(pkg_name,version)` entries\n    \"\"\"\n\n    new_task_groups = [\n        (\n            await _disambiguate_task_groups_not_none(\n                matching_task_groups=list(groups),\n                user_id=user_id,\n                default_group_id=default_group_id,\n                db=db,\n            )\n        )\n        for version, groups in itertools.groupby(\n            task_groups, key=lambda tg: tg.version\n        )\n    ]\n    return new_task_groups\n</code></pre>"},{"location":"reference/app/routes/api/v2/dataset/","title":"dataset","text":""},{"location":"reference/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.create_dataset","title":"<code>create_dataset(project_id, dataset, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Add new dataset to current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/dataset/\",\n    response_model=DatasetRead,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_dataset(\n    project_id: int,\n    dataset: DatasetCreate,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; DatasetRead | None:\n    \"\"\"\n    Add new dataset to current project\n    \"\"\"\n    project = await _get_project_check_access(\n        project_id=project_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n\n    db_dataset = DatasetV2(\n        project_id=project_id,\n        zarr_dir=\"__PLACEHOLDER__\",\n        **dataset.model_dump(exclude={\"project_dir\", \"zarr_subfolder\"}),\n    )\n    db.add(db_dataset)\n    await db.commit()\n    await db.refresh(db_dataset)\n\n    if dataset.project_dir is None:\n        project_dir = user.project_dirs[0]\n    else:\n        if dataset.project_dir not in user.project_dirs:\n            await db.delete(db_dataset)\n            await db.commit()\n            raise HTTPException(\n                status_code=status.HTTP_403_FORBIDDEN,\n                detail=f\"You are not allowed to use {dataset.project_dir=}.\",\n            )\n        project_dir = dataset.project_dir\n\n    if dataset.zarr_subfolder is None:\n        zarr_subfolder = (\n            f\"fractal/{project_id}_{sanitize_string(project.name)}/\"\n            f\"{db_dataset.id}_{sanitize_string(db_dataset.name)}\"\n        )\n    else:\n        zarr_subfolder = dataset.zarr_subfolder\n\n    zarr_dir = os.path.join(project_dir, zarr_subfolder)\n    db_dataset.zarr_dir = normalize_url(zarr_dir)\n\n    db.add(db_dataset)\n    await db.commit()\n    await db.refresh(db_dataset)\n\n    return db_dataset\n</code></pre>"},{"location":"reference/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.delete_dataset","title":"<code>delete_dataset(project_id, dataset_id, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    status_code=204,\n)\nasync def delete_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a dataset associated to the current project\n    \"\"\"\n    output = await _get_dataset_check_access(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current dataset.\n    stm = _get_submitted_jobs_statement().where(JobV2.dataset_id == dataset_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot delete dataset {dataset.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # Delete dataset\n    await db.delete(dataset)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.export_dataset","title":"<code>export_dataset(project_id, dataset_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Export an existing dataset</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/{dataset_id}/export/\",\n    response_model=DatasetExport,\n)\nasync def export_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; DatasetExport | None:\n    \"\"\"\n    Export an existing dataset\n    \"\"\"\n    dict_dataset_project = await _get_dataset_check_access(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n    dataset = dict_dataset_project[\"dataset\"]\n\n    return dataset\n</code></pre>"},{"location":"reference/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.import_dataset","title":"<code>import_dataset(project_id, dataset, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Import an existing dataset into a project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/dataset/import/\",\n    response_model=DatasetRead,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def import_dataset(\n    project_id: int,\n    dataset: DatasetImport,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; DatasetRead | None:\n    \"\"\"\n    Import an existing dataset into a project\n    \"\"\"\n\n    # Preliminary checks\n    await _get_project_check_access(\n        project_id=project_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n\n    if not any(\n        Path(dataset.zarr_dir).is_relative_to(project_dir)\n        for project_dir in user.project_dirs\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"{dataset.zarr_dir=} is not relative to any of user's project \"\n                \"dirs.\"\n            ),\n        )\n\n    # Create new Dataset\n    db_dataset = DatasetV2(\n        project_id=project_id,\n        **dataset.model_dump(exclude_none=True),\n    )\n    db.add(db_dataset)\n    await db.commit()\n    await db.refresh(db_dataset)\n\n    return db_dataset\n</code></pre>"},{"location":"reference/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.read_dataset","title":"<code>read_dataset(project_id, dataset_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    response_model=DatasetRead,\n)\nasync def read_dataset(\n    project_id: int,\n    dataset_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; DatasetRead | None:\n    \"\"\"\n    Get info on a dataset associated to the current project\n    \"\"\"\n    output = await _get_dataset_check_access(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n    dataset = output[\"dataset\"]\n    return dataset\n</code></pre>"},{"location":"reference/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.read_dataset_list","title":"<code>read_dataset_list(project_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get dataset list for given project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/dataset/\",\n    response_model=list[DatasetRead],\n)\nasync def read_dataset_list(\n    project_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[DatasetRead] | None:\n    \"\"\"\n    Get dataset list for given project\n    \"\"\"\n    # Access control\n    project = await _get_project_check_access(\n        project_id=project_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n    # Find datasets of the current project. Note: this select/where approach\n    # has much better scaling than refreshing all elements of\n    # `project.dataset_list` - ref\n    # https://github.com/fractal-analytics-platform/fractal-server/pull/1082#issuecomment-1856676097.\n    stm = select(DatasetV2).where(DatasetV2.project_id == project.id)\n    res = await db.execute(stm)\n    dataset_list = res.scalars().all()\n    return dataset_list\n</code></pre>"},{"location":"reference/app/routes/api/v2/dataset/#fractal_server.app.routes.api.v2.dataset.update_dataset","title":"<code>update_dataset(project_id, dataset_id, dataset_update, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a dataset associated to the current project</p> Source code in <code>fractal_server/app/routes/api/v2/dataset.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/dataset/{dataset_id}/\",\n    response_model=DatasetRead,\n)\nasync def update_dataset(\n    project_id: int,\n    dataset_id: int,\n    dataset_update: DatasetUpdate,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; DatasetRead | None:\n    \"\"\"\n    Edit a dataset associated to the current project\n    \"\"\"\n\n    output = await _get_dataset_check_access(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n    db_dataset = output[\"dataset\"]\n\n    for key, value in dataset_update.model_dump(exclude_unset=True).items():\n        setattr(db_dataset, key, value)\n\n    await db.commit()\n    await db.refresh(db_dataset)\n    return db_dataset\n</code></pre>"},{"location":"reference/app/routes/api/v2/history/","title":"history","text":""},{"location":"reference/app/routes/api/v2/history/#fractal_server.app.routes.api.v2.history.get_dataset_history","title":"<code>get_dataset_history(project_id, dataset_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns a list of all HistoryRuns associated to a given dataset, sorted by timestamp.</p> Source code in <code>fractal_server/app/routes/api/v2/history.py</code> <pre><code>@router.get(\"/project/{project_id}/dataset/{dataset_id}/history/\")\nasync def get_dataset_history(\n    project_id: int,\n    dataset_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[HistoryRunRead]:\n    \"\"\"\n    Returns a list of all HistoryRuns associated to a given dataset, sorted by\n    timestamp.\n    \"\"\"\n    # Access control\n    await _get_dataset_check_access(\n        project_id=project_id,\n        dataset_id=dataset_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n\n    res = await db.execute(\n        select(HistoryRun)\n        .where(HistoryRun.dataset_id == dataset_id)\n        .order_by(HistoryRun.timestamp_started)\n    )\n    history_run_list = res.scalars().all()\n\n    return history_run_list\n</code></pre>"},{"location":"reference/app/routes/api/v2/images/","title":"images","text":""},{"location":"reference/app/routes/api/v2/images/#fractal_server.app.routes.api.v2.images.ImageQuery","title":"<code>ImageQuery</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Query for a list of images.</p> ATTRIBUTE DESCRIPTION <code>type_filters</code> <p> TYPE: <code>TypeFilters</code> </p> <code>attribute_filters</code> <p> TYPE: <code>AttributeFilters</code> </p> Source code in <code>fractal_server/app/routes/api/v2/images.py</code> <pre><code>class ImageQuery(BaseModel):\n    \"\"\"\n    Query for a list of images.\n\n    Attributes:\n        type_filters:\n        attribute_filters:\n    \"\"\"\n\n    type_filters: TypeFilters = Field(default_factory=dict)\n    attribute_filters: AttributeFilters = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/app/routes/api/v2/job/","title":"job","text":""},{"location":"reference/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.download_job_logs","title":"<code>download_job_logs(project_id, job_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Download zipped job folder</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/download/\",\n    response_class=StreamingResponse,\n)\nasync def download_job_logs(\n    project_id: int,\n    job_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; StreamingResponse:\n    \"\"\"\n    Download zipped job folder\n    \"\"\"\n    output = await _get_job_check_access(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n    job = output[\"job\"]\n    zip_name = f\"{Path(job.working_dir).name}_archive.zip\"\n\n    zip_bytes_iterator = await zip_folder_threaded(job.working_dir)\n\n    return StreamingResponse(\n        zip_bytes_iterator,\n        media_type=\"application/x-zip-compressed\",\n        headers={\"Content-Disposition\": f\"attachment;filename={zip_name}\"},\n    )\n</code></pre>"},{"location":"reference/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.get_job_list","title":"<code>get_job_list(project_id, user=Depends(get_api_guest), log=True, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get job list for given project</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/\",\n    response_model=list[JobRead],\n)\nasync def get_job_list(\n    project_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    log: bool = True,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[JobRead] | None:\n    \"\"\"\n    Get job list for given project\n    \"\"\"\n    project = await _get_project_check_access(\n        project_id=project_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n\n    res = await db.execute(\n        select(JobV2)\n        .where(JobV2.project_id == project.id)\n        .order_by(JobV2.start_timestamp.desc())\n    )\n    job_list = res.scalars().all()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.get_user_jobs","title":"<code>get_user_jobs(user=Depends(get_api_guest), log=True, db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the jobs from projects linked to the current user</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\"/job/\", response_model=list[JobRead])\nasync def get_user_jobs(\n    user: UserOAuth = Depends(get_api_guest),\n    log: bool = True,\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[JobRead]:\n    \"\"\"\n    Returns all the jobs from projects linked to the current user\n    \"\"\"\n    stm = (\n        select(JobV2)\n        .join(\n            LinkUserProjectV2, LinkUserProjectV2.project_id == JobV2.project_id\n        )\n        .where(LinkUserProjectV2.user_id == user.id)\n        .where(LinkUserProjectV2.is_verified.is_(True))\n        .order_by(JobV2.start_timestamp.desc())\n    )\n    res = await db.execute(stm)\n    job_list = res.scalars().all()\n    if not log:\n        for job in job_list:\n            setattr(job, \"log\", None)\n\n    return job_list\n</code></pre>"},{"location":"reference/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.get_workflow_jobs","title":"<code>get_workflow_jobs(project_id, workflow_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns all the jobs related to a specific workflow</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/job/\",\n    response_model=list[JobRead],\n)\nasync def get_workflow_jobs(\n    project_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[JobRead] | None:\n    \"\"\"\n    Returns all the jobs related to a specific workflow\n    \"\"\"\n    await _get_workflow_check_access(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n    res = await db.execute(\n        select(JobV2)\n        .where(JobV2.workflow_id == workflow_id)\n        .order_by(JobV2.start_timestamp.desc())\n    )\n    job_list = res.scalars().all()\n    return job_list\n</code></pre>"},{"location":"reference/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.read_job","title":"<code>read_job(project_id, job_id, show_tmp_logs=False, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return info on an existing job</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/\",\n    response_model=JobRead,\n)\nasync def read_job(\n    project_id: int,\n    job_id: int,\n    show_tmp_logs: bool = False,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; JobRead | None:\n    \"\"\"\n    Return info on an existing job\n    \"\"\"\n\n    output = await _get_job_check_access(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n    job = output[\"job\"]\n\n    if show_tmp_logs and (job.status == JobStatusType.SUBMITTED):\n        try:\n            with open(f\"{job.working_dir}/{WORKFLOW_LOG_FILENAME}\") as f:\n                job.log = f.read()\n        except FileNotFoundError:\n            pass\n\n    return job\n</code></pre>"},{"location":"reference/app/routes/api/v2/job/#fractal_server.app.routes.api.v2.job.stop_job","title":"<code>stop_job(project_id, job_id, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Stop execution of a workflow job.</p> Source code in <code>fractal_server/app/routes/api/v2/job.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/job/{job_id}/stop/\",\n    status_code=202,\n)\nasync def stop_job(\n    project_id: int,\n    job_id: int,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Stop execution of a workflow job.\n    \"\"\"\n\n    _check_shutdown_is_supported()\n\n    # Get job from DB\n    output = await _get_job_check_access(\n        project_id=project_id,\n        job_id=job_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.EXECUTE,\n        db=db,\n    )\n    job = output[\"job\"]\n\n    _write_shutdown_file(job=job)\n\n    return Response(status_code=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"reference/app/routes/api/v2/pre_submission_checks/","title":"pre_submission_checks","text":""},{"location":"reference/app/routes/api/v2/project/","title":"project","text":""},{"location":"reference/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.create_project","title":"<code>create_project(project, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create new project</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.post(\"/project/\", response_model=ProjectRead, status_code=201)\nasync def create_project(\n    project: ProjectCreate,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProjectRead | None:\n    \"\"\"\n    Create new project\n    \"\"\"\n\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(\n        user=user,\n        db=db,\n    )\n    resource_id = resource.id\n\n    # Check that there is no project with the same user and name\n    await _check_project_exists(\n        project_name=project.name, user_id=user.id, db=db\n    )\n\n    db_project = ProjectV2(**project.model_dump(), resource_id=resource_id)\n    db.add(db_project)\n    await db.flush()\n\n    link = LinkUserProjectV2(\n        project_id=db_project.id,\n        user_id=user.id,\n        is_owner=True,\n        is_verified=True,\n        permissions=ProjectPermissions.EXECUTE,\n    )\n    db.add(link)\n\n    await db.commit()\n    await db.refresh(db_project)\n\n    return db_project\n</code></pre>"},{"location":"reference/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.delete_project","title":"<code>delete_project(project_id, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete project</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.delete(\"/project/{project_id}/\", status_code=204)\nasync def delete_project(\n    project_id: int,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete project\n    \"\"\"\n\n    project = await _get_project_check_access(\n        project_id=project_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.EXECUTE,\n        db=db,\n    )\n    link_user_project = await db.get(LinkUserProjectV2, (project_id, user.id))\n    if not link_user_project.is_owner:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Only the owner can delete a Project.\",\n        )\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current project.\n    stm = _get_submitted_jobs_statement().where(JobV2.project_id == project_id)\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot delete project {project.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    logger.debug(f\"Add project {project.id} to deletion.\")\n    await db.delete(project)\n\n    logger.debug(\"Commit changes to db\")\n    await db.commit()\n\n    logger.debug(\"Everything  has been deleted correctly.\")\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.get_list_project","title":"<code>get_list_project(is_owner=True, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return list of projects user is member of</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.get(\"/project/\", response_model=list[ProjectRead])\nasync def get_list_project(\n    is_owner: bool = True,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ProjectV2]:\n    \"\"\"\n    Return list of projects user is member of\n    \"\"\"\n    stm = (\n        select(ProjectV2)\n        .join(LinkUserProjectV2, LinkUserProjectV2.project_id == ProjectV2.id)\n        .where(LinkUserProjectV2.user_id == user.id)\n        .where(LinkUserProjectV2.is_owner == is_owner)\n        .where(LinkUserProjectV2.is_verified.is_(True))\n    )\n    res = await db.execute(stm)\n    project_list = res.scalars().all()\n    return project_list\n</code></pre>"},{"location":"reference/app/routes/api/v2/project/#fractal_server.app.routes.api.v2.project.read_project","title":"<code>read_project(project_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return info on an existing project</p> Source code in <code>fractal_server/app/routes/api/v2/project.py</code> <pre><code>@router.get(\"/project/{project_id}/\", response_model=ProjectRead)\nasync def read_project(\n    project_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProjectRead | None:\n    \"\"\"\n    Return info on an existing project\n    \"\"\"\n    project = await _get_project_check_access(\n        project_id=project_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n    return project\n</code></pre>"},{"location":"reference/app/routes/api/v2/sharing/","title":"sharing","text":""},{"location":"reference/app/routes/api/v2/sharing/#fractal_server.app.routes.api.v2.sharing.accept_project_invitation","title":"<code>accept_project_invitation(project_id, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Accept invitation to project <code>project_id</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/sharing.py</code> <pre><code>@router.post(\"/project/{project_id}/access/accept/\", status_code=200)\nasync def accept_project_invitation(\n    project_id: int,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Accept invitation to project `project_id`.\n    \"\"\"\n    link = await get_pending_invitation_or_404(\n        user_id=user.id, project_id=project_id, db=db\n    )\n    link.is_verified = True\n    await db.commit()\n\n    return Response(status_code=status.HTTP_200_OK)\n</code></pre>"},{"location":"reference/app/routes/api/v2/sharing/#fractal_server.app.routes.api.v2.sharing.get_access_info","title":"<code>get_access_info(project_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns information on your relationship with Project[<code>project_id</code>].</p> Source code in <code>fractal_server/app/routes/api/v2/sharing.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/access/\",\n    response_model=ProjectAccessRead,\n)\nasync def get_access_info(\n    project_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; ProjectAccessRead:\n    \"\"\"\n    Returns information on your relationship with Project[`project_id`].\n    \"\"\"\n\n    res = await db.execute(\n        select(\n            LinkUserProjectV2.is_owner,\n            LinkUserProjectV2.permissions,\n            (\n                select(UserOAuth.email)\n                .join(\n                    LinkUserProjectV2,\n                    UserOAuth.id == LinkUserProjectV2.user_id,\n                )\n                .where(LinkUserProjectV2.is_owner.is_(True))\n                .where(LinkUserProjectV2.project_id == project_id)\n                .scalar_subquery()\n            ),\n        )\n        .where(LinkUserProjectV2.project_id == project_id)\n        .where(LinkUserProjectV2.user_id == user.id)\n        .where(LinkUserProjectV2.is_verified.is_(True))\n    )\n\n    guest_project_info = res.one_or_none()\n\n    if guest_project_info is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"User has no access to project {project_id}.\",\n        )\n\n    is_owner, permissions, owner_email = guest_project_info\n\n    return dict(\n        is_owner=is_owner,\n        permissions=permissions,\n        owner_email=owner_email,\n    )\n</code></pre>"},{"location":"reference/app/routes/api/v2/sharing/#fractal_server.app.routes.api.v2.sharing.get_pending_invitations","title":"<code>get_pending_invitations(user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>See your current invitations.</p> Source code in <code>fractal_server/app/routes/api/v2/sharing.py</code> <pre><code>@router.get(\n    \"/project/invitation/\",\n    response_model=list[ProjectInvitationRead],\n)\nasync def get_pending_invitations(\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ProjectInvitationRead]:\n    \"\"\"\n    See your current invitations.\n    \"\"\"\n\n    if user.is_guest:\n        # The user's attribute `is_guest` is used to identify guest accounts,\n        # i.e. accounts with read only permissions on the API.\n        # This is a different concept from a project guest, which is a regular\n        # account with which a project has been shared.\n        return []\n\n    res = await db.execute(\n        select(\n            ProjectV2.id,\n            ProjectV2.name,\n            LinkUserProjectV2.permissions,\n            (\n                select(UserOAuth.email)\n                .join(\n                    LinkUserProjectV2,\n                    UserOAuth.id == LinkUserProjectV2.user_id,\n                )\n                .where(LinkUserProjectV2.is_owner.is_(True))\n                .where(LinkUserProjectV2.project_id == ProjectV2.id)\n                .scalar_subquery()\n                .correlate(ProjectV2)\n            ),\n        )\n        .join(LinkUserProjectV2, LinkUserProjectV2.project_id == ProjectV2.id)\n        .where(LinkUserProjectV2.user_id == user.id)\n        .where(LinkUserProjectV2.is_verified.is_(False))\n        .order_by(ProjectV2.name)\n    )\n\n    guest_project_info = res.all()\n\n    return [\n        dict(\n            project_id=project_id,\n            project_name=project_name,\n            guest_permissions=guest_permissions,\n            owner_email=owner_email,\n        )\n        for (\n            project_id,\n            project_name,\n            guest_permissions,\n            owner_email,\n        ) in guest_project_info\n    ]\n</code></pre>"},{"location":"reference/app/routes/api/v2/sharing/#fractal_server.app.routes.api.v2.sharing.get_project_guests","title":"<code>get_project_guests(project_id, owner=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get the list of all the guests of your project (verified or not).</p> Source code in <code>fractal_server/app/routes/api/v2/sharing.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/guest/\",\n    response_model=list[ProjectGuestRead],\n)\nasync def get_project_guests(\n    project_id: int,\n    owner: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[ProjectGuestRead]:\n    \"\"\"\n    Get the list of all the guests of your project (verified or not).\n    \"\"\"\n    await raise_403_if_not_owner(user_id=owner.id, project_id=project_id, db=db)\n    # Get (email, is_verified, permissions) for all guests\n    res = await db.execute(\n        select(\n            UserOAuth.email,\n            LinkUserProjectV2.is_verified,\n            LinkUserProjectV2.permissions,\n        )\n        .join(LinkUserProjectV2, LinkUserProjectV2.user_id == UserOAuth.id)\n        .where(LinkUserProjectV2.project_id == project_id)\n        .where(LinkUserProjectV2.is_owner.is_(False))\n        .order_by(UserOAuth.email)\n    )\n    guest_tuples = res.all()\n    return [\n        dict(\n            email=guest_email,\n            is_verified=is_verified,\n            permissions=permissions,\n        )\n        for guest_email, is_verified, permissions in guest_tuples\n    ]\n</code></pre>"},{"location":"reference/app/routes/api/v2/sharing/#fractal_server.app.routes.api.v2.sharing.invite_guest","title":"<code>invite_guest(project_id, email, project_invitation, owner=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Add a guest to your project.</p> Source code in <code>fractal_server/app/routes/api/v2/sharing.py</code> <pre><code>@router.post(\"/project/{project_id}/guest/\", status_code=201)\nasync def invite_guest(\n    project_id: int,\n    email: EmailStr,\n    project_invitation: ProjectGuestCreate,\n    owner: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Add a guest to your project.\n    \"\"\"\n    await raise_403_if_not_owner(user_id=owner.id, project_id=project_id, db=db)\n\n    guest_id = await get_user_id_from_email_or_404(user_email=email, db=db)\n\n    await raise_422_if_link_exists(\n        user_id=guest_id,\n        project_id=project_id,\n        db=db,\n    )\n\n    db.add(\n        LinkUserProjectV2(\n            project_id=project_id,\n            user_id=guest_id,\n            is_owner=False,\n            is_verified=False,\n            permissions=project_invitation.permissions,\n        )\n    )\n    await db.commit()\n\n    return Response(status_code=status.HTTP_201_CREATED)\n</code></pre>"},{"location":"reference/app/routes/api/v2/sharing/#fractal_server.app.routes.api.v2.sharing.leave_project","title":"<code>leave_project(project_id, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Decline invitation to project <code>project_id</code> or stop being a guest of that project.</p> Source code in <code>fractal_server/app/routes/api/v2/sharing.py</code> <pre><code>@router.delete(\"/project/{project_id}/access/\", status_code=204)\nasync def leave_project(\n    project_id: int,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Decline invitation to project `project_id` or stop being a guest of that\n    project.\n    \"\"\"\n    link = await get_link_or_404(user_id=user.id, project_id=project_id, db=db)\n\n    if link.is_owner:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"You are the owner of project {project_id}.\",\n        )\n\n    await db.delete(link)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/app/routes/api/v2/sharing/#fractal_server.app.routes.api.v2.sharing.patch_guest","title":"<code>patch_guest(project_id, email, update, owner=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Change guest's permissions on your project.</p> Source code in <code>fractal_server/app/routes/api/v2/sharing.py</code> <pre><code>@router.patch(\"/project/{project_id}/guest/\", status_code=200)\nasync def patch_guest(\n    project_id: int,\n    email: EmailStr,\n    update: ProjectGuestUpdate,\n    owner: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Change guest's permissions on your project.\n    \"\"\"\n    await raise_403_if_not_owner(user_id=owner.id, project_id=project_id, db=db)\n\n    guest_id = await get_user_id_from_email_or_404(user_email=email, db=db)\n\n    if guest_id == owner.id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Cannot perform this operation on project owner.\",\n        )\n\n    link = await get_link_or_404(\n        user_id=guest_id,\n        project_id=project_id,\n        db=db,\n    )\n\n    # Update link and commit\n    for key, value in update.model_dump(exclude_unset=True).items():\n        setattr(link, key, value)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_200_OK)\n</code></pre>"},{"location":"reference/app/routes/api/v2/sharing/#fractal_server.app.routes.api.v2.sharing.revoke_guest_access","title":"<code>revoke_guest_access(project_id, email, owner=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Remove a guest from your project.</p> Source code in <code>fractal_server/app/routes/api/v2/sharing.py</code> <pre><code>@router.delete(\"/project/{project_id}/guest/\", status_code=204)\nasync def revoke_guest_access(\n    project_id: int,\n    email: EmailStr,\n    owner: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Remove a guest from your project.\n    \"\"\"\n    await raise_403_if_not_owner(user_id=owner.id, project_id=project_id, db=db)\n\n    guest_id = await get_user_id_from_email_or_404(user_email=email, db=db)\n\n    if guest_id == owner.id:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Cannot perform this operation on project owner.\",\n        )\n\n    link = await get_link_or_404(\n        user_id=guest_id,\n        project_id=project_id,\n        db=db,\n    )\n\n    # Delete link and commit\n    await db.delete(link)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/app/routes/api/v2/submit/","title":"submit","text":""},{"location":"reference/app/routes/api/v2/task/","title":"task","text":""},{"location":"reference/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.create_task","title":"<code>create_task(task, user_group_id=None, private=False, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create a new task</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.post(\"/\", response_model=TaskRead, status_code=status.HTTP_201_CREATED)\nasync def create_task(\n    task: TaskCreate,\n    user_group_id: int | None = None,\n    private: bool = False,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskRead | None:\n    \"\"\"\n    Create a new task\n    \"\"\"\n\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(\n        user=user,\n        db=db,\n    )\n    resource_id = resource.id\n\n    # Validate query parameters related to user-group ownership\n    user_group_id = await _get_valid_user_group_id(\n        user_group_id=user_group_id,\n        private=private,\n        user_id=user.id,\n        db=db,\n    )\n\n    if task.type == TaskType.PARALLEL and (\n        task.args_schema_non_parallel is not None\n        or task.meta_non_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot set `TaskV2.args_schema_non_parallel` or \"\n                \"`TaskV2.args_schema_non_parallel` if TaskV2 is parallel\"\n            ),\n        )\n    elif task.type == TaskType.NON_PARALLEL and (\n        task.args_schema_parallel is not None or task.meta_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot set `TaskV2.args_schema_parallel` or \"\n                \"`TaskV2.args_schema_parallel` if TaskV2 is non_parallel\"\n            ),\n        )\n\n    # Add task\n\n    db_task = TaskV2(**task.model_dump(exclude_unset=True))\n    pkg_name = db_task.name\n    await _verify_non_duplication_user_constraint(\n        db=db,\n        pkg_name=pkg_name,\n        user_id=user.id,\n        version=db_task.version,\n        user_resource_id=resource_id,\n    )\n    await _verify_non_duplication_group_constraint(\n        db=db,\n        pkg_name=pkg_name,\n        user_group_id=user_group_id,\n        version=db_task.version,\n    )\n    db_task_group = TaskGroupV2(\n        user_id=user.id,\n        user_group_id=user_group_id,\n        resource_id=resource_id,\n        active=True,\n        task_list=[db_task],\n        origin=TaskGroupOriginEnum.OTHER,\n        version=db_task.version,\n        pkg_name=pkg_name,\n    )\n    db.add(db_task_group)\n    async with integrity_error_to_422(db):\n        await db.commit()\n    await db.refresh(db_task)\n\n    return db_task\n</code></pre>"},{"location":"reference/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.delete_task","title":"<code>delete_task(task_id, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a task</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.delete(\"/{task_id}/\", status_code=204)\nasync def delete_task(\n    task_id: int,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a task\n    \"\"\"\n    raise HTTPException(\n        status_code=status.HTTP_405_METHOD_NOT_ALLOWED,\n        detail=(\n            \"Cannot delete single tasks, \"\n            \"please operate directly on task groups.\"\n        ),\n    )\n</code></pre>"},{"location":"reference/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.get_list_task","title":"<code>get_list_task(args_schema=True, category=None, modality=None, author=None, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get list of available tasks</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.get(\"/\", response_model=list[TaskRead])\nasync def get_list_task(\n    args_schema: bool = True,\n    category: str | None = None,\n    modality: str | None = None,\n    author: str | None = None,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[TaskRead]:\n    \"\"\"\n    Get list of available tasks\n    \"\"\"\n\n    user_resource_id = await _get_user_resource_id(user_id=user.id, db=db)\n\n    stm = (\n        select(TaskV2)\n        .join(TaskGroupV2, TaskGroupV2.id == TaskV2.taskgroupv2_id)\n        .where(TaskGroupV2.resource_id == user_resource_id)\n        .where(\n            or_(\n                TaskGroupV2.user_id == user.id,\n                TaskGroupV2.user_group_id.in_(\n                    select(LinkUserGroup.group_id).where(\n                        LinkUserGroup.user_id == user.id\n                    )\n                ),\n            )\n        )\n    )\n    if category is not None:\n        stm = stm.where(func.lower(TaskV2.category) == category.lower())\n    if modality is not None:\n        stm = stm.where(func.lower(TaskV2.modality) == modality.lower())\n    if author is not None:\n        stm = stm.where(TaskV2.authors.icontains(author))\n\n    stm = stm.order_by(TaskV2.id)\n    res = await db.execute(stm)\n    task_list = list(res.scalars().all())\n    if args_schema is False:\n        for task in task_list:\n            setattr(task, \"args_schema_parallel\", None)\n            setattr(task, \"args_schema_non_parallel\", None)\n\n    return task_list\n</code></pre>"},{"location":"reference/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.get_task","title":"<code>get_task(task_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on a specific task</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.get(\"/{task_id}/\", response_model=TaskRead)\nasync def get_task(\n    task_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskRead:\n    \"\"\"\n    Get info on a specific task\n    \"\"\"\n    task = await _get_task_read_access(task_id=task_id, user_id=user.id, db=db)\n    return task\n</code></pre>"},{"location":"reference/app/routes/api/v2/task/#fractal_server.app.routes.api.v2.task.patch_task","title":"<code>patch_task(task_id, task_update, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a specific task (restricted to task owner)</p> Source code in <code>fractal_server/app/routes/api/v2/task.py</code> <pre><code>@router.patch(\"/{task_id}/\", response_model=TaskRead)\nasync def patch_task(\n    task_id: int,\n    task_update: TaskUpdate,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskRead | None:\n    \"\"\"\n    Edit a specific task (restricted to task owner)\n    \"\"\"\n\n    # Retrieve task from database\n    db_task = await _get_task_full_access(\n        task_id=task_id, user_id=user.id, db=db\n    )\n    update = task_update.model_dump(exclude_unset=True)\n\n    # Forbid changes that set a previously unset command\n    if db_task.type == TaskType.NON_PARALLEL and \"command_parallel\" in update:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Cannot set an unset `command_parallel`.\",\n        )\n    if db_task.type == TaskType.PARALLEL and \"command_non_parallel\" in update:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Cannot set an unset `command_non_parallel`.\",\n        )\n\n    for key, value in update.items():\n        setattr(db_task, key, value)\n\n    await db.commit()\n    await db.refresh(db_task)\n    return db_task\n</code></pre>"},{"location":"reference/app/routes/api/v2/task_collection/","title":"task_collection","text":""},{"location":"reference/app/routes/api/v2/task_collection/#fractal_server.app.routes.api.v2.task_collection.CollectionRequestData","title":"<code>CollectionRequestData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Validate form data and wheel file.</p> Source code in <code>fractal_server/app/routes/api/v2/task_collection.py</code> <pre><code>class CollectionRequestData(BaseModel):\n    \"\"\"\n    Validate form data _and_ wheel file.\n    \"\"\"\n\n    task_collect: TaskCollectPip\n    file: UploadFile | None = None\n    origin: TaskGroupOriginEnum\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_data(cls, values):\n        file = values.get(\"file\")\n        package = values.get(\"task_collect\").package\n        package_version = values.get(\"task_collect\").package_version\n\n        if file is None:\n            if package is None:\n                raise ValueError(\n                    \"When no `file` is provided, `package` is required.\"\n                )\n            values[\"origin\"] = TaskGroupOriginEnum.PYPI\n        else:\n            if package is not None:\n                raise ValueError(\n                    \"Cannot set `package` when `file` is provided \"\n                    f\"(given package='{package}').\"\n                )\n            if package_version is not None:\n                raise ValueError(\n                    \"Cannot set `package_version` when `file` is \"\n                    f\"provided (given package_version='{package_version}').\"\n                )\n            values[\"origin\"] = TaskGroupOriginEnum.WHEELFILE\n\n            for forbidden_char in FORBIDDEN_CHAR_WHEEL:\n                if forbidden_char in file.filename:\n                    raise ValueError(\n                        \"Wheel filename has forbidden characters, \"\n                        f\"{FORBIDDEN_CHAR_WHEEL}\"\n                    )\n\n        return values\n</code></pre>"},{"location":"reference/app/routes/api/v2/task_collection/#fractal_server.app.routes.api.v2.task_collection.collect_tasks_pip","title":"<code>collect_tasks_pip(response, background_tasks, request_data=Depends(parse_request_data), private=False, user_group_id=None, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Task-collection endpoint</p> Source code in <code>fractal_server/app/routes/api/v2/task_collection.py</code> <pre><code>@router.post(\n    \"/collect/pip/\",\n    response_model=TaskGroupActivityRead,\n)\nasync def collect_tasks_pip(\n    response: Response,\n    background_tasks: BackgroundTasks,\n    request_data: CollectionRequestData = Depends(parse_request_data),\n    private: bool = False,\n    user_group_id: int | None = None,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupActivityRead:\n    \"\"\"\n    Task-collection endpoint\n    \"\"\"\n\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(\n        user=user,\n        db=db,\n    )\n    resource_id = resource.id\n\n    # Get some validated request data\n    task_collect = request_data.task_collect\n\n    # Initialize task-group attributes\n    task_group_attrs = dict(\n        user_id=user.id,\n        resource_id=resource_id,\n        origin=request_data.origin,\n    )\n\n    # Set/check python version\n    if task_collect.python_version is None:\n        task_group_attrs[\"python_version\"] = resource.tasks_python_config[\n            \"default_version\"\n        ]\n    else:\n        task_group_attrs[\"python_version\"] = task_collect.python_version\n    try:\n        get_python_interpreter(\n            python_version=task_group_attrs[\"python_version\"],\n            resource=resource,\n        )\n    except ValueError:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Python version {task_group_attrs['python_version']} \"\n                \"is not available on this Fractal instance.\"\n            ),\n        )\n\n    # Set pip_extras\n    if task_collect.package_extras is not None:\n        task_group_attrs[\"pip_extras\"] = task_collect.package_extras\n\n    # Set pinned_package_versions\n    if task_collect.pinned_package_versions_pre is not None:\n        task_group_attrs[\"pinned_package_versions_pre\"] = (\n            task_collect.pinned_package_versions_pre\n        )\n    if task_collect.pinned_package_versions_post is not None:\n        task_group_attrs[\"pinned_package_versions_post\"] = (\n            task_collect.pinned_package_versions_post\n        )\n\n    # Initialize wheel_file_content as None\n    wheel_file = None\n\n    # Set pkg_name, version, origin and archive_path\n    if request_data.origin == TaskGroupOriginEnum.WHEELFILE:\n        try:\n            wheel_filename = request_data.file.filename\n            wheel_info = _parse_wheel_filename(wheel_filename)\n            wheel_file_content = await request_data.file.read()\n            wheel_file = FractalUploadedFile(\n                filename=wheel_filename,\n                contents=wheel_file_content,\n            )\n        except ValueError as e:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=(\n                    f\"Invalid wheel-file name {wheel_filename}. \"\n                    f\"Original error: {str(e)}\",\n                ),\n            )\n        task_group_attrs[\"pkg_name\"] = normalize_package_name(\n            wheel_info[\"distribution\"]\n        )\n        task_group_attrs[\"version\"] = wheel_info[\"version\"]\n    elif request_data.origin == TaskGroupOriginEnum.PYPI:\n        pkg_name = task_collect.package\n        task_group_attrs[\"pkg_name\"] = normalize_package_name(pkg_name)\n        latest_version = await get_package_version_from_pypi(\n            task_collect.package,\n            task_collect.package_version,\n        )\n        task_group_attrs[\"version\"] = latest_version\n\n    # Validate query parameters related to user-group ownership\n    user_group_id = await _get_valid_user_group_id(\n        user_group_id=user_group_id,\n        private=private,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Set user_group_id\n    task_group_attrs[\"user_group_id\"] = user_group_id\n\n    # Set path and venv_path\n    if resource.type == ResourceType.SLURM_SSH:\n        base_tasks_path = profile.tasks_remote_dir\n    else:\n        base_tasks_path = resource.tasks_local_dir\n    task_group_path = (\n        Path(base_tasks_path)\n        / str(user.id)\n        / task_group_attrs[\"pkg_name\"]\n        / task_group_attrs[\"version\"]\n    ).as_posix()\n    task_group_attrs[\"path\"] = task_group_path\n    task_group_attrs[\"venv_path\"] = Path(task_group_path, \"venv\").as_posix()\n\n    # Validate TaskGroupV2 attributes\n    try:\n        TaskGroupCreateStrict(**task_group_attrs)\n    except ValidationError as e:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Invalid task-group object. Original error: {e}\",\n        )\n\n    # Database checks\n\n    # Verify non-duplication constraints\n    await _verify_non_duplication_user_constraint(\n        user_id=user.id,\n        pkg_name=task_group_attrs[\"pkg_name\"],\n        version=task_group_attrs[\"version\"],\n        user_resource_id=resource_id,\n        db=db,\n    )\n    await _verify_non_duplication_group_constraint(\n        user_group_id=task_group_attrs[\"user_group_id\"],\n        pkg_name=task_group_attrs[\"pkg_name\"],\n        version=task_group_attrs[\"version\"],\n        db=db,\n    )\n\n    # On-disk checks\n\n    if resource.type != ResourceType.SLURM_SSH:\n        # Verify that folder does not exist (for local collection)\n        if Path(task_group_path).exists():\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=f\"{task_group_path} already exists.\",\n            )\n\n    # Create TaskGroupV2 object\n    task_group = TaskGroupV2(**task_group_attrs)\n    db.add(task_group)\n    async with integrity_error_to_422(db):\n        await db.commit()\n    await db.refresh(task_group)\n    db.expunge(task_group)\n\n    # All checks are OK, proceed with task collection\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatus.PENDING,\n        action=TaskGroupActivityAction.COLLECT,\n        pkg_name=task_group.pkg_name,\n        version=task_group.version,\n        fractal_server_version=__VERSION__,\n    )\n    db.add(task_group_activity)\n    await db.commit()\n    await db.refresh(task_group_activity)\n    logger = set_logger(logger_name=\"collect_tasks_pip\")\n\n    # END of SSH/non-SSH common part\n\n    if resource.type == ResourceType.SLURM_SSH:\n        collect_function = collect_ssh\n    else:\n        collect_function = collect_local\n\n    background_tasks.add_task(\n        collect_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        wheel_file=wheel_file,\n        resource=resource,\n        profile=profile,\n    )\n\n    logger.debug(\n        \"Task-collection endpoint: start background collection \"\n        \"and return task_group_activity\"\n    )\n    reset_logger_handlers(logger)\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/app/routes/api/v2/task_collection/#fractal_server.app.routes.api.v2.task_collection.parse_request_data","title":"<code>parse_request_data(package=Form(None), package_version=Form(None), package_extras=Form(None), python_version=Form(None), pinned_package_versions_pre=Form(None), pinned_package_versions_post=Form(None), file=File(None))</code>","text":"<p>Expand the parsing/validation of <code>parse_form_data</code>, based on <code>file</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/task_collection.py</code> <pre><code>def parse_request_data(\n    package: str | None = Form(None),\n    package_version: str | None = Form(None),\n    package_extras: str | None = Form(None),\n    python_version: str | None = Form(None),\n    pinned_package_versions_pre: str | None = Form(None),\n    pinned_package_versions_post: str | None = Form(None),\n    file: UploadFile | None = File(None),\n) -&gt; CollectionRequestData:\n    \"\"\"\n    Expand the parsing/validation of `parse_form_data`, based on `file`.\n    \"\"\"\n\n    try:\n        # Convert dict_pinned_pkg from a JSON string into a Python dictionary\n        dict_pinned_pkg_pre = (\n            json.loads(pinned_package_versions_pre)\n            if pinned_package_versions_pre\n            else None\n        )\n        dict_pinned_pkg_post = (\n            json.loads(pinned_package_versions_post)\n            if pinned_package_versions_post\n            else None\n        )\n        # Validate and coerce form data\n        task_collect_pip = TaskCollectPip(\n            package=package,\n            package_version=package_version,\n            package_extras=package_extras,\n            python_version=python_version,\n            pinned_package_versions_pre=dict_pinned_pkg_pre,\n            pinned_package_versions_post=dict_pinned_pkg_post,\n        )\n\n        data = CollectionRequestData(\n            task_collect=task_collect_pip,\n            file=file,\n        )\n\n    except (ValidationError, json.JSONDecodeError) as e:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Invalid request-body\\n{str(e)}\",\n        )\n\n    return data\n</code></pre>"},{"location":"reference/app/routes/api/v2/task_collection_custom/","title":"task_collection_custom","text":""},{"location":"reference/app/routes/api/v2/task_collection_pixi/","title":"task_collection_pixi","text":""},{"location":"reference/app/routes/api/v2/task_group/","title":"task_group","text":""},{"location":"reference/app/routes/api/v2/task_group/#fractal_server.app.routes.api.v2.task_group.get_task_group","title":"<code>get_task_group(task_group_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get single TaskGroup</p> Source code in <code>fractal_server/app/routes/api/v2/task_group.py</code> <pre><code>@router.get(\"/{task_group_id}/\", response_model=TaskGroupRead)\nasync def get_task_group(\n    task_group_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupRead:\n    \"\"\"\n    Get single TaskGroup\n    \"\"\"\n    task_group = await _get_task_group_read_access(\n        task_group_id=task_group_id,\n        user_id=user.id,\n        db=db,\n    )\n    return task_group\n</code></pre>"},{"location":"reference/app/routes/api/v2/task_group/#fractal_server.app.routes.api.v2.task_group.get_task_group_list","title":"<code>get_task_group_list(user=Depends(get_api_guest), db=Depends(get_async_db), only_active=False, only_owner=False, args_schema=True)</code>  <code>async</code>","text":"<p>Get all accessible TaskGroups</p> Source code in <code>fractal_server/app/routes/api/v2/task_group.py</code> <pre><code>@router.get(\"/\", response_model=list[tuple[str, list[TaskGroupRead]]])\nasync def get_task_group_list(\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n    only_active: bool = False,\n    only_owner: bool = False,\n    args_schema: bool = True,\n) -&gt; list[tuple[str, list[TaskGroupRead]]]:\n    \"\"\"\n    Get all accessible TaskGroups\n    \"\"\"\n    if only_owner:\n        condition = TaskGroupV2.user_id == user.id\n    else:\n        condition = or_(\n            TaskGroupV2.user_id == user.id,\n            TaskGroupV2.user_group_id.in_(\n                select(LinkUserGroup.group_id).where(\n                    LinkUserGroup.user_id == user.id\n                )\n            ),\n        )\n\n    user_resource_id = await _get_user_resource_id(user_id=user.id, db=db)\n    stm = (\n        select(TaskGroupV2)\n        .where(TaskGroupV2.resource_id == user_resource_id)\n        .where(condition)\n        .order_by(TaskGroupV2.pkg_name)\n    )\n    if only_active:\n        stm = stm.where(TaskGroupV2.active)\n\n    res = await db.execute(stm)\n    task_groups = res.scalars().all()\n\n    if args_schema is False:\n        for taskgroup in task_groups:\n            for task in taskgroup.task_list:\n                db.expunge(task)  # See issue 3101\n                setattr(task, \"args_schema_non_parallel\", None)\n                setattr(task, \"args_schema_parallel\", None)\n\n    default_group_id = await _get_default_usergroup_id_or_none(db)\n    grouped_result = [\n        (\n            pkg_name,\n            (\n                await remove_duplicate_task_groups(\n                    task_groups=sorted(\n                        list(groups),\n                        key=lambda group: _version_sort_key(group.version),\n                        reverse=True,\n                    ),\n                    user_id=user.id,\n                    default_group_id=default_group_id,\n                    db=db,\n                )\n            ),\n        )\n        for pkg_name, groups in itertools.groupby(\n            task_groups, key=lambda tg: tg.pkg_name\n        )\n    ]\n    return grouped_result\n</code></pre>"},{"location":"reference/app/routes/api/v2/task_group/#fractal_server.app.routes.api.v2.task_group.patch_task_group","title":"<code>patch_task_group(task_group_id, task_group_update, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Patch single TaskGroup</p> Source code in <code>fractal_server/app/routes/api/v2/task_group.py</code> <pre><code>@router.patch(\"/{task_group_id}/\", response_model=TaskGroupRead)\nasync def patch_task_group(\n    task_group_id: int,\n    task_group_update: TaskGroupUpdate,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupRead:\n    \"\"\"\n    Patch single TaskGroup\n    \"\"\"\n    task_group = await _get_task_group_full_access(\n        task_group_id=task_group_id,\n        user_id=user.id,\n        db=db,\n    )\n    if (\n        \"user_group_id\" in task_group_update.model_dump(exclude_unset=True)\n        and task_group_update.user_group_id != task_group.user_group_id\n    ):\n        await _verify_non_duplication_group_constraint(\n            db=db,\n            pkg_name=task_group.pkg_name,\n            version=task_group.version,\n            user_group_id=task_group_update.user_group_id,\n        )\n    for key, value in task_group_update.model_dump(exclude_unset=True).items():\n        if (key == \"user_group_id\") and (value is not None):\n            await _verify_user_belongs_to_group(\n                user_id=user.id, user_group_id=value, db=db\n            )\n        setattr(task_group, key, value)\n\n    db.add(task_group)\n    await db.commit()\n    await db.refresh(task_group)\n    return task_group\n</code></pre>"},{"location":"reference/app/routes/api/v2/task_group_lifecycle/","title":"task_group_lifecycle","text":""},{"location":"reference/app/routes/api/v2/task_group_lifecycle/#fractal_server.app.routes.api.v2.task_group_lifecycle.deactivate_task_group","title":"<code>deactivate_task_group(task_group_id, background_tasks, response, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Deactivate task-group venv</p> Source code in <code>fractal_server/app/routes/api/v2/task_group_lifecycle.py</code> <pre><code>@router.post(\n    \"/{task_group_id}/deactivate/\",\n    response_model=TaskGroupActivityRead,\n)\nasync def deactivate_task_group(\n    task_group_id: int,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupActivityRead:\n    \"\"\"\n    Deactivate task-group venv\n    \"\"\"\n\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(user=user, db=db)\n\n    # Check access\n    task_group = await _get_task_group_full_access(\n        task_group_id=task_group_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Check no other activity is ongoing\n    await check_no_ongoing_activity(task_group_id=task_group_id, db=db)\n\n    # Check no submitted jobs use tasks from this task group\n    await check_no_submitted_job(task_group_id=task_group.id, db=db)\n\n    # Check that task-group is active\n    if not task_group.active:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot deactivate a task group with {task_group.active=}.\"\n            ),\n        )\n\n    # Shortcut for task-group with origin=\"other\"\n    if task_group.origin == TaskGroupOriginEnum.OTHER:\n        task_group.active = False\n        task_group_activity = TaskGroupActivityV2(\n            user_id=task_group.user_id,\n            taskgroupv2_id=task_group.id,\n            status=TaskGroupActivityStatus.OK,\n            action=TaskGroupActivityAction.DEACTIVATE,\n            pkg_name=task_group.pkg_name,\n            version=(task_group.version or \"N/A\"),\n            log=(\n                f\"Task group has {task_group.origin=}, set \"\n                \"task_group.active to False and exit.\"\n            ),\n            timestamp_started=get_timestamp(),\n            timestamp_ended=get_timestamp(),\n            fractal_server_version=__VERSION__,\n        )\n        db.add(task_group)\n        db.add(task_group_activity)\n        await db.commit()\n        response.status_code = status.HTTP_202_ACCEPTED\n        return task_group_activity\n\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatus.PENDING,\n        action=TaskGroupActivityAction.DEACTIVATE,\n        pkg_name=task_group.pkg_name,\n        version=task_group.version,\n        timestamp_started=get_timestamp(),\n        fractal_server_version=__VERSION__,\n    )\n    task_group.active = False\n    db.add(task_group)\n    db.add(task_group_activity)\n    await db.commit()\n\n    # Submit background task\n    if resource.type == ResourceType.SLURM_SSH:\n        if task_group.origin == TaskGroupOriginEnum.PIXI:\n            deactivate_function = deactivate_ssh_pixi\n        else:\n            deactivate_function = deactivate_ssh\n    else:\n        if task_group.origin == TaskGroupOriginEnum.PIXI:\n            deactivate_function = deactivate_local_pixi\n        else:\n            deactivate_function = deactivate_local\n    background_tasks.add_task(\n        deactivate_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        resource=resource,\n        profile=profile,\n    )\n\n    logger.debug(\n        \"Task group deactivation endpoint: start deactivate \"\n        \"and return task_group_activity\"\n    )\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/app/routes/api/v2/task_group_lifecycle/#fractal_server.app.routes.api.v2.task_group_lifecycle.delete_task_group","title":"<code>delete_task_group(task_group_id, background_tasks, response, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Deletion of task-group from db and file system</p> Source code in <code>fractal_server/app/routes/api/v2/task_group_lifecycle.py</code> <pre><code>@router.post(\n    \"/{task_group_id}/delete/\",\n    status_code=202,\n)\nasync def delete_task_group(\n    task_group_id: int,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupActivityRead:\n    \"\"\"\n    Deletion of task-group from db and file system\n    \"\"\"\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(user=user, db=db)\n\n    task_group = await _get_task_group_full_access(\n        task_group_id=task_group_id,\n        user_id=user.id,\n        db=db,\n    )\n    await check_no_ongoing_activity(task_group_id=task_group_id, db=db)\n    await check_no_related_workflowtask(task_group=task_group, db=db)\n\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatus.PENDING,\n        action=TaskGroupActivityAction.DELETE,\n        pkg_name=task_group.pkg_name,\n        version=(task_group.version or \"N/A\"),\n        timestamp_started=get_timestamp(),\n        fractal_server_version=__VERSION__,\n    )\n    db.add(task_group_activity)\n    await db.commit()\n\n    if resource.type == ResourceType.SLURM_SSH:\n        delete_function = delete_ssh\n    else:\n        delete_function = delete_local\n\n    background_tasks.add_task(\n        delete_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        resource=resource,\n        profile=profile,\n    )\n\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/app/routes/api/v2/task_group_lifecycle/#fractal_server.app.routes.api.v2.task_group_lifecycle.reactivate_task_group","title":"<code>reactivate_task_group(task_group_id, background_tasks, response, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Deactivate task-group venv</p> Source code in <code>fractal_server/app/routes/api/v2/task_group_lifecycle.py</code> <pre><code>@router.post(\n    \"/{task_group_id}/reactivate/\",\n    response_model=TaskGroupActivityRead,\n)\nasync def reactivate_task_group(\n    task_group_id: int,\n    background_tasks: BackgroundTasks,\n    response: Response,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; TaskGroupRead:\n    \"\"\"\n    Deactivate task-group venv\n    \"\"\"\n    # Get validated resource and profile\n    resource, profile = await validate_user_profile(user=user, db=db)\n\n    # Check access\n    task_group = await _get_task_group_full_access(\n        task_group_id=task_group_id,\n        user_id=user.id,\n        db=db,\n    )\n\n    # Check that task-group is not active\n    if task_group.active:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot reactivate a task group with {task_group.active=}.\"\n            ),\n        )\n\n    # Check no other activity is ongoing\n    await check_no_ongoing_activity(task_group_id=task_group_id, db=db)\n\n    # Check no submitted jobs use tasks from this task group\n    await check_no_submitted_job(task_group_id=task_group.id, db=db)\n\n    # Shortcut for task-group with origin=\"other\"\n    if task_group.origin == TaskGroupOriginEnum.OTHER:\n        task_group.active = True\n        task_group_activity = TaskGroupActivityV2(\n            user_id=task_group.user_id,\n            taskgroupv2_id=task_group.id,\n            status=TaskGroupActivityStatus.OK,\n            action=TaskGroupActivityAction.REACTIVATE,\n            pkg_name=task_group.pkg_name,\n            version=(task_group.version or \"N/A\"),\n            log=(\n                f\"Task group has {task_group.origin=}, set \"\n                \"task_group.active to True and exit.\"\n            ),\n            timestamp_started=get_timestamp(),\n            timestamp_ended=get_timestamp(),\n            fractal_server_version=__VERSION__,\n        )\n        db.add(task_group)\n        db.add(task_group_activity)\n        await db.commit()\n        response.status_code = status.HTTP_202_ACCEPTED\n        return task_group_activity\n\n    if task_group.env_info is None:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot reactivate a task group with {task_group.env_info=}.\"\n            ),\n        )\n\n    task_group_activity = TaskGroupActivityV2(\n        user_id=task_group.user_id,\n        taskgroupv2_id=task_group.id,\n        status=TaskGroupActivityStatus.PENDING,\n        action=TaskGroupActivityAction.REACTIVATE,\n        pkg_name=task_group.pkg_name,\n        version=task_group.version,\n        timestamp_started=get_timestamp(),\n        fractal_server_version=__VERSION__,\n    )\n    db.add(task_group_activity)\n    await db.commit()\n\n    # Submit background task\n    if resource.type == ResourceType.SLURM_SSH:\n        if task_group.origin == TaskGroupOriginEnum.PIXI:\n            reactivate_function = reactivate_ssh_pixi\n        else:\n            reactivate_function = reactivate_ssh\n    else:\n        if task_group.origin == TaskGroupOriginEnum.PIXI:\n            reactivate_function = reactivate_local_pixi\n        else:\n            reactivate_function = reactivate_local\n    background_tasks.add_task(\n        reactivate_function,\n        task_group_id=task_group.id,\n        task_group_activity_id=task_group_activity.id,\n        resource=resource,\n        profile=profile,\n    )\n    logger.debug(\n        \"Task group reactivation endpoint: start reactivate \"\n        \"and return task_group_activity\"\n    )\n    response.status_code = status.HTTP_202_ACCEPTED\n    return task_group_activity\n</code></pre>"},{"location":"reference/app/routes/api/v2/task_version_update/","title":"task_version_update","text":""},{"location":"reference/app/routes/api/v2/workflow/","title":"workflow","text":""},{"location":"reference/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.create_workflow","title":"<code>create_workflow(project_id, workflow, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Create a workflow, associate to a project</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/\",\n    response_model=WorkflowRead,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_workflow(\n    project_id: int,\n    workflow: WorkflowCreate,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowRead | None:\n    \"\"\"\n    Create a workflow, associate to a project\n    \"\"\"\n    await _get_project_check_access(\n        project_id=project_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n    await _check_workflow_exists(\n        name=workflow.name, project_id=project_id, db=db\n    )\n\n    db_workflow = WorkflowV2(project_id=project_id, **workflow.model_dump())\n    db.add(db_workflow)\n    await db.commit()\n    await db.refresh(db_workflow)\n    return db_workflow\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.delete_workflow","title":"<code>delete_workflow(project_id, workflow_id, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    status_code=status.HTTP_204_NO_CONTENT,\n)\nasync def delete_workflow(\n    project_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_access(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n\n    # Fail if there exist jobs that are submitted and in relation with the\n    # current workflow.\n    stm = _get_submitted_jobs_statement().where(\n        JobV2.workflow_id == workflow.id\n    )\n    res = await db.execute(stm)\n    jobs = res.scalars().all()\n    if jobs:\n        string_ids = str([job.id for job in jobs])[1:-1]\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                f\"Cannot delete workflow {workflow.id} because it \"\n                f\"is linked to active job(s) {string_ids}.\"\n            ),\n        )\n\n    # Delete workflow\n    await db.delete(workflow)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.export_workflow","title":"<code>export_workflow(project_id, workflow_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Export an existing workflow, after stripping all IDs</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/export/\",\n    response_model=WorkflowExport,\n)\nasync def export_workflow(\n    project_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowExport | None:\n    \"\"\"\n    Export an existing workflow, after stripping all IDs\n    \"\"\"\n    workflow = await _get_workflow_check_access(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n    wf_task_list = []\n    for wftask in workflow.task_list:\n        task_group = await db.get(TaskGroupV2, wftask.task.taskgroupv2_id)\n        wf_task_list.append(wftask.model_dump())\n        wf_task_list[-1][\"task\"] = dict(\n            pkg_name=task_group.pkg_name,\n            version=task_group.version,\n            name=wftask.task.name,\n        )\n\n    wf = WorkflowExport(\n        **workflow.model_dump(),\n        task_list=wf_task_list,\n    )\n    return wf\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.get_workflow_list","title":"<code>get_workflow_list(project_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get workflow list for given project</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/\",\n    response_model=list[WorkflowRead],\n)\nasync def get_workflow_list(\n    project_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[WorkflowRead] | None:\n    \"\"\"\n    Get workflow list for given project\n    \"\"\"\n    # Access control\n    project = await _get_project_check_access(\n        project_id=project_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n    # Find workflows of the current project. Note: this select/where approach\n    # has much better scaling than refreshing all elements of\n    # `project.workflow_list` - ref\n    # https://github.com/fractal-analytics-platform/fractal-server/pull/1082#issuecomment-1856676097.\n    stm = select(WorkflowV2).where(WorkflowV2.project_id == project.id)\n    workflow_list = (await db.execute(stm)).scalars().all()\n    return workflow_list\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.get_workflow_type_filters","title":"<code>get_workflow_type_filters(project_id, workflow_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on type/type-filters flow for a workflow.</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\"/project/{project_id}/workflow/{workflow_id}/type-filters-flow/\")\nasync def get_workflow_type_filters(\n    project_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[WorkflowTaskTypeFiltersInfo]:\n    \"\"\"\n    Get info on type/type-filters flow for a workflow.\n    \"\"\"\n\n    workflow = await _get_workflow_check_access(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n\n    num_tasks = len(workflow.task_list)\n    if num_tasks == 0:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Workflow has no tasks.\",\n        )\n\n    current_type_filters = {}\n\n    response_items = []\n    for wftask in workflow.task_list:\n        # Compute input_type_filters, based on wftask and task manifest\n        input_type_filters = merge_type_filters(\n            wftask_type_filters=wftask.type_filters,\n            task_input_types=wftask.task.input_types,\n        )\n\n        # Append current item to response list\n        response_items.append(\n            dict(\n                workflowtask_id=wftask.id,\n                current_type_filters=copy(current_type_filters),\n                input_type_filters=copy(input_type_filters),\n                output_type_filters=copy(wftask.task.output_types),\n            )\n        )\n\n        # Update `current_type_filters`\n        current_type_filters.update(wftask.task.output_types)\n\n    return response_items\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.read_workflow","title":"<code>read_workflow(project_id, workflow_id, user=Depends(get_api_guest), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Get info on an existing workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.get(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    response_model=WorkflowReadWithWarnings,\n)\nasync def read_workflow(\n    project_id: int,\n    workflow_id: int,\n    user: UserOAuth = Depends(get_api_guest),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowReadWithWarnings | None:\n    \"\"\"\n    Get info on an existing workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_access(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.READ,\n        db=db,\n    )\n\n    wftask_list_with_warnings = await _add_warnings_to_workflow_tasks(\n        wftask_list=workflow.task_list, user_id=user.id, db=db\n    )\n    workflow_data = dict(\n        **workflow.model_dump(),\n        project=workflow.project,\n        task_list=wftask_list_with_warnings,\n    )\n\n    return workflow_data\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflow/#fractal_server.app.routes.api.v2.workflow.update_workflow","title":"<code>update_workflow(project_id, workflow_id, patch, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflow.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/workflow/{workflow_id}/\",\n    response_model=WorkflowReadWithWarnings,\n)\nasync def update_workflow(\n    project_id: int,\n    workflow_id: int,\n    patch: WorkflowUpdate,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowReadWithWarnings | None:\n    \"\"\"\n    Edit a workflow\n    \"\"\"\n    workflow = await _get_workflow_check_access(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n\n    if patch.name and patch.name != workflow.name:\n        await _check_workflow_exists(\n            name=patch.name, project_id=project_id, db=db\n        )\n\n    for key, value in patch.model_dump(exclude_unset=True).items():\n        if key == \"reordered_workflowtask_ids\":\n            if await _workflow_has_submitted_job(\n                workflow_id=workflow_id, db=db\n            ):\n                raise HTTPException(\n                    status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                    detail=(\n                        \"Cannot re-order WorkflowTasks while a Job is running \"\n                        \"for this Workflow.\"\n                    ),\n                )\n\n            current_workflowtask_ids = [\n                wftask.id for wftask in workflow.task_list\n            ]\n            num_tasks = len(workflow.task_list)\n            if len(value) != num_tasks or set(value) != set(\n                current_workflowtask_ids\n            ):\n                raise HTTPException(\n                    status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                    detail=(\n                        \"`reordered_workflowtask_ids` must be a permutation of\"\n                        f\" {current_workflowtask_ids} (given {value})\"\n                    ),\n                )\n            for ind_wftask in range(num_tasks):\n                new_order = value.index(workflow.task_list[ind_wftask].id)\n                workflow.task_list[ind_wftask].order = new_order\n        else:\n            setattr(workflow, key, value)\n\n    await db.commit()\n    await db.refresh(workflow)\n\n    wftask_list_with_warnings = await _add_warnings_to_workflow_tasks(\n        wftask_list=workflow.task_list, user_id=user.id, db=db\n    )\n    workflow_data = dict(\n        **workflow.model_dump(),\n        project=workflow.project,\n        task_list=wftask_list_with_warnings,\n    )\n\n    return workflow_data\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflow_import/","title":"workflow_import","text":""},{"location":"reference/app/routes/api/v2/workflow_import/#fractal_server.app.routes.api.v2.workflow_import._get_task_id_or_available_tasks","title":"<code>_get_task_id_or_available_tasks(*, task_import, task_groups_list, user_id, default_group_id, db)</code>  <code>async</code>","text":"<p>Find a task id based on <code>task_import</code>. If the task is not found, return the list of available versions.</p> PARAMETER DESCRIPTION <code>task_import</code> <p>Info on task to be imported.</p> <p> TYPE: <code>TaskImport</code> </p> <code>task_groups_list</code> <p>Current list of valid task groups with not-null version.</p> <p> TYPE: <code>list[TaskGroupV2]</code> </p> <code>user_id</code> <p>ID of current user.</p> <p> TYPE: <code>int</code> </p> <code>default_group_id</code> <p>ID of default user group.</p> <p> TYPE: <code>int | None</code> </p> <code>db</code> <p>Asynchronous database session.</p> <p> TYPE: <code>AsyncSession</code> </p> Return <p>A tuple <code>(success, result)</code> where: - <code>success</code> is <code>True</code> if a matching task was found, <code>False</code> otherwise. - <code>result</code> is:     - the <code>id</code> of the matching task when <code>success</code> is <code>True</code>,     - a list of <code>TaskAvailable</code> instances when <code>success</code> is <code>False</code>.</p> Source code in <code>fractal_server/app/routes/api/v2/workflow_import.py</code> <pre><code>async def _get_task_id_or_available_tasks(\n    *,\n    task_import: TaskImport,\n    task_groups_list: list[TaskGroupV2],\n    user_id: int,\n    default_group_id: int | None,\n    db: AsyncSession,\n) -&gt; tuple[bool, int | list[TaskAvailable]]:\n    \"\"\"\n    Find a task id based on `task_import`.\n    If the task is not found, return the list of available versions.\n\n    Args:\n        task_import: Info on task to be imported.\n        task_groups_list: Current list of valid task groups with not-null\n            version.\n        user_id: ID of current user.\n        default_group_id: ID of default user group.\n        db: Asynchronous database session.\n\n    Return:\n        A tuple `(success, result)` where:\n        - `success` is `True` if a matching task was found, `False` otherwise.\n        - `result` is:\n            - the `id` of the matching task when `success` is `True`,\n            - a list of `TaskAvailable` instances when `success` is `False`.\n    \"\"\"\n\n    logger.debug(f\"[_get_task_id_or_available_tasks] START, {task_import=}\")\n\n    # Filter by `pkg_name` and by presence of a task with given `name`.\n    matching_task_groups = [\n        task_group\n        for task_group in task_groups_list\n        if (\n            task_group.pkg_name == task_import.pkg_name\n            and task_import.name in [task.name for task in task_group.task_list]\n        )\n    ]\n    if len(matching_task_groups) &lt; 1:\n        logger.debug(\n            \"[_get_task_id_or_available_tasks] \"\n            f\"No task group with {task_import.pkg_name=} \"\n            f\"and a task with {task_import.name=}.\"\n        )\n        return (False, [])\n\n    if task_import.version is not None:\n        final_matching_task_groups = list(\n            filter(\n                lambda tg: tg.version == task_import.version,\n                matching_task_groups,\n            )\n        )\n    else:\n        final_matching_task_groups = matching_task_groups\n\n    if task_import.version is None or len(final_matching_task_groups) &lt; 1:\n        logger.debug(\n            \"[_get_task_id_or_available_tasks] \"\n            \"No task group left after filtering by version.\"\n        )\n        return (\n            False,\n            [\n                TaskAvailable(version=tg.version, active=tg.active)\n                for tg in matching_task_groups\n            ],\n        )\n    elif len(final_matching_task_groups) == 1:\n        final_task_group = final_matching_task_groups[0]\n        logger.debug(\n            \"[_get_task_id_or_available_tasks] \"\n            \"Found a single task group, after filtering by version.\"\n        )\n    else:\n        logger.debug(\n            \"[_get_task_id_or_available_tasks] \"\n            f\"Found {len(final_matching_task_groups)} task groups, \"\n            \"after filtering by version.\"\n        )\n        final_task_group = await _disambiguate_task_groups(\n            matching_task_groups=final_matching_task_groups,\n            user_id=user_id,\n            db=db,\n            default_group_id=default_group_id,\n        )\n        if final_task_group is None:\n            logger.error(\n                \"[_get_task_id_or_available_tasks] UnreachableBranchError: \"\n                \"disambiguation returned None, likely be due to a race \"\n                \"condition on TaskGroups.\"\n            )\n            return (False, [])\n\n    # Find task with given name\n    task_id = next(\n        iter(\n            task.id\n            for task in final_task_group.task_list\n            if task.name == task_import.name\n        ),\n        None,\n    )\n    if task_id is None:\n        logger.error(\n            \"[_get_task_id_or_available_tasks] UnreachableBranchError: \"\n            \"likely be due to a race condition on TaskGroups.\"\n        )\n        return (False, [])\n\n    logger.debug(\n        f\"[_get_task_id_or_available_tasks] END, {task_import=}, {task_id=}.\"\n    )\n\n    return (True, task_id)\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflow_import/#fractal_server.app.routes.api.v2.workflow_import._get_user_accessible_taskgroups_with_version","title":"<code>_get_user_accessible_taskgroups_with_version(*, user_id, user_resource_id, db)</code>  <code>async</code>","text":"<p>Retrieve list of task groups with non-null version that the user has access to.</p> Source code in <code>fractal_server/app/routes/api/v2/workflow_import.py</code> <pre><code>async def _get_user_accessible_taskgroups_with_version(\n    *,\n    user_id: int,\n    user_resource_id: int,\n    db: AsyncSession,\n) -&gt; list[TaskGroupV2]:\n    \"\"\"\n    Retrieve list of task groups with non-null version that the user has access\n    to.\n    \"\"\"\n\n    stm = (\n        select(TaskGroupV2)\n        .where(\n            or_(\n                TaskGroupV2.user_id == user_id,\n                TaskGroupV2.user_group_id.in_(\n                    select(LinkUserGroup.group_id).where(\n                        LinkUserGroup.user_id == user_id\n                    )\n                ),\n            )\n        )\n        .where(TaskGroupV2.resource_id == user_resource_id)\n        .where(is_not(TaskGroupV2.version, None))\n    )\n    res = await db.execute(stm)\n    accessible_task_groups = res.scalars().all()\n    logger.debug(\n        f\"Found {len(accessible_task_groups)} accessible \"\n        f\"task groups for {user_id=}.\"\n    )\n    return accessible_task_groups\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflow_import/#fractal_server.app.routes.api.v2.workflow_import.import_workflow","title":"<code>import_workflow(project_id, workflow_import, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Import an existing workflow into a project and create required objects.</p> Source code in <code>fractal_server/app/routes/api/v2/workflow_import.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/import/\",\n    status_code=status.HTTP_201_CREATED,\n)\nasync def import_workflow(\n    project_id: int,\n    workflow_import: WorkflowImport,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Import an existing workflow into a project and create required objects.\n    \"\"\"\n\n    user_resource_id = await _get_user_resource_id(user_id=user.id, db=db)\n\n    # Preliminary checks\n    await _get_project_check_access(\n        project_id=project_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n    await _check_workflow_exists(\n        name=workflow_import.name,\n        project_id=project_id,\n        db=db,\n    )\n\n    task_group_list = await _get_user_accessible_taskgroups_with_version(\n        user_id=user.id,\n        db=db,\n        user_resource_id=user_resource_id,\n    )\n    default_group_id = await _get_default_usergroup_id_or_none(db)\n\n    list_wf_tasks = []\n    list_results = [\n        await _get_task_id_or_available_tasks(\n            task_import=wf_task.task,\n            user_id=user.id,\n            default_group_id=default_group_id,\n            task_groups_list=task_group_list,\n            db=db,\n        )\n        for wf_task in workflow_import.task_list\n    ]\n\n    if any(success is False for success, _ in list_results):\n        raise HTTPExceptionWithData(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            data=[\n                {\n                    \"outcome\": \"success\",\n                    \"pkg_name\": wf_task.task.pkg_name,\n                    \"version\": wf_task.task.version,\n                    \"task_name\": wf_task.task.name,\n                    \"task_id\": task_id_or_available_tasks,\n                }\n                if success\n                else {\n                    \"outcome\": \"fail\",\n                    \"pkg_name\": wf_task.task.pkg_name,\n                    \"version\": wf_task.task.version,\n                    \"task_name\": wf_task.task.name,\n                    \"available_tasks\": [\n                        available_task.model_dump()\n                        for available_task in task_id_or_available_tasks\n                    ],\n                }\n                for wf_task, (success, task_id_or_available_tasks) in zip(\n                    workflow_import.task_list, list_results\n                )\n            ],\n        )\n\n    for wf_task, (_, task_id) in zip(workflow_import.task_list, list_results):\n        new_wf_task = WorkflowTaskCreate(\n            **wf_task.model_dump(exclude_none=True, exclude={\"task\"})\n        )\n        list_wf_tasks.append(new_wf_task)\n        task = await db.get(TaskV2, task_id)\n        _check_type_filters_compatibility(\n            task_input_types=task.input_types,\n            wftask_type_filters=new_wf_task.type_filters,\n        )\n\n    # Create new Workflow\n    db_workflow = WorkflowV2(\n        project_id=project_id,\n        **workflow_import.model_dump(exclude_none=True, exclude={\"task_list\"}),\n    )\n    db.add(db_workflow)\n    await db.commit()\n    await db.refresh(db_workflow)\n\n    # Insert task into the workflow\n    for new_wf_task, (_, task_id) in zip(list_wf_tasks, list_results):\n        await _workflow_insert_task(\n            **new_wf_task.model_dump(),\n            workflow_id=db_workflow.id,\n            task_id=task_id,\n            db=db,\n        )\n\n    # Add warnings for non-active tasks (or non-accessible tasks,\n    # although that should never happen)\n    wftask_list_with_warnings = await _add_warnings_to_workflow_tasks(\n        wftask_list=db_workflow.task_list, user_id=user.id, db=db\n    )\n    workflow_data = dict(\n        **db_workflow.model_dump(),\n        project=db_workflow.project,\n        task_list=wftask_list_with_warnings,\n    )\n\n    return workflow_data\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflowtask/","title":"workflowtask","text":""},{"location":"reference/app/routes/api/v2/workflowtask/#fractal_server.app.routes.api.v2.workflowtask.create_workflowtask","title":"<code>create_workflowtask(project_id, workflow_id, task_id, wftask, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Add a WorkflowTask to a Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflowtask.py</code> <pre><code>@router.post(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/\",\n    response_model=WorkflowTaskRead,\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    task_id: int,\n    wftask: WorkflowTaskCreate,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowTaskRead | None:\n    \"\"\"\n    Add a WorkflowTask to a Workflow\n    \"\"\"\n\n    workflow = await _get_workflow_check_access(\n        project_id=project_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n\n    task = await _get_task_read_access(\n        task_id=task_id, user_id=user.id, db=db, require_active=True\n    )\n\n    if task.type == TaskType.PARALLEL:\n        if (\n            wftask.meta_non_parallel is not None\n            or wftask.args_non_parallel is not None\n        ):\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=(\n                    \"Cannot set `WorkflowTaskV2.meta_non_parallel` or \"\n                    \"`WorkflowTask.args_non_parallel` if the associated Task \"\n                    \"is `parallel`.\"\n                ),\n            )\n    elif task.type == TaskType.NON_PARALLEL:\n        if wftask.meta_parallel is not None or wftask.args_parallel is not None:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=(\n                    \"Cannot set `WorkflowTaskV2.meta_parallel` or \"\n                    \"`WorkflowTask.args_parallel` if the associated Task \"\n                    \"is `non_parallel`.\"\n                ),\n            )\n\n    _check_type_filters_compatibility(\n        task_input_types=task.input_types,\n        wftask_type_filters=wftask.type_filters,\n    )\n\n    wftask_db = await _workflow_insert_task(\n        workflow_id=workflow.id,\n        task_id=task_id,\n        meta_non_parallel=wftask.meta_non_parallel,\n        meta_parallel=wftask.meta_parallel,\n        args_non_parallel=wftask.args_non_parallel,\n        args_parallel=wftask.args_parallel,\n        type_filters=wftask.type_filters,\n        description=wftask.description,\n        alias=wftask.alias,\n        db=db,\n    )\n\n    return wftask_db\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflowtask/#fractal_server.app.routes.api.v2.workflowtask.delete_workflowtask","title":"<code>delete_workflowtask(project_id, workflow_id, workflow_task_id, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a WorkflowTask of a Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflowtask.py</code> <pre><code>@router.delete(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}/\",\n    status_code=status.HTTP_204_NO_CONTENT,\n)\nasync def delete_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a WorkflowTask of a Workflow\n    \"\"\"\n\n    db_workflow_task, db_workflow = await _get_workflow_task_check_access(\n        project_id=project_id,\n        workflow_task_id=workflow_task_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n\n    if await _workflow_has_submitted_job(workflow_id=workflow_id, db=db):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot delete a WorkflowTask while a Job is running for this \"\n                \"Workflow.\"\n            ),\n        )\n\n    # Delete WorkflowTask\n    await db.delete(db_workflow_task)\n    await db.commit()\n\n    await db.refresh(db_workflow)\n    db_workflow.task_list.reorder()\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/app/routes/api/v2/workflowtask/#fractal_server.app.routes.api.v2.workflowtask.update_workflowtask","title":"<code>update_workflowtask(project_id, workflow_id, workflow_task_id, workflow_task_update, user=Depends(get_api_user), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Edit a WorkflowTask of a Workflow</p> Source code in <code>fractal_server/app/routes/api/v2/workflowtask.py</code> <pre><code>@router.patch(\n    \"/project/{project_id}/workflow/{workflow_id}/wftask/{workflow_task_id}/\",\n    response_model=WorkflowTaskRead,\n)\nasync def update_workflowtask(\n    project_id: int,\n    workflow_id: int,\n    workflow_task_id: int,\n    workflow_task_update: WorkflowTaskUpdate,\n    user: UserOAuth = Depends(get_api_user),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; WorkflowTaskRead | None:\n    \"\"\"\n    Edit a WorkflowTask of a Workflow\n    \"\"\"\n\n    db_wf_task, db_workflow = await _get_workflow_task_check_access(\n        project_id=project_id,\n        workflow_task_id=workflow_task_id,\n        workflow_id=workflow_id,\n        user_id=user.id,\n        required_permissions=ProjectPermissions.WRITE,\n        db=db,\n    )\n    if workflow_task_update.type_filters is not None:\n        _check_type_filters_compatibility(\n            task_input_types=db_wf_task.task.input_types,\n            wftask_type_filters=workflow_task_update.type_filters,\n        )\n\n    if db_wf_task.task_type == TaskType.PARALLEL and (\n        workflow_task_update.args_non_parallel is not None\n        or workflow_task_update.meta_non_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot patch `WorkflowTaskV2.args_non_parallel` or \"\n                \"`WorkflowTask.meta_non_parallel` if the associated Task is \"\n                \"parallel.\"\n            ),\n        )\n    elif db_wf_task.task_type in [\n        TaskType.NON_PARALLEL,\n        TaskType.CONVERTER_NON_PARALLEL,\n    ] and (\n        workflow_task_update.args_parallel is not None\n        or workflow_task_update.meta_parallel is not None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot patch `WorkflowTaskV2.args_parallel` or \"\n                \"`WorkflowTask.meta_parallel` if the associated Task is \"\n                \"non parallel.\"\n            ),\n        )\n\n    for key, value in workflow_task_update.model_dump(\n        exclude_unset=True\n    ).items():\n        if key == \"args_parallel\":\n            actual_args = deepcopy(value)\n            if not actual_args:\n                actual_args = None\n            setattr(db_wf_task, key, actual_args)\n        elif key == \"args_non_parallel\":\n            actual_args = deepcopy(value)\n            if not actual_args:\n                actual_args = None\n            setattr(db_wf_task, key, actual_args)\n        else:\n            setattr(db_wf_task, key, value)\n\n    await db.commit()\n    await db.refresh(db_wf_task)\n\n    return db_wf_task\n</code></pre>"},{"location":"reference/app/routes/auth/","title":"auth","text":""},{"location":"reference/app/routes/auth/#fractal_server.app.routes.auth.get_api_guest","title":"<code>get_api_guest(user=Depends(current_user_act_ver))</code>  <code>async</code>","text":"<p>Require a active&amp;verified user, with a non-null <code>profile_id</code>.</p> <p>Raises 401 if user does not exist or is not active. Raises 403 if user is not verified or has null <code>profile_id</code>.</p> Source code in <code>fractal_server/app/routes/auth/__init__.py</code> <pre><code>async def get_api_guest(\n    user: UserOAuth = Depends(current_user_act_ver),\n) -&gt; UserOAuth:\n    \"\"\"\n    Require a active&amp;verified user, with a non-null `profile_id`.\n\n    Raises 401 if user does not exist or is not active.\n    Raises 403 if user is not verified or has null `profile_id`.\n    \"\"\"\n    if user.profile_id is None:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=(\n                f\"Forbidden access ({user.is_verified=} {user.profile_id=}).\"\n            ),\n        )\n    return user\n</code></pre>"},{"location":"reference/app/routes/auth/#fractal_server.app.routes.auth.get_api_user","title":"<code>get_api_user(user=Depends(get_api_guest))</code>  <code>async</code>","text":"<p>Require a active&amp;verified non-guest user, with a non-null <code>profile_id</code>.</p> <p>Raises 401 if user does not exist or is not active. Raises 403 if user is not verified, is a guest or has null <code>profile_id</code>.</p> Source code in <code>fractal_server/app/routes/auth/__init__.py</code> <pre><code>async def get_api_user(\n    user: UserOAuth = Depends(get_api_guest),\n) -&gt; UserOAuth:\n    \"\"\"\n    Require a active&amp;verified non-guest user, with a non-null `profile_id`.\n\n    Raises 401 if user does not exist or is not active.\n    Raises 403 if user is not verified, is a guest or has null `profile_id`.\n    \"\"\"\n    if user.is_guest:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"This feature is not available for guest users.\",\n        )\n    return user\n</code></pre>"},{"location":"reference/app/routes/auth/_aux_auth/","title":"_aux_auth","text":""},{"location":"reference/app/routes/auth/_aux_auth/#fractal_server.app.routes.auth._aux_auth._check_project_dirs_update","title":"<code>_check_project_dirs_update(*, old_project_dirs, new_project_dirs, user_id, db)</code>  <code>async</code>","text":"<p>Raises 422 if by replacing user's <code>project_dirs</code> with new ones we are removing the access to a <code>zarr_dir</code> used by some dataset.</p> <p>Note both <code>old_project_dirs</code> and <code>new_project_dirs</code> have been normalized through <code>os.path.normpath</code>, which notably strips any trailing <code>/</code> character. To be safe, we also re-normalize them within this function.</p> Source code in <code>fractal_server/app/routes/auth/_aux_auth.py</code> <pre><code>async def _check_project_dirs_update(\n    *,\n    old_project_dirs: list[str],\n    new_project_dirs: list[str],\n    user_id: int,\n    db: AsyncSession,\n) -&gt; None:\n    \"\"\"\n    Raises 422 if by replacing user's `project_dirs` with new ones we are\n    removing the access to a `zarr_dir` used by some dataset.\n\n    Note both `old_project_dirs` and `new_project_dirs` have been\n    normalized through `os.path.normpath`, which notably strips any trailing\n    `/` character. To be safe, we also re-normalize them within this function.\n    \"\"\"\n    # Create a list of all the old project dirs that will lose privileges.\n    # E.g.:\n    #   old_project_dirs = [\"/a\", \"/b\", \"/c/d\", \"/e/f\"]\n    #   new_project_dirs = [\"/a\", \"/c\", \"/e/f/g1\", \"/e/f/g2\"]\n    #   removed_project_dirs == [\"/b\", \"/e/f\"]\n    removed_project_dirs = [\n        old_project_dir\n        for old_project_dir in old_project_dirs\n        if not any(\n            Path(old_project_dir).is_relative_to(new_project_dir)\n            for new_project_dir in new_project_dirs\n        )\n    ]\n    if removed_project_dirs:\n        # Query all the `zarr_dir`s linked to the user such that `zarr_dir`\n        # starts with one of the project dirs in `removed_project_dirs`.\n        stmt = (\n            select(DatasetV2.zarr_dir)\n            .join(ProjectV2, ProjectV2.id == DatasetV2.project_id)\n            .join(\n                LinkUserProjectV2,\n                LinkUserProjectV2.project_id == ProjectV2.id,\n            )\n            .where(LinkUserProjectV2.user_id == user_id)\n            .where(LinkUserProjectV2.is_verified.is_(True))\n            .where(\n                or_(\n                    *[\n                        DatasetV2.zarr_dir.startswith(normpath(old_project_dir))\n                        for old_project_dir in removed_project_dirs\n                    ]\n                )\n            )\n        )\n        if new_project_dirs:\n            stmt = stmt.where(\n                and_(\n                    *[\n                        not_(\n                            DatasetV2.zarr_dir.startswith(\n                                normpath(new_project_dir)\n                            )\n                        )\n                        for new_project_dir in new_project_dirs\n                    ]\n                )\n            )\n        res = await db.execute(stmt)\n\n        # Raise 422 if one of the query results is relative to a path in\n        # `removed_project_dirs`, but its not relative to any path in\n        # `new_project_dirs`.\n        if any(\n            (\n                any(\n                    Path(zarr_dir).is_relative_to(old_project_dir)\n                    for old_project_dir in removed_project_dirs\n                )\n                and not any(\n                    Path(zarr_dir).is_relative_to(new_project_dir)\n                    for new_project_dir in new_project_dirs\n                )\n            )\n            for zarr_dir in res.scalars().all()\n        ):\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n                detail=(\n                    \"You tried updating the user project_dirs, removing \"\n                    f\"{removed_project_dirs}. This operation is not possible, \"\n                    \"because it would make the user loose access to some of \"\n                    \"their dataset zarr directories.\"\n                ),\n            )\n</code></pre>"},{"location":"reference/app/routes/auth/_aux_auth/#fractal_server.app.routes.auth._aux_auth._get_default_usergroup_id_or_none","title":"<code>_get_default_usergroup_id_or_none(db)</code>  <code>async</code>","text":"<p>Return the ID of the group named <code>\"All\"</code>, if <code>FRACTAL_DEFAULT_GROUP_NAME</code> is set and such group exists. Return <code>None</code>, if <code>FRACTAL_DEFAULT_GROUP_NAME=None</code> or if the <code>\"All\"</code> group does not exist.</p> Source code in <code>fractal_server/app/routes/auth/_aux_auth.py</code> <pre><code>async def _get_default_usergroup_id_or_none(db: AsyncSession) -&gt; int | None:\n    \"\"\"\n    Return the ID of the group named `\"All\"`, if `FRACTAL_DEFAULT_GROUP_NAME`\n    is set and such group exists. Return `None`, if\n    `FRACTAL_DEFAULT_GROUP_NAME=None` or if the `\"All\"` group does not exist.\n    \"\"\"\n    settings = Inject(get_settings)\n    stm = select(UserGroup.id).where(\n        UserGroup.name == settings.FRACTAL_DEFAULT_GROUP_NAME\n    )\n    res = await db.execute(stm)\n    user_group_id = res.scalars().one_or_none()\n\n    if (\n        settings.FRACTAL_DEFAULT_GROUP_NAME is not None\n        and user_group_id is None\n    ):\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=(\n                f\"User group '{settings.FRACTAL_DEFAULT_GROUP_NAME}'\"\n                \" not found.\",\n            ),\n        )\n\n    return user_group_id\n</code></pre>"},{"location":"reference/app/routes/auth/_aux_auth/#fractal_server.app.routes.auth._aux_auth._get_single_user_with_groups","title":"<code>_get_single_user_with_groups(user, db)</code>  <code>async</code>","text":"<p>Enrich a user object by filling its <code>group_ids_names</code> attribute.</p> PARAMETER DESCRIPTION <code>user</code> <p>The current <code>UserOAuth</code> object</p> <p> TYPE: <code>UserOAuth</code> </p> <code>db</code> <p>Async db session</p> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>UserRead</code> <p>A <code>UserRead</code> object with <code>group_ids_names</code> dict</p> Source code in <code>fractal_server/app/routes/auth/_aux_auth.py</code> <pre><code>async def _get_single_user_with_groups(\n    user: UserOAuth,\n    db: AsyncSession,\n) -&gt; UserRead:\n    \"\"\"\n    Enrich a user object by filling its `group_ids_names` attribute.\n\n    Args:\n        user: The current `UserOAuth` object\n        db: Async db session\n\n    Returns:\n        A `UserRead` object with `group_ids_names` dict\n    \"\"\"\n\n    settings = Inject(get_settings)\n\n    stm_groups = (\n        select(UserGroup)\n        .join(LinkUserGroup, LinkUserGroup.group_id == UserGroup.id)\n        .where(LinkUserGroup.user_id == user.id)\n        .order_by(asc(LinkUserGroup.timestamp_created))\n    )\n    res = await db.execute(stm_groups)\n    groups = res.scalars().unique().all()\n    group_ids_names = [(group.id, group.name) for group in groups]\n\n    # Identify the default-group position in the list of groups\n    index = next(\n        (\n            ind\n            for ind, group_tuple in enumerate(group_ids_names)\n            if group_tuple[1] == settings.FRACTAL_DEFAULT_GROUP_NAME\n        ),\n        None,\n    )\n    if (index is None) or (index == 0):\n        # Either the default group does not exist, or it is already the first\n        # one. No action needed.\n        pass\n    else:\n        # Move the default group to the first position\n        default_group = group_ids_names.pop(index)\n        group_ids_names.insert(0, default_group)\n\n    # Create dump of `user.oauth_accounts` relationship\n    oauth_accounts = [\n        oauth_account.model_dump() for oauth_account in user.oauth_accounts\n    ]\n\n    return UserRead(\n        **user.model_dump(),\n        group_ids_names=group_ids_names,\n        oauth_accounts=oauth_accounts,\n    )\n</code></pre>"},{"location":"reference/app/routes/auth/_aux_auth/#fractal_server.app.routes.auth._aux_auth._get_single_usergroup_with_user_ids","title":"<code>_get_single_usergroup_with_user_ids(group_id, db)</code>  <code>async</code>","text":"<p>Get a group, and construct its <code>user_ids</code> list.</p> PARAMETER DESCRIPTION <code>group_id</code> <p> TYPE: <code>int</code> </p> <code>db</code> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>UserGroupRead</code> <p><code>UserGroupRead</code> object, with <code>user_ids</code> attribute populated</p> <code>UserGroupRead</code> <p>from database.</p> Source code in <code>fractal_server/app/routes/auth/_aux_auth.py</code> <pre><code>async def _get_single_usergroup_with_user_ids(\n    group_id: int, db: AsyncSession\n) -&gt; UserGroupRead:\n    \"\"\"\n    Get a group, and construct its `user_ids` list.\n\n    Args:\n        group_id:\n        db:\n\n    Returns:\n        `UserGroupRead` object, with `user_ids` attribute populated\n        from database.\n    \"\"\"\n    group = await _usergroup_or_404(group_id, db)\n\n    # Get all user/group links\n    stm_links = select(LinkUserGroup).where(LinkUserGroup.group_id == group_id)\n    res = await db.execute(stm_links)\n    links = res.scalars().all()\n    user_ids = [link.user_id for link in links]\n\n    return UserGroupRead(**group.model_dump(), user_ids=user_ids)\n</code></pre>"},{"location":"reference/app/routes/auth/_aux_auth/#fractal_server.app.routes.auth._aux_auth._user_or_404","title":"<code>_user_or_404(user_id, db)</code>  <code>async</code>","text":"<p>Get a user from db, or raise a 404 HTTP exception if missing.</p> PARAMETER DESCRIPTION <code>user_id</code> <p>ID of the user</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>Async db session</p> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/routes/auth/_aux_auth.py</code> <pre><code>async def _user_or_404(user_id: int, db: AsyncSession) -&gt; UserOAuth:\n    \"\"\"\n    Get a user from db, or raise a 404 HTTP exception if missing.\n\n    Args:\n        user_id: ID of the user\n        db: Async db session\n    \"\"\"\n    user = await db.get(UserOAuth, user_id, populate_existing=True)\n    if user is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"User {user_id} not found.\",\n        )\n    return user\n</code></pre>"},{"location":"reference/app/routes/auth/current_user/","title":"current_user","text":"<p>Definition of <code>/auth/current-user/</code> endpoints</p>"},{"location":"reference/app/routes/auth/current_user/#fractal_server.app.routes.auth.current_user.get_current_user","title":"<code>get_current_user(group_ids_names=False, user=Depends(current_user_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return current user</p> Source code in <code>fractal_server/app/routes/auth/current_user.py</code> <pre><code>@router_current_user.get(\"/current-user/\", response_model=UserRead)\nasync def get_current_user(\n    group_ids_names: bool = False,\n    user: UserOAuth = Depends(current_user_act),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Return current user\n    \"\"\"\n    if group_ids_names is True:\n        user_with_groups = await _get_single_user_with_groups(user, db)\n        return user_with_groups\n    else:\n        return user\n</code></pre>"},{"location":"reference/app/routes/auth/current_user/#fractal_server.app.routes.auth.current_user.patch_current_user","title":"<code>patch_current_user(user_update, current_user=Depends(current_user_act), user_manager=Depends(get_user_manager), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Note: a user cannot patch their own password (as enforced within the <code>UserUpdateStrict</code> schema).</p> Source code in <code>fractal_server/app/routes/auth/current_user.py</code> <pre><code>@router_current_user.patch(\"/current-user/\", response_model=UserRead)\nasync def patch_current_user(\n    user_update: UserUpdateStrict,\n    current_user: UserOAuth = Depends(current_user_act),\n    user_manager: UserManager = Depends(get_user_manager),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Note: a user cannot patch their own password (as enforced within the\n    `UserUpdateStrict` schema).\n    \"\"\"\n    if current_user.is_guest:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"This feature is not available for guest users.\",\n        )\n\n    update = UserUpdate(**user_update.model_dump(exclude_unset=True))\n\n    # NOTE: here it would be relevant to catch an `InvalidPasswordException`\n    # (from `fastapi_users.exceptions`), if we were to allow users change\n    # their own password\n\n    user = await user_manager.update(update, current_user, safe=True)\n    validated_user = UserOAuth.model_validate(user.model_dump())\n\n    patched_user = await db.get(\n        UserOAuth, validated_user.id, populate_existing=True\n    )\n    patched_user_with_groups = await _get_single_user_with_groups(\n        patched_user, db\n    )\n    return patched_user_with_groups\n</code></pre>"},{"location":"reference/app/routes/auth/group/","title":"group","text":"<p>Definition of <code>/auth/group/</code> routes</p>"},{"location":"reference/app/routes/auth/group/#fractal_server.app.routes.auth.group.delete_single_group","title":"<code>delete_single_group(group_id, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Delete a user group.</p> <p>If <code>FRACTAL_DEFAULT_GROUP_NAME=\"All\"</code>, a group named <code>\"All\"</code> cannot be deleted. If <code>FRACTAL_DEFAULT_GROUP_NAME=None</code>, any group can be deleted.</p> Source code in <code>fractal_server/app/routes/auth/group.py</code> <pre><code>@router_group.delete(\"/group/{group_id}/\", status_code=204)\nasync def delete_single_group(\n    group_id: int,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; Response:\n    \"\"\"\n    Delete a user group.\n\n    If `FRACTAL_DEFAULT_GROUP_NAME=\"All\"`, a group named `\"All\"` cannot be\n    deleted. If `FRACTAL_DEFAULT_GROUP_NAME=None`, any group can be deleted.\n    \"\"\"\n    settings = Inject(get_settings)\n    group = await _usergroup_or_404(group_id, db)\n\n    if group.name == settings.FRACTAL_DEFAULT_GROUP_NAME:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Cannot delete default UserGroup \"\n                f\"'{settings.FRACTAL_DEFAULT_GROUP_NAME}'.\"\n            ),\n        )\n\n    await db.delete(group)\n    await db.commit()\n\n    return Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"reference/app/routes/auth/login/","title":"login","text":"<p>Definition of <code>/auth/{login,logout}/</code>, <code>/auth/token/{login/logout}</code> routes.</p>"},{"location":"reference/app/routes/auth/oauth/","title":"oauth","text":""},{"location":"reference/app/routes/auth/oauth/#fractal_server.app.routes.auth.oauth.FractalOpenID","title":"<code>FractalOpenID</code>","text":"<p>               Bases: <code>OpenID</code></p> <p>Subclass of <code>httpx_oauth.clients.openid.OpenID</code> with customizable name for the <code>\"email\"</code> claim.</p> Source code in <code>fractal_server/app/routes/auth/oauth.py</code> <pre><code>class FractalOpenID(OpenID):\n    \"\"\"\n    Subclass of `httpx_oauth.clients.openid.OpenID` with customizable name for\n    the `\"email\"` claim.\n    \"\"\"\n\n    def __init__(self, *, email_claim: str, **kwargs):\n        super().__init__(**kwargs)\n        self.email_claim = email_claim\n\n    @override\n    async def get_id_email(self, token: str) -&gt; tuple[str, str | None]:\n        \"\"\"\n        Identical to the parent-class method (httpx-oauth version 0.16.1),\n        apart from making `\"email\"` configurable.\n        \"\"\"\n        try:\n            profile = await self.get_profile(token)\n        except GetProfileError as e:\n            raise GetIdEmailError(response=e.response) from e\n        return str(profile[\"sub\"]), profile.get(self.email_claim)\n</code></pre>"},{"location":"reference/app/routes/auth/oauth/#fractal_server.app.routes.auth.oauth.FractalOpenID.get_id_email","title":"<code>get_id_email(token)</code>  <code>async</code>","text":"<p>Identical to the parent-class method (httpx-oauth version 0.16.1), apart from making <code>\"email\"</code> configurable.</p> Source code in <code>fractal_server/app/routes/auth/oauth.py</code> <pre><code>@override\nasync def get_id_email(self, token: str) -&gt; tuple[str, str | None]:\n    \"\"\"\n    Identical to the parent-class method (httpx-oauth version 0.16.1),\n    apart from making `\"email\"` configurable.\n    \"\"\"\n    try:\n        profile = await self.get_profile(token)\n    except GetProfileError as e:\n        raise GetIdEmailError(response=e.response) from e\n    return str(profile[\"sub\"]), profile.get(self.email_claim)\n</code></pre>"},{"location":"reference/app/routes/auth/oauth/#fractal_server.app.routes.auth.oauth.get_oauth_router","title":"<code>get_oauth_router()</code>","text":"<p>Get the <code>APIRouter</code> object for OAuth endpoints.</p> Source code in <code>fractal_server/app/routes/auth/oauth.py</code> <pre><code>def get_oauth_router() -&gt; APIRouter | None:\n    \"\"\"\n    Get the `APIRouter` object for OAuth endpoints.\n    \"\"\"\n    router_oauth = APIRouter()\n    settings = Inject(get_settings)\n    oauth_settings = Inject(get_oauth_settings)\n    if not oauth_settings.is_set:\n        return None\n\n    client_name = oauth_settings.OAUTH_CLIENT_NAME\n    if client_name == \"google\":\n        client = _create_client_google(oauth_settings)\n    elif client_name == \"github\":\n        client = _create_client_github(oauth_settings)\n    else:\n        client = _create_client_oidc(oauth_settings)\n\n    router_oauth.include_router(\n        fastapi_users.get_oauth_router(\n            client,\n            cookie_backend,\n            settings.JWT_SECRET_KEY,\n            is_verified_by_default=False,\n            associate_by_email=True,\n            redirect_url=oauth_settings.OAUTH_REDIRECT_URL,\n        ),\n        prefix=f\"/{client_name}\",\n    )\n\n    # Add trailing slash to all routes' paths\n    for route in router_oauth.routes:\n        if not route.path.endswith(\"/\"):\n            route.path = f\"{route.path}/\"\n\n    return router_oauth\n</code></pre>"},{"location":"reference/app/routes/auth/register/","title":"register","text":"<p>Definition of <code>/auth/register/</code> routes.</p>"},{"location":"reference/app/routes/auth/router/","title":"router","text":""},{"location":"reference/app/routes/auth/users/","title":"users","text":"<p>Definition of <code>/auth/users/</code> routes</p>"},{"location":"reference/app/routes/auth/users/#fractal_server.app.routes.auth.users.list_users","title":"<code>list_users(profile_id=None, user=Depends(current_superuser_act), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Return list of all users</p> Source code in <code>fractal_server/app/routes/auth/users.py</code> <pre><code>@router_users.get(\"/users/\", response_model=list[UserRead])\nasync def list_users(\n    profile_id: int | None = None,\n    user: UserOAuth = Depends(current_superuser_act),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Return list of all users\n    \"\"\"\n    stm = select(UserOAuth)\n    if profile_id is not None:\n        stm = stm.where(UserOAuth.profile_id == profile_id)\n    res = await db.execute(stm)\n    user_list = res.scalars().unique().all()\n\n    # Get all user/group links\n    stm_all_links = select(LinkUserGroup)\n    res = await db.execute(stm_all_links)\n    links = res.scalars().all()\n\n    # TODO: possible optimizations for this construction are listed in\n    # https://github.com/fractal-analytics-platform/fractal-server/issues/1742\n    for ind, user in enumerate(user_list):\n        user_list[ind] = dict(\n            **user.model_dump(),\n            oauth_accounts=user.oauth_accounts,\n            group_ids=[\n                link.group_id for link in links if link.user_id == user.id\n            ],\n        )\n\n    return user_list\n</code></pre>"},{"location":"reference/app/routes/auth/users/#fractal_server.app.routes.auth.users.patch_user","title":"<code>patch_user(user_id, user_update, current_superuser=Depends(current_superuser_act), user_manager=Depends(get_user_manager), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Custom version of the PATCH-user route from <code>fastapi-users</code>.</p> Source code in <code>fractal_server/app/routes/auth/users.py</code> <pre><code>@router_users.patch(\"/users/{user_id}/\", response_model=UserRead)\nasync def patch_user(\n    user_id: int,\n    user_update: UserUpdate,\n    current_superuser: UserOAuth = Depends(current_superuser_act),\n    user_manager: UserManager = Depends(get_user_manager),\n    db: AsyncSession = Depends(get_async_db),\n):\n    \"\"\"\n    Custom version of the PATCH-user route from `fastapi-users`.\n    \"\"\"\n\n    # Check that user exists\n    user_to_patch = await _user_or_404(user_id, db)\n\n    if user_update.profile_id is not None:\n        profile = await db.get(Profile, user_update.profile_id)\n        if profile is None:\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND,\n                detail=f\"Profile {user_update.profile_id} not found.\",\n            )\n\n    if user_update.project_dirs is not None:\n        await _check_project_dirs_update(\n            old_project_dirs=user_to_patch.project_dirs,\n            new_project_dirs=user_update.project_dirs,\n            user_id=user_id,\n            db=db,\n        )\n\n    will_be_superuser = (\n        user_update.is_superuser\n        if user_update.is_superuser is not None\n        else user_to_patch.is_superuser\n    )\n    if user_update.is_guest and will_be_superuser:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=\"Superuser cannot be guest.\",\n        )\n\n    # Modify user attributes\n    try:\n        user = await user_manager.update(\n            user_update,\n            user_to_patch,\n            safe=False,\n            request=None,\n        )\n        validated_user = UserOAuth.model_validate(user.model_dump())\n        patched_user = await db.get(\n            UserOAuth, validated_user.id, populate_existing=True\n        )\n    except exceptions.InvalidPasswordException as e:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail={\n                \"code\": ErrorCode.UPDATE_USER_INVALID_PASSWORD,\n                \"reason\": e.reason,\n            },\n        )\n    except exceptions.UserAlreadyExists:\n        raise HTTPException(\n            status.HTTP_400_BAD_REQUEST,\n            detail=ErrorCode.UPDATE_USER_EMAIL_ALREADY_EXISTS,\n        )\n\n    # Enrich user object with `group_ids_names` attribute\n    patched_user_with_groups = await _get_single_user_with_groups(\n        patched_user, db\n    )\n\n    return patched_user_with_groups\n</code></pre>"},{"location":"reference/app/routes/auth/viewer_paths/","title":"viewer_paths","text":""},{"location":"reference/app/routes/auth/viewer_paths/#fractal_server.app.routes.auth.viewer_paths.get_current_user_allowed_viewer_paths","title":"<code>get_current_user_allowed_viewer_paths(include_shared_projects=True, current_user=Depends(current_user_act_ver), db=Depends(get_async_db))</code>  <code>async</code>","text":"<p>Returns the allowed viewer paths for current user.</p> <p>NOTE: <code>include_shared_projects</code> is an obsolete query-parameter name, because it does not make a difference between owners/guests. A better naming would be e.g. <code>include_zarr_dirs</code>, but it would require a fix in <code>fractal-web</code> as well.</p> Source code in <code>fractal_server/app/routes/auth/viewer_paths.py</code> <pre><code>@router_viewer_paths.get(\n    \"/current-user/allowed-viewer-paths/\", response_model=list[str]\n)\nasync def get_current_user_allowed_viewer_paths(\n    include_shared_projects: bool = True,\n    current_user: UserOAuth = Depends(current_user_act_ver),\n    db: AsyncSession = Depends(get_async_db),\n) -&gt; list[str]:\n    \"\"\"\n    Returns the allowed viewer paths for current user.\n\n    NOTE: `include_shared_projects` is an obsolete query-parameter name,\n    because it does not make a difference between owners/guests. A better\n    naming would be e.g. `include_zarr_dirs`, but it would require a fix\n    in `fractal-web` as well.\n    \"\"\"\n    if include_shared_projects:\n        res = await db.execute(\n            select(DatasetV2.zarr_dir)\n            .join(ProjectV2, ProjectV2.id == DatasetV2.project_id)\n            .join(\n                LinkUserProjectV2, LinkUserProjectV2.project_id == ProjectV2.id\n            )\n            .where(LinkUserProjectV2.user_id == current_user.id)\n            .where(LinkUserProjectV2.is_verified.is_(True))\n        )\n        authorized_zarr_dirs = list(res.unique().scalars().all())\n        # Note that `project_dirs` and the `authorized_zarr_dirs` may have some\n        # common elements, and then the response may include non-unique items.\n        return current_user.project_dirs + authorized_zarr_dirs\n    else:\n        return current_user.project_dirs\n</code></pre>"},{"location":"reference/app/routes/aux/","title":"aux","text":""},{"location":"reference/app/routes/aux/_job/","title":"_job","text":""},{"location":"reference/app/routes/aux/_job/#fractal_server.app.routes.aux._job._write_shutdown_file","title":"<code>_write_shutdown_file(*, job)</code>","text":"<p>Write job's shutdown file.</p> PARAMETER DESCRIPTION <code>job</code> <p> TYPE: <code>JobV2</code> </p> <p>Note: we are not marking the job as failed (by setting its <code>status</code> attribute) here, since this will be done by the runner backend as soon as it detects the shutdown-trigerring file and performs the actual shutdown.</p> Source code in <code>fractal_server/app/routes/aux/_job.py</code> <pre><code>def _write_shutdown_file(*, job: JobV2):\n    \"\"\"\n    Write job's shutdown file.\n\n    Args:\n        job:\n\n    Note: we are **not** marking the job as failed (by setting its `status`\n    attribute) here, since this will be done by the runner backend as soon as\n    it detects the shutdown-trigerring file and performs the actual shutdown.\n    \"\"\"\n    shutdown_file = Path(job.working_dir) / SHUTDOWN_FILENAME\n    with shutdown_file.open(\"w\") as f:\n        f.write(f\"Trigger executor shutdown for {job.id=}.\")\n</code></pre>"},{"location":"reference/app/routes/aux/_runner/","title":"_runner","text":""},{"location":"reference/app/routes/aux/_runner/#fractal_server.app.routes.aux._runner._check_shutdown_is_supported","title":"<code>_check_shutdown_is_supported()</code>","text":"RAISES DESCRIPTION <code>HTTPException(status_code=HTTP_422_UNPROCESSABLE_CONTENT)</code> <p>If FRACTAL_RUNNER_BACKEND is the thread-based 'local' backend.</p> Source code in <code>fractal_server/app/routes/aux/_runner.py</code> <pre><code>def _check_shutdown_is_supported():\n    \"\"\"\n    Raises:\n        HTTPException(status_code=HTTP_422_UNPROCESSABLE_CONTENT):\n            If FRACTAL_RUNNER_BACKEND is the thread-based 'local' backend.\n    \"\"\"\n    settings = Inject(get_settings)\n    backend = settings.FRACTAL_RUNNER_BACKEND\n\n    if not _backend_supports_shutdown(backend):\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=(\n                \"Stopping a job execution is not implemented for \"\n                f\"FRACTAL_RUNNER_BACKEND={backend}.\"\n            ),\n        )\n</code></pre>"},{"location":"reference/app/routes/aux/_versions/","title":"_versions","text":""},{"location":"reference/app/routes/aux/_versions/#fractal_server.app.routes.aux._versions._find_latest_version_or_422","title":"<code>_find_latest_version_or_422(versions)</code>","text":"<p>For PEP 440 versions, this is easy enough for the client to do (using the <code>packaging</code> library [...]. For non-standard versions, there is no well-defined ordering, and clients will need to decide on what rule is appropriate for their needs. (https://peps.python.org/pep-0700/#why-not-provide-a-latest-version-value)</p> <p>The <code>versions</code> array is coming from the PyPI API, and its elements are assumed parsable.</p> Source code in <code>fractal_server/app/routes/aux/_versions.py</code> <pre><code>def _find_latest_version_or_422(versions: list[str]) -&gt; str:\n    \"\"\"\n    &gt; For PEP 440 versions, this is easy enough for the client to do (using\n    &gt; the `packaging` library [...]. For non-standard versions, there is no\n    &gt; well-defined ordering, and clients will need to decide on what rule is\n    &gt; appropriate for their needs.\n    (https://peps.python.org/pep-0700/#why-not-provide-a-latest-version-value)\n\n    The `versions` array is coming from the PyPI API, and its elements are\n    assumed parsable.\n    \"\"\"\n    try:\n        latest = max(versions, key=lambda v_str: Version(v_str))\n        return latest\n    except InvalidVersion as e:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=f\"Cannot find latest version (original error: {str(e)}).\",\n        )\n</code></pre>"},{"location":"reference/app/routes/aux/_versions/#fractal_server.app.routes.aux._versions._version_sort_key","title":"<code>_version_sort_key(version)</code>","text":"<p>Returns a tuple used as (reverse) ordering key for TaskGroups in <code>get_task_group_list</code>. The parsable versions are the first in order, sorted according to the sorting rules of packaging.version.Version. Next in order we have the non-null non-parsable versions, sorted alphabetically.</p> Source code in <code>fractal_server/app/routes/aux/_versions.py</code> <pre><code>def _version_sort_key(version: str | None) -&gt; tuple[int, Version | str | None]:\n    \"\"\"\n    Returns a tuple used as (reverse) ordering key for TaskGroups in\n    `get_task_group_list`.\n    The parsable versions are the first in order, sorted according to the\n    sorting rules of packaging.version.Version.\n    Next in order we have the non-null non-parsable versions, sorted\n    alphabetically.\n    \"\"\"\n    if version is None:\n        return (0, None)\n    try:\n        return (2, Version(version))\n    except InvalidVersion:\n        return (1, version)\n</code></pre>"},{"location":"reference/app/routes/aux/validate_user_profile/","title":"validate_user_profile","text":""},{"location":"reference/app/routes/aux/validate_user_profile/#fractal_server.app.routes.aux.validate_user_profile.validate_user_profile","title":"<code>validate_user_profile(*, user, db)</code>  <code>async</code>","text":"<p>Validate profile and resource associated to a given user.</p> <p>Note: this only returns non-db-bound objects.</p> Source code in <code>fractal_server/app/routes/aux/validate_user_profile.py</code> <pre><code>async def validate_user_profile(\n    *,\n    user: UserOAuth,\n    db: AsyncSession,\n) -&gt; tuple[Resource, Profile]:\n    \"\"\"\n    Validate profile and resource associated to a given user.\n\n    Note: this only returns non-db-bound objects.\n    \"\"\"\n    await user_has_profile_or_422(user=user)\n    profile = await db.get(Profile, user.profile_id)\n    resource = await db.get(Resource, profile.resource_id)\n    try:\n        cast_serialize_resource(\n            resource.model_dump(exclude={\"id\", \"timestamp_created\"}),\n        )\n        cast_serialize_profile(\n            profile.model_dump(exclude={\"resource_id\", \"id\"}),\n        )\n        db.expunge(resource)\n        db.expunge(profile)\n\n        return resource, profile\n\n    except ValidationError as e:\n        error_msg = (\n            \"User resource/profile are not valid for \"\n            f\"resource type '{resource.type}'. \"\n            f\"Original error: {str(e)}\"\n        )\n        logger.warning(error_msg)\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_CONTENT,\n            detail=error_msg,\n        )\n</code></pre>"},{"location":"reference/app/schemas/","title":"schemas","text":""},{"location":"reference/app/schemas/#fractal_server.app.schemas.ListUniqueAbsolutePathStr","title":"<code>ListUniqueAbsolutePathStr = Annotated[list[AbsolutePathStr], AfterValidator(val_unique_list)]</code>  <code>module-attribute</code>","text":"<p>List of unique absolute-path-string items.</p>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.ListUniqueNonEmptyString","title":"<code>ListUniqueNonEmptyString = Annotated[list[NonEmptyStr], AfterValidator(val_unique_list)]</code>  <code>module-attribute</code>","text":"<p>List of unique non-empty-string items.</p>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.ListUniqueNonNegativeInt","title":"<code>ListUniqueNonNegativeInt = Annotated[list[NonNegativeInt], AfterValidator(val_unique_list)]</code>  <code>module-attribute</code>","text":"<p>List of unique non-negative-integer items.</p>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.NonEmptyStr","title":"<code>NonEmptyStr = Annotated[str, StringConstraints(min_length=1, strip_whitespace=True)]</code>  <code>module-attribute</code>","text":"<p>A non-empty string, with no leading/trailing whitespaces.</p>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.OAuthAccountRead","title":"<code>OAuthAccountRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for storing essential <code>OAuthAccount</code> information within <code>UserRead.oauth_accounts</code>.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>ID of the row in fractal-owned <code>oauthaccount</code> table.</p> <p> TYPE: <code>int</code> </p> <code>account_email</code> <p>Email associated to OAuth account</p> <p> TYPE: <code>str</code> </p> <code>oauth_name</code> <p>Name of the OAuth provider (e.g. <code>github</code>)</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class OAuthAccountRead(BaseModel):\n    \"\"\"\n    Schema for storing essential `OAuthAccount` information within\n    `UserRead.oauth_accounts`.\n\n    Attributes:\n        id: ID of the row in fractal-owned `oauthaccount` table.\n        account_email: Email associated to OAuth account\n        oauth_name: Name of the OAuth provider (e.g. `github`)\n    \"\"\"\n\n    id: int\n    account_email: str\n    oauth_name: str\n</code></pre>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.UserCreate","title":"<code>UserCreate</code>","text":"<p>               Bases: <code>BaseUserCreate</code></p> <p>Schema for <code>User</code> creation.</p> ATTRIBUTE DESCRIPTION <code>is_guest</code> <p> TYPE: <code>bool</code> </p> <code>profile_id</code> <p> TYPE: <code>int | None</code> </p> <code>project_dirs</code> <p> TYPE: <code>Annotated[ListUniqueAbsolutePathStr, AfterValidator(_validate_cmd_list)]</code> </p> <code>slurm_accounts</code> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserCreate(schemas.BaseUserCreate):\n    \"\"\"\n    Schema for `User` creation.\n\n    Attributes:\n        is_guest:\n        profile_id:\n        project_dirs:\n        slurm_accounts:\n    \"\"\"\n\n    is_guest: bool = False\n    profile_id: int | None = None\n    project_dirs: Annotated[\n        ListUniqueAbsolutePathStr, AfterValidator(_validate_cmd_list)\n    ] = Field(min_length=1)\n    slurm_accounts: list[str] = Field(default_factory=list)\n</code></pre>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.UserGroupCreate","title":"<code>UserGroupCreate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>UserGroup</code> creation</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Group name</p> <p> TYPE: <code>NonEmptyStr</code> </p> Source code in <code>fractal_server/app/schemas/user_group.py</code> <pre><code>class UserGroupCreate(BaseModel):\n    \"\"\"\n    Schema for `UserGroup` creation\n\n    Attributes:\n        name: Group name\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    name: NonEmptyStr\n</code></pre>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.UserGroupRead","title":"<code>UserGroupRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>UserGroup</code> read</p> <p>NOTE: <code>user_ids</code> does not correspond to a column of the <code>UserGroup</code> table, but it is rather computed dynamically in relevant endpoints.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Group ID</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>Group name</p> <p> TYPE: <code>str</code> </p> <code>timestamp_created</code> <p>Creation timestamp</p> <p> TYPE: <code>AwareDatetime</code> </p> <code>user_ids</code> <p>IDs of users of this group</p> <p> TYPE: <code>list[int] | None</code> </p> Source code in <code>fractal_server/app/schemas/user_group.py</code> <pre><code>class UserGroupRead(BaseModel):\n    \"\"\"\n    Schema for `UserGroup` read\n\n    NOTE: `user_ids` does not correspond to a column of the `UserGroup` table,\n    but it is rather computed dynamically in relevant endpoints.\n\n    Attributes:\n        id: Group ID\n        name: Group name\n        timestamp_created: Creation timestamp\n        user_ids: IDs of users of this group\n    \"\"\"\n\n    id: int\n    name: str\n    timestamp_created: AwareDatetime\n    user_ids: list[int] | None = None\n\n    @field_serializer(\"timestamp_created\")\n    def serialize_datetime(v: datetime) -&gt; str:\n        return v.isoformat()\n</code></pre>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.UserProfileInfo","title":"<code>UserProfileInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> ATTRIBUTE DESCRIPTION <code>has_profile</code> <p> TYPE: <code>bool</code> </p> <code>resource_name</code> <p> TYPE: <code>str | None</code> </p> <code>profile_name</code> <p> TYPE: <code>str | None</code> </p> <code>username</code> <p> TYPE: <code>str | None</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserProfileInfo(BaseModel):\n    \"\"\"\n    Attributes:\n        has_profile:\n        resource_name:\n        profile_name:\n        username:\n    \"\"\"\n\n    has_profile: bool\n    resource_name: str | None = None\n    profile_name: str | None = None\n    username: str | None = None\n</code></pre>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.UserRead","title":"<code>UserRead</code>","text":"<p>               Bases: <code>BaseUser[int]</code></p> <p>Schema for <code>User</code> read from database.</p> ATTRIBUTE DESCRIPTION <code>is_guest</code> <p> TYPE: <code>bool</code> </p> <code>group_ids_names</code> <p> TYPE: <code>list[tuple[int, str]] | None</code> </p> <code>oauth_accounts</code> <p> TYPE: <code>list[OAuthAccountRead]</code> </p> <code>profile_id</code> <p> TYPE: <code>int | None</code> </p> <code>project_dirs</code> <p> TYPE: <code>list[str]</code> </p> <code>slurm_accounts</code> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserRead(schemas.BaseUser[int]):\n    \"\"\"\n    Schema for `User` read from database.\n\n    Attributes:\n        is_guest:\n        group_ids_names:\n        oauth_accounts:\n        profile_id:\n        project_dirs:\n        slurm_accounts:\n\n    \"\"\"\n\n    is_guest: bool\n    group_ids_names: list[tuple[int, str]] | None = None\n    oauth_accounts: list[OAuthAccountRead]\n    profile_id: int | None = None\n    project_dirs: list[str]\n    slurm_accounts: list[str]\n</code></pre>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.UserUpdate","title":"<code>UserUpdate</code>","text":"<p>               Bases: <code>BaseUserUpdate</code></p> <p>Schema for <code>User</code> update.</p> ATTRIBUTE DESCRIPTION <code>password</code> <p> TYPE: <code>NonEmptyStr</code> </p> <code>email</code> <p> TYPE: <code>EmailStr</code> </p> <code>is_active</code> <p> TYPE: <code>bool</code> </p> <code>is_superuser</code> <p> TYPE: <code>bool</code> </p> <code>is_verified</code> <p> TYPE: <code>bool</code> </p> <code>is_guest</code> <p> TYPE: <code>bool</code> </p> <code>profile_id</code> <p> TYPE: <code>int | None</code> </p> <code>project_dirs</code> <p> TYPE: <code>Annotated[ListUniqueAbsolutePathStr, AfterValidator(_validate_cmd_list)]</code> </p> <code>slurm_accounts</code> <p> TYPE: <code>ListUniqueNonEmptyString</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdate(schemas.BaseUserUpdate):\n    \"\"\"\n    Schema for `User` update.\n\n    Attributes:\n        password:\n        email:\n        is_active:\n        is_superuser:\n        is_verified:\n        is_guest:\n        profile_id:\n        project_dirs:\n        slurm_accounts:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    password: NonEmptyStr = None\n    email: EmailStr = None\n    is_active: bool = None\n    is_superuser: bool = None\n    is_verified: bool = None\n    is_guest: bool = None\n    profile_id: int | None = None\n    project_dirs: Annotated[\n        ListUniqueAbsolutePathStr, AfterValidator(_validate_cmd_list)\n    ] = Field(default=None, min_length=1)\n    slurm_accounts: ListUniqueNonEmptyString = None\n</code></pre>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.UserUpdateGroups","title":"<code>UserUpdateGroups</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>POST /auth/users/{user_id}/set-groups/</code></p> ATTRIBUTE DESCRIPTION <code>group_ids</code> <p> TYPE: <code>ListUniqueNonNegativeInt</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdateGroups(BaseModel):\n    \"\"\"\n    Schema for `POST /auth/users/{user_id}/set-groups/`\n\n    Attributes:\n        group_ids:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    group_ids: ListUniqueNonNegativeInt = Field(min_length=1)\n</code></pre>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.UserUpdateStrict","title":"<code>UserUpdateStrict</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>User</code> self-editing.</p> ATTRIBUTE DESCRIPTION <code>slurm_accounts</code> <p> TYPE: <code>ListUniqueNonEmptyString</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdateStrict(BaseModel):\n    \"\"\"\n    Schema for `User` self-editing.\n\n    Attributes:\n        slurm_accounts:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    slurm_accounts: ListUniqueNonEmptyString = None\n</code></pre>"},{"location":"reference/app/schemas/#fractal_server.app.schemas.validate_cmd","title":"<code>validate_cmd(command, *, allow_char=None, attribute_name='Command')</code>","text":"<p>Assert that the provided <code>command</code> does not contain any of the forbidden characters for commands (fractal_server.string_tools.NOT_ALLOWED_FOR_COMMANDS)</p> PARAMETER DESCRIPTION <code>command</code> <p>command to validate.</p> <p> TYPE: <code>str</code> </p> <code>allow_char</code> <p>chars to accept among the forbidden ones</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>attribute_name</code> <p>Name of the attribute, to be used in error message.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'Command'</code> </p> Source code in <code>fractal_server/string_tools.py</code> <pre><code>def validate_cmd(\n    command: str,\n    *,\n    allow_char: str | None = None,\n    attribute_name: str = \"Command\",\n):\n    \"\"\"\n    Assert that the provided `command` does not contain any of the forbidden\n    characters for commands\n    (fractal_server.string_tools.__NOT_ALLOWED_FOR_COMMANDS__)\n\n    Args:\n        command: command to validate.\n        allow_char: chars to accept among the forbidden ones\n        attribute_name: Name of the attribute, to be used in error message.\n    \"\"\"\n    if not isinstance(command, str):\n        raise ValueError(f\"{command=} is not a string.\")\n    forbidden = set(__NOT_ALLOWED_FOR_COMMANDS__)\n    if allow_char is not None:\n        forbidden = forbidden - set(allow_char)\n    if not forbidden.isdisjoint(set(command)):\n        raise ValueError(\n            f\"{attribute_name} must not contain any of this characters: \"\n            f\"'{forbidden}'\\n\"\n            f\"Provided {attribute_name.lower()}: '{command}'.\"\n        )\n</code></pre>"},{"location":"reference/app/schemas/user/","title":"user","text":""},{"location":"reference/app/schemas/user/#fractal_server.app.schemas.user.OAuthAccountRead","title":"<code>OAuthAccountRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for storing essential <code>OAuthAccount</code> information within <code>UserRead.oauth_accounts</code>.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>ID of the row in fractal-owned <code>oauthaccount</code> table.</p> <p> TYPE: <code>int</code> </p> <code>account_email</code> <p>Email associated to OAuth account</p> <p> TYPE: <code>str</code> </p> <code>oauth_name</code> <p>Name of the OAuth provider (e.g. <code>github</code>)</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class OAuthAccountRead(BaseModel):\n    \"\"\"\n    Schema for storing essential `OAuthAccount` information within\n    `UserRead.oauth_accounts`.\n\n    Attributes:\n        id: ID of the row in fractal-owned `oauthaccount` table.\n        account_email: Email associated to OAuth account\n        oauth_name: Name of the OAuth provider (e.g. `github`)\n    \"\"\"\n\n    id: int\n    account_email: str\n    oauth_name: str\n</code></pre>"},{"location":"reference/app/schemas/user/#fractal_server.app.schemas.user.UserCreate","title":"<code>UserCreate</code>","text":"<p>               Bases: <code>BaseUserCreate</code></p> <p>Schema for <code>User</code> creation.</p> ATTRIBUTE DESCRIPTION <code>is_guest</code> <p> TYPE: <code>bool</code> </p> <code>profile_id</code> <p> TYPE: <code>int | None</code> </p> <code>project_dirs</code> <p> TYPE: <code>Annotated[ListUniqueAbsolutePathStr, AfterValidator(_validate_cmd_list)]</code> </p> <code>slurm_accounts</code> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserCreate(schemas.BaseUserCreate):\n    \"\"\"\n    Schema for `User` creation.\n\n    Attributes:\n        is_guest:\n        profile_id:\n        project_dirs:\n        slurm_accounts:\n    \"\"\"\n\n    is_guest: bool = False\n    profile_id: int | None = None\n    project_dirs: Annotated[\n        ListUniqueAbsolutePathStr, AfterValidator(_validate_cmd_list)\n    ] = Field(min_length=1)\n    slurm_accounts: list[str] = Field(default_factory=list)\n</code></pre>"},{"location":"reference/app/schemas/user/#fractal_server.app.schemas.user.UserProfileInfo","title":"<code>UserProfileInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> ATTRIBUTE DESCRIPTION <code>has_profile</code> <p> TYPE: <code>bool</code> </p> <code>resource_name</code> <p> TYPE: <code>str | None</code> </p> <code>profile_name</code> <p> TYPE: <code>str | None</code> </p> <code>username</code> <p> TYPE: <code>str | None</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserProfileInfo(BaseModel):\n    \"\"\"\n    Attributes:\n        has_profile:\n        resource_name:\n        profile_name:\n        username:\n    \"\"\"\n\n    has_profile: bool\n    resource_name: str | None = None\n    profile_name: str | None = None\n    username: str | None = None\n</code></pre>"},{"location":"reference/app/schemas/user/#fractal_server.app.schemas.user.UserRead","title":"<code>UserRead</code>","text":"<p>               Bases: <code>BaseUser[int]</code></p> <p>Schema for <code>User</code> read from database.</p> ATTRIBUTE DESCRIPTION <code>is_guest</code> <p> TYPE: <code>bool</code> </p> <code>group_ids_names</code> <p> TYPE: <code>list[tuple[int, str]] | None</code> </p> <code>oauth_accounts</code> <p> TYPE: <code>list[OAuthAccountRead]</code> </p> <code>profile_id</code> <p> TYPE: <code>int | None</code> </p> <code>project_dirs</code> <p> TYPE: <code>list[str]</code> </p> <code>slurm_accounts</code> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserRead(schemas.BaseUser[int]):\n    \"\"\"\n    Schema for `User` read from database.\n\n    Attributes:\n        is_guest:\n        group_ids_names:\n        oauth_accounts:\n        profile_id:\n        project_dirs:\n        slurm_accounts:\n\n    \"\"\"\n\n    is_guest: bool\n    group_ids_names: list[tuple[int, str]] | None = None\n    oauth_accounts: list[OAuthAccountRead]\n    profile_id: int | None = None\n    project_dirs: list[str]\n    slurm_accounts: list[str]\n</code></pre>"},{"location":"reference/app/schemas/user/#fractal_server.app.schemas.user.UserUpdate","title":"<code>UserUpdate</code>","text":"<p>               Bases: <code>BaseUserUpdate</code></p> <p>Schema for <code>User</code> update.</p> ATTRIBUTE DESCRIPTION <code>password</code> <p> TYPE: <code>NonEmptyStr</code> </p> <code>email</code> <p> TYPE: <code>EmailStr</code> </p> <code>is_active</code> <p> TYPE: <code>bool</code> </p> <code>is_superuser</code> <p> TYPE: <code>bool</code> </p> <code>is_verified</code> <p> TYPE: <code>bool</code> </p> <code>is_guest</code> <p> TYPE: <code>bool</code> </p> <code>profile_id</code> <p> TYPE: <code>int | None</code> </p> <code>project_dirs</code> <p> TYPE: <code>Annotated[ListUniqueAbsolutePathStr, AfterValidator(_validate_cmd_list)]</code> </p> <code>slurm_accounts</code> <p> TYPE: <code>ListUniqueNonEmptyString</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdate(schemas.BaseUserUpdate):\n    \"\"\"\n    Schema for `User` update.\n\n    Attributes:\n        password:\n        email:\n        is_active:\n        is_superuser:\n        is_verified:\n        is_guest:\n        profile_id:\n        project_dirs:\n        slurm_accounts:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    password: NonEmptyStr = None\n    email: EmailStr = None\n    is_active: bool = None\n    is_superuser: bool = None\n    is_verified: bool = None\n    is_guest: bool = None\n    profile_id: int | None = None\n    project_dirs: Annotated[\n        ListUniqueAbsolutePathStr, AfterValidator(_validate_cmd_list)\n    ] = Field(default=None, min_length=1)\n    slurm_accounts: ListUniqueNonEmptyString = None\n</code></pre>"},{"location":"reference/app/schemas/user/#fractal_server.app.schemas.user.UserUpdateGroups","title":"<code>UserUpdateGroups</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>POST /auth/users/{user_id}/set-groups/</code></p> ATTRIBUTE DESCRIPTION <code>group_ids</code> <p> TYPE: <code>ListUniqueNonNegativeInt</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdateGroups(BaseModel):\n    \"\"\"\n    Schema for `POST /auth/users/{user_id}/set-groups/`\n\n    Attributes:\n        group_ids:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    group_ids: ListUniqueNonNegativeInt = Field(min_length=1)\n</code></pre>"},{"location":"reference/app/schemas/user/#fractal_server.app.schemas.user.UserUpdateStrict","title":"<code>UserUpdateStrict</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>User</code> self-editing.</p> ATTRIBUTE DESCRIPTION <code>slurm_accounts</code> <p> TYPE: <code>ListUniqueNonEmptyString</code> </p> Source code in <code>fractal_server/app/schemas/user.py</code> <pre><code>class UserUpdateStrict(BaseModel):\n    \"\"\"\n    Schema for `User` self-editing.\n\n    Attributes:\n        slurm_accounts:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    slurm_accounts: ListUniqueNonEmptyString = None\n</code></pre>"},{"location":"reference/app/schemas/user_group/","title":"user_group","text":""},{"location":"reference/app/schemas/user_group/#fractal_server.app.schemas.user_group.UserGroupCreate","title":"<code>UserGroupCreate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>UserGroup</code> creation</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Group name</p> <p> TYPE: <code>NonEmptyStr</code> </p> Source code in <code>fractal_server/app/schemas/user_group.py</code> <pre><code>class UserGroupCreate(BaseModel):\n    \"\"\"\n    Schema for `UserGroup` creation\n\n    Attributes:\n        name: Group name\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    name: NonEmptyStr\n</code></pre>"},{"location":"reference/app/schemas/user_group/#fractal_server.app.schemas.user_group.UserGroupRead","title":"<code>UserGroupRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>UserGroup</code> read</p> <p>NOTE: <code>user_ids</code> does not correspond to a column of the <code>UserGroup</code> table, but it is rather computed dynamically in relevant endpoints.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Group ID</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>Group name</p> <p> TYPE: <code>str</code> </p> <code>timestamp_created</code> <p>Creation timestamp</p> <p> TYPE: <code>AwareDatetime</code> </p> <code>user_ids</code> <p>IDs of users of this group</p> <p> TYPE: <code>list[int] | None</code> </p> Source code in <code>fractal_server/app/schemas/user_group.py</code> <pre><code>class UserGroupRead(BaseModel):\n    \"\"\"\n    Schema for `UserGroup` read\n\n    NOTE: `user_ids` does not correspond to a column of the `UserGroup` table,\n    but it is rather computed dynamically in relevant endpoints.\n\n    Attributes:\n        id: Group ID\n        name: Group name\n        timestamp_created: Creation timestamp\n        user_ids: IDs of users of this group\n    \"\"\"\n\n    id: int\n    name: str\n    timestamp_created: AwareDatetime\n    user_ids: list[int] | None = None\n\n    @field_serializer(\"timestamp_created\")\n    def serialize_datetime(v: datetime) -&gt; str:\n        return v.isoformat()\n</code></pre>"},{"location":"reference/app/schemas/v2/","title":"v2","text":""},{"location":"reference/app/schemas/v2/accounting/","title":"accounting","text":""},{"location":"reference/app/schemas/v2/accounting/#fractal_server.app.schemas.v2.accounting.AccountingRecordRead","title":"<code>AccountingRecordRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>AccountingRecordRead</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> <code>timestamp</code> <p> TYPE: <code>AwareDatetime</code> </p> <code>num_tasks</code> <p> TYPE: <code>int</code> </p> <code>num_new_images</code> <p> TYPE: <code>int</code> </p> Source code in <code>fractal_server/app/schemas/v2/accounting.py</code> <pre><code>class AccountingRecordRead(BaseModel):\n    \"\"\"\n    AccountingRecordRead\n\n    Attributes:\n        id:\n        user_id:\n        timestamp:\n        num_tasks:\n        num_new_images:\n    \"\"\"\n\n    id: int\n    user_id: int\n    timestamp: AwareDatetime\n    num_tasks: int\n    num_new_images: int\n\n    @field_serializer(\"timestamp\")\n    def serialize_datetime(v: datetime) -&gt; str:\n        return v.isoformat()\n</code></pre>"},{"location":"reference/app/schemas/v2/dataset/","title":"dataset","text":""},{"location":"reference/app/schemas/v2/dataset/#fractal_server.app.schemas.v2.dataset.DatasetCreate","title":"<code>DatasetCreate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DatasetCreate</p> ATTRIBUTE DESCRIPTION <code>name</code> <p> TYPE: <code>SafeNonEmptyStr</code> </p> <code>project_dir</code> <p> TYPE: <code>AbsolutePathStr | None</code> </p> <code>zarr_subfolder</code> <p> TYPE: <code>SafeRelativePathStr | None</code> </p> Source code in <code>fractal_server/app/schemas/v2/dataset.py</code> <pre><code>class DatasetCreate(BaseModel):\n    \"\"\"\n    DatasetCreate\n\n    Attributes:\n        name:\n        project_dir:\n        zarr_subfolder:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    name: SafeNonEmptyStr\n    project_dir: AbsolutePathStr | None = None\n    zarr_subfolder: SafeRelativePathStr | None = None\n\n    @model_validator(mode=\"after\")\n    def validate_zarr_dir(self):\n        if (self.project_dir is None) and (self.zarr_subfolder is not None):\n            raise ValueError(\n                \"Cannot provide `zarr_subfolder` without `project_dir`\"\n            )\n        return self\n</code></pre>"},{"location":"reference/app/schemas/v2/dataset/#fractal_server.app.schemas.v2.dataset.DatasetExport","title":"<code>DatasetExport</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for <code>Dataset</code> export.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p> TYPE: <code>str</code> </p> <code>zarr_dir</code> <p> TYPE: <code>str</code> </p> <code>images</code> <p> TYPE: <code>list[SingleImage]</code> </p> Source code in <code>fractal_server/app/schemas/v2/dataset.py</code> <pre><code>class DatasetExport(BaseModel):\n    \"\"\"\n    Class for `Dataset` export.\n\n    Attributes:\n        name:\n        zarr_dir:\n        images:\n    \"\"\"\n\n    name: str\n    zarr_dir: str\n    images: list[SingleImage]\n</code></pre>"},{"location":"reference/app/schemas/v2/dataset/#fractal_server.app.schemas.v2.dataset.DatasetImport","title":"<code>DatasetImport</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for <code>Dataset</code> import.</p> <p>We are dropping <code>model_config = ConfigDict(extra=\"forbid\")</code> so that any kind of legacy filters can be included in the payload, and ignored in the API.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p> TYPE: <code>SafeNonEmptyStr</code> </p> <code>zarr_dir</code> <p> TYPE: <code>ZarrDirStr</code> </p> <code>images</code> <p> TYPE: <code>list[SingleImage]</code> </p> Source code in <code>fractal_server/app/schemas/v2/dataset.py</code> <pre><code>class DatasetImport(BaseModel):\n    \"\"\"\n    Class for `Dataset` import.\n\n    We are dropping `model_config = ConfigDict(extra=\"forbid\")` so that any\n    kind of legacy filters can be included in the payload, and ignored in the\n    API.\n\n    Attributes:\n        name:\n        zarr_dir:\n        images:\n    \"\"\"\n\n    name: SafeNonEmptyStr\n    zarr_dir: ZarrDirStr\n    images: list[SingleImage] = Field(default_factory=list)\n\n    @model_validator(mode=\"after\")\n    def validate_image_zarr_url(self):\n        for image in self.images:\n            if not Path(image.zarr_url).is_relative_to(self.zarr_dir):\n                raise ValueError(\n                    f\"{image.zarr_url=} is not relative to {self.zarr_dir=}.\"\n                )\n        return self\n</code></pre>"},{"location":"reference/app/schemas/v2/dataset/#fractal_server.app.schemas.v2.dataset.DatasetRead","title":"<code>DatasetRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DatasetRead</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int</code> </p> <code>name</code> <p> TYPE: <code>str</code> </p> <code>project_id</code> <p> TYPE: <code>int</code> </p> <code>project</code> <p> TYPE: <code>ProjectRead</code> </p> <code>timestamp_created</code> <p> TYPE: <code>AwareDatetime</code> </p> <code>zarr_dir</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/app/schemas/v2/dataset.py</code> <pre><code>class DatasetRead(BaseModel):\n    \"\"\"\n    DatasetRead\n\n    Attributes:\n        id:\n        name:\n        project_id:\n        project:\n        timestamp_created:\n        zarr_dir:\n    \"\"\"\n\n    id: int\n    name: str\n\n    project_id: int\n    project: ProjectRead\n\n    timestamp_created: AwareDatetime\n\n    zarr_dir: str\n\n    @field_serializer(\"timestamp_created\")\n    def serialize_datetime(v: datetime) -&gt; str:\n        return v.isoformat()\n</code></pre>"},{"location":"reference/app/schemas/v2/dataset/#fractal_server.app.schemas.v2.dataset.DatasetUpdate","title":"<code>DatasetUpdate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DatasetUpdate</p> ATTRIBUTE DESCRIPTION <code>name</code> <p> TYPE: <code>SafeNonEmptyStr</code> </p> <code>zarr_dir</code> <p> TYPE: <code>SafeNonEmptyStr</code> </p> Source code in <code>fractal_server/app/schemas/v2/dataset.py</code> <pre><code>class DatasetUpdate(BaseModel):\n    \"\"\"\n    DatasetUpdate\n\n    Attributes:\n        name:\n        zarr_dir:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    name: SafeNonEmptyStr = None\n</code></pre>"},{"location":"reference/app/schemas/v2/dumps/","title":"dumps","text":"<p>Dump models differ from their Read counterpart in that: * They are directly JSON-able, without any additional encoder. * They may include only a subset of the available fields.</p> <p>These models are used in at least two situations: 1. In the \"*_dump\" attributes of Job models; 2. In the history items, to trim their size.</p>"},{"location":"reference/app/schemas/v2/dumps/#fractal_server.app.schemas.v2.dumps.DatasetDump","title":"<code>DatasetDump</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>We do not include 'model_config = ConfigDict(extra=\"forbid\")' because legacy data may include 'type_filters' or 'attribute_filters' and we want to avoid response-validation errors.</p> Source code in <code>fractal_server/app/schemas/v2/dumps.py</code> <pre><code>class DatasetDump(BaseModel):\n    \"\"\"\n    We do not include 'model_config = ConfigDict(extra=\"forbid\")' because\n    legacy data may include 'type_filters' or 'attribute_filters' and we\n    want to avoid response-validation errors.\n    \"\"\"\n\n    id: int\n    name: str\n    project_id: int\n    timestamp_created: str\n    zarr_dir: str\n</code></pre>"},{"location":"reference/app/schemas/v2/dumps/#fractal_server.app.schemas.v2.dumps.WorkflowTaskDump","title":"<code>WorkflowTaskDump</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>We do not include 'model_config = ConfigDict(extra=\"forbid\")' because legacy data may include 'input_filters' field and we want to avoid response-validation errors for the endpoints that GET datasets.</p> Source code in <code>fractal_server/app/schemas/v2/dumps.py</code> <pre><code>class WorkflowTaskDump(BaseModel):\n    \"\"\"\n    We do not include 'model_config = ConfigDict(extra=\"forbid\")'\n    because legacy data may include 'input_filters' field and we want to avoid\n    response-validation errors for the endpoints that GET datasets.\n    \"\"\"\n\n    id: int\n    workflow_id: int\n    order: int | None = None\n\n    type_filters: dict[str, bool]\n\n    task_id: int | None = None\n    task: TaskDump | None = None\n</code></pre>"},{"location":"reference/app/schemas/v2/history/","title":"history","text":""},{"location":"reference/app/schemas/v2/history/#fractal_server.app.schemas.v2.history.HistoryUnitStatus","title":"<code>HistoryUnitStatus</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available status for images</p> ATTRIBUTE DESCRIPTION <code>SUBMITTED</code> <p> </p> <code>DONE</code> <p> </p> <code>FAILED</code> <p> </p> Source code in <code>fractal_server/app/schemas/v2/history.py</code> <pre><code>class HistoryUnitStatus(StrEnum):\n    \"\"\"\n    Available status for images\n\n    Attributes:\n        SUBMITTED:\n        DONE:\n        FAILED:\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"reference/app/schemas/v2/history/#fractal_server.app.schemas.v2.history.HistoryUnitStatusWithUnset","title":"<code>HistoryUnitStatusWithUnset</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available status for history queries</p> ATTRIBUTE DESCRIPTION <code>SUBMITTED</code> <p> </p> <code>DONE</code> <p> </p> <code>FAILED</code> <p> </p> Source code in <code>fractal_server/app/schemas/v2/history.py</code> <pre><code>class HistoryUnitStatusWithUnset(StrEnum):\n    \"\"\"\n    Available status for history queries\n\n    Attributes:\n        SUBMITTED:\n        DONE:\n        FAILED:\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n    UNSET = \"unset\"\n</code></pre>"},{"location":"reference/app/schemas/v2/job/","title":"job","text":""},{"location":"reference/app/schemas/v2/job/#fractal_server.app.schemas.v2.job.JobStatusType","title":"<code>JobStatusType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the available job statuses</p> ATTRIBUTE DESCRIPTION <code>SUBMITTED</code> <p>The job was created. This does not guarantee that it was also submitted to an executor (e.g. other errors could have prevented this), nor that it is actually running (e.g. SLURM jobs could be still in the queue).</p> <p> </p> <code>DONE</code> <p>The job successfully reached its end.</p> <p> </p> <code>FAILED</code> <p>The workflow terminated with an error.</p> <p> </p> Source code in <code>fractal_server/app/schemas/v2/job.py</code> <pre><code>class JobStatusType(StrEnum):\n    \"\"\"\n    Define the available job statuses\n\n    Attributes:\n        SUBMITTED:\n            The job was created. This does not guarantee that it was also\n            submitted to an executor (e.g. other errors could have prevented\n            this), nor that it is actually running (e.g. SLURM jobs could be\n            still in the queue).\n        DONE:\n            The job successfully reached its end.\n        FAILED:\n            The workflow terminated with an error.\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"reference/app/schemas/v2/manifest/","title":"manifest","text":""},{"location":"reference/app/schemas/v2/manifest/#fractal_server.app.schemas.v2.manifest.ManifestV2","title":"<code>ManifestV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Packages containing tasks are required to include a special file <code>__FRACTAL_MANIFEST__.json</code> in order to be discovered and used by Fractal.</p> <p>This model class and the model classes it depends on provide the base schema to read, write and validate manifests.</p> ATTRIBUTE DESCRIPTION <code>manifest_version</code> <p>A version string that provides indication for compatibility between manifests as the schema evolves. This is for instance used by Fractal to determine which subclass of the present base class needs be used to read and validate the input.</p> <p> TYPE: <code>Literal['2']</code> </p> <code>task_list</code> <p>The list of tasks, represented as specified by subclasses of the <code>_TaskManifestBase</code> (a.k.a. <code>TaskManifestType</code>)</p> <p> TYPE: <code>list[TaskManifestV2]</code> </p> <code>has_args_schemas</code> <p><code>True</code> if the manifest includes JSON Schemas for the arguments of each task.</p> <p> TYPE: <code>bool</code> </p> <code>args_schema_version</code> <p>Label of how <code>args_schema</code>s were generated (e.g. <code>pydantic_v1</code>).</p> <p> TYPE: <code>str | None</code> </p> Source code in <code>fractal_server/app/schemas/v2/manifest.py</code> <pre><code>class ManifestV2(BaseModel):\n    \"\"\"\n    Packages containing tasks are required to include a special file\n    `__FRACTAL_MANIFEST__.json` in order to be discovered and used by Fractal.\n\n    This model class and the model classes it depends on provide the base\n    schema to read, write and validate manifests.\n\n    Attributes:\n        manifest_version:\n            A version string that provides indication for compatibility between\n            manifests as the schema evolves. This is for instance used by\n            Fractal to determine which subclass of the present base class needs\n            be used to read and validate the input.\n        task_list:\n            The list of tasks, represented as specified by subclasses of the\n            `_TaskManifestBase` (a.k.a. `TaskManifestType`)\n        has_args_schemas:\n            `True` if the manifest includes JSON Schemas for the arguments of\n            each task.\n        args_schema_version:\n            Label of how `args_schema`s were generated (e.g. `pydantic_v1`).\n    \"\"\"\n\n    manifest_version: Literal[\"2\"]\n    task_list: list[TaskManifestV2]\n    has_args_schemas: bool = False\n    args_schema_version: str | None = None\n    authors: NonEmptyStr | None = None\n\n    @model_validator(mode=\"after\")\n    def _check_args_schemas_are_present(self):\n        has_args_schemas = self.has_args_schemas\n        task_list = self.task_list\n        if has_args_schemas is True:\n            for task in task_list:\n                if task.executable_parallel is not None:\n                    if task.args_schema_parallel is None:\n                        raise ValueError(\n                            f\"Manifest has {has_args_schemas=}, but \"\n                            f\"task '{task.name}' has \"\n                            f\"{task.args_schema_parallel=}.\"\n                        )\n                if task.executable_non_parallel is not None:\n                    if task.args_schema_non_parallel is None:\n                        raise ValueError(\n                            f\"Manifest has {has_args_schemas=}, but \"\n                            f\"task '{task.name}' has \"\n                            f\"{task.args_schema_non_parallel=}.\"\n                        )\n        return self\n\n    @model_validator(mode=\"after\")\n    def _unique_task_names(self):\n        task_list = self.task_list\n        task_list_names = [t.name for t in task_list]\n        if len(set(task_list_names)) != len(task_list_names):\n            raise ValueError(\n                (\n                    \"Task names in manifest must be unique.\\n\",\n                    f\"Given: {task_list_names}.\",\n                )\n            )\n        return self\n</code></pre>"},{"location":"reference/app/schemas/v2/manifest/#fractal_server.app.schemas.v2.manifest.TaskManifestV2","title":"<code>TaskManifestV2</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a task within a V2 manifest.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>The task name</p> <p> TYPE: <code>str</code> </p> <code>executable</code> <p>Path to the executable relative to the package root</p> <p>Note: by package root we mean \"as it will be installed\". If a package <code>Pkg</code> installs in the folder <code>pkg</code> the executable <code>pkg/executable.py</code>, this attribute must contain only <code>executable.py</code>.</p> <p> TYPE: <code>str</code> </p> <code>input_type</code> <p>The input type accepted by the task</p> <p> TYPE: <code>str</code> </p> <code>output_type</code> <p>The output type returned by the task</p> <p> TYPE: <code>str</code> </p> <code>meta</code> <p>Additional information about the package, such as hash of the executable, specific runtime requirements (e.g., need_gpu=True), etc.</p> <p> TYPE: <code>str</code> </p> <code>args_schema</code> <p>JSON Schema for task arguments</p> <p> TYPE: <code>str</code> </p> <code>docs_info</code> <p>Additional information about the Task, coming from the docstring.</p> <p> TYPE: <code>str | None</code> </p> <code>docs_link</code> <p>Link to Task docs.</p> <p> TYPE: <code>HttpUrlStr | None</code> </p> Source code in <code>fractal_server/app/schemas/v2/manifest.py</code> <pre><code>class TaskManifestV2(BaseModel):\n    \"\"\"\n    Represents a task within a V2 manifest.\n\n    Attributes:\n        name:\n            The task name\n        executable:\n            Path to the executable relative to the package root\n\n            Note: by package root we mean \"as it will be installed\". If a\n            package `Pkg` installs in the folder `pkg` the executable\n            `pkg/executable.py`, this attribute must contain only\n            `executable.py`.\n        input_type:\n            The input type accepted by the task\n        output_type:\n            The output type returned by the task\n        meta:\n            Additional information about the package, such as hash of the\n            executable, specific runtime requirements (e.g., need_gpu=True),\n            etc.\n        args_schema:\n            JSON Schema for task arguments\n        docs_info:\n            Additional information about the Task, coming from the docstring.\n        docs_link:\n            Link to Task docs.\n    \"\"\"\n\n    name: str\n    executable_non_parallel: str | None = None\n    executable_parallel: str | None = None\n    input_types: dict[str, bool] = Field(default_factory=dict)\n    output_types: dict[str, bool] = Field(default_factory=dict)\n    meta_non_parallel: DictStrAny = Field(default_factory=dict)\n    meta_parallel: DictStrAny = Field(default_factory=dict)\n    args_schema_non_parallel: DictStrAny | None = None\n    args_schema_parallel: DictStrAny | None = None\n    docs_info: str | None = None\n    docs_link: HttpUrlStr | None = None\n\n    category: str | None = None\n    modality: str | None = None\n    tags: list[str] = Field(default_factory=list)\n\n    type: None | TaskType = None\n\n    @model_validator(mode=\"after\")\n    def validate_executable_args_meta(self):\n        executable_non_parallel = self.executable_non_parallel\n        executable_parallel = self.executable_parallel\n        if (executable_non_parallel is None) and (executable_parallel is None):\n            raise ValueError(\n                \"`TaskManifestV2.executable_non_parallel` and \"\n                \"`TaskManifestV2.executable_parallel` cannot be both None.\"\n            )\n\n        elif executable_non_parallel is None:\n            meta_non_parallel = self.meta_non_parallel\n            if meta_non_parallel != {}:\n                raise ValueError(\n                    \"`TaskManifestV2.meta_non_parallel` must be an empty dict \"\n                    \"if `TaskManifestV2.executable_non_parallel` is None. \"\n                    f\"Given: {meta_non_parallel}.\"\n                )\n\n            args_schema_non_parallel = self.args_schema_non_parallel\n            if args_schema_non_parallel is not None:\n                raise ValueError(\n                    \"`TaskManifestV2.args_schema_non_parallel` must be None \"\n                    \"if `TaskManifestV2.executable_non_parallel` is None. \"\n                    f\"Given: {args_schema_non_parallel}.\"\n                )\n\n        elif executable_parallel is None:\n            meta_parallel = self.meta_parallel\n            if meta_parallel != {}:\n                raise ValueError(\n                    \"`TaskManifestV2.meta_parallel` must be an empty dict if \"\n                    \"`TaskManifestV2.executable_parallel` is None. \"\n                    f\"Given: {meta_parallel}.\"\n                )\n\n            args_schema_parallel = self.args_schema_parallel\n            if args_schema_parallel is not None:\n                raise ValueError(\n                    \"`TaskManifestV2.args_schema_parallel` must be None if \"\n                    \"`TaskManifestV2.executable_parallel` is None. \"\n                    f\"Given: {args_schema_parallel}.\"\n                )\n\n        return self\n</code></pre>"},{"location":"reference/app/schemas/v2/profile/","title":"profile","text":""},{"location":"reference/app/schemas/v2/profile/#fractal_server.app.schemas.v2.profile.ProfileRead","title":"<code>ProfileRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Profile schema for GET endpoints.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p> TYPE: <code>int</code> </p> <code>name</code> <p> TYPE: <code>str</code> </p> <code>resource_id</code> <p> TYPE: <code>int</code> </p> <code>resource_type</code> <p> TYPE: <code>str</code> </p> <code>username</code> <p> TYPE: <code>str | None</code> </p> <code>ssh_key_path</code> <p> TYPE: <code>str | None</code> </p> <code>jobs_remote_dir</code> <p> TYPE: <code>str | None</code> </p> <code>tasks_remote_dir</code> <p> TYPE: <code>str | None</code> </p> Source code in <code>fractal_server/app/schemas/v2/profile.py</code> <pre><code>class ProfileRead(BaseModel):\n    \"\"\"\n    Profile schema for GET endpoints.\n\n    Attributes:\n        id:\n        name:\n        resource_id:\n        resource_type:\n        username:\n        ssh_key_path:\n        jobs_remote_dir:\n        tasks_remote_dir:\n    \"\"\"\n\n    id: int\n    name: str\n    resource_id: int\n    resource_type: str\n    username: str | None = None\n    ssh_key_path: str | None = None\n    jobs_remote_dir: str | None = None\n    tasks_remote_dir: str | None = None\n</code></pre>"},{"location":"reference/app/schemas/v2/profile/#fractal_server.app.schemas.v2.profile.ValidProfileLocal","title":"<code>ValidProfileLocal</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Valid local profile.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Profile name.</p> <p> TYPE: <code>NonEmptyStr</code> </p> <code>resource_type</code> <p>Type of the corresponding resource.</p> <p> TYPE: <code>ResourceType</code> </p> Source code in <code>fractal_server/app/schemas/v2/profile.py</code> <pre><code>class ValidProfileLocal(BaseModel):\n    \"\"\"\n    Valid local profile.\n\n    Attributes:\n        name: Profile name.\n        resource_type: Type of the corresponding resource.\n    \"\"\"\n\n    name: NonEmptyStr\n    resource_type: ResourceType\n    username: None = None\n    ssh_key_path: None = None\n    jobs_remote_dir: None = None\n    tasks_remote_dir: None = None\n</code></pre>"},{"location":"reference/app/schemas/v2/profile/#fractal_server.app.schemas.v2.profile.ValidProfileSlurmSSH","title":"<code>ValidProfileSlurmSSH</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Valid SLURM/sudo profile.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Profile name.</p> <p> TYPE: <code>NonEmptyStr</code> </p> <code>resource_type</code> <p>Type of the corresponding resource.</p> <p> TYPE: <code>ResourceType</code> </p> <code>username</code> <p>SLURM user to impersonate (e.g. as in <code>ssh username@cluster sbatch /some/script.sh</code>).</p> <p> TYPE: <code>NonEmptyStr</code> </p> <code>ssh_key_path</code> <p>Local path of SSH private key for user <code>username</code>.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>tasks_remote_dir</code> <p>Base folder for task environments on the remote SLURM cluster.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>jobs_remote_dir</code> <p>Base folder for job directories on the remote SLURM cluster.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> Source code in <code>fractal_server/app/schemas/v2/profile.py</code> <pre><code>class ValidProfileSlurmSSH(BaseModel):\n    \"\"\"\n    Valid SLURM/sudo profile.\n\n    Attributes:\n        name: Profile name.\n        resource_type: Type of the corresponding resource.\n        username:\n            SLURM user to impersonate (e.g. as in\n            `ssh username@cluster sbatch /some/script.sh`).\n        ssh_key_path:\n            Local path of SSH private key for user `username`.\n        tasks_remote_dir:\n            Base folder for task environments on the remote SLURM cluster.\n        jobs_remote_dir:\n            Base folder for job directories on the remote SLURM cluster.\n    \"\"\"\n\n    name: NonEmptyStr\n    resource_type: ResourceType\n    username: NonEmptyStr\n    ssh_key_path: AbsolutePathStr\n    jobs_remote_dir: AbsolutePathStr\n    tasks_remote_dir: AbsolutePathStr\n</code></pre>"},{"location":"reference/app/schemas/v2/profile/#fractal_server.app.schemas.v2.profile.ValidProfileSlurmSudo","title":"<code>ValidProfileSlurmSudo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Valid SLURM/sudo profile.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Profile name.</p> <p> TYPE: <code>NonEmptyStr</code> </p> <code>resource_type</code> <p>Type of the corresponding resource.</p> <p> TYPE: <code>ResourceType</code> </p> <code>username</code> <p>SLURM user to impersonate (e.g. as in <code>sudo -u username sbatch /some/script.sh</code>).</p> <p> TYPE: <code>NonEmptyStr</code> </p> Source code in <code>fractal_server/app/schemas/v2/profile.py</code> <pre><code>class ValidProfileSlurmSudo(BaseModel):\n    \"\"\"\n    Valid SLURM/sudo profile.\n\n    Attributes:\n        name: Profile name.\n        resource_type: Type of the corresponding resource.\n        username:\n            SLURM user to impersonate (e.g. as in\n            `sudo -u username sbatch /some/script.sh`).\n    \"\"\"\n\n    name: NonEmptyStr\n    resource_type: ResourceType\n    username: NonEmptyStr\n    ssh_key_path: None = None\n    jobs_remote_dir: None = None\n    tasks_remote_dir: None = None\n</code></pre>"},{"location":"reference/app/schemas/v2/profile/#fractal_server.app.schemas.v2.profile.cast_serialize_profile","title":"<code>cast_serialize_profile(_data)</code>","text":"<p>Cast/serialize round-trip for <code>Profile</code> data.</p> <p>We use <code>@validate_call</code> because <code>ProfileCreate</code> is a <code>Union</code> type and it cannot be instantiated directly.</p> Return <p>Serialized version of a valid profile object.</p> Source code in <code>fractal_server/app/schemas/v2/profile.py</code> <pre><code>@validate_call\ndef cast_serialize_profile(_data: ProfileCreate) -&gt; dict[str, Any]:\n    \"\"\"\n    Cast/serialize round-trip for `Profile` data.\n\n    We use `@validate_call` because `ProfileCreate` is a `Union` type and it\n    cannot be instantiated directly.\n\n    Return:\n        Serialized version of a valid profile object.\n    \"\"\"\n    return _data.model_dump()\n</code></pre>"},{"location":"reference/app/schemas/v2/project/","title":"project","text":""},{"location":"reference/app/schemas/v2/resource/","title":"resource","text":""},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.ResourceCreate","title":"<code>ResourceCreate = Annotated[Annotated[ValidResourceLocal, Tag(ResourceType.LOCAL)] | Annotated[ValidResourceSlurmSudo, Tag(ResourceType.SLURM_SUDO)] | Annotated[ValidResourceSlurmSSH, Tag(ResourceType.SLURM_SSH)], Discriminator(get_discriminator_value)]</code>  <code>module-attribute</code>","text":"<p>Schema for resources in API request bodies.</p>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.ResourceRead","title":"<code>ResourceRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for resources in API response bodies.</p> Source code in <code>fractal_server/app/schemas/v2/resource.py</code> <pre><code>class ResourceRead(BaseModel):\n    \"\"\"\n    Schema for resources in API response bodies.\n    \"\"\"\n\n    id: int\n    name: str\n    type: str\n    prevent_new_submissions: bool\n    timestamp_created: AwareDatetime\n\n    host: str | None\n\n    jobs_local_dir: str\n    jobs_runner_config: dict[str, Any]\n    jobs_slurm_python_worker: str | None\n    jobs_poll_interval: int\n\n    tasks_local_dir: str\n    tasks_python_config: dict[str, Any]\n    tasks_pixi_config: dict[str, Any]\n</code></pre>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.ResourceType","title":"<code>ResourceType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for the possible resource types.</p> Source code in <code>fractal_server/app/schemas/v2/resource.py</code> <pre><code>class ResourceType(StrEnum):\n    \"\"\"\n    Enum for the possible resource types.\n    \"\"\"\n\n    SLURM_SUDO = \"slurm_sudo\"\n    \"\"\"\n    Enum entry for resource type `slurm_sudo`.\n    \"\"\"\n\n    SLURM_SSH = \"slurm_ssh\"\n    \"\"\"\n    Enum entry for resource type `slurm_ssh`.\n    \"\"\"\n\n    LOCAL = \"local\"\n    \"\"\"\n    Enum entry for resource type `local`.\n    \"\"\"\n</code></pre>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.ResourceType.LOCAL","title":"<code>LOCAL = 'local'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Enum entry for resource type <code>local</code>.</p>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.ResourceType.SLURM_SSH","title":"<code>SLURM_SSH = 'slurm_ssh'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Enum entry for resource type <code>slurm_ssh</code>.</p>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.ResourceType.SLURM_SUDO","title":"<code>SLURM_SUDO = 'slurm_sudo'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Enum entry for resource type <code>slurm_sudo</code>.</p>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.ValidResourceBase","title":"<code>ValidResourceBase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base resource schema.</p> Source code in <code>fractal_server/app/schemas/v2/resource.py</code> <pre><code>class ValidResourceBase(BaseModel):\n    \"\"\"\n    Base resource schema.\n    \"\"\"\n\n    type: ResourceType\n    name: NonEmptyStr\n\n    # Tasks\n    tasks_python_config: TasksPythonSettings\n    tasks_pixi_config: Annotated[\n        dict[NonEmptyStr, Any],\n        AfterValidator(cast_serialize_pixi_settings),\n    ]\n    tasks_local_dir: AbsolutePathStr\n\n    # Jobs\n    jobs_local_dir: AbsolutePathStr\n    jobs_runner_config: dict[NonEmptyStr, Any]\n    jobs_poll_interval: int = 5\n\n    prevent_new_submissions: bool = False\n\n    @model_validator(mode=\"after\")\n    def _pixi_slurm_config(self) -&gt; Self:\n        if (\n            self.tasks_pixi_config != {}\n            and self.type == ResourceType.SLURM_SSH\n            and self.tasks_pixi_config[\"SLURM_CONFIG\"] is None\n        ):\n            raise ValueError(\"`tasks_pixi_config` must include `SLURM_CONFIG`.\")\n        return self\n</code></pre>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.ValidResourceLocal","title":"<code>ValidResourceLocal</code>","text":"<p>               Bases: <code>ValidResourceBase</code></p> <p>Valid local resource.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Resource name.</p> <p> TYPE: <code>NonEmptyStr</code> </p> <code>type</code> <p>Resource type.</p> <p> TYPE: <code>Literal[LOCAL]</code> </p> <code>prevent_new_submissions</code> <p>When set to true: Prevent new job submissions and stop execution of ongoing jobs as soon as the current task is complete.</p> <p> TYPE: <code>bool</code> </p> <code>tasks_python_config</code> <p>Configuration of Python interpreters used for task collection.</p> <p> TYPE: <code>TasksPythonSettings</code> </p> <code>tasks_pixi_config</code> <p>Configuration of <code>pixi</code> interpreters used for task collection.</p> <p> TYPE: <code>Annotated[dict[NonEmptyStr, Any], AfterValidator(cast_serialize_pixi_settings)]</code> </p> <code>tasks_local_dir</code> <p>Local base folder for task environments.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>jobs_local_dir</code> <p>Local base folder for job folders.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>jobs_runner_config</code> <p>Runner configuration.</p> <p> TYPE: <code>JobRunnerConfigLocal</code> </p> Source code in <code>fractal_server/app/schemas/v2/resource.py</code> <pre><code>class ValidResourceLocal(ValidResourceBase):\n    \"\"\"\n    Valid local resource.\n\n    Attributes:\n        name: Resource name.\n        type: Resource type.\n        prevent_new_submissions:\n            When set to true: Prevent new job submissions and stop execution of\n            ongoing jobs as soon as the current task is complete.\n        tasks_python_config:\n            Configuration of Python interpreters used for task collection.\n        tasks_pixi_config:\n            Configuration of `pixi` interpreters used for task collection.\n        tasks_local_dir:\n            Local base folder for task environments.\n        jobs_local_dir:\n            Local base folder for job folders.\n        jobs_runner_config:\n            Runner configuration.\n    \"\"\"\n\n    type: Literal[ResourceType.LOCAL]\n    jobs_runner_config: JobRunnerConfigLocal\n    jobs_slurm_python_worker: None = None\n    host: None = None\n</code></pre>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.ValidResourceSlurmSSH","title":"<code>ValidResourceSlurmSSH</code>","text":"<p>               Bases: <code>ValidResourceBase</code></p> <p>Valid SLURM-SSH resource.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Resource name</p> <p> TYPE: <code>NonEmptyStr</code> </p> <code>type</code> <p>Resource type.</p> <p> TYPE: <code>Literal[SLURM_SSH]</code> </p> <code>prevent_new_submissions</code> <p>When set to true: Prevent new job submissions and stop execution of ongoing jobs as soon as the current task is complete.</p> <p> TYPE: <code>bool</code> </p> <code>tasks_python_config</code> <p>Configuration of Python interpreters used for task collection.</p> <p> TYPE: <code>TasksPythonSettings</code> </p> <code>tasks_pixi_config</code> <p>Configuration of <code>pixi</code> interpreters used for task collection.</p> <p> TYPE: <code>Annotated[dict[NonEmptyStr, Any], AfterValidator(cast_serialize_pixi_settings)]</code> </p> <code>tasks_local_dir</code> <p>Local base folder for task environments.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>jobs_local_dir</code> <p>Local base folder for job folders.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>jobs_runner_config</code> <p>Runner configuration.</p> <p> TYPE: <code>JobRunnerConfigSLURM</code> </p> <code>jobs_poll_interval</code> <p><code>squeue</code> polling interval.</p> <p> TYPE: <code>int</code> </p> <code>jobs_slurm_python_worker</code> <p>Python worker to be used in SLURM jobs.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>host</code> <p>Hostname or IP address of remote SLURM cluster.</p> <p> TYPE: <code>NonEmptyStr</code> </p> Source code in <code>fractal_server/app/schemas/v2/resource.py</code> <pre><code>class ValidResourceSlurmSSH(ValidResourceBase):\n    \"\"\"\n    Valid SLURM-SSH resource.\n\n    Attributes:\n        name: Resource name\n        type: Resource type.\n        prevent_new_submissions:\n            When set to true: Prevent new job submissions and stop execution of\n            ongoing jobs as soon as the current task is complete.\n        tasks_python_config:\n            Configuration of Python interpreters used for task collection.\n        tasks_pixi_config:\n            Configuration of `pixi` interpreters used for task collection.\n        tasks_local_dir:\n            Local base folder for task environments.\n        jobs_local_dir:\n            Local base folder for job folders.\n        jobs_runner_config:\n            Runner configuration.\n        jobs_poll_interval:\n            `squeue` polling interval.\n        jobs_slurm_python_worker:\n            Python worker to be used in SLURM jobs.\n        host:\n            Hostname or IP address of remote SLURM cluster.\n    \"\"\"\n\n    type: Literal[ResourceType.SLURM_SSH]\n    host: NonEmptyStr\n    jobs_slurm_python_worker: AbsolutePathStr\n    jobs_runner_config: JobRunnerConfigSLURM\n</code></pre>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.ValidResourceSlurmSudo","title":"<code>ValidResourceSlurmSudo</code>","text":"<p>               Bases: <code>ValidResourceBase</code></p> <p>Valid SLURM-sudo resource.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Resource name.</p> <p> TYPE: <code>NonEmptyStr</code> </p> <code>type</code> <p>Resource type.</p> <p> TYPE: <code>Literal[SLURM_SUDO]</code> </p> <code>prevent_new_submissions</code> <p>When set to true: Prevent new job submissions and stop execution of ongoing jobs as soon as the current task is complete.</p> <p> TYPE: <code>bool</code> </p> <code>tasks_python_config</code> <p>Configuration of Python interpreters used for task collection.</p> <p> TYPE: <code>TasksPythonSettings</code> </p> <code>tasks_pixi_config</code> <p>Configuration of <code>pixi</code> interpreters used for task collection.</p> <p> TYPE: <code>Annotated[dict[NonEmptyStr, Any], AfterValidator(cast_serialize_pixi_settings)]</code> </p> <code>tasks_local_dir</code> <p>Local base folder for task environments.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>jobs_local_dir</code> <p>Local base folder for job folders.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>jobs_runner_config</code> <p>Runner configuration.</p> <p> TYPE: <code>JobRunnerConfigSLURM</code> </p> <code>jobs_poll_interval</code> <p><code>squeue</code> polling interval.</p> <p> TYPE: <code>int</code> </p> <code>jobs_slurm_python_worker</code> <p>Python worker to be used in SLURM jobs.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> Source code in <code>fractal_server/app/schemas/v2/resource.py</code> <pre><code>class ValidResourceSlurmSudo(ValidResourceBase):\n    \"\"\"\n    Valid SLURM-sudo resource.\n\n    Attributes:\n        name: Resource name.\n        type: Resource type.\n        prevent_new_submissions:\n            When set to true: Prevent new job submissions and stop execution of\n            ongoing jobs as soon as the current task is complete.\n        tasks_python_config:\n            Configuration of Python interpreters used for task collection.\n        tasks_pixi_config:\n            Configuration of `pixi` interpreters used for task collection.\n        tasks_local_dir:\n            Local base folder for task environments.\n        jobs_local_dir:\n            Local base folder for job folders.\n        jobs_runner_config:\n            Runner configuration.\n        jobs_poll_interval:\n            `squeue` polling interval.\n        jobs_slurm_python_worker:\n            Python worker to be used in SLURM jobs.\n    \"\"\"\n\n    type: Literal[ResourceType.SLURM_SUDO]\n    jobs_slurm_python_worker: AbsolutePathStr\n    jobs_runner_config: JobRunnerConfigSLURM\n    host: None = None\n</code></pre>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.cast_serialize_pixi_settings","title":"<code>cast_serialize_pixi_settings(value)</code>","text":"<p>Cast/serialize round trip for <code>tasks_pixi_config</code> through the <code>TasksPixiSettings</code> schema.</p> PARAMETER DESCRIPTION <code>value</code> <p>Current <code>tasks_pixi_config</code> value.</p> <p> TYPE: <code>dict[NonEmptyStr, Any]</code> </p> Source code in <code>fractal_server/app/schemas/v2/resource.py</code> <pre><code>def cast_serialize_pixi_settings(\n    value: dict[NonEmptyStr, Any],\n) -&gt; dict[NonEmptyStr, Any]:\n    \"\"\"\n    Cast/serialize round trip for `tasks_pixi_config` through the\n    `TasksPixiSettings` schema.\n\n    Arguments:\n        value: Current `tasks_pixi_config` value.\n    \"\"\"\n    if value != {}:\n        value = TasksPixiSettings(**value).model_dump()\n    return value\n</code></pre>"},{"location":"reference/app/schemas/v2/resource/#fractal_server.app.schemas.v2.resource.cast_serialize_resource","title":"<code>cast_serialize_resource(_data)</code>","text":"<p>Cast/serialize round-trip for <code>Resource</code> data.</p> <p>We use <code>@validate_call</code> because <code>ResourceCreate</code> is a <code>Union</code> type and it cannot be instantiated directly.</p> PARAMETER DESCRIPTION <code>_data</code> <p> TYPE: <code>ResourceCreate</code> </p> Return <p>Serialized version of a valid resource object.</p> Source code in <code>fractal_server/app/schemas/v2/resource.py</code> <pre><code>@validate_call\ndef cast_serialize_resource(_data: ResourceCreate) -&gt; dict[str, Any]:\n    \"\"\"\n    Cast/serialize round-trip for `Resource` data.\n\n    We use `@validate_call` because `ResourceCreate` is a `Union` type and it\n    cannot be instantiated directly.\n\n    Args:\n        _data:\n\n    Return:\n        Serialized version of a valid resource object.\n    \"\"\"\n    return _data.model_dump()\n</code></pre>"},{"location":"reference/app/schemas/v2/sharing/","title":"sharing","text":""},{"location":"reference/app/schemas/v2/sharing/#fractal_server.app.schemas.v2.sharing.ProjectAccessRead","title":"<code>ProjectAccessRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Project-access information for current user.</p> ATTRIBUTE DESCRIPTION <code>is_owner</code> <p>Whether current user is owner.</p> <p> TYPE: <code>bool</code> </p> <code>permissions</code> <p>Current user permissions.</p> <p> TYPE: <code>str</code> </p> <code>owner_email</code> <p>Email of project owner</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/app/schemas/v2/sharing.py</code> <pre><code>class ProjectAccessRead(BaseModel):\n    \"\"\"\n    Project-access information for current user.\n\n    Attributes:\n        is_owner: Whether current user is owner.\n        permissions: Current user permissions.\n        owner_email: Email of project owner\n    \"\"\"\n\n    is_owner: bool\n    permissions: str\n    owner_email: str\n</code></pre>"},{"location":"reference/app/schemas/v2/sharing/#fractal_server.app.schemas.v2.sharing.ProjectGuestCreate","title":"<code>ProjectGuestCreate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request body for project-sharing invitation.</p> ATTRIBUTE DESCRIPTION <code>permissions</code> <p> TYPE: <code>ProjectPermissions</code> </p> Source code in <code>fractal_server/app/schemas/v2/sharing.py</code> <pre><code>class ProjectGuestCreate(BaseModel):\n    \"\"\"\n    Request body for project-sharing invitation.\n\n    Attributes:\n        permissions:\n    \"\"\"\n\n    permissions: ProjectPermissions\n</code></pre>"},{"location":"reference/app/schemas/v2/sharing/#fractal_server.app.schemas.v2.sharing.ProjectGuestRead","title":"<code>ProjectGuestRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about a guest.</p> ATTRIBUTE DESCRIPTION <code>email</code> <p>Guest email.</p> <p> TYPE: <code>str</code> </p> <code>is_verified</code> <p>Project/guest verification status.</p> <p> TYPE: <code>bool</code> </p> <code>permissions</code> <p>Guest permissions for project.</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/app/schemas/v2/sharing.py</code> <pre><code>class ProjectGuestRead(BaseModel):\n    \"\"\"\n    Information about a guest.\n\n    Attributes:\n        email: Guest email.\n        is_verified: Project/guest verification status.\n        permissions: Guest permissions for project.\n    \"\"\"\n\n    email: str\n    is_verified: bool\n    permissions: str\n</code></pre>"},{"location":"reference/app/schemas/v2/sharing/#fractal_server.app.schemas.v2.sharing.ProjectGuestUpdate","title":"<code>ProjectGuestUpdate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request body for updating permissions of an existing guest.</p> ATTRIBUTE DESCRIPTION <code>permissions</code> <p>New permissions for guest.</p> <p> TYPE: <code>ProjectPermissions</code> </p> Source code in <code>fractal_server/app/schemas/v2/sharing.py</code> <pre><code>class ProjectGuestUpdate(BaseModel):\n    \"\"\"\n    Request body for updating permissions of an existing guest.\n\n    Attributes:\n        permissions: New permissions for guest.\n    \"\"\"\n\n    permissions: ProjectPermissions\n</code></pre>"},{"location":"reference/app/schemas/v2/sharing/#fractal_server.app.schemas.v2.sharing.ProjectInvitationRead","title":"<code>ProjectInvitationRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Info about a pending invitation.</p> ATTRIBUTE DESCRIPTION <code>project_id</code> <p> TYPE: <code>int</code> </p> <code>project_name</code> <p> TYPE: <code>str</code> </p> <code>owner_email</code> <p> TYPE: <code>str</code> </p> <code>guest_permissions</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/app/schemas/v2/sharing.py</code> <pre><code>class ProjectInvitationRead(BaseModel):\n    \"\"\"\n    Info about a pending invitation.\n\n    Attributes:\n        project_id:\n        project_name:\n        owner_email:\n        guest_permissions:\n    \"\"\"\n\n    project_id: int\n    project_name: str\n    owner_email: str\n    guest_permissions: str\n</code></pre>"},{"location":"reference/app/schemas/v2/sharing/#fractal_server.app.schemas.v2.sharing.ProjectPermissions","title":"<code>ProjectPermissions</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available permissions for accessing Project Attributes:     READ:     WRITE:     EXECUTE:</p> Source code in <code>fractal_server/app/schemas/v2/sharing.py</code> <pre><code>class ProjectPermissions(StrEnum):\n    \"\"\"\n    Available permissions for accessing Project\n    Attributes:\n        READ:\n        WRITE:\n        EXECUTE:\n    \"\"\"\n\n    READ = \"r\"\n    WRITE = \"rw\"\n    EXECUTE = \"rwx\"\n</code></pre>"},{"location":"reference/app/schemas/v2/status_legacy/","title":"status_legacy","text":""},{"location":"reference/app/schemas/v2/status_legacy/#fractal_server.app.schemas.v2.status_legacy.LegacyStatusRead","title":"<code>LegacyStatusRead</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response type for the <code>/project/{project_id}/status/</code> endpoint</p> Source code in <code>fractal_server/app/schemas/v2/status_legacy.py</code> <pre><code>class LegacyStatusRead(BaseModel):\n    \"\"\"\n    Response type for the\n    `/project/{project_id}/status/` endpoint\n    \"\"\"\n\n    status: dict[\n        str,\n        WorkflowTaskStatusType,\n    ] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/app/schemas/v2/status_legacy/#fractal_server.app.schemas.v2.status_legacy.WorkflowTaskStatusType","title":"<code>WorkflowTaskStatusType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the available values for the status of a <code>WorkflowTask</code>.</p> <p>This model is used within the <code>Dataset.history</code> attribute, which is constructed in the runner and then used in the API (e.g. in the <code>api/v2/project/{project_id}/dataset/{dataset_id}/status</code> endpoint).</p> ATTRIBUTE DESCRIPTION <code>SUBMITTED</code> <p>The <code>WorkflowTask</code> is part of a running job.</p> <p> </p> <code>DONE</code> <p>The most-recent execution of this <code>WorkflowTask</code> was successful.</p> <p> </p> <code>FAILED</code> <p>The most-recent execution of this <code>WorkflowTask</code> failed.</p> <p> </p> Source code in <code>fractal_server/app/schemas/v2/status_legacy.py</code> <pre><code>class WorkflowTaskStatusType(StrEnum):\n    \"\"\"\n    Define the available values for the status of a `WorkflowTask`.\n\n    This model is used within the `Dataset.history` attribute, which is\n    constructed in the runner and then used in the API (e.g. in the\n    `api/v2/project/{project_id}/dataset/{dataset_id}/status` endpoint).\n\n    Attributes:\n        SUBMITTED: The `WorkflowTask` is part of a running job.\n        DONE: The most-recent execution of this `WorkflowTask` was successful.\n        FAILED: The most-recent execution of this `WorkflowTask` failed.\n    \"\"\"\n\n    SUBMITTED = \"submitted\"\n    DONE = \"done\"\n    FAILED = \"failed\"\n</code></pre>"},{"location":"reference/app/schemas/v2/task/","title":"task","text":""},{"location":"reference/app/schemas/v2/task/#fractal_server.app.schemas.v2.task.TaskType","title":"<code>TaskType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Define the available task types.</p> Source code in <code>fractal_server/app/schemas/v2/task.py</code> <pre><code>class TaskType(StrEnum):\n    \"\"\"\n    Define the available task types.\n    \"\"\"\n\n    COMPOUND = \"compound\"\n    CONVERTER_COMPOUND = \"converter_compound\"\n    NON_PARALLEL = \"non_parallel\"\n    CONVERTER_NON_PARALLEL = \"converter_non_parallel\"\n    PARALLEL = \"parallel\"\n</code></pre>"},{"location":"reference/app/schemas/v2/task_collection/","title":"task_collection","text":""},{"location":"reference/app/schemas/v2/task_collection/#fractal_server.app.schemas.v2.task_collection.FractalUploadedFile","title":"<code>FractalUploadedFile</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for data sent from the endpoint to the background task.</p> Source code in <code>fractal_server/app/schemas/v2/task_collection.py</code> <pre><code>class FractalUploadedFile(BaseModel):\n    \"\"\"\n    Model for data sent from the endpoint to the background task.\n    \"\"\"\n\n    filename: str\n    contents: bytes\n</code></pre>"},{"location":"reference/app/schemas/v2/task_collection/#fractal_server.app.schemas.v2.task_collection.TaskCollectCustom","title":"<code>TaskCollectCustom</code>","text":"<p>               Bases: <code>BaseModel</code></p> ATTRIBUTE DESCRIPTION <code>manifest</code> <p>Manifest of a Fractal task package (this is typically the content of <code>__FRACTAL_MANIFEST__.json</code>).</p> <p> TYPE: <code>ManifestV2</code> </p> <code>python_interpreter</code> <p>Absolute path to the Python interpreter to be used for running tasks.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>name</code> <p>A name identifying this package, that will fill the <code>TaskGroup.pkg_name</code> column.</p> <p> TYPE: <code>AbsolutePathStr</code> </p> <code>package_root</code> <p>The folder where the package is installed. If not provided, it will be extracted via <code>pip show</code> (requires <code>package_name</code> to be set).</p> <p> TYPE: <code>AbsolutePathStr | None</code> </p> <code>package_name</code> <p>Name of the package, as used for <code>import &lt;package_name&gt;</code>; this is then used to extract the package directory (<code>package_root</code>) via <code>pip show &lt;package_name&gt;</code>.</p> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>version</code> <p>Optional version of tasks to be collected.</p> <p> TYPE: <code>NonEmptyStr | None</code> </p> Source code in <code>fractal_server/app/schemas/v2/task_collection.py</code> <pre><code>class TaskCollectCustom(BaseModel):\n    \"\"\"\n    Attributes:\n        manifest: Manifest of a Fractal task package (this is typically the\n            content of `__FRACTAL_MANIFEST__.json`).\n        python_interpreter: Absolute path to the Python interpreter to be used\n            for running tasks.\n        name: A name identifying this package, that will fill the\n            `TaskGroup.pkg_name` column.\n        package_root: The folder where the package is installed.\n            If not provided, it will be extracted via `pip show`\n            (requires `package_name` to be set).\n        package_name: Name of the package, as used for `import &lt;package_name&gt;`;\n            this is then used to extract the package directory (`package_root`)\n            via `pip show &lt;package_name&gt;`.\n        version: Optional version of tasks to be collected.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    manifest: ManifestV2\n    python_interpreter: AbsolutePathStr\n    label: NonEmptyStr\n    package_root: AbsolutePathStr | None = None\n    package_name: NonEmptyStr | None = None\n    version: NonEmptyStr | None = None\n\n    @field_validator(\"package_name\", mode=\"after\")\n    @classmethod\n    def validate_package_name(cls, value):\n        if value is not None:\n            validate_cmd(value)\n        return value\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def one_of_package_root_or_name(cls, values):\n        package_root = values.get(\"package_root\")\n        package_name = values.get(\"package_name\")\n        if (package_root is None and package_name is None) or (\n            package_root is not None and package_name is not None\n        ):\n            raise ValueError(\n                \"One and only one must be set between \"\n                \"'package_root' and 'package_name'\"\n            )\n        return values\n</code></pre>"},{"location":"reference/app/schemas/v2/task_collection/#fractal_server.app.schemas.v2.task_collection.TaskCollectPip","title":"<code>TaskCollectPip</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>TaskCollectPip class</p> <p>This class only encodes the attributes required to trigger a task-collection operation. Other attributes (that are assigned during task collection) are defined as part of fractal-server.</p> <p>Two cases are supported:</p> <pre><code>1. `package` is the name of a package that can be installed via `pip`.\n1. `package=None`, and a wheel file is uploaded within the API request.\n</code></pre> ATTRIBUTE DESCRIPTION <code>package</code> <p>The name of a <code>pip</code>-installable package, or <code>None</code>.</p> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>package_version</code> <p>Version of the package</p> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>package_extras</code> <p>Package extras to include in the <code>pip install</code> command</p> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>python_version</code> <p>Python version to install and run the package tasks</p> <p> TYPE: <code>Literal['3.9', '3.10', '3.11', '3.12', '3.13', '3.14'] | None</code> </p> <code>pinned_package_versions_pre</code> <p>dictionary 'package':'version' used to pre-pin versions for specific packages.</p> <p> TYPE: <code>DictStrStr | None</code> </p> <code>pinned_package_versions_post</code> <p>dictionary 'package':'version' used to post-pin versions for specific packages.</p> <p> TYPE: <code>DictStrStr | None</code> </p> Source code in <code>fractal_server/app/schemas/v2/task_collection.py</code> <pre><code>class TaskCollectPip(BaseModel):\n    \"\"\"\n    TaskCollectPip class\n\n    This class only encodes the attributes required to trigger a\n    task-collection operation. Other attributes (that are assigned *during*\n    task collection) are defined as part of fractal-server.\n\n    Two cases are supported:\n\n        1. `package` is the name of a package that can be installed via `pip`.\n        1. `package=None`, and a wheel file is uploaded within the API request.\n\n    Attributes:\n        package: The name of a `pip`-installable package, or `None`.\n        package_version: Version of the package\n        package_extras: Package extras to include in the `pip install` command\n        python_version: Python version to install and run the package tasks\n        pinned_package_versions_pre:\n            dictionary 'package':'version' used to pre-pin versions for\n            specific packages.\n        pinned_package_versions_post:\n            dictionary 'package':'version' used to post-pin versions for\n            specific packages.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    package: NonEmptyStr | None = None\n    package_version: NonEmptyStr | None = None\n    package_extras: NonEmptyStr | None = None\n    python_version: (\n        Literal[\n            \"3.9\",\n            \"3.10\",\n            \"3.11\",\n            \"3.12\",\n            \"3.13\",\n            \"3.14\",\n        ]\n        | None\n    ) = None\n    pinned_package_versions_pre: DictStrStr | None = None\n    pinned_package_versions_post: DictStrStr | None = None\n\n    @field_validator(\n        \"package\",\n        \"package_version\",\n        \"package_extras\",\n        mode=\"after\",\n    )\n    @classmethod\n    def validate_commands(cls, value):\n        if value is not None:\n            validate_cmd(value)\n        return value\n\n    @field_validator(\n        \"pinned_package_versions_pre\",\n        \"pinned_package_versions_post\",\n        mode=\"after\",\n    )\n    @classmethod\n    def validate_pinned_package_versions(cls, value):\n        if value is not None:\n            for pkg, version in value.items():\n                validate_cmd(pkg, allow_char=\"[]\")\n                validate_cmd(version)\n        return value\n</code></pre>"},{"location":"reference/app/schemas/v2/task_group/","title":"task_group","text":""},{"location":"reference/app/schemas/v2/task_group/#fractal_server.app.schemas.v2.task_group.TaskGroupCreateStrict","title":"<code>TaskGroupCreateStrict</code>","text":"<p>               Bases: <code>TaskGroupCreate</code></p> <p>A strict version of TaskGroupCreate, to be used for task collection.</p> Source code in <code>fractal_server/app/schemas/v2/task_group.py</code> <pre><code>class TaskGroupCreateStrict(TaskGroupCreate):\n    \"\"\"\n    A strict version of TaskGroupCreate, to be used for task collection.\n    \"\"\"\n\n    path: AbsolutePathStr\n    version: NonEmptyStr\n    venv_path: AbsolutePathStr\n    python_version: NonEmptyStr\n</code></pre>"},{"location":"reference/app/schemas/v2/workflow/","title":"workflow","text":""},{"location":"reference/app/schemas/v2/workflow/#fractal_server.app.schemas.v2.workflow.WorkflowExport","title":"<code>WorkflowExport</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for <code>Workflow</code> export.</p> ATTRIBUTE DESCRIPTION <code>task_list</code> <p> TYPE: <code>list[WorkflowTaskExport]</code> </p> Source code in <code>fractal_server/app/schemas/v2/workflow.py</code> <pre><code>class WorkflowExport(BaseModel):\n    \"\"\"\n    Class for `Workflow` export.\n\n    Attributes:\n        task_list:\n    \"\"\"\n\n    name: str\n    description: str | None\n    task_list: list[WorkflowTaskExport]\n</code></pre>"},{"location":"reference/app/schemas/v2/workflow/#fractal_server.app.schemas.v2.workflow.WorkflowImport","title":"<code>WorkflowImport</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for <code>Workflow</code> import.</p> ATTRIBUTE DESCRIPTION <code>task_list</code> <p> TYPE: <code>list[WorkflowTaskImport]</code> </p> Source code in <code>fractal_server/app/schemas/v2/workflow.py</code> <pre><code>class WorkflowImport(BaseModel):\n    \"\"\"\n    Class for `Workflow` import.\n\n    Attributes:\n        task_list:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    name: NonEmptyStr\n    description: NonEmptyStr | None = None\n    task_list: list[WorkflowTaskImport]\n</code></pre>"},{"location":"reference/app/schemas/v2/workflowtask/","title":"workflowtask","text":""},{"location":"reference/app/schemas/v2/workflowtask/#fractal_server.app.schemas.v2.workflowtask.WorkflowTaskImport","title":"<code>WorkflowTaskImport</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>fractal_server/app/schemas/v2/workflowtask.py</code> <pre><code>class WorkflowTaskImport(BaseModel):\n    model_config = ConfigDict(extra=\"forbid\")\n\n    meta_non_parallel: DictStrAny | None = None\n    meta_parallel: DictStrAny | None = None\n    args_non_parallel: DictStrAny | None = None\n    args_parallel: DictStrAny | None = None\n    type_filters: TypeFilters | None = None\n    input_filters: dict[str, Any] | None = None\n    description: NonEmptyStr | None = None\n    alias: NonEmptyStr | None = None\n\n    task: TaskImport\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def update_legacy_filters(cls, values: dict):\n        \"\"\"\n        Transform legacy filters (created with fractal-server&lt;2.11.0)\n        into type filters\n        \"\"\"\n        if values.get(\"input_filters\") is not None:\n            if \"type_filters\" in values.keys():\n                raise ValueError(\n                    \"Cannot set filters both through the legacy field \"\n                    \"('filters') and the new one ('type_filters').\"\n                )\n            else:\n                # As of 2.11.0, WorkflowTask do not have attribute filters\n                # any more.\n                if values[\"input_filters\"][\"attributes\"] != {}:\n                    raise ValueError(\n                        \"Cannot set attribute filters for WorkflowTasks.\"\n                    )\n                # Convert legacy filters.types into new type_filters\n                values[\"type_filters\"] = values[\"input_filters\"].get(\n                    \"types\", {}\n                )\n                values[\"input_filters\"] = None\n\n        return values\n</code></pre>"},{"location":"reference/app/schemas/v2/workflowtask/#fractal_server.app.schemas.v2.workflowtask.WorkflowTaskImport.update_legacy_filters","title":"<code>update_legacy_filters(values)</code>  <code>classmethod</code>","text":"<p>Transform legacy filters (created with fractal-server&lt;2.11.0) into type filters</p> Source code in <code>fractal_server/app/schemas/v2/workflowtask.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef update_legacy_filters(cls, values: dict):\n    \"\"\"\n    Transform legacy filters (created with fractal-server&lt;2.11.0)\n    into type filters\n    \"\"\"\n    if values.get(\"input_filters\") is not None:\n        if \"type_filters\" in values.keys():\n            raise ValueError(\n                \"Cannot set filters both through the legacy field \"\n                \"('filters') and the new one ('type_filters').\"\n            )\n        else:\n            # As of 2.11.0, WorkflowTask do not have attribute filters\n            # any more.\n            if values[\"input_filters\"][\"attributes\"] != {}:\n                raise ValueError(\n                    \"Cannot set attribute filters for WorkflowTasks.\"\n                )\n            # Convert legacy filters.types into new type_filters\n            values[\"type_filters\"] = values[\"input_filters\"].get(\n                \"types\", {}\n            )\n            values[\"input_filters\"] = None\n\n    return values\n</code></pre>"},{"location":"reference/app/schemas/v2/workflowtask/#fractal_server.app.schemas.v2.workflowtask.WorkflowTaskReplace","title":"<code>WorkflowTaskReplace</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Used by 'replace-task' endpoint</p> Source code in <code>fractal_server/app/schemas/v2/workflowtask.py</code> <pre><code>class WorkflowTaskReplace(BaseModel):\n    \"\"\"Used by 'replace-task' endpoint\"\"\"\n\n    args_non_parallel: dict[str, Any] | None = None\n    args_parallel: dict[str, Any] | None = None\n</code></pre>"},{"location":"reference/app/security/","title":"security","text":"<p>Auth subsystem</p> <p>This module implements the authorisation/authentication subsystem of the Fractal Server. It is based on the FastAPI Users library with support for the SQLModel database adapter.</p> <p>In particular, this module links the appropriate database models, sets up FastAPIUsers with Barer Token and cookie transports and register local routes. Then, for each OAuth client defined in the Fractal Settings configuration, it registers the client and the relative routes.</p> <p>All routes are registered under the <code>auth/</code> prefix.</p>"},{"location":"reference/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync","title":"<code>SQLModelUserDatabaseAsync</code>","text":"<p>               Bases: <code>Generic[UP, ID]</code>, <code>BaseUserDatabase[UP, ID]</code></p> <p>This class is from fastapi_users_db_sqlmodel Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence</p> <p>Database adapter for SQLModel working purely asynchronously.</p> PARAMETER DESCRIPTION <code>user_model</code> <p>SQLModel model of a DB representation of a user.</p> <p> TYPE: <code>type[UP]</code> </p> <code>session</code> <p>SQLAlchemy async session.</p> <p> TYPE: <code>AsyncSession</code> </p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>class SQLModelUserDatabaseAsync(Generic[UP, ID], BaseUserDatabase[UP, ID]):\n    \"\"\"\n    This class is from fastapi_users_db_sqlmodel\n    Original Copyright: 2022 Fran\u00e7ois Voron, released under MIT licence\n\n    Database adapter for SQLModel working purely asynchronously.\n\n    Args:\n        user_model: SQLModel model of a DB representation of a user.\n        session: SQLAlchemy async session.\n    \"\"\"\n\n    session: AsyncSession\n    user_model: type[UP]\n    oauth_account_model: type[OAuthAccount] | None = None\n\n    def __init__(\n        self,\n        session: AsyncSession,\n        user_model: type[UP],\n        oauth_account_model: type[OAuthAccount] | None = None,\n    ):\n        self.session = session\n        self.user_model = user_model\n        self.oauth_account_model = oauth_account_model\n\n    async def get(self, id: ID) -&gt; UP | None:\n        \"\"\"Get a single user by id.\"\"\"\n        return await self.session.get(self.user_model, id)\n\n    async def get_by_email(self, email: str) -&gt; UP | None:\n        \"\"\"Get a single user by email.\"\"\"\n        statement = select(self.user_model).where(\n            func.lower(self.user_model.email) == func.lower(email)\n        )\n        results = await self.session.execute(statement)\n        object = results.first()\n        if object is None:\n            return None\n        return object[0]\n\n    async def get_by_oauth_account(\n        self, oauth: str, account_id: str\n    ) -&gt; UP | None:  # noqa\n        \"\"\"Get a single user by OAuth account id.\"\"\"\n        if self.oauth_account_model is None:\n            raise NotImplementedError()\n        statement = (\n            select(self.oauth_account_model)\n            .where(self.oauth_account_model.oauth_name == oauth)\n            .where(self.oauth_account_model.account_id == account_id)\n            .options(selectinload(self.oauth_account_model.user))  # type: ignore  # noqa\n        )\n        results = await self.session.execute(statement)\n        oauth_account = results.first()\n        if oauth_account:\n            user = oauth_account[0].user  # type: ignore\n            return user\n        return None\n\n    async def create(self, create_dict: dict[str, Any]) -&gt; UP:\n        \"\"\"Create a user.\"\"\"\n        user = self.user_model(**create_dict)\n        self.session.add(user)\n        await self.session.commit()\n        await self.session.refresh(user)\n        return user\n\n    async def update(self, user: UP, update_dict: dict[str, Any]) -&gt; UP:\n        for key, value in update_dict.items():\n            setattr(user, key, value)\n        self.session.add(user)\n        await self.session.commit()\n        await self.session.refresh(user)\n        return user\n\n    async def delete(self, user: UP) -&gt; None:\n        await self.session.delete(user)\n        await self.session.commit()\n\n    async def add_oauth_account(\n        self, user: UP, create_dict: dict[str, Any]\n    ) -&gt; UP:  # noqa\n        if self.oauth_account_model is None:\n            raise NotImplementedError()\n\n        oauth_account = self.oauth_account_model(**create_dict)\n        user.oauth_accounts.append(oauth_account)  # type: ignore\n        self.session.add(user)\n\n        await self.session.commit()\n\n        return user\n\n    async def update_oauth_account(\n        self, user: UP, oauth_account: OAP, update_dict: dict[str, Any]\n    ) -&gt; UP:\n        if self.oauth_account_model is None:\n            raise NotImplementedError()\n\n        for key, value in update_dict.items():\n            setattr(oauth_account, key, value)\n        self.session.add(oauth_account)\n        await self.session.commit()\n\n        return user\n</code></pre>"},{"location":"reference/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.create","title":"<code>create(create_dict)</code>  <code>async</code>","text":"<p>Create a user.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def create(self, create_dict: dict[str, Any]) -&gt; UP:\n    \"\"\"Create a user.\"\"\"\n    user = self.user_model(**create_dict)\n    self.session.add(user)\n    await self.session.commit()\n    await self.session.refresh(user)\n    return user\n</code></pre>"},{"location":"reference/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.get","title":"<code>get(id)</code>  <code>async</code>","text":"<p>Get a single user by id.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def get(self, id: ID) -&gt; UP | None:\n    \"\"\"Get a single user by id.\"\"\"\n    return await self.session.get(self.user_model, id)\n</code></pre>"},{"location":"reference/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.get_by_email","title":"<code>get_by_email(email)</code>  <code>async</code>","text":"<p>Get a single user by email.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def get_by_email(self, email: str) -&gt; UP | None:\n    \"\"\"Get a single user by email.\"\"\"\n    statement = select(self.user_model).where(\n        func.lower(self.user_model.email) == func.lower(email)\n    )\n    results = await self.session.execute(statement)\n    object = results.first()\n    if object is None:\n        return None\n    return object[0]\n</code></pre>"},{"location":"reference/app/security/#fractal_server.app.security.SQLModelUserDatabaseAsync.get_by_oauth_account","title":"<code>get_by_oauth_account(oauth, account_id)</code>  <code>async</code>","text":"<p>Get a single user by OAuth account id.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def get_by_oauth_account(\n    self, oauth: str, account_id: str\n) -&gt; UP | None:  # noqa\n    \"\"\"Get a single user by OAuth account id.\"\"\"\n    if self.oauth_account_model is None:\n        raise NotImplementedError()\n    statement = (\n        select(self.oauth_account_model)\n        .where(self.oauth_account_model.oauth_name == oauth)\n        .where(self.oauth_account_model.account_id == account_id)\n        .options(selectinload(self.oauth_account_model.user))  # type: ignore  # noqa\n    )\n    results = await self.session.execute(statement)\n    oauth_account = results.first()\n    if oauth_account:\n        user = oauth_account[0].user  # type: ignore\n        return user\n    return None\n</code></pre>"},{"location":"reference/app/security/#fractal_server.app.security.UserManager","title":"<code>UserManager</code>","text":"<p>               Bases: <code>IntegerIDMixin</code>, <code>BaseUserManager[UserOAuth, int]</code></p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>class UserManager(IntegerIDMixin, BaseUserManager[UserOAuth, int]):\n    def __init__(self, user_db):\n        \"\"\"\n        Override `__init__` of `BaseUserManager` to define custom\n        `password_helper`.\n        \"\"\"\n        super().__init__(\n            user_db=user_db,\n            password_helper=password_helper,\n        )\n\n    @override\n    async def validate_password(self, password: str, user: UserOAuth) -&gt; None:\n        min_length = 4\n        len_password = len(password)\n        if len_password &lt; min_length:\n            raise InvalidPasswordException(\n                \"The password is too short \"\n                f\"(length = {len_password}, minimum length = {min_length}).\"\n            )\n        max_length_in_bytes = 72\n        len_password_in_bytes = len(password.encode(\"utf-8\"))\n        if len_password_in_bytes &gt; max_length_in_bytes:\n            # See:\n            # https://github.com/pyca/bcrypt/blob/f0451e42e3ab6f6e1b9ac8b09bf04104bf8bdef8/src/_bcrypt/src/lib.rs#L85-L89\n            raise InvalidPasswordException(\n                \"The password is too long \"\n                f\"(length = {len_password_in_bytes} bytes, \"\n                f\"maximum length = {max_length_in_bytes} bytes).\"\n            )\n\n    @override\n    async def oauth_callback(\n        self: Self,\n        oauth_name: str,\n        access_token: str,\n        account_id: str,\n        account_email: str,\n        expires_at: int | None = None,\n        refresh_token: str | None = None,\n        request: Request | None = None,\n        *,\n        associate_by_email: bool = False,\n        is_verified_by_default: bool = False,\n    ) -&gt; UserOAuth:\n        \"\"\"\n        Handle the callback after a successful OAuth authentication.\n\n        This method extends the corresponding `BaseUserManager` method of\n        &gt; fastapi-users v14.0.1, Copyright (c) 2019 Fran\u00e7ois Voron, MIT License\n\n        If the user already exists with this OAuth account, the token is\n        updated.\n\n        If a user with the same e-mail already exists and `associate_by_email`\n        is True, the OAuth account is associated to this user.\n        Otherwise, the `UserNotExists` exception is raised.\n\n        If the user does not exist, send an email to the Fractal admins (if\n        configured) and respond with a 400 error status. NOTE: This is the\n        function branch where the `fractal-server` implementation deviates\n        from the original `fastapi-users` one.\n\n        :param oauth_name: Name of the OAuth client.\n        :param access_token: Valid access token for the service provider.\n        :param account_id: models.ID of the user on the service provider.\n        :param account_email: E-mail of the user on the service provider.\n        :param expires_at: Optional timestamp at which the access token\n        expires.\n        :param refresh_token: Optional refresh token to get a\n        fresh access token from the service provider.\n        :param request: Optional FastAPI request that\n        triggered the operation, defaults to None\n        :param associate_by_email: If True, any existing user with the same\n        e-mail address will be associated to this user. Defaults to False.\n        :param is_verified_by_default: If True, the `is_verified` flag will be\n        set to `True` on newly created user. Make sure the OAuth Provider you\n        are using does verify the email address before enabling this flag.\n        Defaults to False.\n        :return: A user.\n        \"\"\"\n        from fastapi import HTTPException\n        from fastapi import status\n        from fastapi_users import exceptions\n\n        oauth_account_dict = {\n            \"oauth_name\": oauth_name,\n            \"access_token\": access_token,\n            \"account_id\": account_id,\n            \"account_email\": account_email,\n            \"expires_at\": expires_at,\n            \"refresh_token\": refresh_token,\n        }\n\n        try:\n            user = await self.get_by_oauth_account(oauth_name, account_id)\n        except exceptions.UserNotExists:\n            try:\n                # Associate account\n                user = await self.get_by_email(account_email)\n                if not associate_by_email:\n                    raise exceptions.UserAlreadyExists()\n                user = await self.user_db.add_oauth_account(\n                    user, oauth_account_dict\n                )\n            except exceptions.UserNotExists:\n                # (0) Log\n                logger.warning(f\"Self-registration attempt by {account_email}.\")\n\n                # (1) Prepare user-facing error message\n                error_msg = (\n                    \"Thank you for registering for the Fractal service. \"\n                    \"Administrators have been informed to configure your \"\n                    \"account and will get back to you.\"\n                )\n                settings = Inject(get_settings)\n                if settings.FRACTAL_HELP_URL is not None:\n                    error_msg = (\n                        f\"{error_msg}\\n\"\n                        \"You can find more information about the onboarding \"\n                        f\"process at {settings.FRACTAL_HELP_URL}.\"\n                    )\n\n                # (2) Send email to admins\n                email_settings = Inject(get_email_settings)\n                send_fractal_email_or_log_failure(\n                    subject=\"New OAuth self-registration\",\n                    msg=(\n                        f\"User '{account_email}' tried to \"\n                        \"self-register through OAuth.\\n\"\n                        \"Please create the Fractal account manually.\\n\"\n                        \"Here is the error message displayed to the \"\n                        f\"user:\\n{error_msg}\"\n                    ),\n                    email_settings=email_settings.public,\n                )\n\n                # (3) Raise\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=error_msg,\n                )\n        else:\n            # Update oauth\n            for existing_oauth_account in user.oauth_accounts:\n                if (\n                    existing_oauth_account.account_id == account_id\n                    and existing_oauth_account.oauth_name == oauth_name\n                ):\n                    user = await self.user_db.update_oauth_account(\n                        user, existing_oauth_account, oauth_account_dict\n                    )\n\n        return user\n\n    @override\n    async def on_after_register(\n        self, user: UserOAuth, request: Request | None = None\n    ):\n        settings = Inject(get_settings)\n        logger.info(\n            f\"New-user registration completed ({user.id=}, {user.email=}).\"\n        )\n        async for db in get_async_db():\n            # Note: if `FRACTAL_DEFAULT_GROUP_NAME=None`, this query will\n            # result into `None`\n            settings = Inject(get_settings)\n            stm = select(UserGroup.id).where(\n                UserGroup.name == settings.FRACTAL_DEFAULT_GROUP_NAME\n            )\n            res = await db.execute(stm)\n            default_group_id_or_none = res.scalars().one_or_none()\n            if default_group_id_or_none is not None:\n                link = LinkUserGroup(\n                    user_id=user.id, group_id=default_group_id_or_none\n                )\n                db.add(link)\n                await db.commit()\n                logger.info(\n                    f\"Added {user.email} user to group \"\n                    f\"{default_group_id_or_none=}.\"\n                )\n            elif settings.FRACTAL_DEFAULT_GROUP_NAME is not None:\n                logger.error(\n                    \"No group found with name \"\n                    f\"{settings.FRACTAL_DEFAULT_GROUP_NAME}\"\n                )\n</code></pre>"},{"location":"reference/app/security/#fractal_server.app.security.UserManager.__init__","title":"<code>__init__(user_db)</code>","text":"<p>Override <code>__init__</code> of <code>BaseUserManager</code> to define custom <code>password_helper</code>.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>def __init__(self, user_db):\n    \"\"\"\n    Override `__init__` of `BaseUserManager` to define custom\n    `password_helper`.\n    \"\"\"\n    super().__init__(\n        user_db=user_db,\n        password_helper=password_helper,\n    )\n</code></pre>"},{"location":"reference/app/security/#fractal_server.app.security.UserManager.oauth_callback","title":"<code>oauth_callback(oauth_name, access_token, account_id, account_email, expires_at=None, refresh_token=None, request=None, *, associate_by_email=False, is_verified_by_default=False)</code>  <code>async</code>","text":"<p>Handle the callback after a successful OAuth authentication.</p> <p>This method extends the corresponding <code>BaseUserManager</code> method of</p> <p>fastapi-users v14.0.1, Copyright (c) 2019 Fran\u00e7ois Voron, MIT License</p> <p>If the user already exists with this OAuth account, the token is updated.</p> <p>If a user with the same e-mail already exists and <code>associate_by_email</code> is True, the OAuth account is associated to this user. Otherwise, the <code>UserNotExists</code> exception is raised.</p> <p>If the user does not exist, send an email to the Fractal admins (if configured) and respond with a 400 error status. NOTE: This is the function branch where the <code>fractal-server</code> implementation deviates from the original <code>fastapi-users</code> one.</p> <p>:param oauth_name: Name of the OAuth client. :param access_token: Valid access token for the service provider. :param account_id: models.ID of the user on the service provider. :param account_email: E-mail of the user on the service provider. :param expires_at: Optional timestamp at which the access token expires. :param refresh_token: Optional refresh token to get a fresh access token from the service provider. :param request: Optional FastAPI request that triggered the operation, defaults to None :param associate_by_email: If True, any existing user with the same e-mail address will be associated to this user. Defaults to False. :param is_verified_by_default: If True, the <code>is_verified</code> flag will be set to <code>True</code> on newly created user. Make sure the OAuth Provider you are using does verify the email address before enabling this flag. Defaults to False. :return: A user.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>@override\nasync def oauth_callback(\n    self: Self,\n    oauth_name: str,\n    access_token: str,\n    account_id: str,\n    account_email: str,\n    expires_at: int | None = None,\n    refresh_token: str | None = None,\n    request: Request | None = None,\n    *,\n    associate_by_email: bool = False,\n    is_verified_by_default: bool = False,\n) -&gt; UserOAuth:\n    \"\"\"\n    Handle the callback after a successful OAuth authentication.\n\n    This method extends the corresponding `BaseUserManager` method of\n    &gt; fastapi-users v14.0.1, Copyright (c) 2019 Fran\u00e7ois Voron, MIT License\n\n    If the user already exists with this OAuth account, the token is\n    updated.\n\n    If a user with the same e-mail already exists and `associate_by_email`\n    is True, the OAuth account is associated to this user.\n    Otherwise, the `UserNotExists` exception is raised.\n\n    If the user does not exist, send an email to the Fractal admins (if\n    configured) and respond with a 400 error status. NOTE: This is the\n    function branch where the `fractal-server` implementation deviates\n    from the original `fastapi-users` one.\n\n    :param oauth_name: Name of the OAuth client.\n    :param access_token: Valid access token for the service provider.\n    :param account_id: models.ID of the user on the service provider.\n    :param account_email: E-mail of the user on the service provider.\n    :param expires_at: Optional timestamp at which the access token\n    expires.\n    :param refresh_token: Optional refresh token to get a\n    fresh access token from the service provider.\n    :param request: Optional FastAPI request that\n    triggered the operation, defaults to None\n    :param associate_by_email: If True, any existing user with the same\n    e-mail address will be associated to this user. Defaults to False.\n    :param is_verified_by_default: If True, the `is_verified` flag will be\n    set to `True` on newly created user. Make sure the OAuth Provider you\n    are using does verify the email address before enabling this flag.\n    Defaults to False.\n    :return: A user.\n    \"\"\"\n    from fastapi import HTTPException\n    from fastapi import status\n    from fastapi_users import exceptions\n\n    oauth_account_dict = {\n        \"oauth_name\": oauth_name,\n        \"access_token\": access_token,\n        \"account_id\": account_id,\n        \"account_email\": account_email,\n        \"expires_at\": expires_at,\n        \"refresh_token\": refresh_token,\n    }\n\n    try:\n        user = await self.get_by_oauth_account(oauth_name, account_id)\n    except exceptions.UserNotExists:\n        try:\n            # Associate account\n            user = await self.get_by_email(account_email)\n            if not associate_by_email:\n                raise exceptions.UserAlreadyExists()\n            user = await self.user_db.add_oauth_account(\n                user, oauth_account_dict\n            )\n        except exceptions.UserNotExists:\n            # (0) Log\n            logger.warning(f\"Self-registration attempt by {account_email}.\")\n\n            # (1) Prepare user-facing error message\n            error_msg = (\n                \"Thank you for registering for the Fractal service. \"\n                \"Administrators have been informed to configure your \"\n                \"account and will get back to you.\"\n            )\n            settings = Inject(get_settings)\n            if settings.FRACTAL_HELP_URL is not None:\n                error_msg = (\n                    f\"{error_msg}\\n\"\n                    \"You can find more information about the onboarding \"\n                    f\"process at {settings.FRACTAL_HELP_URL}.\"\n                )\n\n            # (2) Send email to admins\n            email_settings = Inject(get_email_settings)\n            send_fractal_email_or_log_failure(\n                subject=\"New OAuth self-registration\",\n                msg=(\n                    f\"User '{account_email}' tried to \"\n                    \"self-register through OAuth.\\n\"\n                    \"Please create the Fractal account manually.\\n\"\n                    \"Here is the error message displayed to the \"\n                    f\"user:\\n{error_msg}\"\n                ),\n                email_settings=email_settings.public,\n            )\n\n            # (3) Raise\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=error_msg,\n            )\n    else:\n        # Update oauth\n        for existing_oauth_account in user.oauth_accounts:\n            if (\n                existing_oauth_account.account_id == account_id\n                and existing_oauth_account.oauth_name == oauth_name\n            ):\n                user = await self.user_db.update_oauth_account(\n                    user, existing_oauth_account, oauth_account_dict\n                )\n\n    return user\n</code></pre>"},{"location":"reference/app/security/#fractal_server.app.security._create_first_group","title":"<code>_create_first_group()</code>","text":"<p>Create a <code>UserGroup</code> named <code>FRACTAL_DEFAULT_GROUP_NAME</code>, if this variable is set and if such a group does not already exist.</p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>def _create_first_group():\n    \"\"\"\n    Create a `UserGroup` named `FRACTAL_DEFAULT_GROUP_NAME`, if this variable\n    is set and if such a group does not already exist.\n    \"\"\"\n    settings = Inject(get_settings)\n    function_logger = set_logger(\"fractal_server.create_first_group\")\n\n    if settings.FRACTAL_DEFAULT_GROUP_NAME is None:\n        function_logger.info(\n            f\"SKIP because '{settings.FRACTAL_DEFAULT_GROUP_NAME=}'\"\n        )\n        return\n\n    function_logger.info(f\"START, name '{settings.FRACTAL_DEFAULT_GROUP_NAME}'\")\n    with next(get_sync_db()) as db:\n        group_all = db.execute(\n            select(UserGroup).where(\n                UserGroup.name == settings.FRACTAL_DEFAULT_GROUP_NAME\n            )\n        )\n        if group_all.scalars().one_or_none() is None:\n            first_group = UserGroup(name=settings.FRACTAL_DEFAULT_GROUP_NAME)\n            db.add(first_group)\n            db.commit()\n            function_logger.info(\n                f\"Created group '{settings.FRACTAL_DEFAULT_GROUP_NAME}'\"\n            )\n        else:\n            function_logger.info(\n                f\"Group '{settings.FRACTAL_DEFAULT_GROUP_NAME}' \"\n                \"already exists, skip.\"\n            )\n    function_logger.info(\"END\")\n    close_logger(function_logger)\n</code></pre>"},{"location":"reference/app/security/#fractal_server.app.security._create_first_user","title":"<code>_create_first_user(email, password, project_dir, profile_id=None, is_superuser=False, is_verified=False)</code>  <code>async</code>","text":"<p>Private method to create the first fractal-server user</p> <p>Create a user with the given default arguments and return a message with the relevant information. If the user already exists, for example after a restart, it returns a message to inform that user already exists.</p> <p>WARNING: This function is only meant to create the first user, and then it catches and ignores <code>IntegrityError</code>s (when multiple workers may be trying to concurrently create the first user). This is not the expected behavior for regular user creation, which must rather happen via the /auth/register endpoint.</p> <p>See fastapi_users docs</p> PARAMETER DESCRIPTION <code>email</code> <p>New user's email</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>New user's password</p> <p> TYPE: <code>str</code> </p> <code>is_superuser</code> <p><code>True</code> if the new user is a superuser</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_verified</code> <p><code>True</code> if the new user is verified</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>fractal_server/app/security/__init__.py</code> <pre><code>async def _create_first_user(\n    email: str,\n    password: str,\n    project_dir: str,\n    profile_id: int | None = None,\n    is_superuser: bool = False,\n    is_verified: bool = False,\n) -&gt; None:\n    \"\"\"\n    Private method to create the first fractal-server user\n\n    Create a user with the given default arguments and return a message with\n    the relevant information. If the user already exists, for example after a\n    restart, it returns a message to inform that user already exists.\n\n    **WARNING**: This function is only meant to create the first user, and then\n    it catches and ignores `IntegrityError`s (when multiple workers may be\n    trying to concurrently create the first user). This is not the expected\n    behavior for regular user creation, which must rather happen via the\n    /auth/register endpoint.\n\n    See [fastapi_users docs](https://fastapi-users.github.io/fastapi-users/\n    12.1/cookbook/create-user-programmatically)\n\n    Args:\n        email: New user's email\n        password: New user's password\n        is_superuser: `True` if the new user is a superuser\n        is_verified: `True` if the new user is verified\n    \"\"\"\n    function_logger = set_logger(\"fractal_server.create_first_user\")\n    function_logger.info(f\"START _create_first_user, with email '{email}'\")\n    try:\n        async with get_async_session_context() as session:\n            if is_superuser is True:\n                # If a superuser already exists, exit\n                stm = select(UserOAuth).where(  # noqa\n                    UserOAuth.is_superuser == True  # noqa\n                )  # noqa\n                res = await session.execute(stm)\n                existing_superuser = res.scalars().first()\n                if existing_superuser is not None:\n                    function_logger.info(\n                        f\"'{existing_superuser.email}' superuser already \"\n                        f\"exists, skip creation of '{email}'\"\n                    )\n                    return None\n\n            async with get_user_db_context(session) as user_db:\n                async with get_user_manager_context(user_db) as user_manager:\n                    kwargs = dict(\n                        email=email,\n                        password=password,\n                        project_dirs=[project_dir],\n                        profile_id=profile_id,\n                        is_superuser=is_superuser,\n                        is_verified=is_verified,\n                    )\n                    user = await user_manager.create(UserCreate(**kwargs))\n                    function_logger.info(f\"User '{user.email}' created\")\n    except UserAlreadyExists:\n        function_logger.warning(f\"User '{email}' already exists\")\n    except Exception as e:\n        function_logger.error(\n            f\"ERROR in _create_first_user, original error {str(e)}\"\n        )\n        raise e\n    finally:\n        function_logger.info(f\"END   _create_first_user, with email '{email}'\")\n        close_logger(function_logger)\n</code></pre>"},{"location":"reference/app/security/signup_email/","title":"signup_email","text":""},{"location":"reference/app/security/signup_email/#fractal_server.app.security.signup_email.send_fractal_email_or_log_failure","title":"<code>send_fractal_email_or_log_failure(*, subject, msg, email_settings)</code>","text":"<p>Send an email using the specified settings, or log about failure.</p> Source code in <code>fractal_server/app/security/signup_email.py</code> <pre><code>def send_fractal_email_or_log_failure(\n    *,\n    subject: str,\n    msg: str,\n    email_settings: PublicEmailSettings | None,\n) -&gt; None:\n    \"\"\"\n    Send an email using the specified settings, or log about failure.\n    \"\"\"\n\n    if email_settings is None:\n        logger.error(\n            f\"Cannot send email with {subject=}, because {email_settings=}.\"\n        )\n        return\n\n    try:\n        logger.info(f\"START sending email with {subject=}.\")\n        mail_msg = EmailMessage()\n        mail_msg.set_content(msg)\n        mail_msg[\"From\"] = formataddr(\n            (email_settings.sender, email_settings.sender)\n        )\n        mail_msg[\"To\"] = \", \".join(\n            [\n                formataddr((recipient, recipient))\n                for recipient in email_settings.recipients\n            ]\n        )\n        mail_msg[\"Subject\"] = (\n            f\"[Fractal, {email_settings.instance_name}] {subject}\"\n        )\n        with smtplib.SMTP(\n            email_settings.smtp_server,\n            email_settings.port,\n        ) as server:\n            server.ehlo()\n            if email_settings.use_starttls:\n                server.starttls()\n                server.ehlo()\n            if email_settings.use_login:\n                server.login(\n                    user=email_settings.sender,\n                    password=email_settings.password.get_secret_value(),\n                )\n            server.sendmail(\n                from_addr=email_settings.sender,\n                to_addrs=email_settings.recipients,\n                msg=mail_msg.as_string(),\n            )\n        logger.info(f\"END sending email with {subject=}.\")\n\n    except Exception as e:\n        logger.error(\n            f\"Could not send self-registration email, original error: {str(e)}.\"\n        )\n</code></pre>"},{"location":"reference/config/","title":"config","text":""},{"location":"reference/config/_data/","title":"_data","text":""},{"location":"reference/config/_database/","title":"_database","text":""},{"location":"reference/config/_database/#fractal_server.config._database.DatabaseSettings","title":"<code>DatabaseSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Minimal set of configurations needed for operating on the database (e.g for schema migrations).</p> ATTRIBUTE DESCRIPTION <code>DB_ECHO</code> <p>If <code>\"true\"</code>, make database operations verbose.</p> <p> TYPE: <code>Literal['true', 'false']</code> </p> <code>POSTGRES_USER</code> <p>User to use when connecting to the PostgreSQL database.</p> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>POSTGRES_PASSWORD</code> <p>Password to use when connecting to the PostgreSQL database.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>POSTGRES_HOST</code> <p>URL to the PostgreSQL server or path to a UNIX domain socket.</p> <p> TYPE: <code>NonEmptyStr</code> </p> <code>POSTGRES_PORT</code> <p>Port number to use when connecting to the PostgreSQL server.</p> <p> TYPE: <code>NonNegativeInt</code> </p> <code>POSTGRES_DB</code> <p>Name of the PostgreSQL database to connect to.</p> <p> TYPE: <code>NonEmptyStr</code> </p> Source code in <code>fractal_server/config/_database.py</code> <pre><code>class DatabaseSettings(BaseSettings):\n    \"\"\"\n    Minimal set of configurations needed for operating on the database (e.g\n    for schema migrations).\n\n    Attributes:\n        DB_ECHO:\n            If `\"true\"`, make database operations verbose.\n        POSTGRES_USER:\n            User to use when connecting to the PostgreSQL database.\n        POSTGRES_PASSWORD:\n            Password to use when connecting to the PostgreSQL database.\n        POSTGRES_HOST:\n            URL to the PostgreSQL server or path to a UNIX domain socket.\n        POSTGRES_PORT:\n            Port number to use when connecting to the PostgreSQL server.\n        POSTGRES_DB:\n            Name of the PostgreSQL database to connect to.\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    DB_ECHO: Literal[\"true\", \"false\"] = \"false\"\n    POSTGRES_USER: NonEmptyStr | None = None\n    POSTGRES_PASSWORD: SecretStr | None = None\n    POSTGRES_HOST: NonEmptyStr = \"localhost\"\n    POSTGRES_PORT: NonNegativeInt = 5432\n    POSTGRES_DB: NonEmptyStr\n\n    @property\n    def DATABASE_URL(self) -&gt; URL:\n        if self.POSTGRES_PASSWORD is None:\n            password = None\n        else:\n            password = self.POSTGRES_PASSWORD.get_secret_value()\n\n        url = URL.create(\n            drivername=\"postgresql+psycopg\",\n            username=self.POSTGRES_USER,\n            password=password,\n            host=self.POSTGRES_HOST,\n            port=self.POSTGRES_PORT,\n            database=self.POSTGRES_DB,\n        )\n        return url\n</code></pre>"},{"location":"reference/config/_email/","title":"_email","text":""},{"location":"reference/config/_email/#fractal_server.config._email.EmailSettings","title":"<code>EmailSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Class with settings for email-sending feature.</p> ATTRIBUTE DESCRIPTION <code>FRACTAL_EMAIL_SENDER</code> <p>Address of the OAuth-signup email sender.</p> <p> TYPE: <code>EmailStr | None</code> </p> <code>FRACTAL_EMAIL_PASSWORD</code> <p>Password for the OAuth-signup email sender.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>FRACTAL_EMAIL_SMTP_SERVER</code> <p>SMTP server for the OAuth-signup emails.</p> <p> TYPE: <code>str | None</code> </p> <code>FRACTAL_EMAIL_SMTP_PORT</code> <p>SMTP server port for the OAuth-signup emails.</p> <p> TYPE: <code>int | None</code> </p> <code>FRACTAL_EMAIL_INSTANCE_NAME</code> <p>Fractal instance name, to be included in the OAuth-signup emails.</p> <p> TYPE: <code>str | None</code> </p> <code>FRACTAL_EMAIL_RECIPIENTS</code> <p>Comma-separated list of recipients of the OAuth-signup emails.</p> <p> TYPE: <code>str | None</code> </p> <code>FRACTAL_EMAIL_USE_STARTTLS</code> <p>Whether to use StartTLS when using the SMTP server.</p> <p> TYPE: <code>Literal['true', 'false']</code> </p> <code>FRACTAL_EMAIL_USE_LOGIN</code> <p>Whether to use login when using the SMTP server. If 'true', FRACTAL_EMAIL_PASSWORD  must be provided.</p> <p> TYPE: <code>Literal['true', 'false']</code> </p> Source code in <code>fractal_server/config/_email.py</code> <pre><code>class EmailSettings(BaseSettings):\n    \"\"\"\n    Class with settings for email-sending feature.\n\n    Attributes:\n        FRACTAL_EMAIL_SENDER:\n            Address of the OAuth-signup email sender.\n        FRACTAL_EMAIL_PASSWORD:\n            Password for the OAuth-signup email sender.\n        FRACTAL_EMAIL_SMTP_SERVER:\n            SMTP server for the OAuth-signup emails.\n        FRACTAL_EMAIL_SMTP_PORT:\n            SMTP server port for the OAuth-signup emails.\n        FRACTAL_EMAIL_INSTANCE_NAME:\n            Fractal instance name, to be included in the OAuth-signup emails.\n        FRACTAL_EMAIL_RECIPIENTS:\n            Comma-separated list of recipients of the OAuth-signup emails.\n        FRACTAL_EMAIL_USE_STARTTLS:\n            Whether to use StartTLS when using the SMTP server.\n        FRACTAL_EMAIL_USE_LOGIN:\n            Whether to use login when using the SMTP server.\n            If 'true', FRACTAL_EMAIL_PASSWORD  must be provided.\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    FRACTAL_EMAIL_SENDER: EmailStr | None = None\n    FRACTAL_EMAIL_PASSWORD: SecretStr | None = None\n    FRACTAL_EMAIL_SMTP_SERVER: str | None = None\n    FRACTAL_EMAIL_SMTP_PORT: int | None = None\n    FRACTAL_EMAIL_INSTANCE_NAME: str | None = None\n    FRACTAL_EMAIL_RECIPIENTS: str | None = None\n    FRACTAL_EMAIL_USE_STARTTLS: Literal[\"true\", \"false\"] = \"true\"\n    FRACTAL_EMAIL_USE_LOGIN: Literal[\"true\", \"false\"] = \"true\"\n\n    public: PublicEmailSettings | None = None\n    \"\"\"\n    The validated field which is actually used in `fractal-server`,\n    automatically populated upon creation.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def validate_email_settings(self: Self) -&gt; Self:\n        \"\"\"\n        Set `self.public`.\n        \"\"\"\n\n        email_values = [\n            self.FRACTAL_EMAIL_SENDER,\n            self.FRACTAL_EMAIL_SMTP_SERVER,\n            self.FRACTAL_EMAIL_SMTP_PORT,\n            self.FRACTAL_EMAIL_INSTANCE_NAME,\n            self.FRACTAL_EMAIL_RECIPIENTS,\n        ]\n        if len(set(email_values)) == 1:\n            # All required EMAIL attributes are None\n            pass\n        elif None in email_values:\n            # Not all required EMAIL attributes are set\n            error_msg = (\n                \"Invalid FRACTAL_EMAIL configuration. \"\n                f\"Given values: {email_values}.\"\n            )\n            raise ValueError(error_msg)\n        else:\n            use_starttls = self.FRACTAL_EMAIL_USE_STARTTLS == \"true\"\n            use_login = self.FRACTAL_EMAIL_USE_LOGIN == \"true\"\n\n            if use_login and self.FRACTAL_EMAIL_PASSWORD is None:\n                raise ValueError(\n                    \"'FRACTAL_EMAIL_USE_LOGIN' is 'true' but \"\n                    \"'FRACTAL_EMAIL_PASSWORD' is not provided.\"\n                )\n\n            self.public = PublicEmailSettings(\n                sender=self.FRACTAL_EMAIL_SENDER,\n                recipients=self.FRACTAL_EMAIL_RECIPIENTS.split(\",\"),\n                smtp_server=self.FRACTAL_EMAIL_SMTP_SERVER,\n                port=self.FRACTAL_EMAIL_SMTP_PORT,\n                password=self.FRACTAL_EMAIL_PASSWORD,\n                instance_name=self.FRACTAL_EMAIL_INSTANCE_NAME,\n                use_starttls=use_starttls,\n                use_login=use_login,\n            )\n\n        return self\n</code></pre>"},{"location":"reference/config/_email/#fractal_server.config._email.EmailSettings.public","title":"<code>public = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The validated field which is actually used in <code>fractal-server</code>, automatically populated upon creation.</p>"},{"location":"reference/config/_email/#fractal_server.config._email.EmailSettings.validate_email_settings","title":"<code>validate_email_settings()</code>","text":"<p>Set <code>self.public</code>.</p> Source code in <code>fractal_server/config/_email.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_email_settings(self: Self) -&gt; Self:\n    \"\"\"\n    Set `self.public`.\n    \"\"\"\n\n    email_values = [\n        self.FRACTAL_EMAIL_SENDER,\n        self.FRACTAL_EMAIL_SMTP_SERVER,\n        self.FRACTAL_EMAIL_SMTP_PORT,\n        self.FRACTAL_EMAIL_INSTANCE_NAME,\n        self.FRACTAL_EMAIL_RECIPIENTS,\n    ]\n    if len(set(email_values)) == 1:\n        # All required EMAIL attributes are None\n        pass\n    elif None in email_values:\n        # Not all required EMAIL attributes are set\n        error_msg = (\n            \"Invalid FRACTAL_EMAIL configuration. \"\n            f\"Given values: {email_values}.\"\n        )\n        raise ValueError(error_msg)\n    else:\n        use_starttls = self.FRACTAL_EMAIL_USE_STARTTLS == \"true\"\n        use_login = self.FRACTAL_EMAIL_USE_LOGIN == \"true\"\n\n        if use_login and self.FRACTAL_EMAIL_PASSWORD is None:\n            raise ValueError(\n                \"'FRACTAL_EMAIL_USE_LOGIN' is 'true' but \"\n                \"'FRACTAL_EMAIL_PASSWORD' is not provided.\"\n            )\n\n        self.public = PublicEmailSettings(\n            sender=self.FRACTAL_EMAIL_SENDER,\n            recipients=self.FRACTAL_EMAIL_RECIPIENTS.split(\",\"),\n            smtp_server=self.FRACTAL_EMAIL_SMTP_SERVER,\n            port=self.FRACTAL_EMAIL_SMTP_PORT,\n            password=self.FRACTAL_EMAIL_PASSWORD,\n            instance_name=self.FRACTAL_EMAIL_INSTANCE_NAME,\n            use_starttls=use_starttls,\n            use_login=use_login,\n        )\n\n    return self\n</code></pre>"},{"location":"reference/config/_email/#fractal_server.config._email.PublicEmailSettings","title":"<code>PublicEmailSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for <code>EmailSettings.public</code>, namely the ready-to-use settings.</p> ATTRIBUTE DESCRIPTION <code>sender</code> <p>Sender email address.</p> <p> TYPE: <code>EmailStr</code> </p> <code>recipients</code> <p>List of recipients email address.</p> <p> TYPE: <code>list[EmailStr]</code> </p> <code>smtp_server</code> <p>SMTP server address.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>SMTP server port.</p> <p> TYPE: <code>int</code> </p> <code>password</code> <p>Sender password.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>instance_name</code> <p>Name of SMTP server instance.</p> <p> TYPE: <code>str</code> </p> <code>use_starttls</code> <p>Whether to use the security protocol.</p> <p> TYPE: <code>bool</code> </p> <code>use_login</code> <p>Whether to use login.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>fractal_server/config/_email.py</code> <pre><code>class PublicEmailSettings(BaseModel):\n    \"\"\"\n    Schema for `EmailSettings.public`, namely the ready-to-use settings.\n\n    Attributes:\n        sender: Sender email address.\n        recipients: List of recipients email address.\n        smtp_server: SMTP server address.\n        port: SMTP server port.\n        password: Sender password.\n        instance_name: Name of SMTP server instance.\n        use_starttls: Whether to use the security protocol.\n        use_login: Whether to use login.\n    \"\"\"\n\n    sender: EmailStr\n    recipients: list[EmailStr] = Field(min_length=1)\n    smtp_server: str\n    port: int\n    password: SecretStr | None = None\n    instance_name: str\n    use_starttls: bool\n    use_login: bool\n</code></pre>"},{"location":"reference/config/_main/","title":"_main","text":""},{"location":"reference/config/_main/#fractal_server.config._main.Settings","title":"<code>Settings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Contains the general configuration variables for Fractal Server.</p> ATTRIBUTE DESCRIPTION <code>JWT_EXPIRE_SECONDS</code> <p>JWT token lifetime, in seconds.</p> <p> TYPE: <code>int</code> </p> <code>JWT_SECRET_KEY</code> <p>JWT secret. \u26a0\ufe0f Set this variable to a secure string, and do not disclose it.</p> <p> TYPE: <code>SecretStr</code> </p> <code>COOKIE_EXPIRE_SECONDS</code> <p>Cookie token lifetime, in seconds.</p> <p> TYPE: <code>int</code> </p> <code>FRACTAL_RUNNER_BACKEND</code> <p>Select which runner backend to use.</p> <p> TYPE: <code>Literal['local', 'slurm_ssh', 'slurm_sudo']</code> </p> <code>FRACTAL_LOGGING_LEVEL</code> <p>Logging-level threshold for logging Only logs of with this level (or higher) will appear in the console logs.</p> <p> TYPE: <code>int</code> </p> <code>FRACTAL_API_MAX_JOB_LIST_LENGTH</code> <p>Number of ids that can be stored in the <code>jobs</code> attribute of <code>app.state</code>.</p> <p> TYPE: <code>int</code> </p> <code>FRACTAL_GRACEFUL_SHUTDOWN_TIME</code> <p>Waiting time for the shutdown phase of executors, in seconds.</p> <p> TYPE: <code>float</code> </p> <code>FRACTAL_HELP_URL</code> <p>The URL of an instance-specific Fractal help page.</p> <p> TYPE: <code>HttpUrl | None</code> </p> <code>FRACTAL_DEFAULT_GROUP_NAME</code> <p>Name of the default user group.</p> <p>If set to <code>\"All\"</code>, then the user group with that name is a special user group (e.g. it cannot be deleted, and new users are automatically added to it). If set to <code>None</code> (the default value), then user groups are all equivalent, independently on their name.</p> <p> TYPE: <code>Literal['All'] | None</code> </p> <code>FRACTAL_LONG_REQUEST_TIME</code> <p>Time limit beyond which the execution of an API request is considered slow and an appropriate warning is logged by the middleware.</p> <p> TYPE: <code>float</code> </p> Source code in <code>fractal_server/config/_main.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"\n    Contains the general configuration variables for Fractal Server.\n\n    Attributes:\n        JWT_EXPIRE_SECONDS:\n            JWT token lifetime, in seconds.\n        JWT_SECRET_KEY:\n            JWT secret.&lt;br&gt;\n            \u26a0\ufe0f Set this variable to a secure string, and do not disclose it.\n        COOKIE_EXPIRE_SECONDS:\n            Cookie token lifetime, in seconds.\n        FRACTAL_RUNNER_BACKEND:\n            Select which runner backend to use.\n        FRACTAL_LOGGING_LEVEL:\n            Logging-level threshold for logging\n            Only logs of with this level (or higher) will appear in the console\n            logs.\n        FRACTAL_API_MAX_JOB_LIST_LENGTH:\n            Number of ids that can be stored in the `jobs` attribute of\n            `app.state`.\n        FRACTAL_GRACEFUL_SHUTDOWN_TIME:\n            Waiting time for the shutdown phase of executors, in seconds.\n        FRACTAL_HELP_URL:\n            The URL of an instance-specific Fractal help page.\n        FRACTAL_DEFAULT_GROUP_NAME:\n            Name of the default user group.\n\n            If set to `\"All\"`, then the user group with that name is a special\n            user group (e.g. it cannot be deleted, and new users are\n            automatically added to it). If set to `None` (the default value),\n            then user groups are all equivalent, independently on their name.\n        FRACTAL_LONG_REQUEST_TIME:\n            Time limit beyond which the execution of an API request is\n            considered *slow* and an appropriate warning is logged by the\n            middleware.\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    JWT_EXPIRE_SECONDS: int = 180\n    JWT_SECRET_KEY: SecretStr\n    COOKIE_EXPIRE_SECONDS: int = 86400\n    # Note: we do not use ResourceType here to avoid circular imports\n    FRACTAL_RUNNER_BACKEND: Literal[\"local\", \"slurm_ssh\", \"slurm_sudo\"] = (\n        \"local\"\n    )\n    FRACTAL_LOGGING_LEVEL: int = logging.INFO\n    FRACTAL_API_MAX_JOB_LIST_LENGTH: int = 25\n    FRACTAL_GRACEFUL_SHUTDOWN_TIME: float = 30.0\n    FRACTAL_HELP_URL: HttpUrl | None = None\n    FRACTAL_DEFAULT_GROUP_NAME: Literal[\"All\"] | None = None\n    FRACTAL_LONG_REQUEST_TIME: float = 30.0\n</code></pre>"},{"location":"reference/config/_oauth/","title":"_oauth","text":""},{"location":"reference/config/_oauth/#fractal_server.config._oauth.OAuthSettings","title":"<code>OAuthSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings for integration with an OAuth identity provider.</p> ATTRIBUTE DESCRIPTION <code>OAUTH_CLIENT_NAME</code> <p>Name of the client.</p> <p> TYPE: <code>Annotated[NonEmptyStr, StringConstraints(to_lower=True)] | None</code> </p> <code>OAUTH_CLIENT_ID</code> <p>ID of client.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>OAUTH_CLIENT_SECRET</code> <p>Secret to authorise against the identity provider.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>OAUTH_OIDC_CONFIG_ENDPOINT</code> <p>OpenID Connect configuration endpoint, for autodiscovery of relevant endpoints.</p> <p> TYPE: <code>SecretStr | None</code> </p> <code>OAUTH_REDIRECT_URL</code> <p>String to be used as <code>redirect_url</code> argument in <code>fastapi_users.get_oauth_router</code>, and then in <code>httpx_oauth.integrations.fastapi.OAuth2AuthorizeCallback</code>.</p> <p> TYPE: <code>str | None</code> </p> <code>OAUTH_EMAIL_CLAIM</code> <p>Name of the OIDC claim with the user's email address. This is <code>\"email\"</code> by default, but can be customized (e.g. to <code>\"mail\"</code>) to fit with the response from the userinfo endpoint - see https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/config/_oauth.py</code> <pre><code>class OAuthSettings(BaseSettings):\n    \"\"\"\n    Settings for integration with an OAuth identity provider.\n\n    Attributes:\n        OAUTH_CLIENT_NAME: Name of the client.\n        OAUTH_CLIENT_ID: ID of client.\n        OAUTH_CLIENT_SECRET:\n            Secret to authorise against the identity provider.\n        OAUTH_OIDC_CONFIG_ENDPOINT:\n            OpenID Connect configuration endpoint, for autodiscovery of\n            relevant endpoints.\n        OAUTH_REDIRECT_URL:\n            String to be used as `redirect_url` argument in\n            `fastapi_users.get_oauth_router`, and then in\n            `httpx_oauth.integrations.fastapi.OAuth2AuthorizeCallback`.\n        OAUTH_EMAIL_CLAIM:\n            Name of the OIDC claim with the user's email address. This is\n            `\"email\"` by default, but can be customized (e.g. to `\"mail\"`) to\n            fit with the response from the userinfo endpoint - see\n            https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse\n    \"\"\"\n\n    model_config = SettingsConfigDict(**SETTINGS_CONFIG_DICT)\n\n    OAUTH_CLIENT_NAME: (\n        Annotated[\n            NonEmptyStr,\n            StringConstraints(to_lower=True),\n        ]\n        | None\n    ) = None\n    OAUTH_CLIENT_ID: SecretStr | None = None\n    OAUTH_CLIENT_SECRET: SecretStr | None = None\n    OAUTH_OIDC_CONFIG_ENDPOINT: SecretStr | None = None\n    OAUTH_REDIRECT_URL: str | None = None\n    OAUTH_EMAIL_CLAIM: str = \"email\"\n\n    @model_validator(mode=\"after\")\n    def check_configuration(self: Self) -&gt; Self:\n        if (\n            self.OAUTH_CLIENT_NAME not in [\"google\", \"github\", None]\n            and self.OAUTH_OIDC_CONFIG_ENDPOINT is None\n        ):\n            raise ValueError(\n                f\"self.OAUTH_OIDC_CONFIG_ENDPOINT=None but \"\n                f\"{self.OAUTH_CLIENT_NAME=}\"\n            )\n        return self\n\n    @property\n    def is_set(self) -&gt; bool:\n        return None not in (\n            self.OAUTH_CLIENT_NAME,\n            self.OAUTH_CLIENT_ID,\n            self.OAUTH_CLIENT_SECRET,\n        )\n</code></pre>"},{"location":"reference/config/_settings_config/","title":"_settings_config","text":""},{"location":"reference/images/","title":"images","text":""},{"location":"reference/images/models/","title":"models","text":""},{"location":"reference/images/models/#fractal_server.images.models.SingleImage","title":"<code>SingleImage</code>","text":"<p>               Bases: <code>SingleImageBase</code></p> <p><code>SingleImageBase</code>, with scalar <code>attributes</code> values (<code>None</code> excluded).</p> Source code in <code>fractal_server/images/models.py</code> <pre><code>class SingleImage(SingleImageBase):\n    \"\"\"\n    `SingleImageBase`, with scalar `attributes` values (`None` excluded).\n    \"\"\"\n\n    attributes: ImageAttributes = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/images/models/#fractal_server.images.models.SingleImageBase","title":"<code>SingleImageBase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base for <code>SingleImage</code> and <code>SingleImageTaskOutput</code>.</p> ATTRIBUTE DESCRIPTION <code>zarr_url</code> <p> TYPE: <code>ZarrUrlStr</code> </p> <code>origin</code> <p> TYPE: <code>ZarrUrlStr | None</code> </p> <code>attributes</code> <p> TYPE: <code>DictStrAny</code> </p> <code>types</code> <p> TYPE: <code>ImageTypes</code> </p> Source code in <code>fractal_server/images/models.py</code> <pre><code>class SingleImageBase(BaseModel):\n    \"\"\"\n    Base for `SingleImage` and `SingleImageTaskOutput`.\n\n    Attributes:\n        zarr_url:\n        origin:\n        attributes:\n        types:\n    \"\"\"\n\n    zarr_url: ZarrUrlStr\n    origin: ZarrUrlStr | None = None\n\n    attributes: DictStrAny = Field(default_factory=dict)\n    types: ImageTypes = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/images/models/#fractal_server.images.models.SingleImageTaskOutput","title":"<code>SingleImageTaskOutput</code>","text":"<p>               Bases: <code>SingleImageBase</code></p> <p><code>SingleImageBase</code>, with scalar <code>attributes</code> values (<code>None</code> included).</p> Source code in <code>fractal_server/images/models.py</code> <pre><code>class SingleImageTaskOutput(SingleImageBase):\n    \"\"\"\n    `SingleImageBase`, with scalar `attributes` values (`None` included).\n    \"\"\"\n\n    attributes: ImageAttributesWithNone = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/images/status_tools/","title":"status_tools","text":""},{"location":"reference/images/status_tools/#fractal_server.images.status_tools._postprocess_image_lists","title":"<code>_postprocess_image_lists(target_images, list_query_url_status)</code>","text":"Source code in <code>fractal_server/images/status_tools.py</code> <pre><code>def _postprocess_image_lists(\n    target_images: list[dict[str, Any]],\n    list_query_url_status: list[tuple[str, str]],\n) -&gt; list[dict[str, Any]]:\n    \"\"\" \"\"\"\n    t_1 = time.perf_counter()\n\n    # Select only processed images that are part of the target image set\n    zarr_url_to_image = {img[\"zarr_url\"]: img for img in target_images}\n    target_zarr_urls = zarr_url_to_image.keys()\n    list_processed_url_status = [\n        url_status\n        for url_status in list_query_url_status\n        if url_status[0] in target_zarr_urls\n    ]\n\n    set_processed_urls = set(\n        url_status[0] for url_status in list_processed_url_status\n    )\n    processed_images_with_status = [\n        _enriched_image(\n            img=zarr_url_to_image[item[0]],\n            status=item[1],\n        )\n        for item in list_processed_url_status\n    ]\n\n    non_processed_urls = target_zarr_urls - set_processed_urls\n    non_processed_images_with_status = [\n        _enriched_image(\n            img=zarr_url_to_image[zarr_url],\n            status=HistoryUnitStatusWithUnset.UNSET,\n        )\n        for zarr_url in non_processed_urls\n    ]\n    t_2 = time.perf_counter()\n    logger.debug(\n        f\"[enrich_images_async] post-processing, elapsed={t_2 - t_1:.5f} s\"\n    )\n\n    return processed_images_with_status + non_processed_images_with_status\n</code></pre>"},{"location":"reference/images/status_tools/#fractal_server.images.status_tools._prepare_query","title":"<code>_prepare_query(*, dataset_id, workflowtask_id)</code>","text":"<p>Note: the query does not include <code>.order_by</code>.</p> Source code in <code>fractal_server/images/status_tools.py</code> <pre><code>def _prepare_query(\n    *,\n    dataset_id: int,\n    workflowtask_id: int,\n) -&gt; Select:\n    \"\"\"\n    Note: the query does not include `.order_by`.\n    \"\"\"\n    stm = (\n        select(HistoryImageCache.zarr_url, HistoryUnit.status)\n        .join(\n            HistoryUnit,\n            HistoryImageCache.latest_history_unit_id == HistoryUnit.id,\n        )\n        .where(HistoryImageCache.dataset_id == dataset_id)\n        .where(HistoryImageCache.workflowtask_id == workflowtask_id)\n    )\n    return stm\n</code></pre>"},{"location":"reference/images/status_tools/#fractal_server.images.status_tools.enrich_images_unsorted_async","title":"<code>enrich_images_unsorted_async(*, images, dataset_id, workflowtask_id, db)</code>  <code>async</code>","text":"<p>Enrich images with a status-related attribute.</p> PARAMETER DESCRIPTION <code>images</code> <p>The input image list</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> <code>dataset_id</code> <p>The dataset ID</p> <p> TYPE: <code>int</code> </p> <code>workflowtask_id</code> <p>The workflow-task ID</p> <p> TYPE: <code>int</code> </p> <code>db</code> <p>An async db session</p> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>The list of enriched images, not necessarily in the same order as</p> <code>list[dict[str, Any]]</code> <p>the input.</p> Source code in <code>fractal_server/images/status_tools.py</code> <pre><code>async def enrich_images_unsorted_async(\n    *,\n    images: list[dict[str, Any]],\n    dataset_id: int,\n    workflowtask_id: int,\n    db: AsyncSession,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Enrich images with a status-related attribute.\n\n    Args:\n        images: The input image list\n        dataset_id: The dataset ID\n        workflowtask_id: The workflow-task ID\n        db: An async db session\n\n    Returns:\n        The list of enriched images, not necessarily in the same order as\n        the input.\n    \"\"\"\n    t_0 = time.perf_counter()\n    logger.info(\n        f\"[enrich_images_async] START, {dataset_id=}, {workflowtask_id=}\"\n    )\n\n    # Get `(zarr_url, status)` for _all_ processed images (including those that\n    # are not part of the target image set)\n    res = await db.execute(\n        _prepare_query(\n            dataset_id=dataset_id,\n            workflowtask_id=workflowtask_id,\n        )\n    )\n    list_query_url_status = res.all()\n    t_1 = time.perf_counter()\n    logger.debug(f\"[enrich_images_async] query, elapsed={t_1 - t_0:.5f} s\")\n\n    output = _postprocess_image_lists(\n        target_images=images,\n        list_query_url_status=list_query_url_status,\n    )\n\n    return output\n</code></pre>"},{"location":"reference/images/status_tools/#fractal_server.images.status_tools.enrich_images_unsorted_sync","title":"<code>enrich_images_unsorted_sync(*, images, dataset_id, workflowtask_id)</code>","text":"<p>Enrich images with a status-related attribute.</p> PARAMETER DESCRIPTION <code>images</code> <p>The input image list</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> <code>dataset_id</code> <p>The dataset ID</p> <p> TYPE: <code>int</code> </p> <code>workflowtask_id</code> <p>The workflow-task ID</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>The list of enriched images, not necessarily in the same order as</p> <code>list[dict[str, Any]]</code> <p>the input.</p> Source code in <code>fractal_server/images/status_tools.py</code> <pre><code>def enrich_images_unsorted_sync(\n    *,\n    images: list[dict[str, Any]],\n    dataset_id: int,\n    workflowtask_id: int,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Enrich images with a status-related attribute.\n\n\n    Args:\n        images: The input image list\n        dataset_id: The dataset ID\n        workflowtask_id: The workflow-task ID\n\n    Returns:\n        The list of enriched images, not necessarily in the same order as\n        the input.\n    \"\"\"\n\n    t_0 = time.perf_counter()\n    logger.info(\n        f\"[enrich_images_async] START, {dataset_id=}, {workflowtask_id=}\"\n    )\n\n    # Get `(zarr_url, status)` for _all_ processed images (including those that\n    # are not part of the target image set)\n    with next(get_sync_db()) as db:\n        res = db.execute(\n            _prepare_query(\n                dataset_id=dataset_id,\n                workflowtask_id=workflowtask_id,\n            )\n        )\n        list_query_url_status = res.all()\n    t_1 = time.perf_counter()\n    logger.debug(f\"[enrich_images_async] query, elapsed={t_1 - t_0:.5f} s\")\n\n    output = _postprocess_image_lists(\n        target_images=images,\n        list_query_url_status=list_query_url_status,\n    )\n\n    return output\n</code></pre>"},{"location":"reference/images/tools/","title":"tools","text":""},{"location":"reference/images/tools/#fractal_server.images.tools.aggregate_attributes","title":"<code>aggregate_attributes(images)</code>","text":"<p>Given a list of images, this function returns a dictionary of all image attributes, each mapped to a sorted list of existing values.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def aggregate_attributes(images: list[dict[str, Any]]) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Given a list of images, this function returns a dictionary of all image\n    attributes, each mapped to a sorted list of existing values.\n    \"\"\"\n    attributes = {}\n    for image in images:\n        for k, v in image[\"attributes\"].items():\n            attributes.setdefault(k, []).append(v)\n    for k, v in attributes.items():\n        attributes[k] = list(set(v))\n    sorted_attributes = {\n        key: sorted(value) for key, value in attributes.items()\n    }\n    return sorted_attributes\n</code></pre>"},{"location":"reference/images/tools/#fractal_server.images.tools.aggregate_types","title":"<code>aggregate_types(images)</code>","text":"<p>Given a list of images, this function returns a list of all image types.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def aggregate_types(images: list[dict[str, Any]]) -&gt; list[str]:\n    \"\"\"\n    Given a list of images, this function returns a list of all image types.\n    \"\"\"\n    return list({type for image in images for type in image[\"types\"].keys()})\n</code></pre>"},{"location":"reference/images/tools/#fractal_server.images.tools.filter_image_list","title":"<code>filter_image_list(images, type_filters=None, attribute_filters=None)</code>","text":"<p>Compute a sublist with images that match a filter set.</p> PARAMETER DESCRIPTION <code>images</code> <p>A list of images.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> <code>type_filters</code> <p> TYPE: <code>dict[str, bool] | None</code> DEFAULT: <code>None</code> </p> <code>attribute_filters</code> <p> TYPE: <code>AttributeFilters | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>List of the <code>images</code> elements which match the filter set.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def filter_image_list(\n    images: list[dict[str, Any]],\n    type_filters: dict[str, bool] | None = None,\n    attribute_filters: AttributeFilters | None = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Compute a sublist with images that match a filter set.\n\n    Args:\n        images: A list of images.\n        type_filters:\n        attribute_filters:\n\n    Returns:\n        List of the `images` elements which match the filter set.\n    \"\"\"\n\n    # When no filter is provided, return all images\n    if type_filters is None and attribute_filters is None:\n        return images\n    actual_type_filters = type_filters or {}\n    actual_attribute_filters = attribute_filters or {}\n\n    filtered_images = [\n        copy(this_image)\n        for this_image in images\n        if match_filter(\n            image=this_image,\n            type_filters=actual_type_filters,\n            attribute_filters=actual_attribute_filters,\n        )\n    ]\n    return filtered_images\n</code></pre>"},{"location":"reference/images/tools/#fractal_server.images.tools.find_image_by_zarr_url","title":"<code>find_image_by_zarr_url(*, images, zarr_url)</code>","text":"<p>Return a copy of the image with a given zarr_url, and its positional index.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> <code>zarr_url</code> <p>Path that the returned image must have.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ImageSearch | None</code> <p>The first image from <code>images</code> which has zarr_url equal to <code>zarr_url</code>.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def find_image_by_zarr_url(\n    *,\n    images: list[dict[str, Any]],\n    zarr_url: str,\n) -&gt; ImageSearch | None:\n    \"\"\"\n    Return a copy of the image with a given zarr_url, and its positional index.\n\n    Args:\n        images: List of images.\n        zarr_url: Path that the returned image must have.\n\n    Returns:\n        The first image from `images` which has zarr_url equal to `zarr_url`.\n    \"\"\"\n    image_urls = [img[\"zarr_url\"] for img in images]\n    try:\n        ind = image_urls.index(zarr_url)\n    except ValueError:\n        return None\n    return dict(image=copy(images[ind]), index=ind)\n</code></pre>"},{"location":"reference/images/tools/#fractal_server.images.tools.match_filter","title":"<code>match_filter(*, image, type_filters, attribute_filters)</code>","text":"<p>Find whether an image matches a filter set.</p> PARAMETER DESCRIPTION <code>image</code> <p>A single image.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>type_filters</code> <p> TYPE: <code>dict[str, bool]</code> </p> <code>attribute_filters</code> <p> TYPE: <code>AttributeFilters</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the image matches the filter set.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def match_filter(\n    *,\n    image: dict[str, Any],\n    type_filters: dict[str, bool],\n    attribute_filters: AttributeFilters,\n) -&gt; bool:\n    \"\"\"\n    Find whether an image matches a filter set.\n\n    Args:\n        image: A single image.\n        type_filters:\n        attribute_filters:\n\n    Returns:\n        Whether the image matches the filter set.\n    \"\"\"\n\n    # Verify match with types (using a False default)\n    for key, value in type_filters.items():\n        if image[\"types\"].get(key, False) != value:\n            return False\n\n    # Verify match with attributes (only for not-None filters)\n    for key, values in attribute_filters.items():\n        if image[\"attributes\"].get(key) not in values:\n            return False\n\n    return True\n</code></pre>"},{"location":"reference/images/tools/#fractal_server.images.tools.merge_type_filters","title":"<code>merge_type_filters(*, task_input_types, wftask_type_filters)</code>","text":"<p>Merge two type-filters sets, if they are compatible.</p> Source code in <code>fractal_server/images/tools.py</code> <pre><code>def merge_type_filters(\n    *,\n    task_input_types: dict[str, bool],\n    wftask_type_filters: dict[str, bool],\n) -&gt; dict[str, bool]:\n    \"\"\"\n    Merge two type-filters sets, if they are compatible.\n    \"\"\"\n    all_keys = set(task_input_types.keys()) | set(wftask_type_filters.keys())\n    for key in all_keys:\n        if (\n            key in task_input_types.keys()\n            and key in wftask_type_filters.keys()\n            and task_input_types[key] != wftask_type_filters[key]\n        ):\n            raise ValueError(\n                \"Cannot merge type filters \"\n                f\"`{task_input_types}` (from task) \"\n                f\"and `{wftask_type_filters}` (from workflowtask).\"\n            )\n    merged_dict = task_input_types.copy()\n    merged_dict.update(wftask_type_filters)\n    return merged_dict\n</code></pre>"},{"location":"reference/runner/","title":"runner","text":""},{"location":"reference/runner/components/","title":"components","text":""},{"location":"reference/runner/exceptions/","title":"exceptions","text":""},{"location":"reference/runner/exceptions/#fractal_server.runner.exceptions.JobExecutionError","title":"<code>JobExecutionError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>JobExecutionError</p> ATTRIBUTE DESCRIPTION <code>info</code> <p>A free field for additional information</p> <p> TYPE: <code>str | None</code> </p> Source code in <code>fractal_server/runner/exceptions.py</code> <pre><code>class JobExecutionError(RuntimeError):\n    \"\"\"\n    JobExecutionError\n\n    Attributes:\n        info:\n            A free field for additional information\n    \"\"\"\n\n    info: str | None = None\n\n    def __init__(\n        self,\n        *args,\n        info: str | None = None,\n    ):\n        super().__init__(*args)\n        self.info = info\n\n    def assemble_error(self) -&gt; str:\n        if self.info:\n            content = f\"\\n{self.info}\\n\\n\"\n        else:\n            content = str(self)\n        message = f\"JobExecutionError\\n{content}\"\n        return message\n</code></pre>"},{"location":"reference/runner/exceptions/#fractal_server.runner.exceptions.TaskExecutionError","title":"<code>TaskExecutionError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Forwards errors occurred during the execution of a task</p> <p>This error wraps and forwards errors occurred during the execution of tasks, when the exit code is larger than 0 (i.e. the error took place within the task). This error also adds information that is useful to track down and debug the failing task within a workflow.</p> ATTRIBUTE DESCRIPTION <code>workflow_task_id</code> <p>ID of the workflow task that failed.</p> <p> TYPE: <code>int | None</code> </p> <code>workflow_task_order</code> <p>Order of the task within the workflow.</p> <p> TYPE: <code>int | None</code> </p> <code>task_name</code> <p>Human readable name of the failing task.</p> <p> TYPE: <code>str | None</code> </p> Source code in <code>fractal_server/runner/exceptions.py</code> <pre><code>class TaskExecutionError(RuntimeError):\n    \"\"\"\n    Forwards errors occurred during the execution of a task\n\n    This error wraps and forwards errors occurred during the execution of\n    tasks, when the exit code is larger than 0 (i.e. the error took place\n    within the task). This error also adds information that is useful to track\n    down and debug the failing task within a workflow.\n\n    Attributes:\n        workflow_task_id:\n            ID of the workflow task that failed.\n        workflow_task_order:\n            Order of the task within the workflow.\n        task_name:\n            Human readable name of the failing task.\n    \"\"\"\n\n    workflow_task_id: int | None = None\n    workflow_task_order: int | None = None\n    task_name: str | None = None\n\n    def __init__(\n        self,\n        *args,\n        workflow_task_id: int | None = None,\n        workflow_task_order: int | None = None,\n        task_name: str | None = None,\n    ):\n        super().__init__(*args)\n        self.workflow_task_id = workflow_task_id\n        self.workflow_task_order = workflow_task_order\n        self.task_name = task_name\n</code></pre>"},{"location":"reference/runner/filenames/","title":"filenames","text":""},{"location":"reference/runner/set_start_and_last_task_index/","title":"set_start_and_last_task_index","text":""},{"location":"reference/runner/set_start_and_last_task_index/#fractal_server.runner.set_start_and_last_task_index.set_start_and_last_task_index","title":"<code>set_start_and_last_task_index(num_tasks, first_task_index=None, last_task_index=None)</code>","text":"<p>Handle <code>first_task_index</code> and <code>last_task_index</code>, by setting defaults and validating values.</p> num_tasks <p>Total number of tasks in a workflow task list</p> <p>first_task_index:     Positional index of the first task to execute last_task_index:     Positional index of the last task to execute</p> Source code in <code>fractal_server/runner/set_start_and_last_task_index.py</code> <pre><code>def set_start_and_last_task_index(\n    num_tasks: int,\n    first_task_index: int | None = None,\n    last_task_index: int | None = None,\n) -&gt; tuple[int, int]:\n    \"\"\"\n    Handle `first_task_index` and `last_task_index`, by setting defaults and\n    validating values.\n\n    num_tasks:\n        Total number of tasks in a workflow task list\n    first_task_index:\n        Positional index of the first task to execute\n    last_task_index:\n        Positional index of the last task to execute\n    \"\"\"\n    # Set default values\n    if first_task_index is None:\n        first_task_index = 0\n    if last_task_index is None:\n        last_task_index = num_tasks - 1\n\n    # Perform checks\n    if first_task_index &lt; 0:\n        raise ValueError(f\"{first_task_index=} cannot be negative\")\n    if last_task_index &lt; 0:\n        raise ValueError(f\"{last_task_index=} cannot be negative\")\n    if last_task_index &gt; num_tasks - 1:\n        raise ValueError(\n            f\"{last_task_index=} cannot be larger than {(num_tasks-1)=}\"\n        )\n    if first_task_index &gt; last_task_index:\n        raise ValueError(\n            f\"{first_task_index=} cannot be larger than {last_task_index=}\"\n        )\n    return (first_task_index, last_task_index)\n</code></pre>"},{"location":"reference/runner/task_files/","title":"task_files","text":""},{"location":"reference/runner/task_files/#fractal_server.runner.task_files.TaskFiles","title":"<code>TaskFiles</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Files related to a task.</p> ATTRIBUTE DESCRIPTION <code>root_dir_local</code> <p> TYPE: <code>Path</code> </p> <code>root_dir_remote</code> <p> TYPE: <code>Path</code> </p> <code>task_name</code> <p> TYPE: <code>str</code> </p> <code>task_order</code> <p> TYPE: <code>int</code> </p> <code>component</code> <p> TYPE: <code>str | None</code> </p> <code>prefix</code> <p> TYPE: <code>str | None</code> </p> Source code in <code>fractal_server/runner/task_files.py</code> <pre><code>class TaskFiles(BaseModel):\n    \"\"\"\n    Files related to a task.\n\n    Attributes:\n        root_dir_local:\n        root_dir_remote:\n        task_name:\n        task_order:\n        component:\n        prefix:\n    \"\"\"\n\n    # Parent directory\n    root_dir_local: Path\n    root_dir_remote: Path\n\n    # Per-wftask\n    task_name: str\n    task_order: int\n\n    # Per-single-component\n    component: str | None = None\n    prefix: str | None = None\n\n    def _check_component(self):\n        if self.component is None:\n            raise ValueError(\"`component` cannot be None\")\n\n    @property\n    def subfolder_name(self) -&gt; str:\n        order = str(self.task_order or 0)\n        return task_subfolder_name(\n            order=order,\n            task_name=self.task_name,\n        )\n\n    @property\n    def wftask_subfolder_remote(self) -&gt; Path:\n        return self.root_dir_remote / self.subfolder_name\n\n    @property\n    def wftask_subfolder_local(self) -&gt; Path:\n        return self.root_dir_local / self.subfolder_name\n\n    @property\n    def prefix_component(self):\n        if self.prefix is None:\n            return self.component\n        else:\n            return f\"{self.prefix}-{self.component}\"\n\n    @property\n    def log_file_local(self) -&gt; str:\n        self._check_component()\n        return (\n            self.wftask_subfolder_local / f\"{self.prefix_component}-log.txt\"\n        ).as_posix()\n\n    @property\n    def log_file_remote_path(self) -&gt; Path:\n        self._check_component()\n        return self.wftask_subfolder_remote / f\"{self.prefix_component}-log.txt\"\n\n    @property\n    def log_file_remote(self) -&gt; str:\n        return self.log_file_remote_path.as_posix()\n\n    @property\n    def args_file_local(self) -&gt; str:\n        self._check_component()\n        return (\n            self.wftask_subfolder_local / f\"{self.prefix_component}-args.json\"\n        ).as_posix()\n\n    @property\n    def args_file_remote_path(self) -&gt; Path:\n        self._check_component()\n        return (\n            self.wftask_subfolder_remote / f\"{self.prefix_component}-args.json\"\n        )\n\n    @property\n    def args_file_remote(self) -&gt; str:\n        return self.args_file_remote_path.as_posix()\n\n    @property\n    def metadiff_file_local(self) -&gt; str:\n        self._check_component()\n        return (\n            self.wftask_subfolder_local\n            / f\"{self.prefix_component}-metadiff.json\"\n        ).as_posix()\n\n    @property\n    def metadiff_file_remote_path(self) -&gt; Path:\n        self._check_component()\n        return (\n            self.wftask_subfolder_remote\n            / f\"{self.prefix_component}-metadiff.json\"\n        )\n\n    @property\n    def metadiff_file_remote(self) -&gt; str:\n        return self.metadiff_file_remote_path.as_posix()\n</code></pre>"},{"location":"reference/runner/task_files/#fractal_server.runner.task_files.enrich_task_files_multisubmit","title":"<code>enrich_task_files_multisubmit(*, tot_tasks, batch_size, base_task_files)</code>","text":"<p>Expand <code>TaskFiles</code> objects with <code>component</code> and <code>prefix</code>.</p> PARAMETER DESCRIPTION <code>tot_tasks</code> <p>Total number of images to process.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>Batch size, where <code>0</code> means <code>batch_size=tot_tasks</code>.</p> <p> TYPE: <code>int</code> </p> <code>base_task_files</code> <p>Original <code>TaskFiles</code> object to be enriched.</p> <p> TYPE: <code>TaskFiles</code> </p> Source code in <code>fractal_server/runner/task_files.py</code> <pre><code>def enrich_task_files_multisubmit(\n    *,\n    tot_tasks: int,\n    batch_size: int,\n    base_task_files: TaskFiles,\n) -&gt; list[TaskFiles]:\n    \"\"\"\n    Expand `TaskFiles` objects with `component` and `prefix`.\n\n    Args:\n        tot_tasks: Total number of images to process.\n        batch_size: Batch size, where `0` means `batch_size=tot_tasks`.\n        base_task_files: Original `TaskFiles` object to be enriched.\n    \"\"\"\n\n    # Replace `batch_size=0` with `batch_size=tot_tasks`\n    batch_size = batch_size or tot_tasks\n\n    new_list_task_files: list[TaskFiles] = []\n    for absolute_index in range(tot_tasks):\n        ind_batch = absolute_index // batch_size\n        new_list_task_files.append(\n            TaskFiles(\n                **base_task_files.model_dump(\n                    exclude={\n                        \"component\",\n                        \"prefix\",\n                    }\n                ),\n                prefix=f\"{MULTISUBMIT_PREFIX}-{ind_batch:06d}\",\n                component=_index_to_component(absolute_index),\n            )\n        )\n    return new_list_task_files\n</code></pre>"},{"location":"reference/runner/task_files/#fractal_server.runner.task_files.task_subfolder_name","title":"<code>task_subfolder_name(order, task_name)</code>","text":"<p>Get name of task-specific subfolder.</p> PARAMETER DESCRIPTION <code>order</code> <p> TYPE: <code>int | str</code> </p> <code>task_name</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/runner/task_files.py</code> <pre><code>def task_subfolder_name(\n    order: int | str,\n    task_name: str,\n) -&gt; str:\n    \"\"\"\n    Get name of task-specific subfolder.\n\n    Args:\n        order:\n        task_name:\n    \"\"\"\n    task_name_slug = sanitize_string(task_name)\n    return f\"{order}_{task_name_slug}\"\n</code></pre>"},{"location":"reference/runner/versions/","title":"versions","text":""},{"location":"reference/runner/versions/#fractal_server.runner.versions.get_versions","title":"<code>get_versions()</code>","text":"<p>Extract versions of Python and fractal-server.</p> Source code in <code>fractal_server/runner/versions.py</code> <pre><code>def get_versions() -&gt; VersionsType:\n    \"\"\"\n    Extract versions of Python and fractal-server.\n    \"\"\"\n    return dict(\n        python=tuple(sys.version_info[:3]),\n        fractal_server=fractal_server.__VERSION__,\n    )\n</code></pre>"},{"location":"reference/runner/config/","title":"config","text":""},{"location":"reference/runner/config/_local/","title":"_local","text":""},{"location":"reference/runner/config/_local/#fractal_server.runner.config._local.JobRunnerConfigLocal","title":"<code>JobRunnerConfigLocal</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Runner-configuration specifications, for a <code>local</code> resource.</p> <p>The typical use case is that setting <code>parallel_tasks_per_job</code> to a small number (e.g. 1) will limit parallelism when executing tasks requiring a large amount of resources (e.g. memory) on a local machine.</p> ATTRIBUTE DESCRIPTION <code>parallel_tasks_per_job</code> <p>Maximum number of tasks to be run in parallel within a local runner. If <code>None</code>, then all tasks may start at the same time.</p> <p> TYPE: <code>int | None</code> </p> Source code in <code>fractal_server/runner/config/_local.py</code> <pre><code>class JobRunnerConfigLocal(BaseModel):\n    \"\"\"\n    Runner-configuration specifications, for a `local` resource.\n\n    The typical use case is that setting `parallel_tasks_per_job` to a\n    small number (e.g. 1) will limit parallelism when executing tasks\n    requiring a large amount of resources (e.g. memory) on a local machine.\n\n    Attributes:\n        parallel_tasks_per_job:\n            Maximum number of tasks to be run in parallel within a local\n            runner. If `None`, then all tasks may start at the same time.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    parallel_tasks_per_job: int | None = None\n\n    @property\n    def batch_size(self) -&gt; int:\n        return self.parallel_tasks_per_job or 0\n</code></pre>"},{"location":"reference/runner/config/_slurm/","title":"_slurm","text":""},{"location":"reference/runner/config/_slurm/#fractal_server.runner.config._slurm.MemMBType","title":"<code>MemMBType = Annotated[PositiveInt | NonEmptyStr, AfterValidator(slurm_mem_to_MB)]</code>  <code>module-attribute</code>","text":"<p>Memory expressed in MB.</p>"},{"location":"reference/runner/config/_slurm/#fractal_server.runner.config._slurm.BatchingConfigSet","title":"<code>BatchingConfigSet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options to configure the batching strategy (that is, how to combine several tasks in a single SLURM job).</p> ATTRIBUTE DESCRIPTION <code>target_cpus_per_job</code> <p> TYPE: <code>PositiveInt</code> </p> <code>max_cpus_per_job</code> <p> TYPE: <code>PositiveInt</code> </p> <code>target_mem_per_job</code> <p> TYPE: <code>MemMBType</code> </p> <code>max_mem_per_job</code> <p> TYPE: <code>MemMBType</code> </p> <code>target_num_jobs</code> <p> TYPE: <code>PositiveInt</code> </p> <code>max_num_jobs</code> <p> TYPE: <code>PositiveInt</code> </p> Source code in <code>fractal_server/runner/config/_slurm.py</code> <pre><code>class BatchingConfigSet(BaseModel):\n    \"\"\"\n    Options to configure the batching strategy (that is, how to combine\n    several tasks in a single SLURM job).\n\n    Attributes:\n        target_cpus_per_job:\n        max_cpus_per_job:\n        target_mem_per_job:\n        max_mem_per_job:\n        target_num_jobs:\n        max_num_jobs:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    target_num_jobs: PositiveInt\n    max_num_jobs: PositiveInt\n    target_cpus_per_job: PositiveInt\n    max_cpus_per_job: PositiveInt\n    target_mem_per_job: MemMBType\n    max_mem_per_job: MemMBType\n</code></pre>"},{"location":"reference/runner/config/_slurm/#fractal_server.runner.config._slurm.JobRunnerConfigSLURM","title":"<code>JobRunnerConfigSLURM</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Runner-configuration specifications, for a <code>slurm_sudo</code> or <code>slurm_ssh</code> resource.</p> <p>Note: this is a common class, which is processed and transformed into more specific configuration objects during job execution.</p> <p>Valid JSON example <pre><code>{\n    \"default_slurm_config\": {\n        \"partition\": \"partition-name\",\n        \"cpus_per_task\": 1,\n        \"mem\": \"100M\"\n    },\n    \"gpu_slurm_config\": {\n        \"partition\": \"gpu\",\n        \"extra_lines\": [\n            \"#SBATCH --gres=gpu:v100:1\"\n        ]\n    },\n    \"user_local_exports\": {\n        \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n        \"NAPARI_CONFIG\": \"napari_config.json\"\n    },\n    \"batching_config\": {\n        \"target_cpus_per_job\": 1,\n        \"max_cpus_per_job\": 1,\n        \"target_mem_per_job\": 200,\n        \"max_mem_per_job\": 500,\n        \"target_num_jobs\": 2,\n        \"max_num_jobs\": 4\n    }\n}\n</code></pre></p> ATTRIBUTE DESCRIPTION <code>default_slurm_config</code> <p>Common default options for all tasks.</p> <p> TYPE: <code>SlurmConfigSet</code> </p> <code>gpu_slurm_config</code> <p>Default configuration for all GPU tasks.</p> <p> TYPE: <code>SlurmConfigSet | None</code> </p> <code>batching_config</code> <p>Configuration of the batching strategy.</p> <p> TYPE: <code>BatchingConfigSet</code> </p> <code>user_local_exports</code> <p>Key-value pairs to be included as <code>export</code>-ed variables in SLURM submission script, after prepending values with the user's cache directory.</p> <p> TYPE: <code>DictStrStr</code> </p> Source code in <code>fractal_server/runner/config/_slurm.py</code> <pre><code>class JobRunnerConfigSLURM(BaseModel):\n    \"\"\"\n    Runner-configuration specifications, for a `slurm_sudo` or\n    `slurm_ssh` resource.\n\n    Note: this is a common class, which is processed and transformed into more\n    specific configuration objects during job execution.\n\n    Valid JSON example\n    ```json\n    {\n        \"default_slurm_config\": {\n            \"partition\": \"partition-name\",\n            \"cpus_per_task\": 1,\n            \"mem\": \"100M\"\n        },\n        \"gpu_slurm_config\": {\n            \"partition\": \"gpu\",\n            \"extra_lines\": [\n                \"#SBATCH --gres=gpu:v100:1\"\n            ]\n        },\n        \"user_local_exports\": {\n            \"CELLPOSE_LOCAL_MODELS_PATH\": \"CELLPOSE_LOCAL_MODELS_PATH\",\n            \"NAPARI_CONFIG\": \"napari_config.json\"\n        },\n        \"batching_config\": {\n            \"target_cpus_per_job\": 1,\n            \"max_cpus_per_job\": 1,\n            \"target_mem_per_job\": 200,\n            \"max_mem_per_job\": 500,\n            \"target_num_jobs\": 2,\n            \"max_num_jobs\": 4\n        }\n    }\n    ```\n\n    Attributes:\n        default_slurm_config:\n            Common default options for all tasks.\n        gpu_slurm_config:\n            Default configuration for all GPU tasks.\n        batching_config:\n            Configuration of the batching strategy.\n        user_local_exports:\n            Key-value pairs to be included as `export`-ed variables in SLURM\n            submission script, after prepending values with the user's cache\n            directory.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    default_slurm_config: SlurmConfigSet\n    gpu_slurm_config: SlurmConfigSet | None = None\n    batching_config: BatchingConfigSet\n    user_local_exports: DictStrStr = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/runner/config/_slurm/#fractal_server.runner.config._slurm.SlurmConfigSet","title":"<code>SlurmConfigSet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options for the default or gpu SLURM config.</p> ATTRIBUTE DESCRIPTION <code>partition</code> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>cpus_per_task</code> <p> TYPE: <code>PositiveInt | None</code> </p> <code>mem</code> <p> TYPE: <code>MemMBType | None</code> </p> <code>constraint</code> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>gres</code> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>time</code> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>exclude</code> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>nodelist</code> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>nodes</code> <p> TYPE: <code>int | None</code> </p> <code>account</code> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>extra_lines</code> <p> TYPE: <code>list[NonEmptyStr]</code> </p> <code>gpus</code> <p> TYPE: <code>NonEmptyStr | None</code> </p> <code>shebang_line</code> <p>The shell shebang to use for SLURM jobs.</p> <p> TYPE: <code>str</code> </p> <code>use_mem_per_cpu</code> <p> TYPE: <code>bool</code> </p> Source code in <code>fractal_server/runner/config/_slurm.py</code> <pre><code>class SlurmConfigSet(BaseModel):\n    \"\"\"\n    Options for the default or gpu SLURM config.\n\n    Attributes:\n        partition:\n        cpus_per_task:\n        mem:\n        constraint:\n        gres:\n        time:\n        exclude:\n        nodelist:\n        nodes:\n        account:\n        extra_lines:\n        gpus:\n        shebang_line: The shell shebang to use for SLURM jobs.\n        use_mem_per_cpu:\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    partition: NonEmptyStr | None = None\n    cpus_per_task: PositiveInt | None = None\n    mem: MemMBType | None = None\n    constraint: NonEmptyStr | None = None\n    gres: NonEmptyStr | None = None\n    exclude: NonEmptyStr | None = None\n    nodelist: NonEmptyStr | None = None\n    nodes: int | None = None\n    time: NonEmptyStr | None = None\n    account: NonEmptyStr | None = None\n    extra_lines: list[NonEmptyStr] = Field(default_factory=list)\n    gpus: NonEmptyStr | None = None\n    shebang_line: str = \"#!/bin/sh\"\n    use_mem_per_cpu: bool = False\n</code></pre>"},{"location":"reference/runner/config/slurm_mem_to_MB/","title":"slurm_mem_to_MB","text":""},{"location":"reference/runner/config/slurm_mem_to_MB/#fractal_server.runner.config.slurm_mem_to_MB.slurm_mem_to_MB","title":"<code>slurm_mem_to_MB(raw_mem)</code>","text":"<p>Convert a memory-specification string into an integer (in MB units), or simply return the input if it is already an integer.</p> <p>Supported units are <code>\"M\", \"G\", \"T\"</code>, with <code>\"M\"</code> being the default; some parsing examples are: <code>\"10M\" -&gt; 10000</code>, <code>\"3G\" -&gt; 3000000</code>.</p> PARAMETER DESCRIPTION <code>raw_mem</code> <p>A string (e.g. <code>\"100M\"</code>) or an integer (in MB).</p> <p> TYPE: <code>str | int</code> </p> RETURNS DESCRIPTION <code>int</code> <p>Integer value of memory in MB units.</p> Source code in <code>fractal_server/runner/config/slurm_mem_to_MB.py</code> <pre><code>def slurm_mem_to_MB(raw_mem: str | int) -&gt; int:\n    \"\"\"\n    Convert a memory-specification string into an integer (in MB units), or\n    simply return the input if it is already an integer.\n\n    Supported units are `\"M\", \"G\", \"T\"`, with `\"M\"` being the default; some\n    parsing examples are: `\"10M\" -&gt; 10000`, `\"3G\" -&gt; 3000000`.\n\n    Args:\n        raw_mem:\n            A string (e.g. `\"100M\"`) or an integer (in MB).\n\n    Returns:\n        Integer value of memory in MB units.\n    \"\"\"\n\n    info = f\"[_parse_mem_value] {raw_mem=}\"\n    error_msg = (\n        f\"{info}, invalid specification of memory requirements \"\n        \"(valid examples: 93, 71M, 93G, 71T).\"\n    )\n\n    # Handle integer argument\n    if type(raw_mem) is int:\n        return raw_mem\n\n    # Handle string argument\n    if not raw_mem[0].isdigit():  # fail e.g. for raw_mem=\"M100\"\n        logger.error(error_msg)\n        raise SlurmConfigError(error_msg)\n    if raw_mem.isdigit():\n        mem_MB = int(raw_mem)\n    elif raw_mem.endswith(\"M\"):\n        stripped_raw_mem = raw_mem.strip(\"M\")\n        if not stripped_raw_mem.isdigit():\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        mem_MB = int(stripped_raw_mem)\n    elif raw_mem.endswith(\"G\"):\n        stripped_raw_mem = raw_mem.strip(\"G\")\n        if not stripped_raw_mem.isdigit():\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        mem_MB = int(stripped_raw_mem) * 10**3\n    elif raw_mem.endswith(\"T\"):\n        stripped_raw_mem = raw_mem.strip(\"T\")\n        if not stripped_raw_mem.isdigit():\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        mem_MB = int(stripped_raw_mem) * 10**6\n    else:\n        logger.error(error_msg)\n        raise SlurmConfigError(error_msg)\n\n    logger.debug(f\"{info}, return {mem_MB}\")\n    return mem_MB\n</code></pre>"},{"location":"reference/runner/executors/","title":"executors","text":""},{"location":"reference/runner/executors/base_runner/","title":"base_runner","text":""},{"location":"reference/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.TASK_TYPES_MULTISUBMIT","title":"<code>TASK_TYPES_MULTISUBMIT = [TaskType.COMPOUND, TaskType.CONVERTER_COMPOUND, TaskType.PARALLEL]</code>  <code>module-attribute</code>","text":"<p>List of valid task types for <code>BaseRunner.multisubmit</code>.</p>"},{"location":"reference/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.TASK_TYPES_SUBMIT","title":"<code>TASK_TYPES_SUBMIT = [TaskType.COMPOUND, TaskType.CONVERTER_COMPOUND, TaskType.NON_PARALLEL, TaskType.CONVERTER_NON_PARALLEL]</code>  <code>module-attribute</code>","text":"<p>List of valid task types for <code>BaseRunner.submit</code>.</p>"},{"location":"reference/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.BaseRunner","title":"<code>BaseRunner</code>","text":"<p>Base class for Fractal runners.</p> Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>class BaseRunner:\n    \"\"\"\n    Base class for Fractal runners.\n    \"\"\"\n\n    shared_config: JobRunnerConfigLocal | JobRunnerConfigSLURM\n\n    executor_error_log: str | None = None\n\n    def submit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        parameters: dict[str, Any],\n        history_unit_id: int,\n        task_type: SubmitTaskType,\n        task_files: TaskFiles,\n        user_id: int,\n        config: Any,\n    ) -&gt; tuple[Any, BaseException | None]:\n        \"\"\"\n        Run a single fractal task.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            parameters: Dictionary of parameters.\n            history_unit_id:\n                Database ID of the corresponding `HistoryUnit` entry.\n            task_type: Task type.\n            task_files: `TaskFiles` object.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n        raise NotImplementedError()\n\n    def multisubmit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        list_parameters: list[dict],\n        history_unit_ids: list[int],\n        list_task_files: list[TaskFiles],\n        task_type: MultisubmitTaskType,\n        config: Any,\n        user_id: int,\n    ) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n        \"\"\"\n        Run a parallel fractal task.\n\n        Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n        have the same size. For parallel tasks, this is also the number of\n        input images, while for compound tasks these can differ.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            list_parameters:\n                List of dictionaries of parameters (each one must include\n                `zarr_urls` key).\n            history_unit_ids:\n                Database IDs of the corresponding `HistoryUnit` entries.\n            list_task_files: `TaskFiles` objects.\n            task_type: Task type.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n        raise NotImplementedError()\n\n    def validate_submit_parameters(\n        self,\n        parameters: dict[str, Any],\n        task_type: SubmitTaskType | MultisubmitTaskType,\n    ) -&gt; None:\n        \"\"\"\n        Validate parameters for `submit` method\n\n        Args:\n            parameters: Parameters dictionary.\n            task_type: Task type.\n        \"\"\"\n        logger.info(\"[validate_submit_parameters] START\")\n        if task_type not in TASK_TYPES_SUBMIT:\n            raise ValueError(f\"Invalid {task_type=} for `submit`.\")\n        if not isinstance(parameters, dict):\n            raise ValueError(\"`parameters` must be a dictionary.\")\n        if task_type in [\n            TaskType.NON_PARALLEL,\n            TaskType.COMPOUND,\n        ]:\n            if \"zarr_urls\" not in parameters.keys():\n                raise ValueError(\n                    f\"No 'zarr_urls' key in in {list(parameters.keys())}\"\n                )\n        elif task_type in [\n            TaskType.CONVERTER_NON_PARALLEL,\n            TaskType.CONVERTER_COMPOUND,\n        ]:\n            if \"zarr_urls\" in parameters.keys():\n                raise ValueError(\n                    f\"Forbidden 'zarr_urls' key in {list(parameters.keys())}\"\n                )\n        logger.info(\"[validate_submit_parameters] END\")\n\n    def validate_multisubmit_parameters(\n        self,\n        *,\n        task_type: MultisubmitTaskType,\n        list_parameters: list[dict[str, Any]],\n        list_task_files: list[TaskFiles],\n        history_unit_ids: list[int],\n    ) -&gt; None:\n        \"\"\"\n        Validate parameters for `multisubmit` method\n\n        Args:\n            task_type: Task type.\n            list_parameters: List of parameters dictionaries.\n            list_task_files:\n            history_unit_ids:\n        \"\"\"\n        if task_type not in TASK_TYPES_MULTISUBMIT:\n            raise ValueError(f\"Invalid {task_type=} for `multisubmit`.\")\n\n        if not isinstance(list_parameters, list):\n            raise ValueError(\"`parameters` must be a list.\")\n\n        if len(list_parameters) != len(list_task_files):\n            raise ValueError(\n                f\"{len(list_task_files)=} differs from {len(list_parameters)=}.\"\n            )\n        if len(history_unit_ids) != len(list_parameters):\n            raise ValueError(\n                f\"{len(history_unit_ids)=} differs from \"\n                f\"{len(list_parameters)=}.\"\n            )\n\n        subfolders = {\n            task_file.wftask_subfolder_local for task_file in list_task_files\n        }\n        if len(subfolders) != 1:\n            raise ValueError(f\"More than one subfolders: {subfolders}.\")\n\n        for single_kwargs in list_parameters:\n            if not isinstance(single_kwargs, dict):\n                raise ValueError(\"kwargs itemt must be a dictionary.\")\n            if \"zarr_url\" not in single_kwargs.keys():\n                raise ValueError(\n                    f\"No 'zarr_url' key in in {list(single_kwargs.keys())}\"\n                )\n        if task_type == TaskType.PARALLEL:\n            zarr_urls = [kwargs[\"zarr_url\"] for kwargs in list_parameters]\n            if len(zarr_urls) != len(set(zarr_urls)):\n                raise ValueError(\"Non-unique zarr_urls\")\n</code></pre>"},{"location":"reference/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.BaseRunner.multisubmit","title":"<code>multisubmit(*, base_command, workflow_task_order, workflow_task_id, task_name, list_parameters, history_unit_ids, list_task_files, task_type, config, user_id)</code>","text":"<p>Run a parallel fractal task.</p> <p>Note: <code>list_parameters</code>, <code>list_task_files</code> and <code>history_unit_ids</code> have the same size. For parallel tasks, this is also the number of input images, while for compound tasks these can differ.</p> PARAMETER DESCRIPTION <code>base_command</code> <p> TYPE: <code>str</code> </p> <code>workflow_task_order</code> <p> TYPE: <code>int</code> </p> <code>workflow_task_id</code> <p> TYPE: <code>int</code> </p> <code>task_name</code> <p> TYPE: <code>str</code> </p> <code>list_parameters</code> <p>List of dictionaries of parameters (each one must include <code>zarr_urls</code> key).</p> <p> TYPE: <code>list[dict]</code> </p> <code>history_unit_ids</code> <p>Database IDs of the corresponding <code>HistoryUnit</code> entries.</p> <p> TYPE: <code>list[int]</code> </p> <code>list_task_files</code> <p><code>TaskFiles</code> objects.</p> <p> TYPE: <code>list[TaskFiles]</code> </p> <code>task_type</code> <p>Task type.</p> <p> TYPE: <code>MultisubmitTaskType</code> </p> <code>config</code> <p>Runner-specific parameters.</p> <p> TYPE: <code>Any</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>def multisubmit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    list_parameters: list[dict],\n    history_unit_ids: list[int],\n    list_task_files: list[TaskFiles],\n    task_type: MultisubmitTaskType,\n    config: Any,\n    user_id: int,\n) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n    \"\"\"\n    Run a parallel fractal task.\n\n    Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n    have the same size. For parallel tasks, this is also the number of\n    input images, while for compound tasks these can differ.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        list_parameters:\n            List of dictionaries of parameters (each one must include\n            `zarr_urls` key).\n        history_unit_ids:\n            Database IDs of the corresponding `HistoryUnit` entries.\n        list_task_files: `TaskFiles` objects.\n        task_type: Task type.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.BaseRunner.submit","title":"<code>submit(*, base_command, workflow_task_order, workflow_task_id, task_name, parameters, history_unit_id, task_type, task_files, user_id, config)</code>","text":"<p>Run a single fractal task.</p> PARAMETER DESCRIPTION <code>base_command</code> <p> TYPE: <code>str</code> </p> <code>workflow_task_order</code> <p> TYPE: <code>int</code> </p> <code>workflow_task_id</code> <p> TYPE: <code>int</code> </p> <code>task_name</code> <p> TYPE: <code>str</code> </p> <code>parameters</code> <p>Dictionary of parameters.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>history_unit_id</code> <p>Database ID of the corresponding <code>HistoryUnit</code> entry.</p> <p> TYPE: <code>int</code> </p> <code>task_type</code> <p>Task type.</p> <p> TYPE: <code>SubmitTaskType</code> </p> <code>task_files</code> <p><code>TaskFiles</code> object.</p> <p> TYPE: <code>TaskFiles</code> </p> <code>config</code> <p>Runner-specific parameters.</p> <p> TYPE: <code>Any</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>def submit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    parameters: dict[str, Any],\n    history_unit_id: int,\n    task_type: SubmitTaskType,\n    task_files: TaskFiles,\n    user_id: int,\n    config: Any,\n) -&gt; tuple[Any, BaseException | None]:\n    \"\"\"\n    Run a single fractal task.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        parameters: Dictionary of parameters.\n        history_unit_id:\n            Database ID of the corresponding `HistoryUnit` entry.\n        task_type: Task type.\n        task_files: `TaskFiles` object.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.BaseRunner.validate_multisubmit_parameters","title":"<code>validate_multisubmit_parameters(*, task_type, list_parameters, list_task_files, history_unit_ids)</code>","text":"<p>Validate parameters for <code>multisubmit</code> method</p> PARAMETER DESCRIPTION <code>task_type</code> <p>Task type.</p> <p> TYPE: <code>MultisubmitTaskType</code> </p> <code>list_parameters</code> <p>List of parameters dictionaries.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> <code>list_task_files</code> <p> TYPE: <code>list[TaskFiles]</code> </p> <code>history_unit_ids</code> <p> TYPE: <code>list[int]</code> </p> Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>def validate_multisubmit_parameters(\n    self,\n    *,\n    task_type: MultisubmitTaskType,\n    list_parameters: list[dict[str, Any]],\n    list_task_files: list[TaskFiles],\n    history_unit_ids: list[int],\n) -&gt; None:\n    \"\"\"\n    Validate parameters for `multisubmit` method\n\n    Args:\n        task_type: Task type.\n        list_parameters: List of parameters dictionaries.\n        list_task_files:\n        history_unit_ids:\n    \"\"\"\n    if task_type not in TASK_TYPES_MULTISUBMIT:\n        raise ValueError(f\"Invalid {task_type=} for `multisubmit`.\")\n\n    if not isinstance(list_parameters, list):\n        raise ValueError(\"`parameters` must be a list.\")\n\n    if len(list_parameters) != len(list_task_files):\n        raise ValueError(\n            f\"{len(list_task_files)=} differs from {len(list_parameters)=}.\"\n        )\n    if len(history_unit_ids) != len(list_parameters):\n        raise ValueError(\n            f\"{len(history_unit_ids)=} differs from \"\n            f\"{len(list_parameters)=}.\"\n        )\n\n    subfolders = {\n        task_file.wftask_subfolder_local for task_file in list_task_files\n    }\n    if len(subfolders) != 1:\n        raise ValueError(f\"More than one subfolders: {subfolders}.\")\n\n    for single_kwargs in list_parameters:\n        if not isinstance(single_kwargs, dict):\n            raise ValueError(\"kwargs itemt must be a dictionary.\")\n        if \"zarr_url\" not in single_kwargs.keys():\n            raise ValueError(\n                f\"No 'zarr_url' key in in {list(single_kwargs.keys())}\"\n            )\n    if task_type == TaskType.PARALLEL:\n        zarr_urls = [kwargs[\"zarr_url\"] for kwargs in list_parameters]\n        if len(zarr_urls) != len(set(zarr_urls)):\n            raise ValueError(\"Non-unique zarr_urls\")\n</code></pre>"},{"location":"reference/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.BaseRunner.validate_submit_parameters","title":"<code>validate_submit_parameters(parameters, task_type)</code>","text":"<p>Validate parameters for <code>submit</code> method</p> PARAMETER DESCRIPTION <code>parameters</code> <p>Parameters dictionary.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>task_type</code> <p>Task type.</p> <p> TYPE: <code>SubmitTaskType | MultisubmitTaskType</code> </p> Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>def validate_submit_parameters(\n    self,\n    parameters: dict[str, Any],\n    task_type: SubmitTaskType | MultisubmitTaskType,\n) -&gt; None:\n    \"\"\"\n    Validate parameters for `submit` method\n\n    Args:\n        parameters: Parameters dictionary.\n        task_type: Task type.\n    \"\"\"\n    logger.info(\"[validate_submit_parameters] START\")\n    if task_type not in TASK_TYPES_SUBMIT:\n        raise ValueError(f\"Invalid {task_type=} for `submit`.\")\n    if not isinstance(parameters, dict):\n        raise ValueError(\"`parameters` must be a dictionary.\")\n    if task_type in [\n        TaskType.NON_PARALLEL,\n        TaskType.COMPOUND,\n    ]:\n        if \"zarr_urls\" not in parameters.keys():\n            raise ValueError(\n                f\"No 'zarr_urls' key in in {list(parameters.keys())}\"\n            )\n    elif task_type in [\n        TaskType.CONVERTER_NON_PARALLEL,\n        TaskType.CONVERTER_COMPOUND,\n    ]:\n        if \"zarr_urls\" in parameters.keys():\n            raise ValueError(\n                f\"Forbidden 'zarr_urls' key in {list(parameters.keys())}\"\n            )\n    logger.info(\"[validate_submit_parameters] END\")\n</code></pre>"},{"location":"reference/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.MultisubmitTaskType","title":"<code>MultisubmitTaskType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Valid task types for <code>BaseRunner.multisubmit</code>.</p> ATTRIBUTE DESCRIPTION <code>PARALLEL</code> <p>Parallel task.</p> <p> </p> <code>COMPOUND</code> <p>Compound task.</p> <p> </p> <code>CONVERTER_COMPOUND</code> <p>Compound converter task.</p> <p> </p> Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>class MultisubmitTaskType(StrEnum):\n    \"\"\"\n    Valid task types for `BaseRunner.multisubmit`.\n\n    Attributes:\n        PARALLEL: Parallel task.\n        COMPOUND: Compound task.\n        CONVERTER_COMPOUND: Compound converter task.\n    \"\"\"\n\n    PARALLEL = TaskType.PARALLEL\n    COMPOUND = TaskType.COMPOUND\n    CONVERTER_COMPOUND = TaskType.CONVERTER_COMPOUND\n</code></pre>"},{"location":"reference/runner/executors/base_runner/#fractal_server.runner.executors.base_runner.SubmitTaskType","title":"<code>SubmitTaskType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Valid task types for <code>BaseRunner.submit</code>.</p> ATTRIBUTE DESCRIPTION <code>NON_PARALLEL</code> <p>Non-parallel task.</p> <p> </p> <code>COMPOUND</code> <p>Compound task.</p> <p> </p> <code>CONVERTER_NON_PARALLEL</code> <p>Non-parallel converter task.</p> <p> </p> <code>CONVERTER_COMPOUND</code> <p>Compound converter task.</p> <p> </p> Source code in <code>fractal_server/runner/executors/base_runner.py</code> <pre><code>class SubmitTaskType(StrEnum):\n    \"\"\"\n    Valid task types for `BaseRunner.submit`.\n\n    Attributes:\n        NON_PARALLEL: Non-parallel task.\n        COMPOUND: Compound task.\n        CONVERTER_NON_PARALLEL: Non-parallel converter task.\n        CONVERTER_COMPOUND: Compound converter task.\n    \"\"\"\n\n    NON_PARALLEL = TaskType.NON_PARALLEL\n    COMPOUND = TaskType.COMPOUND\n    CONVERTER_NON_PARALLEL = TaskType.CONVERTER_NON_PARALLEL\n    CONVERTER_COMPOUND = TaskType.CONVERTER_COMPOUND\n</code></pre>"},{"location":"reference/runner/executors/call_command_wrapper/","title":"call_command_wrapper","text":""},{"location":"reference/runner/executors/call_command_wrapper/#fractal_server.runner.executors.call_command_wrapper.call_command_wrapper","title":"<code>call_command_wrapper(*, cmd, log_path)</code>","text":"<p>Call a command and write its stdout and stderr to files</p> PARAMETER DESCRIPTION <code>cmd</code> <p> TYPE: <code>str</code> </p> <code>log_path</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/runner/executors/call_command_wrapper.py</code> <pre><code>def call_command_wrapper(*, cmd: str, log_path: str) -&gt; None:\n    \"\"\"\n    Call a command and write its stdout and stderr to files\n\n    Args:\n        cmd:\n        log_path:\n    \"\"\"\n    try:\n        validate_cmd(cmd)\n    except ValueError as e:\n        raise TaskExecutionError(f\"Invalid command. Original error: {str(e)}\")\n\n    split_cmd = shlex.split(cmd)\n\n    # Verify that task command is executable\n    if shutil.which(split_cmd[0]) is None:\n        msg = (\n            f'Command \"{split_cmd[0]}\" is not valid. '\n            \"Hint: make sure that it is executable.\"\n        )\n        raise TaskExecutionError(msg)\n\n    with open(log_path, \"w\") as fp_log:\n        try:\n            result = subprocess.run(  # nosec\n                split_cmd,\n                stderr=fp_log,\n                stdout=fp_log,\n            )\n        except Exception as e:\n            # This is likely unreachable\n            raise e\n\n    if result.returncode != 0:\n        stderr = \"\"\n        if os.path.isfile(log_path):\n            with open(log_path) as fp_stderr:\n                stderr = fp_stderr.read()\n            stderr = placeholder_if_too_long(stderr)\n        raise TaskExecutionError(\n            f\"Task failed with returncode={result.returncode}.\\n\"\n            f\"STDERR: {stderr}\"\n        )\n</code></pre>"},{"location":"reference/runner/executors/call_command_wrapper/#fractal_server.runner.executors.call_command_wrapper.placeholder_if_too_long","title":"<code>placeholder_if_too_long(stderr)</code>","text":"<p>Returns a placeholder if the string is too long</p> Source code in <code>fractal_server/runner/executors/call_command_wrapper.py</code> <pre><code>def placeholder_if_too_long(stderr: str) -&gt; str:\n    \"\"\"Returns a placeholder if the string is too long\"\"\"\n    if len(stderr) &gt; MAX_LEN_STDERR:\n        return (\n            f\"Cannot display stderr of length {len(stderr)}. You can find the \"\n            \"detailed logs by downloading the job-log folder.\"\n        )\n    return stderr\n</code></pre>"},{"location":"reference/runner/executors/local/","title":"local","text":""},{"location":"reference/runner/executors/local/get_local_config/","title":"get_local_config","text":"<p>Submodule to handle the local-backend configuration for a WorkflowTask</p>"},{"location":"reference/runner/executors/local/get_local_config/#fractal_server.runner.executors.local.get_local_config.get_local_backend_config","title":"<code>get_local_backend_config(shared_config, wftask, which_type, tot_tasks=1)</code>","text":"<p>Prepare a specific <code>LocalBackendConfig</code> configuration.</p> <p>The base configuration is the runner-level <code>shared_config</code> object, based on <code>resource.jobs_runner_config</code>. We then incorporate attributes from <code>wftask.meta_{non_parallel,parallel}</code> - with higher priority.</p> PARAMETER DESCRIPTION <code>shared_config</code> <p>Configuration object based on <code>resource.jobs_runner_config</code>.</p> <p> TYPE: <code>JobRunnerConfigLocal</code> </p> <code>wftask</code> <p>WorkflowTaskV2 for which the backend configuration should be prepared.</p> <p> TYPE: <code>WorkflowTaskV2</code> </p> <code>which_type</code> <p>Whether we should look at the non-parallel or parallel part of <code>wftask</code>.</p> <p> TYPE: <code>Literal['non_parallel', 'parallel']</code> </p> <code>tot_tasks</code> <p>Not used here, only present as a common interface.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>JobRunnerConfigLocal</code> <p>A ready-to-use local-backend configuration object.</p> Source code in <code>fractal_server/runner/executors/local/get_local_config.py</code> <pre><code>def get_local_backend_config(\n    shared_config: JobRunnerConfigLocal,\n    wftask: WorkflowTaskV2,\n    which_type: Literal[\"non_parallel\", \"parallel\"],\n    tot_tasks: int = 1,\n) -&gt; JobRunnerConfigLocal:\n    \"\"\"\n    Prepare a specific `LocalBackendConfig` configuration.\n\n    The base configuration is the runner-level `shared_config` object, based\n    on `resource.jobs_runner_config`. We then incorporate attributes from\n    `wftask.meta_{non_parallel,parallel}` - with higher priority.\n\n    Args:\n        shared_config:\n            Configuration object based on `resource.jobs_runner_config`.\n        wftask:\n            WorkflowTaskV2 for which the backend configuration should\n            be prepared.\n        which_type:\n            Whether we should look at the non-parallel or parallel part\n            of `wftask`.\n        tot_tasks: Not used here, only present as a common interface.\n\n    Returns:\n        A ready-to-use local-backend configuration object.\n    \"\"\"\n\n    if which_type == \"non_parallel\":\n        wftask_meta = wftask.meta_non_parallel\n    elif which_type == \"parallel\":\n        wftask_meta = wftask.meta_parallel\n    else:\n        raise ValueError(\n            f\"Invalid {which_type=} in `get_local_backend_config`.\"\n        )\n\n    __KEY__ = \"parallel_tasks_per_job\"\n    output = shared_config.model_copy(deep=True)\n    if wftask_meta and __KEY__ in wftask_meta:\n        output.parallel_tasks_per_job = wftask_meta[__KEY__]\n    return output\n</code></pre>"},{"location":"reference/runner/executors/local/runner/","title":"runner","text":""},{"location":"reference/runner/executors/local/runner/#fractal_server.runner.executors.local.runner.LocalRunner","title":"<code>LocalRunner</code>","text":"<p>               Bases: <code>BaseRunner</code></p> <p>Runner implementation for a computational <code>local</code> resource.</p> <p>Tasks are executed through a <code>concurrent.futures.ThreadPoolExecutor</code> executor.</p> Source code in <code>fractal_server/runner/executors/local/runner.py</code> <pre><code>class LocalRunner(BaseRunner):\n    \"\"\"\n    Runner implementation for a computational `local` resource.\n\n    Tasks are executed through a `concurrent.futures.ThreadPoolExecutor`\n    executor.\n    \"\"\"\n\n    executor: ThreadPoolExecutor\n    root_dir_local: Path\n    shared_config: JobRunnerConfigLocal\n\n    def __init__(\n        self,\n        root_dir_local: Path,\n        resource: Resource,\n        profile: Profile,\n    ):\n        self.root_dir_local = root_dir_local\n        self.root_dir_local.mkdir(parents=True, exist_ok=True)\n        self.executor = ThreadPoolExecutor()\n        logger.debug(\"Create LocalRunner\")\n        self.shared_config = JobRunnerConfigLocal(**resource.jobs_runner_config)\n\n    def __enter__(self):\n        logger.debug(\"Enter LocalRunner\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        logger.debug(\"Exit LocalRunner\")\n        self.executor.shutdown(\n            wait=False,\n            cancel_futures=True,\n        )\n        return self.executor.__exit__(exc_type, exc_val, exc_tb)\n\n    @override\n    def submit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        parameters: dict[str, Any],\n        history_unit_id: int,\n        task_files: TaskFiles,\n        config: JobRunnerConfigLocal,\n        task_type: SubmitTaskType,\n        user_id: int,\n    ) -&gt; tuple[Any, Exception | None]:\n        \"\"\"\n        Run a single fractal task.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            parameters: Dictionary of parameters.\n            history_unit_id:\n                Database ID of the corresponding `HistoryUnit` entry.\n            task_type: Task type.\n            task_files: `TaskFiles` object.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n        logger.debug(\"[submit] START\")\n\n        try:\n            self.validate_submit_parameters(parameters, task_type=task_type)\n            workdir_local = task_files.wftask_subfolder_local\n            workdir_local.mkdir()\n\n            # SUBMISSION PHASE\n            future = self.executor.submit(\n                run_single_task,\n                base_command=base_command,\n                parameters=parameters,\n                task_files=task_files,\n            )\n        except Exception as e:\n            logger.error(\n                \"[submit] Unexpected exception during submission. \"\n                f\"Original error {str(e)}\"\n            )\n            result = None\n            exception = TaskExecutionError(str(e))\n            with next(get_sync_db()) as db:\n                update_status_of_history_unit(\n                    history_unit_id=history_unit_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n                return None, exception\n\n        # RETRIEVAL PHASE\n        with next(get_sync_db()) as db:\n            try:\n                result = future.result()\n                logger.debug(\"[submit] END with result\")\n                if task_type not in [\n                    TaskType.COMPOUND,\n                    TaskType.CONVERTER_COMPOUND,\n                ]:\n                    update_status_of_history_unit(\n                        history_unit_id=history_unit_id,\n                        status=HistoryUnitStatus.DONE,\n                        db_sync=db,\n                    )\n                return result, None\n            except Exception as e:\n                logger.debug(\"[submit] END with exception\")\n                update_status_of_history_unit(\n                    history_unit_id=history_unit_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n                return None, TaskExecutionError(str(e))\n\n    def multisubmit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        list_parameters: list[dict[str, Any]],\n        history_unit_ids: list[int],\n        list_task_files: list[TaskFiles],\n        task_type: MultisubmitTaskType,\n        config: JobRunnerConfigLocal,\n        user_id: int,\n    ) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n        \"\"\"\n        Run a parallel fractal task.\n\n        Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n        have the same size. For parallel tasks, this is also the number of\n        input images, while for compound tasks these can differ.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            list_parameters:\n                List of dictionaries of parameters (each one must include\n                `zarr_urls` key).\n            history_unit_ids:\n                Database IDs of the corresponding `HistoryUnit` entries.\n            list_task_files: `TaskFiles` objects.\n            task_type: Task type.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n\n        logger.debug(f\"[multisubmit] START, {len(list_parameters)=}\")\n        results: dict[int, Any] = {}\n        exceptions: dict[int, BaseException] = {}\n\n        try:\n            self.validate_multisubmit_parameters(\n                list_parameters=list_parameters,\n                task_type=task_type,\n                list_task_files=list_task_files,\n                history_unit_ids=history_unit_ids,\n            )\n\n            workdir_local = list_task_files[0].wftask_subfolder_local\n            # Note: the `mkdir` is not needed for compound tasks, but it is\n            # needed for parallel tasks\n            workdir_local.mkdir(exist_ok=True)\n\n            # Set `n_elements` and `parallel_tasks_per_job`\n            n_elements = len(list_parameters)\n            parallel_tasks_per_job = config.parallel_tasks_per_job\n            if parallel_tasks_per_job is None:\n                parallel_tasks_per_job = n_elements\n\n        except Exception as e:\n            logger.error(\n                \"[multisubmit] Unexpected exception during preliminary phase. \"\n                f\"Original error {str(e)}\"\n            )\n            exception = TaskExecutionError(str(e))\n            exceptions = {ind: exception for ind in range(len(list_parameters))}\n            if task_type == TaskType.PARALLEL:\n                with next(get_sync_db()) as db:\n                    bulk_update_status_of_history_unit(\n                        history_unit_ids=history_unit_ids,\n                        status=HistoryUnitStatus.FAILED,\n                        db_sync=db,\n                    )\n            return results, exceptions\n\n        # Execute tasks, in chunks of size `parallel_tasks_per_job`\n        for ind_chunk in range(0, n_elements, parallel_tasks_per_job):\n            list_parameters_chunk = list_parameters[\n                ind_chunk : ind_chunk + parallel_tasks_per_job\n            ]\n\n            active_futures: dict[int, Future] = {}\n            for ind_within_chunk, kwargs in enumerate(list_parameters_chunk):\n                positional_index = ind_chunk + ind_within_chunk\n                try:\n                    future = self.executor.submit(\n                        run_single_task,\n                        base_command=base_command,\n                        parameters=list_parameters[positional_index],\n                        task_files=list_task_files[positional_index],\n                    )\n                    active_futures[positional_index] = future\n                except Exception as e:\n                    logger.error(\n                        \"[multisubmit] Unexpected exception during submission.\"\n                        f\" Original error {str(e)}\"\n                    )\n                    current_history_unit_id = history_unit_ids[positional_index]\n                    exceptions[positional_index] = TaskExecutionError(str(e))\n                    if task_type == TaskType.PARALLEL:\n                        with next(get_sync_db()) as db:\n                            update_status_of_history_unit(\n                                history_unit_id=current_history_unit_id,\n                                status=HistoryUnitStatus.FAILED,\n                                db_sync=db,\n                            )\n            while active_futures:\n                finished_futures = [\n                    index_and_future\n                    for index_and_future in active_futures.items()\n                    if not index_and_future[1].running()\n                ]\n                if len(finished_futures) == 0:\n                    continue\n\n                with next(get_sync_db()) as db:\n                    for positional_index, fut in finished_futures:\n                        active_futures.pop(positional_index)\n                        if task_type == TaskType.PARALLEL:\n                            current_history_unit_id = history_unit_ids[\n                                positional_index\n                            ]\n\n                        try:\n                            results[positional_index] = fut.result()\n                            if task_type == TaskType.PARALLEL:\n                                update_status_of_history_unit(\n                                    history_unit_id=current_history_unit_id,\n                                    status=HistoryUnitStatus.DONE,\n                                    db_sync=db,\n                                )\n\n                        except Exception as e:\n                            logger.debug(\n                                \"Multisubmit failed in retrieval \"\n                                \"phase with the following error \"\n                                f\"{str(e)}\"\n                            )\n                            exceptions[positional_index] = TaskExecutionError(\n                                str(e)\n                            )\n                            if task_type == TaskType.PARALLEL:\n                                update_status_of_history_unit(\n                                    history_unit_id=current_history_unit_id,\n                                    status=HistoryUnitStatus.FAILED,\n                                    db_sync=db,\n                                )\n\n        logger.debug(f\"[multisubmit] END, {len(results)=}, {len(exceptions)=}\")\n\n        return results, exceptions\n</code></pre>"},{"location":"reference/runner/executors/local/runner/#fractal_server.runner.executors.local.runner.LocalRunner.multisubmit","title":"<code>multisubmit(*, base_command, workflow_task_order, workflow_task_id, task_name, list_parameters, history_unit_ids, list_task_files, task_type, config, user_id)</code>","text":"<p>Run a parallel fractal task.</p> <p>Note: <code>list_parameters</code>, <code>list_task_files</code> and <code>history_unit_ids</code> have the same size. For parallel tasks, this is also the number of input images, while for compound tasks these can differ.</p> PARAMETER DESCRIPTION <code>base_command</code> <p> TYPE: <code>str</code> </p> <code>workflow_task_order</code> <p> TYPE: <code>int</code> </p> <code>workflow_task_id</code> <p> TYPE: <code>int</code> </p> <code>task_name</code> <p> TYPE: <code>str</code> </p> <code>list_parameters</code> <p>List of dictionaries of parameters (each one must include <code>zarr_urls</code> key).</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> <code>history_unit_ids</code> <p>Database IDs of the corresponding <code>HistoryUnit</code> entries.</p> <p> TYPE: <code>list[int]</code> </p> <code>list_task_files</code> <p><code>TaskFiles</code> objects.</p> <p> TYPE: <code>list[TaskFiles]</code> </p> <code>task_type</code> <p>Task type.</p> <p> TYPE: <code>MultisubmitTaskType</code> </p> <code>config</code> <p>Runner-specific parameters.</p> <p> TYPE: <code>JobRunnerConfigLocal</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> Source code in <code>fractal_server/runner/executors/local/runner.py</code> <pre><code>def multisubmit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    list_parameters: list[dict[str, Any]],\n    history_unit_ids: list[int],\n    list_task_files: list[TaskFiles],\n    task_type: MultisubmitTaskType,\n    config: JobRunnerConfigLocal,\n    user_id: int,\n) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n    \"\"\"\n    Run a parallel fractal task.\n\n    Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n    have the same size. For parallel tasks, this is also the number of\n    input images, while for compound tasks these can differ.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        list_parameters:\n            List of dictionaries of parameters (each one must include\n            `zarr_urls` key).\n        history_unit_ids:\n            Database IDs of the corresponding `HistoryUnit` entries.\n        list_task_files: `TaskFiles` objects.\n        task_type: Task type.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n\n    logger.debug(f\"[multisubmit] START, {len(list_parameters)=}\")\n    results: dict[int, Any] = {}\n    exceptions: dict[int, BaseException] = {}\n\n    try:\n        self.validate_multisubmit_parameters(\n            list_parameters=list_parameters,\n            task_type=task_type,\n            list_task_files=list_task_files,\n            history_unit_ids=history_unit_ids,\n        )\n\n        workdir_local = list_task_files[0].wftask_subfolder_local\n        # Note: the `mkdir` is not needed for compound tasks, but it is\n        # needed for parallel tasks\n        workdir_local.mkdir(exist_ok=True)\n\n        # Set `n_elements` and `parallel_tasks_per_job`\n        n_elements = len(list_parameters)\n        parallel_tasks_per_job = config.parallel_tasks_per_job\n        if parallel_tasks_per_job is None:\n            parallel_tasks_per_job = n_elements\n\n    except Exception as e:\n        logger.error(\n            \"[multisubmit] Unexpected exception during preliminary phase. \"\n            f\"Original error {str(e)}\"\n        )\n        exception = TaskExecutionError(str(e))\n        exceptions = {ind: exception for ind in range(len(list_parameters))}\n        if task_type == TaskType.PARALLEL:\n            with next(get_sync_db()) as db:\n                bulk_update_status_of_history_unit(\n                    history_unit_ids=history_unit_ids,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n        return results, exceptions\n\n    # Execute tasks, in chunks of size `parallel_tasks_per_job`\n    for ind_chunk in range(0, n_elements, parallel_tasks_per_job):\n        list_parameters_chunk = list_parameters[\n            ind_chunk : ind_chunk + parallel_tasks_per_job\n        ]\n\n        active_futures: dict[int, Future] = {}\n        for ind_within_chunk, kwargs in enumerate(list_parameters_chunk):\n            positional_index = ind_chunk + ind_within_chunk\n            try:\n                future = self.executor.submit(\n                    run_single_task,\n                    base_command=base_command,\n                    parameters=list_parameters[positional_index],\n                    task_files=list_task_files[positional_index],\n                )\n                active_futures[positional_index] = future\n            except Exception as e:\n                logger.error(\n                    \"[multisubmit] Unexpected exception during submission.\"\n                    f\" Original error {str(e)}\"\n                )\n                current_history_unit_id = history_unit_ids[positional_index]\n                exceptions[positional_index] = TaskExecutionError(str(e))\n                if task_type == TaskType.PARALLEL:\n                    with next(get_sync_db()) as db:\n                        update_status_of_history_unit(\n                            history_unit_id=current_history_unit_id,\n                            status=HistoryUnitStatus.FAILED,\n                            db_sync=db,\n                        )\n        while active_futures:\n            finished_futures = [\n                index_and_future\n                for index_and_future in active_futures.items()\n                if not index_and_future[1].running()\n            ]\n            if len(finished_futures) == 0:\n                continue\n\n            with next(get_sync_db()) as db:\n                for positional_index, fut in finished_futures:\n                    active_futures.pop(positional_index)\n                    if task_type == TaskType.PARALLEL:\n                        current_history_unit_id = history_unit_ids[\n                            positional_index\n                        ]\n\n                    try:\n                        results[positional_index] = fut.result()\n                        if task_type == TaskType.PARALLEL:\n                            update_status_of_history_unit(\n                                history_unit_id=current_history_unit_id,\n                                status=HistoryUnitStatus.DONE,\n                                db_sync=db,\n                            )\n\n                    except Exception as e:\n                        logger.debug(\n                            \"Multisubmit failed in retrieval \"\n                            \"phase with the following error \"\n                            f\"{str(e)}\"\n                        )\n                        exceptions[positional_index] = TaskExecutionError(\n                            str(e)\n                        )\n                        if task_type == TaskType.PARALLEL:\n                            update_status_of_history_unit(\n                                history_unit_id=current_history_unit_id,\n                                status=HistoryUnitStatus.FAILED,\n                                db_sync=db,\n                            )\n\n    logger.debug(f\"[multisubmit] END, {len(results)=}, {len(exceptions)=}\")\n\n    return results, exceptions\n</code></pre>"},{"location":"reference/runner/executors/local/runner/#fractal_server.runner.executors.local.runner.LocalRunner.submit","title":"<code>submit(*, base_command, workflow_task_order, workflow_task_id, task_name, parameters, history_unit_id, task_files, config, task_type, user_id)</code>","text":"<p>Run a single fractal task.</p> PARAMETER DESCRIPTION <code>base_command</code> <p> TYPE: <code>str</code> </p> <code>workflow_task_order</code> <p> TYPE: <code>int</code> </p> <code>workflow_task_id</code> <p> TYPE: <code>int</code> </p> <code>task_name</code> <p> TYPE: <code>str</code> </p> <code>parameters</code> <p>Dictionary of parameters.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>history_unit_id</code> <p>Database ID of the corresponding <code>HistoryUnit</code> entry.</p> <p> TYPE: <code>int</code> </p> <code>task_type</code> <p>Task type.</p> <p> TYPE: <code>SubmitTaskType</code> </p> <code>task_files</code> <p><code>TaskFiles</code> object.</p> <p> TYPE: <code>TaskFiles</code> </p> <code>config</code> <p>Runner-specific parameters.</p> <p> TYPE: <code>JobRunnerConfigLocal</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> Source code in <code>fractal_server/runner/executors/local/runner.py</code> <pre><code>@override\ndef submit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    parameters: dict[str, Any],\n    history_unit_id: int,\n    task_files: TaskFiles,\n    config: JobRunnerConfigLocal,\n    task_type: SubmitTaskType,\n    user_id: int,\n) -&gt; tuple[Any, Exception | None]:\n    \"\"\"\n    Run a single fractal task.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        parameters: Dictionary of parameters.\n        history_unit_id:\n            Database ID of the corresponding `HistoryUnit` entry.\n        task_type: Task type.\n        task_files: `TaskFiles` object.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n    logger.debug(\"[submit] START\")\n\n    try:\n        self.validate_submit_parameters(parameters, task_type=task_type)\n        workdir_local = task_files.wftask_subfolder_local\n        workdir_local.mkdir()\n\n        # SUBMISSION PHASE\n        future = self.executor.submit(\n            run_single_task,\n            base_command=base_command,\n            parameters=parameters,\n            task_files=task_files,\n        )\n    except Exception as e:\n        logger.error(\n            \"[submit] Unexpected exception during submission. \"\n            f\"Original error {str(e)}\"\n        )\n        result = None\n        exception = TaskExecutionError(str(e))\n        with next(get_sync_db()) as db:\n            update_status_of_history_unit(\n                history_unit_id=history_unit_id,\n                status=HistoryUnitStatus.FAILED,\n                db_sync=db,\n            )\n            return None, exception\n\n    # RETRIEVAL PHASE\n    with next(get_sync_db()) as db:\n        try:\n            result = future.result()\n            logger.debug(\"[submit] END with result\")\n            if task_type not in [\n                TaskType.COMPOUND,\n                TaskType.CONVERTER_COMPOUND,\n            ]:\n                update_status_of_history_unit(\n                    history_unit_id=history_unit_id,\n                    status=HistoryUnitStatus.DONE,\n                    db_sync=db,\n                )\n            return result, None\n        except Exception as e:\n            logger.debug(\"[submit] END with exception\")\n            update_status_of_history_unit(\n                history_unit_id=history_unit_id,\n                status=HistoryUnitStatus.FAILED,\n                db_sync=db,\n            )\n            return None, TaskExecutionError(str(e))\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/","title":"slurm_common","text":""},{"location":"reference/runner/executors/slurm_common/_batching/","title":"_batching","text":"<p>Submodule to determine the number of total/parallel tasks per SLURM job.</p>"},{"location":"reference/runner/executors/slurm_common/_batching/#fractal_server.runner.executors.slurm_common._batching.SlurmHeuristicsError","title":"<code>SlurmHeuristicsError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Error in SLURM-batching heuristics.</p> Source code in <code>fractal_server/runner/executors/slurm_common/_batching.py</code> <pre><code>class SlurmHeuristicsError(ValueError):\n    \"\"\"\n    Error in SLURM-batching heuristics.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/_batching/#fractal_server.runner.executors.slurm_common._batching._estimate_parallel_tasks_per_job","title":"<code>_estimate_parallel_tasks_per_job(*, cpus_per_task, mem_per_task, max_cpus_per_job, max_mem_per_job)</code>","text":"<p>Compute how many parallel tasks can fit in a given SLURM job</p> <p>Note: If more resources than available are requested, return 1. This assumes that further checks will be performed on the output of the current function, as is the case in the <code>heuristics</code> function below.</p> PARAMETER DESCRIPTION <code>cpus_per_task</code> <p>Number of CPUs needed for one task.</p> <p> TYPE: <code>int</code> </p> <code>mem_per_task</code> <p>Memory (in MB) needed for one task.</p> <p> TYPE: <code>int</code> </p> <code>max_cpus_per_job</code> <p>Maximum number of CPUs available for one job.</p> <p> TYPE: <code>int</code> </p> <code>max_mem_per_job</code> <p>Maximum memory (in MB) available for one job.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>int</code> <p>Number of parallel tasks per job</p> Source code in <code>fractal_server/runner/executors/slurm_common/_batching.py</code> <pre><code>def _estimate_parallel_tasks_per_job(\n    *,\n    cpus_per_task: int,\n    mem_per_task: int,\n    max_cpus_per_job: int,\n    max_mem_per_job: int,\n) -&gt; int:\n    \"\"\"\n    Compute how many parallel tasks can fit in a given SLURM job\n\n    Note: If more resources than available are requested, return 1. This\n    assumes that further checks will be performed on the output of the current\n    function, as is the case in the `heuristics` function below.\n\n    Args:\n        cpus_per_task: Number of CPUs needed for one task.\n        mem_per_task: Memory (in MB) needed for one task.\n        max_cpus_per_job: Maximum number of CPUs available for one job.\n        max_mem_per_job: Maximum memory (in MB) available for one job.\n\n    Returns:\n        Number of parallel tasks per job\n    \"\"\"\n    if cpus_per_task &gt; max_cpus_per_job or mem_per_task &gt; max_mem_per_job:\n        return 1\n    val_based_on_cpus = max_cpus_per_job // cpus_per_task\n    val_based_on_mem = max_mem_per_job // mem_per_task\n    return min(val_based_on_cpus, val_based_on_mem)\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/_batching/#fractal_server.runner.executors.slurm_common._batching.heuristics","title":"<code>heuristics(*, tot_tasks, tasks_per_job=None, parallel_tasks_per_job=None, cpus_per_task, mem_per_task, target_cpus_per_job, max_cpus_per_job, target_mem_per_job, max_mem_per_job, target_num_jobs, max_num_jobs)</code>","text":"<p>Heuristically determine parameters for multi-task batching</p> <p>\"In-job queues\" refer to the case where <code>parallel_tasks_per_job&lt;tasks_per_job</code>, that is, where not all tasks of a given SLURM job will be executed at the same time.</p> <p>This function goes through the following branches:</p> <ol> <li>Validate/fix parameters, if they are provided as input.</li> <li>Heuristically determine parameters based on the per-task resource    requirements and on the target amount of per-job resources, without    resorting to in-job queues.</li> <li>Heuristically determine parameters based on the per-task resource    requirements and on the maximum amount of per-job resources, without    resorting to in-job queues.</li> <li>Heuristically determine parameters (based on the per-task resource    requirements and on the maximum amount of per-job resources) and then    introduce in-job queues to satisfy the hard constraint on the maximum    number of jobs.</li> </ol> PARAMETER DESCRIPTION <code>tot_tasks</code> <p>Total number of elements to be processed (e.g. number of images in a OME-NGFF array).</p> <p> TYPE: <code>int</code> </p> <code>tasks_per_job</code> <p>If <code>tasks_per_job</code> and <code>parallel_tasks_per_job</code> are not <code>None</code>, validate/edit this choice.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>parallel_tasks_per_job</code> <p>If <code>tasks_per_job</code> and <code>parallel_tasks_per_job</code> are not <code>None</code>, validate/edit this choice.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>cpus_per_task</code> <p>Number of CPUs needed for each parallel task.</p> <p> TYPE: <code>int</code> </p> <code>mem_per_task</code> <p>Memory (in MB) needed for each parallel task.</p> <p> TYPE: <code>int</code> </p> <code>target_cpus_per_job</code> <p>Optimal number of CPUs for each SLURM job.</p> <p> TYPE: <code>int</code> </p> <code>max_cpus_per_job</code> <p>Maximum number of CPUs for each SLURM job.</p> <p> TYPE: <code>int</code> </p> <code>target_mem_per_job</code> <p>Optimal amount of memory (in MB) for each SLURM job.</p> <p> TYPE: <code>int</code> </p> <code>max_mem_per_job</code> <p>Maximum amount of memory (in MB) for each SLURM job.</p> <p> TYPE: <code>int</code> </p> <code>target_num_jobs</code> <p>Optimal total number of SLURM jobs for a given WorkflowTask.</p> <p> TYPE: <code>int</code> </p> <code>max_num_jobs</code> <p>Maximum total number of SLURM jobs for a given WorkflowTask.</p> <p> TYPE: <code>int</code> </p> <p>Return:     Valid values of <code>tasks_per_job</code> and <code>parallel_tasks_per_job</code>.</p> Source code in <code>fractal_server/runner/executors/slurm_common/_batching.py</code> <pre><code>def heuristics(\n    *,\n    # Number of parallel components (always known)\n    tot_tasks: int,\n    # Optional WorkflowTask attributes:\n    tasks_per_job: int | None = None,\n    parallel_tasks_per_job: int | None = None,\n    # Task requirements (multiple possible sources):\n    cpus_per_task: int,\n    mem_per_task: int,\n    # Fractal configuration variables (soft/hard limits):\n    target_cpus_per_job: int,\n    max_cpus_per_job: int,\n    target_mem_per_job: int,  # in MB\n    max_mem_per_job: int,  # in MB\n    target_num_jobs: int,\n    max_num_jobs: int,\n) -&gt; tuple[int, int]:\n    \"\"\"\n    Heuristically determine parameters for multi-task batching\n\n    \"In-job queues\" refer to the case where\n    `parallel_tasks_per_job&lt;tasks_per_job`, that is, where not all\n    tasks of a given SLURM job will be executed at the same time.\n\n    This function goes through the following branches:\n\n    1. Validate/fix parameters, if they are provided as input.\n    2. Heuristically determine parameters based on the per-task resource\n       requirements and on the target amount of per-job resources, without\n       resorting to in-job queues.\n    3. Heuristically determine parameters based on the per-task resource\n       requirements and on the maximum amount of per-job resources, without\n       resorting to in-job queues.\n    4. Heuristically determine parameters (based on the per-task resource\n       requirements and on the maximum amount of per-job resources) and then\n       introduce in-job queues to satisfy the hard constraint on the maximum\n       number of jobs.\n\n    Args:\n        tot_tasks:\n            Total number of elements to be processed (e.g. number of images in\n            a OME-NGFF array).\n        tasks_per_job:\n            If `tasks_per_job` and `parallel_tasks_per_job` are not\n            `None`, validate/edit this choice.\n        parallel_tasks_per_job:\n            If `tasks_per_job` and `parallel_tasks_per_job` are not\n            `None`, validate/edit this choice.\n        cpus_per_task:\n            Number of CPUs needed for each parallel task.\n        mem_per_task:\n            Memory (in MB) needed for each parallel task.\n        target_cpus_per_job:\n            Optimal number of CPUs for each SLURM job.\n        max_cpus_per_job:\n            Maximum number of CPUs for each SLURM job.\n        target_mem_per_job:\n            Optimal amount of memory (in MB) for each SLURM job.\n        max_mem_per_job:\n            Maximum amount of memory (in MB) for each SLURM job.\n        target_num_jobs:\n            Optimal total number of SLURM jobs for a given WorkflowTask.\n        max_num_jobs:\n            Maximum total number of SLURM jobs for a given WorkflowTask.\n    Return:\n        Valid values of `tasks_per_job` and `parallel_tasks_per_job`.\n    \"\"\"\n    # Preliminary checks\n    if bool(tasks_per_job) != bool(parallel_tasks_per_job):\n        msg = (\n            \"tasks_per_job and parallel_tasks_per_job must \"\n            \"be both set or both unset\"\n        )\n        logger.error(msg)\n        raise SlurmHeuristicsError(msg)\n    if cpus_per_task &gt; max_cpus_per_job:\n        msg = (\n            f\"[heuristics] Requested {cpus_per_task=} but {max_cpus_per_job=}.\"\n        )\n        logger.error(msg)\n        raise SlurmHeuristicsError(msg)\n    if mem_per_task &gt; max_mem_per_job:\n        msg = f\"[heuristics] Requested {mem_per_task=} but {max_mem_per_job=}.\"\n        logger.error(msg)\n        raise SlurmHeuristicsError(msg)\n\n    # Branch 1: validate/update given parameters\n    if tasks_per_job and parallel_tasks_per_job:\n        # Reduce parallel_tasks_per_job if it exceeds tasks_per_job\n        if parallel_tasks_per_job &gt; tasks_per_job:\n            logger.warning(\n                \"[heuristics] Set parallel_tasks_per_job=\"\n                f\"tasks_per_job={tasks_per_job}\"\n            )\n            parallel_tasks_per_job = tasks_per_job\n\n        # Check requested cpus_per_job\n        cpus_per_job = parallel_tasks_per_job * cpus_per_task\n        if cpus_per_job &gt; target_cpus_per_job:\n            logger.warning(\n                f\"[heuristics] Requested {cpus_per_job=} \"\n                f\"but {target_cpus_per_job=}.\"\n            )\n        if cpus_per_job &gt; max_cpus_per_job:\n            msg = (\n                f\"[heuristics] Requested {cpus_per_job=} \"\n                f\"but {max_cpus_per_job=}.\"\n            )\n            logger.error(msg)\n            raise SlurmHeuristicsError(msg)\n\n        # Check requested mem_per_job\n        mem_per_job = parallel_tasks_per_job * mem_per_task\n        if mem_per_job &gt; target_mem_per_job:\n            logger.warning(\n                f\"[heuristics] Requested {mem_per_job=} \"\n                f\"but {target_mem_per_job=}.\"\n            )\n        if mem_per_job &gt; max_mem_per_job:\n            msg = (\n                f\"[heuristics] Requested {mem_per_job=} but {max_mem_per_job=}.\"\n            )\n            logger.error(msg)\n            raise SlurmHeuristicsError(msg)\n\n        # Check number of jobs\n        num_jobs = math.ceil(tot_tasks / tasks_per_job)\n        if num_jobs &gt; target_num_jobs:\n            logger.debug(\n                f\"[heuristics] Requested {num_jobs=} but {target_num_jobs=}.\"\n            )\n        if num_jobs &gt; max_num_jobs:\n            msg = f\"[heuristics] Requested {num_jobs=} but {max_num_jobs=}.\"\n            logger.error(msg)\n            raise SlurmHeuristicsError(msg)\n        logger.debug(\"[heuristics] Return from branch 1\")\n        return (tasks_per_job, parallel_tasks_per_job)\n\n    # 2: Target-resources-based heuristics, without in-job queues\n    parallel_tasks_per_job = _estimate_parallel_tasks_per_job(\n        cpus_per_task=cpus_per_task,\n        mem_per_task=mem_per_task,\n        max_cpus_per_job=target_cpus_per_job,\n        max_mem_per_job=target_mem_per_job,\n    )\n    tasks_per_job = parallel_tasks_per_job\n    num_jobs = math.ceil(tot_tasks / tasks_per_job)\n    if num_jobs &lt;= target_num_jobs:\n        logger.debug(\"[heuristics] Return from branch 2\")\n        return (tasks_per_job, parallel_tasks_per_job)\n\n    # Branch 3: Max-resources-based heuristics, without in-job queues\n    parallel_tasks_per_job = _estimate_parallel_tasks_per_job(\n        cpus_per_task=cpus_per_task,\n        mem_per_task=mem_per_task,\n        max_cpus_per_job=max_cpus_per_job,\n        max_mem_per_job=max_mem_per_job,\n    )\n    tasks_per_job = parallel_tasks_per_job\n    num_jobs = math.ceil(tot_tasks / tasks_per_job)\n    if num_jobs &lt;= max_num_jobs:\n        logger.debug(\"[heuristics] Return from branch 3\")\n        return (tasks_per_job, parallel_tasks_per_job)\n\n    # Branch 4: Max-resources-based heuristics, with in-job queues\n    parallel_tasks_per_job = _estimate_parallel_tasks_per_job(\n        cpus_per_task=cpus_per_task,\n        mem_per_task=mem_per_task,\n        max_cpus_per_job=max_cpus_per_job,\n        max_mem_per_job=max_mem_per_job,\n    )\n    tasks_per_job = math.ceil(tot_tasks / max_num_jobs)\n    logger.debug(\"[heuristics] Return from branch 4\")\n    return (tasks_per_job, parallel_tasks_per_job)\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/_job_states/","title":"_job_states","text":""},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/","title":"base_slurm_runner","text":""},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner","title":"<code>BaseSlurmRunner</code>","text":"<p>               Bases: <code>BaseRunner</code></p> <p>Base class for SLURM runners.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>class BaseSlurmRunner(BaseRunner):\n    \"\"\"\n    Base class for SLURM runners.\n    \"\"\"\n\n    shutdown_file: Path\n    common_script_lines: list[str]\n    user_cache_dir: str\n    root_dir_local: Path\n    root_dir_remote: Path\n    poll_interval: int\n    poll_interval_internal: float\n    jobs: dict[str, SlurmJob]\n    python_worker_interpreter: str\n    slurm_runner_type: Literal[\"ssh\", \"sudo\"]\n    slurm_account: str | None = None\n    shared_config: JobRunnerConfigSLURM\n\n    def __init__(\n        self,\n        *,\n        root_dir_local: Path,\n        root_dir_remote: Path,\n        slurm_runner_type: Literal[\"ssh\", \"sudo\"],\n        python_worker_interpreter: str,\n        poll_interval: int,\n        common_script_lines: list[str] | None = None,\n        user_cache_dir: str,\n        slurm_account: str | None = None,\n    ):\n        self.slurm_runner_type = slurm_runner_type\n        self.root_dir_local = root_dir_local\n        self.root_dir_remote = root_dir_remote\n        self.common_script_lines = common_script_lines or []\n        self._check_slurm_account()\n        self.user_cache_dir = user_cache_dir\n        self.python_worker_interpreter = python_worker_interpreter\n        self.slurm_account = slurm_account\n\n        self.poll_interval = poll_interval\n        self.poll_interval_internal = self.poll_interval / 10.0\n\n        self.check_fractal_server_versions()\n\n        # Create job folders. Note that the local one may or may not exist\n        # depending on whether it is a test or an actual run\n        try:\n            if not self.root_dir_local.is_dir():\n                self._mkdir_local_folder(self.root_dir_local.as_posix())\n            self._mkdir_remote_folder(self.root_dir_remote.as_posix())\n        except Exception as e:\n            error_msg = (\n                f\"Could not mkdir {self.root_dir_local.as_posix()} or \"\n                f\"{self.root_dir_remote.as_posix()}. \"\n                f\"Original error: {str(e)}.\"\n            )\n            logger.error(error_msg)\n            raise RuntimeError(error_msg)\n\n        self.shutdown_file = self.root_dir_local / SHUTDOWN_FILENAME\n        self.jobs = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        return False\n\n    def _run_remote_cmd(self, cmd: str) -&gt; str:\n        raise NotImplementedError(\"Implement in child class.\")\n\n    def run_squeue(self, *, job_ids: list[str]) -&gt; str:\n        raise NotImplementedError(\"Implement in child class.\")\n\n    def _is_squeue_error_recoverable(self, exception: BaseException) -&gt; bool:\n        \"\"\"\n        Determine whether a `squeue` error is considered recoverable.\n\n        A _recoverable_ error is one which will disappear after some time,\n        without any specific action from the `fractal-server` side.\n\n        Note: if this function returns `True` for an error that does not\n        actually recover, this leads to an infinite loop  where\n        `fractal-server` keeps polling `squeue` information forever.\n\n        More info at\n        https://github.com/fractal-analytics-platform/fractal-server/issues/2682\n\n        Args:\n            exception: The exception raised by `self.run_squeue`.\n        Returns:\n            Whether the error is considered recoverable.\n        \"\"\"\n        str_exception = str(exception)\n        if (\n            \"slurm_load_jobs\" in str_exception\n            and \"Socket timed out on send/recv operation\" in str_exception\n        ):\n            return True\n        else:\n            return False\n\n    def _get_finished_jobs(self, job_ids: list[str]) -&gt; set[str]:\n        #  If there is no Slurm job to check, return right away\n        if not job_ids:\n            return set()\n\n        try:\n            stdout = self.run_squeue(job_ids=job_ids)\n            slurm_statuses = {\n                out.split()[0]: out.split()[1] for out in stdout.splitlines()\n            }\n        except Exception as e:\n            logger.warning(\n                \"[_get_finished_jobs] `squeue` failed, \"\n                \"retry with individual job IDs. \"\n                f\"Original error: {str(e)}.\"\n            )\n            slurm_statuses = dict()\n            for job_id in job_ids:\n                try:\n                    stdout = self.run_squeue(job_ids=[job_id])\n                    slurm_statuses.update(\n                        {stdout.split()[0]: stdout.split()[1]}\n                    )\n                except Exception as e:\n                    msg = (\n                        f\"[_get_finished_jobs] `squeue` failed for {job_id=}. \"\n                        f\"Original error: {str(e)}.\"\n                    )\n                    logger.warning(msg)\n                    if self._is_squeue_error_recoverable(e):\n                        logger.warning(\n                            \"[_get_finished_jobs] Recoverable `squeue` \"\n                            f\"error - mark {job_id=} as FRACTAL_UNDEFINED and\"\n                            \" retry later.\"\n                        )\n                        slurm_statuses.update(\n                            {str(job_id): \"FRACTAL_UNDEFINED\"}\n                        )\n                    else:\n                        logger.warning(\n                            \"[_get_finished_jobs] Non-recoverable `squeue`\"\n                            f\"error - mark {job_id=} as completed.\"\n                        )\n                        slurm_statuses.update({str(job_id): \"COMPLETED\"})\n\n        # If a job is not in `squeue` output, mark it as completed.\n        finished_jobs = {\n            job_id\n            for job_id in job_ids\n            if slurm_statuses.get(job_id, \"COMPLETED\") in STATES_FINISHED\n        }\n        return finished_jobs\n\n    def _mkdir_local_folder(self, folder: str) -&gt; None:\n        raise NotImplementedError(\"Implement in child class.\")\n\n    def _mkdir_remote_folder(self, folder: str) -&gt; None:\n        raise NotImplementedError(\"Implement in child class.\")\n\n    def _enrich_slurm_config(\n        self,\n        slurm_config: SlurmConfig,\n    ) -&gt; SlurmConfig:\n        \"\"\"\n        Return an enriched `SlurmConfig` object\n\n        Include `self.account` and `self.common_script_lines` into a\n        `SlurmConfig` object. Extracting this logic into an independent\n        class method is useful to fix issue #2659 (which was due to\n        performing this same operation multiple times rather than once).\n\n        Args:\n            slurm_config: The original `SlurmConfig` object.\n\n        Returns:\n            A new, up-to-date, `SlurmConfig` object.\n        \"\"\"\n\n        new_slurm_config = slurm_config.model_copy(deep=True)\n\n        # Include SLURM account in `slurm_config`.\n        if self.slurm_account is not None:\n            new_slurm_config.account = self.slurm_account\n\n        # Include common_script_lines in extra_lines\n        if len(self.common_script_lines) &gt; 0:\n            logger.debug(\n                f\"Add {self.common_script_lines} to \"\n                f\"{new_slurm_config.extra_lines=}.\"\n            )\n            current_extra_lines = new_slurm_config.extra_lines\n            new_slurm_config.extra_lines = (\n                current_extra_lines + self.common_script_lines\n            )\n\n        return new_slurm_config\n\n    def _prepare_single_slurm_job(\n        self,\n        *,\n        base_command: str,\n        slurm_job: SlurmJob,\n        slurm_config: SlurmConfig,\n    ) -&gt; str:\n        \"\"\"\n        Prepare submission script locally.\n\n        Args:\n            base_command: Base of task executable command.\n            slurm_job: `SlurmJob` object\n            slurm_config: Configuration for SLURM job\n\n        Returns:\n            Command to submit the SLURM job.\n        \"\"\"\n        logger.debug(\"[_prepare_single_slurm_job] START\")\n\n        for task in slurm_job.tasks:\n            # Write input file\n            if self.slurm_runner_type == \"ssh\":\n                args_file_remote = task.task_files.args_file_remote\n            else:\n                args_file_remote = task.task_files.args_file_local\n            metadiff_file_remote = task.task_files.metadiff_file_remote\n            full_command = (\n                f\"{base_command} \"\n                f\"--args-json {args_file_remote} \"\n                f\"--out-json {metadiff_file_remote}\"\n            )\n\n            input_data = RemoteInputData(\n                full_command=full_command,\n                python_version=sys.version_info[:3],\n                fractal_server_version=__VERSION__,\n                metadiff_file_remote=task.task_files.metadiff_file_remote,\n                log_file_remote=task.task_files.log_file_remote,\n            )\n\n            with open(task.input_file_local, \"w\") as f:\n                json.dump(input_data.model_dump(), f, indent=2)\n\n            with open(task.task_files.args_file_local, \"w\") as f:\n                json.dump(task.parameters, f, indent=2)\n\n            logger.debug(\n                f\"[_prepare_single_slurm_job] Written {task.input_file_local=}\"\n            )\n\n        # Prepare commands to be included in SLURM submission script\n        cmdlines = []\n        for task in slurm_job.tasks:\n            if self.slurm_runner_type == \"ssh\":\n                input_file = task.input_file_remote\n            else:\n                input_file = task.input_file_local\n            output_file = task.output_file_remote\n            cmdlines.append(\n                f\"{self.python_worker_interpreter}\"\n                \" -m fractal_server.runner.\"\n                \"executors.slurm_common.remote \"\n                f\"--input-file {input_file} \"\n                f\"--output-file {output_file}\"\n            )\n\n        # Set ntasks\n        num_tasks_max_running = slurm_config.parallel_tasks_per_job\n        ntasks = min(len(cmdlines), num_tasks_max_running)\n        slurm_config.parallel_tasks_per_job = ntasks\n\n        # Prepare SLURM preamble based on SlurmConfig object\n        script_lines = slurm_config.to_sbatch_preamble(\n            remote_export_dir=self.user_cache_dir,\n            use_mem_per_cpu=slurm_config.use_mem_per_cpu,\n        )\n\n        # Extend SLURM preamble with variable which are not in SlurmConfig, and\n        # fix their order\n        script_lines.extend(\n            [\n                f\"#SBATCH --err={slurm_job.slurm_stderr_remote}\",\n                f\"#SBATCH --out={slurm_job.slurm_stdout_remote}\",\n                f\"#SBATCH -D {slurm_job.workdir_remote}\",\n            ]\n        )\n        script_lines = slurm_config.sort_script_lines(script_lines)\n        logger.debug(f\"[_prepare_single_slurm_job] {script_lines=}\")\n\n        # Always print output of `uname -n` and `pwd`\n        script_lines.append('\\necho \"Hostname: $(uname -n)\"')\n        script_lines.append('echo \"Current directory: $(pwd)\"')\n        script_lines.append('echo \"Start time: $(date +\"%Y-%m-%dT%H:%M:%S%z\")\"')\n\n        # Complete script preamble\n        script_lines.append(\"\\n\")\n\n        # Include command lines\n        for cmd in cmdlines:\n            if slurm_config.use_mem_per_cpu:\n                mem_specific = f\"--mem-per-cpu={slurm_config.mem_per_cpu_MB}MB\"\n            else:\n                mem_specific = f\"--mem={slurm_config.mem_per_task_MB}MB\"\n            script_lines.append(\n                \"srun --ntasks=1 --cpus-per-task=$SLURM_CPUS_PER_TASK \"\n                f\"{mem_specific} \"\n                f\"{cmd} &amp;\"\n            )\n        script_lines.append(\"wait\\n\\n\")\n        script_lines.append('echo \"End time:   $(date +\"%Y-%m-%dT%H:%M:%S%z\")\"')\n        script = \"\\n\".join(script_lines)\n\n        # Write submission script\n        with open(slurm_job.slurm_submission_script_local, \"w\") as f:\n            f.write(script)\n        logger.debug(\n            \"[_prepare_single_slurm_job] Written \"\n            f\"{slurm_job.slurm_submission_script_local=}\"\n        )\n\n        if self.slurm_runner_type == \"ssh\":\n            submit_command = (\n                f\"sbatch --parsable {slurm_job.slurm_submission_script_remote}\"\n            )\n        else:\n            submit_command = (\n                f\"sbatch --parsable {slurm_job.slurm_submission_script_local}\"\n            )\n        logger.debug(\"[_prepare_single_slurm_job] END\")\n        return submit_command\n\n    def _send_many_job_inputs(\n        self, *, workdir_local: Path, workdir_remote: Path\n    ) -&gt; None:\n        \"\"\"\n        Placeholder method.\n\n        This method is intentionally left unimplemented in the base class.\n        Subclasses must override it to provide the logic for transferring\n        input data.\n        \"\"\"\n        pass\n\n    def _submit_single_sbatch(\n        self,\n        *,\n        submit_command: str,\n        slurm_job: SlurmJob,\n    ) -&gt; None:\n        \"\"\"\n        Run `sbatch` and add the `slurm_job` to `self.jobs`.\n\n        Args:\n            submit_command:\n                The SLURM submission command prepared in\n                `self._prepare_single_slurm_job`.\n            slurm_job: The `SlurmJob` object.\n        \"\"\"\n\n        logger.debug(\"[_submit_single_sbatch] START\")\n\n        # Submit SLURM job and retrieve job ID\n        logger.debug(f\"[_submit_single_sbatch] Now run {submit_command=}\")\n        sbatch_stdout = self._run_remote_cmd(submit_command)\n        logger.info(f\"[_submit_single_sbatch] {sbatch_stdout=}\")\n        stdout = sbatch_stdout.strip(\"\\n\")\n        submitted_job_id = int(stdout)\n        slurm_job.slurm_job_id = str(submitted_job_id)\n\n        # Add job to self.jobs\n        self.jobs[slurm_job.slurm_job_id] = slurm_job\n        logger.debug(\n            \"[_submit_single_sbatch] Added \"\n            f\"{slurm_job.slurm_job_id} to self.jobs.\"\n        )\n        logger.debug(\"[_submit_single_sbatch] END\")\n\n    def _fetch_artifacts(\n        self,\n        finished_slurm_jobs: list[SlurmJob],\n    ) -&gt; None:\n        raise NotImplementedError(\"Implement in child class.\")\n\n    def _check_slurm_account(self) -&gt; None:\n        \"\"\"\n        Check that SLURM account is not set here in `common_script_lines`.\n        \"\"\"\n        try:\n            invalid_line = next(\n                line\n                for line in self.common_script_lines\n                if line.startswith(\"#SBATCH --account=\")\n            )\n            raise RuntimeError(\n                \"Invalid line in `common_script_lines`: \"\n                f\"'{invalid_line}'.\\n\"\n                \"SLURM account must be set via the request body of the \"\n                \"apply-workflow endpoint, or by modifying the user properties.\"\n            )\n        except StopIteration:\n            pass\n\n    def _postprocess_single_task(\n        self,\n        *,\n        task: SlurmTask,\n        was_job_scancelled: bool = False,\n    ) -&gt; tuple[Any, Exception | None]:\n        try:\n            with open(task.output_file_local) as f:\n                output = json.load(f)\n            success = output[0]\n            if success:\n                # Task succeeded\n                result = output[1]\n                return (result, None)\n            else:\n                # Task failed in a controlled way, and produced an `output`\n                # object which is a dictionary with required keys\n                # `exc_type_name` and `traceback_string` and with optional\n                # keys `workflow_task_order`, `workflow_task_id` and\n                # `task_name`.\n                exc_proxy = output[1]\n                exc_type_name = exc_proxy.get(\"exc_type_name\")\n                logger.debug(\n                    f\"Output file contains a '{exc_type_name}' exception.\"\n                )\n                traceback_string = output[1].get(\"traceback_string\")\n                exception = TaskExecutionError(\n                    traceback_string,\n                    workflow_task_id=task.workflow_task_id,\n                    workflow_task_order=task.workflow_task_order,\n                    task_name=task.task_name,\n                )\n                return (None, exception)\n\n        except Exception as e:\n            exception = JobExecutionError(f\"ERROR, {str(e)}\")\n            # If job was scancelled and task failed, replace\n            # exception with a shutdown-related one.\n            if was_job_scancelled:\n                logger.debug(\n                    \"Replacing exception with a shutdown-related one, \"\n                    f\"for {task.index=}.\"\n                )\n                exception = SHUTDOWN_EXCEPTION\n            return (None, exception)\n        finally:\n            Path(task.input_file_local).unlink(missing_ok=True)\n            Path(task.output_file_local).unlink(missing_ok=True)\n\n    def _extract_slurm_error(self, slurm_job: SlurmJob) -&gt; str | None:\n        \"\"\"\n        Extract stderr of SLURM job, or `None`.\n\n        Note: this method reads the _local_ stderr file, and then it should\n        always be called _after_ fetching remote artifacts (e.g. in an SSH\n        deployment).\n        \"\"\"\n\n        stderr_path = slurm_job.slurm_stderr_local_path\n\n        if not stderr_path.exists():\n            return None\n\n        try:\n            with open(stderr_path) as f:\n                stderr_lines = [\n                    line\n                    for line in f.readlines()\n                    if not ignore_stderr_line(line)\n                ]\n                stderr_content = \"\\n\".join(stderr_lines)\n                stderr_content = stderr_content.strip()\n            if stderr_content:\n                return stderr_content\n        except Exception as e:\n            logger.error(f\"Failed to read SLURM stderr file: {e}\")\n\n        return None\n\n    def _set_executor_error_log(self, slurm_jobs: list[SlurmJob]) -&gt; None:\n        \"\"\"\n        If `executor_error_log` is unset, update it based on a list of jobs.\n\n        Notes:\n        1. This method must be executed **after** `_fetch_artifacts`.\n        2. This method only captures the first error it finds.\n        \"\"\"\n        if self.executor_error_log is not None:\n            # `executor_error_log` is already set, exit\n            return\n        for slurm_job in slurm_jobs:\n            slurm_error = self._extract_slurm_error(slurm_job)\n            if slurm_error is not None:\n                logger.warning(f\"SLURM error detected: {slurm_error}\")\n                self.executor_error_log = slurm_error\n                return\n\n    def is_shutdown(self) -&gt; bool:\n        return self.shutdown_file.exists()\n\n    @property\n    def job_ids(self) -&gt; list[str]:\n        return list(self.jobs.keys())\n\n    @property\n    def job_ids_int(self) -&gt; list[int]:\n        return list(map(int, self.jobs.keys()))\n\n    def wait_and_check_shutdown(self) -&gt; list[str]:\n        \"\"\"\n        Wait at most `self.poll_interval`, while also checking for shutdown.\n        \"\"\"\n        # Sleep for `self.poll_interval`, but keep checking for shutdowns\n        start_time = time.perf_counter()\n        # Always wait at least 0.2 (note: this is for cases where\n        # `poll_interval=0`).\n        waiting_time = max(self.poll_interval, 0.2)\n        max_time = start_time + waiting_time\n        logger.debug(\n            \"[wait_and_check_shutdown] \"\n            f\"I will wait at most {self.poll_interval} s, \"\n            f\"in blocks of {self.poll_interval_internal} s.\"\n        )\n\n        while time.perf_counter() &lt; max_time:\n            if self.is_shutdown():\n                logger.info(\"[wait_and_check_shutdown] Shutdown file detected\")\n                scancelled_job_ids = self.scancel_jobs()\n                logger.info(f\"[wait_and_check_shutdown] {scancelled_job_ids=}\")\n                return scancelled_job_ids\n            time.sleep(self.poll_interval_internal)\n\n        logger.debug(\"[wait_and_check_shutdown] No shutdown file detected\")\n        return []\n\n    def _check_no_active_jobs(self):\n        if self.jobs != {}:\n            raise JobExecutionError(\n                \"Unexpected branch: jobs must be empty before new submissions.\"\n            )\n\n    def submit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        parameters: dict[str, Any],\n        history_unit_id: int,\n        task_files: TaskFiles,\n        config: SlurmConfig,\n        task_type: SubmitTaskType,\n        user_id: int,\n    ) -&gt; tuple[Any, Exception | None]:\n        \"\"\"\n        Run a single fractal task.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            parameters: Dictionary of parameters.\n            history_unit_id:\n                Database ID of the corresponding `HistoryUnit` entry.\n            task_type: Task type.\n            task_files: `TaskFiles` object.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n        logger.debug(\"[submit] START\")\n\n        # Always refresh `executor_error_log` before starting a task\n        self.executor_error_log = None\n\n        config = self._enrich_slurm_config(config)\n\n        try:\n            workdir_local = task_files.wftask_subfolder_local\n            workdir_remote = task_files.wftask_subfolder_remote\n\n            if self.is_shutdown():\n                with next(get_sync_db()) as db:\n                    update_status_of_history_unit(\n                        history_unit_id=history_unit_id,\n                        status=HistoryUnitStatus.FAILED,\n                        db_sync=db,\n                    )\n\n                return None, SHUTDOWN_EXCEPTION\n\n            self._check_no_active_jobs()\n\n            # Validation phase\n            self.validate_submit_parameters(\n                parameters=parameters,\n                task_type=task_type,\n            )\n\n            # Create task subfolder\n            logger.debug(\"[submit] Create local/remote folders - START\")\n            self._mkdir_local_folder(folder=workdir_local.as_posix())\n            self._mkdir_remote_folder(folder=workdir_remote.as_posix())\n            logger.debug(\"[submit] Create local/remote folders - END\")\n\n            # Submission phase\n            slurm_job = SlurmJob(\n                prefix=task_files.prefix,\n                workdir_local=workdir_local,\n                workdir_remote=workdir_remote,\n                tasks=[\n                    SlurmTask(\n                        prefix=task_files.prefix,\n                        index=0,\n                        component=task_files.component,\n                        parameters=parameters,\n                        workdir_remote=workdir_remote,\n                        workdir_local=workdir_local,\n                        task_files=task_files,\n                        workflow_task_order=workflow_task_order,\n                        workflow_task_id=workflow_task_id,\n                        task_name=task_name,\n                    )\n                ],\n            )\n\n            config.parallel_tasks_per_job = 1\n            submit_command = self._prepare_single_slurm_job(\n                base_command=base_command,\n                slurm_job=slurm_job,\n                slurm_config=config,\n            )\n            self._send_many_job_inputs(\n                workdir_local=workdir_local,\n                workdir_remote=workdir_remote,\n            )\n            self._submit_single_sbatch(\n                submit_command=submit_command,\n                slurm_job=slurm_job,\n            )\n            logger.debug(f\"[submit] END submission phase, {self.job_ids=}\")\n\n            create_accounting_record_slurm(\n                user_id=user_id,\n                slurm_job_ids=self.job_ids_int,\n            )\n\n            # Retrieval phase\n            logger.debug(\"[submit] START retrieval phase\")\n            scancelled_job_ids = []\n            while len(self.jobs) &gt; 0:\n                # Look for finished jobs\n                finished_job_ids = self._get_finished_jobs(job_ids=self.job_ids)\n                logger.debug(f\"[submit] {finished_job_ids=}\")\n                finished_jobs = [\n                    self.jobs[_slurm_job_id]\n                    for _slurm_job_id in finished_job_ids\n                ]\n\n                self._fetch_artifacts(finished_jobs)\n\n                # Extract SLURM errors\n                self._set_executor_error_log(finished_jobs)\n\n                with next(get_sync_db()) as db:\n                    for slurm_job_id in finished_job_ids:\n                        logger.debug(f\"[submit] Now process {slurm_job_id=}\")\n                        slurm_job = self.jobs.pop(slurm_job_id)\n                        was_job_scancelled = slurm_job_id in scancelled_job_ids\n                        result, exception = self._postprocess_single_task(\n                            task=slurm_job.tasks[0],\n                            was_job_scancelled=was_job_scancelled,\n                        )\n\n                        if exception is not None:\n                            update_status_of_history_unit(\n                                history_unit_id=history_unit_id,\n                                status=HistoryUnitStatus.FAILED,\n                                db_sync=db,\n                            )\n                        else:\n                            if task_type not in [\n                                TaskType.COMPOUND,\n                                TaskType.CONVERTER_COMPOUND,\n                            ]:\n                                update_status_of_history_unit(\n                                    history_unit_id=history_unit_id,\n                                    status=HistoryUnitStatus.DONE,\n                                    db_sync=db,\n                                )\n\n                if len(self.jobs) &gt; 0:\n                    scancelled_job_ids = self.wait_and_check_shutdown()\n\n            logger.debug(\"[submit] END\")\n            return result, exception\n\n        except Exception as e:\n            logger.error(\n                f\"[submit] Unexpected exception. Original error: {str(e)}\"\n            )\n            with next(get_sync_db()) as db:\n                update_status_of_history_unit(\n                    history_unit_id=history_unit_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n            self.scancel_jobs()\n            return None, e\n\n    def multisubmit(\n        self,\n        *,\n        base_command: str,\n        workflow_task_order: int,\n        workflow_task_id: int,\n        task_name: str,\n        list_parameters: list[dict[str, Any]],\n        history_unit_ids: list[int],\n        list_task_files: list[TaskFiles],\n        task_type: MultisubmitTaskType,\n        config: SlurmConfig,\n        user_id: int,\n    ) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n        \"\"\"\n        Run a parallel fractal task.\n\n        Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n        have the same size. For parallel tasks, this is also the number of\n        input images, while for compound tasks these can differ.\n\n        Args:\n            base_command:\n            workflow_task_order:\n            workflow_task_id:\n            task_name:\n            list_parameters:\n                List of dictionaries of parameters (each one must include\n                `zarr_urls` key).\n            history_unit_ids:\n                Database IDs of the corresponding `HistoryUnit` entries.\n            list_task_files: `TaskFiles` objects.\n            task_type: Task type.\n            config: Runner-specific parameters.\n            user_id:\n        \"\"\"\n\n        # Always refresh `executor_error_log` before starting a task\n        self.executor_error_log = None\n\n        config = self._enrich_slurm_config(config)\n\n        results: dict[int, Any] = {}\n        exceptions: dict[int, BaseException] = {}\n\n        logger.debug(f\"[multisubmit] START, {len(list_parameters)=}\")\n        try:\n            if self.is_shutdown():\n                if task_type == TaskType.PARALLEL:\n                    with next(get_sync_db()) as db:\n                        bulk_update_status_of_history_unit(\n                            history_unit_ids=history_unit_ids,\n                            status=HistoryUnitStatus.FAILED,\n                            db_sync=db,\n                        )\n                results = {}\n                exceptions = {\n                    ind: SHUTDOWN_EXCEPTION\n                    for ind in range(len(list_parameters))\n                }\n                return results, exceptions\n\n            self._check_no_active_jobs()\n            self.validate_multisubmit_parameters(\n                list_parameters=list_parameters,\n                task_type=task_type,\n                list_task_files=list_task_files,\n                history_unit_ids=history_unit_ids,\n            )\n\n            workdir_local = list_task_files[0].wftask_subfolder_local\n            workdir_remote = list_task_files[0].wftask_subfolder_remote\n\n            # Create local&amp;remote task subfolders\n            if task_type == TaskType.PARALLEL:\n                self._mkdir_local_folder(workdir_local.as_posix())\n                self._mkdir_remote_folder(folder=workdir_remote.as_posix())\n\n            # NOTE: chunking has already taken place in `get_slurm_config`,\n            # so that `config.tasks_per_job` is now set.\n\n            # Divide arguments in batches of `tasks_per_job` tasks each\n            tot_tasks = len(list_parameters)\n            args_batches = []\n            batch_size = config.tasks_per_job\n            for ind_chunk in range(0, tot_tasks, batch_size):\n                args_batches.append(\n                    list_parameters[ind_chunk : ind_chunk + batch_size]  # noqa\n                )\n            if len(args_batches) != math.ceil(tot_tasks / config.tasks_per_job):\n                raise RuntimeError(\"Something wrong here while batching tasks\")\n\n            # Part 1/3: Iterate over chunks, prepare SlurmJob objects\n            logger.debug(\"[multisubmit] Prepare `SlurmJob`s.\")\n            jobs_to_submit = []\n            for ind_batch, chunk in enumerate(args_batches):\n                # Read prefix based on the first task of this batch\n                prefix = list_task_files[ind_batch * batch_size].prefix\n                tasks = []\n                for ind_chunk, parameters in enumerate(chunk):\n                    index = (ind_batch * batch_size) + ind_chunk\n                    tasks.append(\n                        SlurmTask(\n                            prefix=prefix,\n                            index=index,\n                            component=list_task_files[index].component,\n                            workdir_local=workdir_local,\n                            workdir_remote=workdir_remote,\n                            parameters=parameters,\n                            zarr_url=parameters[\"zarr_url\"],\n                            task_files=list_task_files[index],\n                            workflow_task_order=workflow_task_order,\n                            workflow_task_id=workflow_task_id,\n                            task_name=task_name,\n                        ),\n                    )\n                jobs_to_submit.append(\n                    SlurmJob(\n                        prefix=prefix,\n                        workdir_local=workdir_local,\n                        workdir_remote=workdir_remote,\n                        tasks=tasks,\n                    )\n                )\n\n            submit_commands = []\n            for slurm_job in jobs_to_submit:\n                submit_commands.append(\n                    self._prepare_single_slurm_job(\n                        base_command=base_command,\n                        slurm_job=slurm_job,\n                        slurm_config=config,\n                    )\n                )\n            self._send_many_job_inputs(\n                workdir_local=workdir_local,\n                workdir_remote=workdir_remote,\n            )\n            for slurm_job, submit_command in zip(\n                jobs_to_submit, submit_commands\n            ):\n                self._submit_single_sbatch(\n                    submit_command=submit_command,\n                    slurm_job=slurm_job,\n                )\n\n            logger.info(f\"[multisubmit] END submission phase, {self.job_ids=}\")\n\n            create_accounting_record_slurm(\n                user_id=user_id,\n                slurm_job_ids=self.job_ids_int,\n            )\n\n        except Exception as e:\n            logger.error(\n                \"[multisubmit] Unexpected exception during submission.\"\n                f\" Original error {str(e)}\"\n            )\n            self.scancel_jobs()\n            if task_type == TaskType.PARALLEL:\n                with next(get_sync_db()) as db:\n                    bulk_update_status_of_history_unit(\n                        history_unit_ids=history_unit_ids,\n                        status=HistoryUnitStatus.FAILED,\n                        db_sync=db,\n                    )\n            results: dict[int, Any] = {}\n            exceptions: dict[int, BaseException] = {\n                ind: e for ind in range(len(list_parameters))\n            }\n            return results, exceptions\n\n        # Retrieval phase\n        logger.debug(\"[multisubmit] START retrieval phase\")\n        scancelled_job_ids = []\n        while len(self.jobs) &gt; 0:\n            # Look for finished jobs\n            finished_job_ids = self._get_finished_jobs(job_ids=self.job_ids)\n            logger.debug(f\"[multisubmit] {finished_job_ids=}\")\n            finished_jobs = [\n                self.jobs[_slurm_job_id] for _slurm_job_id in finished_job_ids\n            ]\n\n            fetch_artifacts_exception = None\n            try:\n                self._fetch_artifacts(finished_jobs)\n            except Exception as e:\n                logger.error(\n                    \"[multisubmit] Unexpected exception in \"\n                    \"`_fetch_artifacts`. \"\n                    f\"Original error: {str(e)}\"\n                )\n                fetch_artifacts_exception = e\n\n            # Extract SLURM errors\n            self._set_executor_error_log(finished_jobs)\n\n            with next(get_sync_db()) as db:\n                for slurm_job_id in finished_job_ids:\n                    logger.debug(f\"[multisubmit] Now process {slurm_job_id=}\")\n                    slurm_job = self.jobs.pop(slurm_job_id)\n                    for task in slurm_job.tasks:\n                        logger.debug(f\"[multisubmit] Now process {task.index=}\")\n                        was_job_scancelled = slurm_job_id in scancelled_job_ids\n                        if fetch_artifacts_exception is not None:\n                            result = None\n                            exception = fetch_artifacts_exception\n                        else:\n                            try:\n                                (\n                                    result,\n                                    exception,\n                                ) = self._postprocess_single_task(\n                                    task=task,\n                                    was_job_scancelled=was_job_scancelled,\n                                )\n                            except Exception as e:\n                                logger.error(\n                                    \"[multisubmit] Unexpected exception in \"\n                                    \"`_postprocess_single_task`. \"\n                                    f\"Original error: {str(e)}\"\n                                )\n                                result = None\n                                exception = e\n                        # Note: the relevant done/failed check is based on\n                        # whether `exception is None`. The fact that\n                        # `result is None` is not relevant for this purpose.\n                        if exception is not None:\n                            exceptions[task.index] = exception\n                            if task_type == TaskType.PARALLEL:\n                                update_status_of_history_unit(\n                                    history_unit_id=history_unit_ids[\n                                        task.index\n                                    ],\n                                    status=HistoryUnitStatus.FAILED,\n                                    db_sync=db,\n                                )\n                        else:\n                            results[task.index] = result\n                            if task_type == TaskType.PARALLEL:\n                                update_status_of_history_unit(\n                                    history_unit_id=history_unit_ids[\n                                        task.index\n                                    ],\n                                    status=HistoryUnitStatus.DONE,\n                                    db_sync=db,\n                                )\n\n            if len(self.jobs) &gt; 0:\n                scancelled_job_ids = self.wait_and_check_shutdown()\n\n        logger.debug(\"[multisubmit] END\")\n        return results, exceptions\n\n    def check_fractal_server_versions(self) -&gt; None:\n        \"\"\"\n        Compare fractal-server versions of local/remote Python interpreters.\n        \"\"\"\n\n        # Skip check when the local and remote interpreters are the same\n        # (notably for some sudo-slurm deployments)\n        if self.python_worker_interpreter == sys.executable:\n            return\n\n        # Fetch remote fractal-server version\n        cmd = (\n            f\"{self.python_worker_interpreter} \"\n            \"-m fractal_server.runner.versions\"\n        )\n        stdout = self._run_remote_cmd(cmd)\n        remote_version = json.loads(stdout.strip(\"\\n\"))[\"fractal_server\"]\n\n        # Verify local/remote version match\n        if remote_version != __VERSION__:\n            error_msg = (\n                \"Fractal-server version mismatch.\\n\"\n                \"Local interpreter: \"\n                f\"({sys.executable}): {__VERSION__}.\\n\"\n                \"Remote interpreter: \"\n                f\"({self.python_worker_interpreter}): {remote_version}.\"\n            )\n            logger.error(error_msg)\n            raise RuntimeError(error_msg)\n\n    def scancel_jobs(self) -&gt; list[str]:\n        logger.info(\"[scancel_jobs] START\")\n        scancelled_job_ids = self.job_ids\n        if self.jobs:\n            scancel_string = \" \".join(scancelled_job_ids)\n            scancel_cmd = f\"scancel {scancel_string}\"\n            logger.warning(f\"[scancel_jobs] {scancel_string}\")\n            try:\n                self._run_remote_cmd(scancel_cmd)\n            except Exception as e:\n                logger.error(\n                    \"[scancel_jobs] `scancel` command failed. \"\n                    f\"Original error:\\n{str(e)}\"\n                )\n        logger.info(\"[scancel_jobs] END\")\n        return scancelled_job_ids\n\n    def validate_slurm_jobs_workdirs(\n        self,\n        slurm_jobs: list[SlurmJob],\n    ) -&gt; None:\n        \"\"\"\n        Check that a list of `SlurmJob`s have homogeneous working folders.\n        \"\"\"\n        set_workdir_local = {_job.workdir_local for _job in slurm_jobs}\n        set_workdir_remote = {_job.workdir_remote for _job in slurm_jobs}\n        if len(set_workdir_local) &gt; 1:\n            raise ValueError(f\"Non-unique values in {set_workdir_local=}.\")\n        if len(set_workdir_remote) &gt; 1:\n            raise ValueError(f\"Non-unique values in {set_workdir_remote=}.\")\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._check_slurm_account","title":"<code>_check_slurm_account()</code>","text":"<p>Check that SLURM account is not set here in <code>common_script_lines</code>.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _check_slurm_account(self) -&gt; None:\n    \"\"\"\n    Check that SLURM account is not set here in `common_script_lines`.\n    \"\"\"\n    try:\n        invalid_line = next(\n            line\n            for line in self.common_script_lines\n            if line.startswith(\"#SBATCH --account=\")\n        )\n        raise RuntimeError(\n            \"Invalid line in `common_script_lines`: \"\n            f\"'{invalid_line}'.\\n\"\n            \"SLURM account must be set via the request body of the \"\n            \"apply-workflow endpoint, or by modifying the user properties.\"\n        )\n    except StopIteration:\n        pass\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._enrich_slurm_config","title":"<code>_enrich_slurm_config(slurm_config)</code>","text":"<p>Return an enriched <code>SlurmConfig</code> object</p> <p>Include <code>self.account</code> and <code>self.common_script_lines</code> into a <code>SlurmConfig</code> object. Extracting this logic into an independent class method is useful to fix issue #2659 (which was due to performing this same operation multiple times rather than once).</p> PARAMETER DESCRIPTION <code>slurm_config</code> <p>The original <code>SlurmConfig</code> object.</p> <p> TYPE: <code>SlurmConfig</code> </p> RETURNS DESCRIPTION <code>SlurmConfig</code> <p>A new, up-to-date, <code>SlurmConfig</code> object.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _enrich_slurm_config(\n    self,\n    slurm_config: SlurmConfig,\n) -&gt; SlurmConfig:\n    \"\"\"\n    Return an enriched `SlurmConfig` object\n\n    Include `self.account` and `self.common_script_lines` into a\n    `SlurmConfig` object. Extracting this logic into an independent\n    class method is useful to fix issue #2659 (which was due to\n    performing this same operation multiple times rather than once).\n\n    Args:\n        slurm_config: The original `SlurmConfig` object.\n\n    Returns:\n        A new, up-to-date, `SlurmConfig` object.\n    \"\"\"\n\n    new_slurm_config = slurm_config.model_copy(deep=True)\n\n    # Include SLURM account in `slurm_config`.\n    if self.slurm_account is not None:\n        new_slurm_config.account = self.slurm_account\n\n    # Include common_script_lines in extra_lines\n    if len(self.common_script_lines) &gt; 0:\n        logger.debug(\n            f\"Add {self.common_script_lines} to \"\n            f\"{new_slurm_config.extra_lines=}.\"\n        )\n        current_extra_lines = new_slurm_config.extra_lines\n        new_slurm_config.extra_lines = (\n            current_extra_lines + self.common_script_lines\n        )\n\n    return new_slurm_config\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._extract_slurm_error","title":"<code>_extract_slurm_error(slurm_job)</code>","text":"<p>Extract stderr of SLURM job, or <code>None</code>.</p> <p>Note: this method reads the local stderr file, and then it should always be called after fetching remote artifacts (e.g. in an SSH deployment).</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _extract_slurm_error(self, slurm_job: SlurmJob) -&gt; str | None:\n    \"\"\"\n    Extract stderr of SLURM job, or `None`.\n\n    Note: this method reads the _local_ stderr file, and then it should\n    always be called _after_ fetching remote artifacts (e.g. in an SSH\n    deployment).\n    \"\"\"\n\n    stderr_path = slurm_job.slurm_stderr_local_path\n\n    if not stderr_path.exists():\n        return None\n\n    try:\n        with open(stderr_path) as f:\n            stderr_lines = [\n                line\n                for line in f.readlines()\n                if not ignore_stderr_line(line)\n            ]\n            stderr_content = \"\\n\".join(stderr_lines)\n            stderr_content = stderr_content.strip()\n        if stderr_content:\n            return stderr_content\n    except Exception as e:\n        logger.error(f\"Failed to read SLURM stderr file: {e}\")\n\n    return None\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._is_squeue_error_recoverable","title":"<code>_is_squeue_error_recoverable(exception)</code>","text":"<p>Determine whether a <code>squeue</code> error is considered recoverable.</p> <p>A recoverable error is one which will disappear after some time, without any specific action from the <code>fractal-server</code> side.</p> <p>Note: if this function returns <code>True</code> for an error that does not actually recover, this leads to an infinite loop  where <code>fractal-server</code> keeps polling <code>squeue</code> information forever.</p> <p>More info at https://github.com/fractal-analytics-platform/fractal-server/issues/2682</p> PARAMETER DESCRIPTION <code>exception</code> <p>The exception raised by <code>self.run_squeue</code>.</p> <p> TYPE: <code>BaseException</code> </p> <p>Returns:     Whether the error is considered recoverable.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _is_squeue_error_recoverable(self, exception: BaseException) -&gt; bool:\n    \"\"\"\n    Determine whether a `squeue` error is considered recoverable.\n\n    A _recoverable_ error is one which will disappear after some time,\n    without any specific action from the `fractal-server` side.\n\n    Note: if this function returns `True` for an error that does not\n    actually recover, this leads to an infinite loop  where\n    `fractal-server` keeps polling `squeue` information forever.\n\n    More info at\n    https://github.com/fractal-analytics-platform/fractal-server/issues/2682\n\n    Args:\n        exception: The exception raised by `self.run_squeue`.\n    Returns:\n        Whether the error is considered recoverable.\n    \"\"\"\n    str_exception = str(exception)\n    if (\n        \"slurm_load_jobs\" in str_exception\n        and \"Socket timed out on send/recv operation\" in str_exception\n    ):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._prepare_single_slurm_job","title":"<code>_prepare_single_slurm_job(*, base_command, slurm_job, slurm_config)</code>","text":"<p>Prepare submission script locally.</p> PARAMETER DESCRIPTION <code>base_command</code> <p>Base of task executable command.</p> <p> TYPE: <code>str</code> </p> <code>slurm_job</code> <p><code>SlurmJob</code> object</p> <p> TYPE: <code>SlurmJob</code> </p> <code>slurm_config</code> <p>Configuration for SLURM job</p> <p> TYPE: <code>SlurmConfig</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Command to submit the SLURM job.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _prepare_single_slurm_job(\n    self,\n    *,\n    base_command: str,\n    slurm_job: SlurmJob,\n    slurm_config: SlurmConfig,\n) -&gt; str:\n    \"\"\"\n    Prepare submission script locally.\n\n    Args:\n        base_command: Base of task executable command.\n        slurm_job: `SlurmJob` object\n        slurm_config: Configuration for SLURM job\n\n    Returns:\n        Command to submit the SLURM job.\n    \"\"\"\n    logger.debug(\"[_prepare_single_slurm_job] START\")\n\n    for task in slurm_job.tasks:\n        # Write input file\n        if self.slurm_runner_type == \"ssh\":\n            args_file_remote = task.task_files.args_file_remote\n        else:\n            args_file_remote = task.task_files.args_file_local\n        metadiff_file_remote = task.task_files.metadiff_file_remote\n        full_command = (\n            f\"{base_command} \"\n            f\"--args-json {args_file_remote} \"\n            f\"--out-json {metadiff_file_remote}\"\n        )\n\n        input_data = RemoteInputData(\n            full_command=full_command,\n            python_version=sys.version_info[:3],\n            fractal_server_version=__VERSION__,\n            metadiff_file_remote=task.task_files.metadiff_file_remote,\n            log_file_remote=task.task_files.log_file_remote,\n        )\n\n        with open(task.input_file_local, \"w\") as f:\n            json.dump(input_data.model_dump(), f, indent=2)\n\n        with open(task.task_files.args_file_local, \"w\") as f:\n            json.dump(task.parameters, f, indent=2)\n\n        logger.debug(\n            f\"[_prepare_single_slurm_job] Written {task.input_file_local=}\"\n        )\n\n    # Prepare commands to be included in SLURM submission script\n    cmdlines = []\n    for task in slurm_job.tasks:\n        if self.slurm_runner_type == \"ssh\":\n            input_file = task.input_file_remote\n        else:\n            input_file = task.input_file_local\n        output_file = task.output_file_remote\n        cmdlines.append(\n            f\"{self.python_worker_interpreter}\"\n            \" -m fractal_server.runner.\"\n            \"executors.slurm_common.remote \"\n            f\"--input-file {input_file} \"\n            f\"--output-file {output_file}\"\n        )\n\n    # Set ntasks\n    num_tasks_max_running = slurm_config.parallel_tasks_per_job\n    ntasks = min(len(cmdlines), num_tasks_max_running)\n    slurm_config.parallel_tasks_per_job = ntasks\n\n    # Prepare SLURM preamble based on SlurmConfig object\n    script_lines = slurm_config.to_sbatch_preamble(\n        remote_export_dir=self.user_cache_dir,\n        use_mem_per_cpu=slurm_config.use_mem_per_cpu,\n    )\n\n    # Extend SLURM preamble with variable which are not in SlurmConfig, and\n    # fix their order\n    script_lines.extend(\n        [\n            f\"#SBATCH --err={slurm_job.slurm_stderr_remote}\",\n            f\"#SBATCH --out={slurm_job.slurm_stdout_remote}\",\n            f\"#SBATCH -D {slurm_job.workdir_remote}\",\n        ]\n    )\n    script_lines = slurm_config.sort_script_lines(script_lines)\n    logger.debug(f\"[_prepare_single_slurm_job] {script_lines=}\")\n\n    # Always print output of `uname -n` and `pwd`\n    script_lines.append('\\necho \"Hostname: $(uname -n)\"')\n    script_lines.append('echo \"Current directory: $(pwd)\"')\n    script_lines.append('echo \"Start time: $(date +\"%Y-%m-%dT%H:%M:%S%z\")\"')\n\n    # Complete script preamble\n    script_lines.append(\"\\n\")\n\n    # Include command lines\n    for cmd in cmdlines:\n        if slurm_config.use_mem_per_cpu:\n            mem_specific = f\"--mem-per-cpu={slurm_config.mem_per_cpu_MB}MB\"\n        else:\n            mem_specific = f\"--mem={slurm_config.mem_per_task_MB}MB\"\n        script_lines.append(\n            \"srun --ntasks=1 --cpus-per-task=$SLURM_CPUS_PER_TASK \"\n            f\"{mem_specific} \"\n            f\"{cmd} &amp;\"\n        )\n    script_lines.append(\"wait\\n\\n\")\n    script_lines.append('echo \"End time:   $(date +\"%Y-%m-%dT%H:%M:%S%z\")\"')\n    script = \"\\n\".join(script_lines)\n\n    # Write submission script\n    with open(slurm_job.slurm_submission_script_local, \"w\") as f:\n        f.write(script)\n    logger.debug(\n        \"[_prepare_single_slurm_job] Written \"\n        f\"{slurm_job.slurm_submission_script_local=}\"\n    )\n\n    if self.slurm_runner_type == \"ssh\":\n        submit_command = (\n            f\"sbatch --parsable {slurm_job.slurm_submission_script_remote}\"\n        )\n    else:\n        submit_command = (\n            f\"sbatch --parsable {slurm_job.slurm_submission_script_local}\"\n        )\n    logger.debug(\"[_prepare_single_slurm_job] END\")\n    return submit_command\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._send_many_job_inputs","title":"<code>_send_many_job_inputs(*, workdir_local, workdir_remote)</code>","text":"<p>Placeholder method.</p> <p>This method is intentionally left unimplemented in the base class. Subclasses must override it to provide the logic for transferring input data.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _send_many_job_inputs(\n    self, *, workdir_local: Path, workdir_remote: Path\n) -&gt; None:\n    \"\"\"\n    Placeholder method.\n\n    This method is intentionally left unimplemented in the base class.\n    Subclasses must override it to provide the logic for transferring\n    input data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._set_executor_error_log","title":"<code>_set_executor_error_log(slurm_jobs)</code>","text":"<p>If <code>executor_error_log</code> is unset, update it based on a list of jobs.</p> <p>Notes: 1. This method must be executed after <code>_fetch_artifacts</code>. 2. This method only captures the first error it finds.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _set_executor_error_log(self, slurm_jobs: list[SlurmJob]) -&gt; None:\n    \"\"\"\n    If `executor_error_log` is unset, update it based on a list of jobs.\n\n    Notes:\n    1. This method must be executed **after** `_fetch_artifacts`.\n    2. This method only captures the first error it finds.\n    \"\"\"\n    if self.executor_error_log is not None:\n        # `executor_error_log` is already set, exit\n        return\n    for slurm_job in slurm_jobs:\n        slurm_error = self._extract_slurm_error(slurm_job)\n        if slurm_error is not None:\n            logger.warning(f\"SLURM error detected: {slurm_error}\")\n            self.executor_error_log = slurm_error\n            return\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner._submit_single_sbatch","title":"<code>_submit_single_sbatch(*, submit_command, slurm_job)</code>","text":"<p>Run <code>sbatch</code> and add the <code>slurm_job</code> to <code>self.jobs</code>.</p> PARAMETER DESCRIPTION <code>submit_command</code> <p>The SLURM submission command prepared in <code>self._prepare_single_slurm_job</code>.</p> <p> TYPE: <code>str</code> </p> <code>slurm_job</code> <p>The <code>SlurmJob</code> object.</p> <p> TYPE: <code>SlurmJob</code> </p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def _submit_single_sbatch(\n    self,\n    *,\n    submit_command: str,\n    slurm_job: SlurmJob,\n) -&gt; None:\n    \"\"\"\n    Run `sbatch` and add the `slurm_job` to `self.jobs`.\n\n    Args:\n        submit_command:\n            The SLURM submission command prepared in\n            `self._prepare_single_slurm_job`.\n        slurm_job: The `SlurmJob` object.\n    \"\"\"\n\n    logger.debug(\"[_submit_single_sbatch] START\")\n\n    # Submit SLURM job and retrieve job ID\n    logger.debug(f\"[_submit_single_sbatch] Now run {submit_command=}\")\n    sbatch_stdout = self._run_remote_cmd(submit_command)\n    logger.info(f\"[_submit_single_sbatch] {sbatch_stdout=}\")\n    stdout = sbatch_stdout.strip(\"\\n\")\n    submitted_job_id = int(stdout)\n    slurm_job.slurm_job_id = str(submitted_job_id)\n\n    # Add job to self.jobs\n    self.jobs[slurm_job.slurm_job_id] = slurm_job\n    logger.debug(\n        \"[_submit_single_sbatch] Added \"\n        f\"{slurm_job.slurm_job_id} to self.jobs.\"\n    )\n    logger.debug(\"[_submit_single_sbatch] END\")\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner.check_fractal_server_versions","title":"<code>check_fractal_server_versions()</code>","text":"<p>Compare fractal-server versions of local/remote Python interpreters.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def check_fractal_server_versions(self) -&gt; None:\n    \"\"\"\n    Compare fractal-server versions of local/remote Python interpreters.\n    \"\"\"\n\n    # Skip check when the local and remote interpreters are the same\n    # (notably for some sudo-slurm deployments)\n    if self.python_worker_interpreter == sys.executable:\n        return\n\n    # Fetch remote fractal-server version\n    cmd = (\n        f\"{self.python_worker_interpreter} \"\n        \"-m fractal_server.runner.versions\"\n    )\n    stdout = self._run_remote_cmd(cmd)\n    remote_version = json.loads(stdout.strip(\"\\n\"))[\"fractal_server\"]\n\n    # Verify local/remote version match\n    if remote_version != __VERSION__:\n        error_msg = (\n            \"Fractal-server version mismatch.\\n\"\n            \"Local interpreter: \"\n            f\"({sys.executable}): {__VERSION__}.\\n\"\n            \"Remote interpreter: \"\n            f\"({self.python_worker_interpreter}): {remote_version}.\"\n        )\n        logger.error(error_msg)\n        raise RuntimeError(error_msg)\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner.multisubmit","title":"<code>multisubmit(*, base_command, workflow_task_order, workflow_task_id, task_name, list_parameters, history_unit_ids, list_task_files, task_type, config, user_id)</code>","text":"<p>Run a parallel fractal task.</p> <p>Note: <code>list_parameters</code>, <code>list_task_files</code> and <code>history_unit_ids</code> have the same size. For parallel tasks, this is also the number of input images, while for compound tasks these can differ.</p> PARAMETER DESCRIPTION <code>base_command</code> <p> TYPE: <code>str</code> </p> <code>workflow_task_order</code> <p> TYPE: <code>int</code> </p> <code>workflow_task_id</code> <p> TYPE: <code>int</code> </p> <code>task_name</code> <p> TYPE: <code>str</code> </p> <code>list_parameters</code> <p>List of dictionaries of parameters (each one must include <code>zarr_urls</code> key).</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> <code>history_unit_ids</code> <p>Database IDs of the corresponding <code>HistoryUnit</code> entries.</p> <p> TYPE: <code>list[int]</code> </p> <code>list_task_files</code> <p><code>TaskFiles</code> objects.</p> <p> TYPE: <code>list[TaskFiles]</code> </p> <code>task_type</code> <p>Task type.</p> <p> TYPE: <code>MultisubmitTaskType</code> </p> <code>config</code> <p>Runner-specific parameters.</p> <p> TYPE: <code>SlurmConfig</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def multisubmit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    list_parameters: list[dict[str, Any]],\n    history_unit_ids: list[int],\n    list_task_files: list[TaskFiles],\n    task_type: MultisubmitTaskType,\n    config: SlurmConfig,\n    user_id: int,\n) -&gt; tuple[dict[int, Any], dict[int, BaseException]]:\n    \"\"\"\n    Run a parallel fractal task.\n\n    Note: `list_parameters`, `list_task_files` and `history_unit_ids`\n    have the same size. For parallel tasks, this is also the number of\n    input images, while for compound tasks these can differ.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        list_parameters:\n            List of dictionaries of parameters (each one must include\n            `zarr_urls` key).\n        history_unit_ids:\n            Database IDs of the corresponding `HistoryUnit` entries.\n        list_task_files: `TaskFiles` objects.\n        task_type: Task type.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n\n    # Always refresh `executor_error_log` before starting a task\n    self.executor_error_log = None\n\n    config = self._enrich_slurm_config(config)\n\n    results: dict[int, Any] = {}\n    exceptions: dict[int, BaseException] = {}\n\n    logger.debug(f\"[multisubmit] START, {len(list_parameters)=}\")\n    try:\n        if self.is_shutdown():\n            if task_type == TaskType.PARALLEL:\n                with next(get_sync_db()) as db:\n                    bulk_update_status_of_history_unit(\n                        history_unit_ids=history_unit_ids,\n                        status=HistoryUnitStatus.FAILED,\n                        db_sync=db,\n                    )\n            results = {}\n            exceptions = {\n                ind: SHUTDOWN_EXCEPTION\n                for ind in range(len(list_parameters))\n            }\n            return results, exceptions\n\n        self._check_no_active_jobs()\n        self.validate_multisubmit_parameters(\n            list_parameters=list_parameters,\n            task_type=task_type,\n            list_task_files=list_task_files,\n            history_unit_ids=history_unit_ids,\n        )\n\n        workdir_local = list_task_files[0].wftask_subfolder_local\n        workdir_remote = list_task_files[0].wftask_subfolder_remote\n\n        # Create local&amp;remote task subfolders\n        if task_type == TaskType.PARALLEL:\n            self._mkdir_local_folder(workdir_local.as_posix())\n            self._mkdir_remote_folder(folder=workdir_remote.as_posix())\n\n        # NOTE: chunking has already taken place in `get_slurm_config`,\n        # so that `config.tasks_per_job` is now set.\n\n        # Divide arguments in batches of `tasks_per_job` tasks each\n        tot_tasks = len(list_parameters)\n        args_batches = []\n        batch_size = config.tasks_per_job\n        for ind_chunk in range(0, tot_tasks, batch_size):\n            args_batches.append(\n                list_parameters[ind_chunk : ind_chunk + batch_size]  # noqa\n            )\n        if len(args_batches) != math.ceil(tot_tasks / config.tasks_per_job):\n            raise RuntimeError(\"Something wrong here while batching tasks\")\n\n        # Part 1/3: Iterate over chunks, prepare SlurmJob objects\n        logger.debug(\"[multisubmit] Prepare `SlurmJob`s.\")\n        jobs_to_submit = []\n        for ind_batch, chunk in enumerate(args_batches):\n            # Read prefix based on the first task of this batch\n            prefix = list_task_files[ind_batch * batch_size].prefix\n            tasks = []\n            for ind_chunk, parameters in enumerate(chunk):\n                index = (ind_batch * batch_size) + ind_chunk\n                tasks.append(\n                    SlurmTask(\n                        prefix=prefix,\n                        index=index,\n                        component=list_task_files[index].component,\n                        workdir_local=workdir_local,\n                        workdir_remote=workdir_remote,\n                        parameters=parameters,\n                        zarr_url=parameters[\"zarr_url\"],\n                        task_files=list_task_files[index],\n                        workflow_task_order=workflow_task_order,\n                        workflow_task_id=workflow_task_id,\n                        task_name=task_name,\n                    ),\n                )\n            jobs_to_submit.append(\n                SlurmJob(\n                    prefix=prefix,\n                    workdir_local=workdir_local,\n                    workdir_remote=workdir_remote,\n                    tasks=tasks,\n                )\n            )\n\n        submit_commands = []\n        for slurm_job in jobs_to_submit:\n            submit_commands.append(\n                self._prepare_single_slurm_job(\n                    base_command=base_command,\n                    slurm_job=slurm_job,\n                    slurm_config=config,\n                )\n            )\n        self._send_many_job_inputs(\n            workdir_local=workdir_local,\n            workdir_remote=workdir_remote,\n        )\n        for slurm_job, submit_command in zip(\n            jobs_to_submit, submit_commands\n        ):\n            self._submit_single_sbatch(\n                submit_command=submit_command,\n                slurm_job=slurm_job,\n            )\n\n        logger.info(f\"[multisubmit] END submission phase, {self.job_ids=}\")\n\n        create_accounting_record_slurm(\n            user_id=user_id,\n            slurm_job_ids=self.job_ids_int,\n        )\n\n    except Exception as e:\n        logger.error(\n            \"[multisubmit] Unexpected exception during submission.\"\n            f\" Original error {str(e)}\"\n        )\n        self.scancel_jobs()\n        if task_type == TaskType.PARALLEL:\n            with next(get_sync_db()) as db:\n                bulk_update_status_of_history_unit(\n                    history_unit_ids=history_unit_ids,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n        results: dict[int, Any] = {}\n        exceptions: dict[int, BaseException] = {\n            ind: e for ind in range(len(list_parameters))\n        }\n        return results, exceptions\n\n    # Retrieval phase\n    logger.debug(\"[multisubmit] START retrieval phase\")\n    scancelled_job_ids = []\n    while len(self.jobs) &gt; 0:\n        # Look for finished jobs\n        finished_job_ids = self._get_finished_jobs(job_ids=self.job_ids)\n        logger.debug(f\"[multisubmit] {finished_job_ids=}\")\n        finished_jobs = [\n            self.jobs[_slurm_job_id] for _slurm_job_id in finished_job_ids\n        ]\n\n        fetch_artifacts_exception = None\n        try:\n            self._fetch_artifacts(finished_jobs)\n        except Exception as e:\n            logger.error(\n                \"[multisubmit] Unexpected exception in \"\n                \"`_fetch_artifacts`. \"\n                f\"Original error: {str(e)}\"\n            )\n            fetch_artifacts_exception = e\n\n        # Extract SLURM errors\n        self._set_executor_error_log(finished_jobs)\n\n        with next(get_sync_db()) as db:\n            for slurm_job_id in finished_job_ids:\n                logger.debug(f\"[multisubmit] Now process {slurm_job_id=}\")\n                slurm_job = self.jobs.pop(slurm_job_id)\n                for task in slurm_job.tasks:\n                    logger.debug(f\"[multisubmit] Now process {task.index=}\")\n                    was_job_scancelled = slurm_job_id in scancelled_job_ids\n                    if fetch_artifacts_exception is not None:\n                        result = None\n                        exception = fetch_artifacts_exception\n                    else:\n                        try:\n                            (\n                                result,\n                                exception,\n                            ) = self._postprocess_single_task(\n                                task=task,\n                                was_job_scancelled=was_job_scancelled,\n                            )\n                        except Exception as e:\n                            logger.error(\n                                \"[multisubmit] Unexpected exception in \"\n                                \"`_postprocess_single_task`. \"\n                                f\"Original error: {str(e)}\"\n                            )\n                            result = None\n                            exception = e\n                    # Note: the relevant done/failed check is based on\n                    # whether `exception is None`. The fact that\n                    # `result is None` is not relevant for this purpose.\n                    if exception is not None:\n                        exceptions[task.index] = exception\n                        if task_type == TaskType.PARALLEL:\n                            update_status_of_history_unit(\n                                history_unit_id=history_unit_ids[\n                                    task.index\n                                ],\n                                status=HistoryUnitStatus.FAILED,\n                                db_sync=db,\n                            )\n                    else:\n                        results[task.index] = result\n                        if task_type == TaskType.PARALLEL:\n                            update_status_of_history_unit(\n                                history_unit_id=history_unit_ids[\n                                    task.index\n                                ],\n                                status=HistoryUnitStatus.DONE,\n                                db_sync=db,\n                            )\n\n        if len(self.jobs) &gt; 0:\n            scancelled_job_ids = self.wait_and_check_shutdown()\n\n    logger.debug(\"[multisubmit] END\")\n    return results, exceptions\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner.submit","title":"<code>submit(*, base_command, workflow_task_order, workflow_task_id, task_name, parameters, history_unit_id, task_files, config, task_type, user_id)</code>","text":"<p>Run a single fractal task.</p> PARAMETER DESCRIPTION <code>base_command</code> <p> TYPE: <code>str</code> </p> <code>workflow_task_order</code> <p> TYPE: <code>int</code> </p> <code>workflow_task_id</code> <p> TYPE: <code>int</code> </p> <code>task_name</code> <p> TYPE: <code>str</code> </p> <code>parameters</code> <p>Dictionary of parameters.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>history_unit_id</code> <p>Database ID of the corresponding <code>HistoryUnit</code> entry.</p> <p> TYPE: <code>int</code> </p> <code>task_type</code> <p>Task type.</p> <p> TYPE: <code>SubmitTaskType</code> </p> <code>task_files</code> <p><code>TaskFiles</code> object.</p> <p> TYPE: <code>TaskFiles</code> </p> <code>config</code> <p>Runner-specific parameters.</p> <p> TYPE: <code>SlurmConfig</code> </p> <code>user_id</code> <p> TYPE: <code>int</code> </p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def submit(\n    self,\n    *,\n    base_command: str,\n    workflow_task_order: int,\n    workflow_task_id: int,\n    task_name: str,\n    parameters: dict[str, Any],\n    history_unit_id: int,\n    task_files: TaskFiles,\n    config: SlurmConfig,\n    task_type: SubmitTaskType,\n    user_id: int,\n) -&gt; tuple[Any, Exception | None]:\n    \"\"\"\n    Run a single fractal task.\n\n    Args:\n        base_command:\n        workflow_task_order:\n        workflow_task_id:\n        task_name:\n        parameters: Dictionary of parameters.\n        history_unit_id:\n            Database ID of the corresponding `HistoryUnit` entry.\n        task_type: Task type.\n        task_files: `TaskFiles` object.\n        config: Runner-specific parameters.\n        user_id:\n    \"\"\"\n    logger.debug(\"[submit] START\")\n\n    # Always refresh `executor_error_log` before starting a task\n    self.executor_error_log = None\n\n    config = self._enrich_slurm_config(config)\n\n    try:\n        workdir_local = task_files.wftask_subfolder_local\n        workdir_remote = task_files.wftask_subfolder_remote\n\n        if self.is_shutdown():\n            with next(get_sync_db()) as db:\n                update_status_of_history_unit(\n                    history_unit_id=history_unit_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n\n            return None, SHUTDOWN_EXCEPTION\n\n        self._check_no_active_jobs()\n\n        # Validation phase\n        self.validate_submit_parameters(\n            parameters=parameters,\n            task_type=task_type,\n        )\n\n        # Create task subfolder\n        logger.debug(\"[submit] Create local/remote folders - START\")\n        self._mkdir_local_folder(folder=workdir_local.as_posix())\n        self._mkdir_remote_folder(folder=workdir_remote.as_posix())\n        logger.debug(\"[submit] Create local/remote folders - END\")\n\n        # Submission phase\n        slurm_job = SlurmJob(\n            prefix=task_files.prefix,\n            workdir_local=workdir_local,\n            workdir_remote=workdir_remote,\n            tasks=[\n                SlurmTask(\n                    prefix=task_files.prefix,\n                    index=0,\n                    component=task_files.component,\n                    parameters=parameters,\n                    workdir_remote=workdir_remote,\n                    workdir_local=workdir_local,\n                    task_files=task_files,\n                    workflow_task_order=workflow_task_order,\n                    workflow_task_id=workflow_task_id,\n                    task_name=task_name,\n                )\n            ],\n        )\n\n        config.parallel_tasks_per_job = 1\n        submit_command = self._prepare_single_slurm_job(\n            base_command=base_command,\n            slurm_job=slurm_job,\n            slurm_config=config,\n        )\n        self._send_many_job_inputs(\n            workdir_local=workdir_local,\n            workdir_remote=workdir_remote,\n        )\n        self._submit_single_sbatch(\n            submit_command=submit_command,\n            slurm_job=slurm_job,\n        )\n        logger.debug(f\"[submit] END submission phase, {self.job_ids=}\")\n\n        create_accounting_record_slurm(\n            user_id=user_id,\n            slurm_job_ids=self.job_ids_int,\n        )\n\n        # Retrieval phase\n        logger.debug(\"[submit] START retrieval phase\")\n        scancelled_job_ids = []\n        while len(self.jobs) &gt; 0:\n            # Look for finished jobs\n            finished_job_ids = self._get_finished_jobs(job_ids=self.job_ids)\n            logger.debug(f\"[submit] {finished_job_ids=}\")\n            finished_jobs = [\n                self.jobs[_slurm_job_id]\n                for _slurm_job_id in finished_job_ids\n            ]\n\n            self._fetch_artifacts(finished_jobs)\n\n            # Extract SLURM errors\n            self._set_executor_error_log(finished_jobs)\n\n            with next(get_sync_db()) as db:\n                for slurm_job_id in finished_job_ids:\n                    logger.debug(f\"[submit] Now process {slurm_job_id=}\")\n                    slurm_job = self.jobs.pop(slurm_job_id)\n                    was_job_scancelled = slurm_job_id in scancelled_job_ids\n                    result, exception = self._postprocess_single_task(\n                        task=slurm_job.tasks[0],\n                        was_job_scancelled=was_job_scancelled,\n                    )\n\n                    if exception is not None:\n                        update_status_of_history_unit(\n                            history_unit_id=history_unit_id,\n                            status=HistoryUnitStatus.FAILED,\n                            db_sync=db,\n                        )\n                    else:\n                        if task_type not in [\n                            TaskType.COMPOUND,\n                            TaskType.CONVERTER_COMPOUND,\n                        ]:\n                            update_status_of_history_unit(\n                                history_unit_id=history_unit_id,\n                                status=HistoryUnitStatus.DONE,\n                                db_sync=db,\n                            )\n\n            if len(self.jobs) &gt; 0:\n                scancelled_job_ids = self.wait_and_check_shutdown()\n\n        logger.debug(\"[submit] END\")\n        return result, exception\n\n    except Exception as e:\n        logger.error(\n            f\"[submit] Unexpected exception. Original error: {str(e)}\"\n        )\n        with next(get_sync_db()) as db:\n            update_status_of_history_unit(\n                history_unit_id=history_unit_id,\n                status=HistoryUnitStatus.FAILED,\n                db_sync=db,\n            )\n        self.scancel_jobs()\n        return None, e\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner.validate_slurm_jobs_workdirs","title":"<code>validate_slurm_jobs_workdirs(slurm_jobs)</code>","text":"<p>Check that a list of <code>SlurmJob</code>s have homogeneous working folders.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def validate_slurm_jobs_workdirs(\n    self,\n    slurm_jobs: list[SlurmJob],\n) -&gt; None:\n    \"\"\"\n    Check that a list of `SlurmJob`s have homogeneous working folders.\n    \"\"\"\n    set_workdir_local = {_job.workdir_local for _job in slurm_jobs}\n    set_workdir_remote = {_job.workdir_remote for _job in slurm_jobs}\n    if len(set_workdir_local) &gt; 1:\n        raise ValueError(f\"Non-unique values in {set_workdir_local=}.\")\n    if len(set_workdir_remote) &gt; 1:\n        raise ValueError(f\"Non-unique values in {set_workdir_remote=}.\")\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.BaseSlurmRunner.wait_and_check_shutdown","title":"<code>wait_and_check_shutdown()</code>","text":"<p>Wait at most <code>self.poll_interval</code>, while also checking for shutdown.</p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def wait_and_check_shutdown(self) -&gt; list[str]:\n    \"\"\"\n    Wait at most `self.poll_interval`, while also checking for shutdown.\n    \"\"\"\n    # Sleep for `self.poll_interval`, but keep checking for shutdowns\n    start_time = time.perf_counter()\n    # Always wait at least 0.2 (note: this is for cases where\n    # `poll_interval=0`).\n    waiting_time = max(self.poll_interval, 0.2)\n    max_time = start_time + waiting_time\n    logger.debug(\n        \"[wait_and_check_shutdown] \"\n        f\"I will wait at most {self.poll_interval} s, \"\n        f\"in blocks of {self.poll_interval_internal} s.\"\n    )\n\n    while time.perf_counter() &lt; max_time:\n        if self.is_shutdown():\n            logger.info(\"[wait_and_check_shutdown] Shutdown file detected\")\n            scancelled_job_ids = self.scancel_jobs()\n            logger.info(f\"[wait_and_check_shutdown] {scancelled_job_ids=}\")\n            return scancelled_job_ids\n        time.sleep(self.poll_interval_internal)\n\n    logger.debug(\"[wait_and_check_shutdown] No shutdown file detected\")\n    return []\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/base_slurm_runner/#fractal_server.runner.executors.slurm_common.base_slurm_runner.ignore_stderr_line","title":"<code>ignore_stderr_line(line)</code>","text":"<p>Whether to ignore a SLURM-job stderr line, based on <code>STDERR_EXCLUDE_PATTERNS</code>.</p> <p>The goal is not to flag some stderr files as relevant <code>executor_error_log</code> if they only include lines matching a given set of patterns to ignore. See https://github.com/fractal-analytics-platform/fractal-server/issues/2835</p> PARAMETER DESCRIPTION <code>line</code> <p>The line to be considered.</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/runner/executors/slurm_common/base_slurm_runner.py</code> <pre><code>def ignore_stderr_line(line: str) -&gt; bool:\n    \"\"\"\n    Whether to ignore a SLURM-job stderr line, based on\n    `STDERR_EXCLUDE_PATTERNS`.\n\n    The goal is not to flag some stderr files as relevant `executor_error_log`\n    if they only include lines matching a given set of patterns to ignore. See\n    https://github.com/fractal-analytics-platform/fractal-server/issues/2835\n\n    Args:\n        line: The line to be considered.\n    \"\"\"\n    line_lower = line.lower()\n    for pattern in STDERR_IGNORE_PATTERNS:\n        if pattern in line_lower:\n            return True\n    return False\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/get_slurm_config/","title":"get_slurm_config","text":""},{"location":"reference/runner/executors/slurm_common/get_slurm_config/#fractal_server.runner.executors.slurm_common.get_slurm_config._get_slurm_config_internal","title":"<code>_get_slurm_config_internal(shared_config, wftask, which_type)</code>","text":"<p>Prepare a specific <code>SlurmConfig</code> configuration.</p> <p>The base configuration is the runner-level <code>shared_config</code> object, based on <code>resource.jobs_runner_config</code> (note that GPU-specific properties take priority, when <code>needs_gpu=True</code>). We then incorporate attributes from <code>wftask.meta_{non_parallel,parallel}</code> - with higher priority.</p> PARAMETER DESCRIPTION <code>shared_config</code> <p>Configuration object based on <code>resource.jobs_runner_config</code>.</p> <p> TYPE: <code>JobRunnerConfigSLURM</code> </p> <code>wftask</code> <p>WorkflowTaskV2 for which the backend configuration should be prepared.</p> <p> TYPE: <code>WorkflowTaskV2</code> </p> <code>which_type</code> <p>Whether we should look at the non-parallel or parallel part of <code>wftask</code>.</p> <p> TYPE: <code>Literal['non_parallel', 'parallel']</code> </p> RETURNS DESCRIPTION <code>SlurmConfig</code> <p>A ready-to-use <code>SlurmConfig</code> object.</p> Source code in <code>fractal_server/runner/executors/slurm_common/get_slurm_config.py</code> <pre><code>def _get_slurm_config_internal(\n    shared_config: JobRunnerConfigSLURM,\n    wftask: WorkflowTaskV2,\n    which_type: Literal[\"non_parallel\", \"parallel\"],\n) -&gt; SlurmConfig:\n    \"\"\"\n\n    Prepare a specific `SlurmConfig` configuration.\n\n    The base configuration is the runner-level `shared_config` object, based\n    on `resource.jobs_runner_config` (note that GPU-specific properties take\n    priority, when `needs_gpu=True`). We then incorporate attributes from\n    `wftask.meta_{non_parallel,parallel}` - with higher priority.\n\n    Args:\n        shared_config:\n            Configuration object based on `resource.jobs_runner_config`.\n        wftask:\n            WorkflowTaskV2 for which the backend configuration should\n            be prepared.\n        which_type:\n            Whether we should look at the non-parallel or parallel part\n            of `wftask`.\n\n    Returns:\n        A ready-to-use `SlurmConfig` object.\n    \"\"\"\n\n    if which_type == \"non_parallel\":\n        wftask_meta = wftask.meta_non_parallel\n    elif which_type == \"parallel\":\n        wftask_meta = wftask.meta_parallel\n    else:\n        raise ValueError(\n            f\"get_slurm_config received invalid argument {which_type=}.\"\n        )\n\n    logger.debug(\n        f\"[get_slurm_config] WorkflowTask meta attribute: {wftask_meta=}\"\n    )\n\n    # Start from `shared_config`\n    slurm_dict = shared_config.default_slurm_config.model_dump(\n        exclude_unset=True, exclude={\"mem\"}\n    )\n    if shared_config.default_slurm_config.mem:\n        slurm_dict[\"mem_per_task_MB\"] = shared_config.default_slurm_config.mem\n\n    # Incorporate slurm_env.batching_config\n    for key, value in shared_config.batching_config.model_dump().items():\n        slurm_dict[key] = value\n\n    # Incorporate slurm_env.user_local_exports\n    slurm_dict[\"user_local_exports\"] = shared_config.user_local_exports\n\n    # GPU-related options\n    # Notes about priority:\n    # 1. This block of definitions takes priority over other definitions from\n    #    slurm_env which are not under the `needs_gpu` subgroup\n    # 2. This block of definitions has lower priority than whatever comes next\n    #    (i.e. from WorkflowTask.meta_parallel).\n    if wftask_meta is not None:\n        needs_gpu = interpret_as_bool(wftask_meta.get(\"needs_gpu\", False))\n    else:\n        needs_gpu = False\n    logger.debug(f\"[get_slurm_config] {needs_gpu=}\")\n    if needs_gpu and shared_config.gpu_slurm_config is not None:\n        for key, value in shared_config.gpu_slurm_config.model_dump(\n            exclude_unset=True, exclude={\"mem\"}\n        ).items():\n            slurm_dict[key] = value\n        if shared_config.gpu_slurm_config.mem:\n            slurm_dict[\"mem_per_task_MB\"] = shared_config.gpu_slurm_config.mem\n\n    # Number of CPUs per task, for multithreading\n    if wftask_meta is not None and \"cpus_per_task\" in wftask_meta:\n        cpus_per_task = int(wftask_meta[\"cpus_per_task\"])\n        slurm_dict[\"cpus_per_task\"] = cpus_per_task\n\n    # Required memory per task, in MB\n    if wftask_meta is not None and \"mem\" in wftask_meta:\n        raw_mem = wftask_meta[\"mem\"]\n        mem_per_task_MB = slurm_mem_to_MB(raw_mem)\n        slurm_dict[\"mem_per_task_MB\"] = mem_per_task_MB\n\n    # Job name\n    job_name = wftask.task.name.replace(\" \", \"_\")\n    slurm_dict[\"job_name\"] = job_name\n\n    # Optional SLURM arguments and extra lines\n    if wftask_meta is not None:\n        account = wftask_meta.get(\"account\", None)\n        if account is not None:\n            error_msg = (\n                f\"Invalid {account=} property in WorkflowTask `meta` \"\n                \"attribute.\\n\"\n                \"SLURM account must be set in the request body of the \"\n                \"apply-workflow endpoint, or by modifying the user properties.\"\n            )\n            logger.error(error_msg)\n            raise SlurmConfigError(error_msg)\n        for key in [\n            \"time\",\n            \"gres\",\n            \"gpus\",\n            \"constraint\",\n            \"nodelist\",\n            \"nodes\",\n            \"exclude\",\n        ]:\n            value = wftask_meta.get(key, None)\n            if value is not None:\n                slurm_dict[key] = value\n    if wftask_meta is not None:\n        extra_lines = wftask_meta.get(\"extra_lines\", [])\n    else:\n        extra_lines = []\n    extra_lines = slurm_dict.get(\"extra_lines\", []) + extra_lines\n    if len(set(extra_lines)) != len(extra_lines):\n        logger.debug(\n            f\"[get_slurm_config] Removing repeated elements in {extra_lines=}.\"\n        )\n        extra_lines = list(set(extra_lines))\n    slurm_dict[\"extra_lines\"] = extra_lines\n\n    # Job-batching parameters (if None, they will be determined heuristically)\n    if wftask_meta is not None:\n        tasks_per_job = wftask_meta.get(\"tasks_per_job\", None)\n        parallel_tasks_per_job = wftask_meta.get(\"parallel_tasks_per_job\", None)\n    else:\n        tasks_per_job = None\n        parallel_tasks_per_job = None\n    slurm_dict[\"tasks_per_job\"] = tasks_per_job\n    slurm_dict[\"parallel_tasks_per_job\"] = parallel_tasks_per_job\n\n    # Put everything together\n    logger.debug(\n        f\"[get_slurm_config] Create SlurmConfig object based on {slurm_dict=}\"\n    )\n    slurm_config = SlurmConfig(**slurm_dict)\n\n    return slurm_config\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/get_slurm_config/#fractal_server.runner.executors.slurm_common.get_slurm_config.get_slurm_config","title":"<code>get_slurm_config(shared_config, wftask, which_type, tot_tasks=1)</code>","text":"<p>Get <code>SlurmConfig</code> object.</p> PARAMETER DESCRIPTION <code>shared_config</code> <p> TYPE: <code>JobRunnerConfigSLURM</code> </p> <code>wftask</code> <p> TYPE: <code>WorkflowTaskV2</code> </p> <code>which_type</code> <p> TYPE: <code>Literal['non_parallel', 'parallel']</code> </p> <code>tot_tasks</code> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>fractal_server/runner/executors/slurm_common/get_slurm_config.py</code> <pre><code>def get_slurm_config(\n    shared_config: JobRunnerConfigSLURM,\n    wftask: WorkflowTaskV2,\n    which_type: Literal[\"non_parallel\", \"parallel\"],\n    tot_tasks: int = 1,\n) -&gt; SlurmConfig:\n    \"\"\"\n    Get `SlurmConfig` object.\n\n    Args:\n        shared_config:\n        wftask:\n        which_type:\n        tot_tasks:\n    \"\"\"\n    config = _get_slurm_config_internal(\n        shared_config=shared_config,\n        wftask=wftask,\n        which_type=which_type,\n    )\n\n    # Set/validate parameters for task batching\n    tasks_per_job, parallel_tasks_per_job = heuristics(\n        # Number of parallel components (always known)\n        tot_tasks=tot_tasks,\n        # Optional WorkflowTask attributes:\n        tasks_per_job=config.tasks_per_job,\n        parallel_tasks_per_job=config.parallel_tasks_per_job,  # noqa\n        # Task requirements (multiple possible sources):\n        cpus_per_task=config.cpus_per_task,\n        mem_per_task=config.mem_per_task_MB,\n        # Fractal configuration variables (soft/hard limits):\n        target_cpus_per_job=config.target_cpus_per_job,\n        target_mem_per_job=config.target_mem_per_job,\n        target_num_jobs=config.target_num_jobs,\n        max_cpus_per_job=config.max_cpus_per_job,\n        max_mem_per_job=config.max_mem_per_job,\n        max_num_jobs=config.max_num_jobs,\n    )\n    config.parallel_tasks_per_job = parallel_tasks_per_job\n    config.tasks_per_job = tasks_per_job\n\n    return config\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/remote/","title":"remote","text":""},{"location":"reference/runner/executors/slurm_common/remote/#fractal_server.runner.executors.slurm_common.remote.FractalVersionMismatch","title":"<code>FractalVersionMismatch</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Custom exception for version mismatch</p> Source code in <code>fractal_server/runner/executors/slurm_common/remote.py</code> <pre><code>class FractalVersionMismatch(RuntimeError):\n    \"\"\"\n    Custom exception for version mismatch\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/remote/#fractal_server.runner.executors.slurm_common.remote.worker","title":"<code>worker(*, in_fname, out_fname)</code>","text":"<p>Execute a job, possibly on a remote node.</p> PARAMETER DESCRIPTION <code>in_fname</code> <p>Absolute path to the input file (must be readable).</p> <p> TYPE: <code>str</code> </p> <code>out_fname</code> <p>Absolute path of the output file (must be writeable).</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/runner/executors/slurm_common/remote.py</code> <pre><code>def worker(\n    *,\n    in_fname: str,\n    out_fname: str,\n) -&gt; None:\n    \"\"\"\n    Execute a job, possibly on a remote node.\n\n    Args:\n        in_fname: Absolute path to the input file (must be readable).\n        out_fname: Absolute path of the output file (must be writeable).\n    \"\"\"\n\n    # Create output folder, if missing\n    out_dir = os.path.dirname(out_fname)\n    if not os.path.exists(out_dir):\n        os.mkdir(out_dir)\n\n    # Execute the job and capture exceptions\n    try:\n        with open(in_fname) as f:\n            input_data = json.load(f)\n\n        # Fractal-server version must be identical\n        server_fractal_server_version = input_data[\"fractal_server_version\"]\n        worker_fractal_server_version = __VERSION__\n        if worker_fractal_server_version != server_fractal_server_version:\n            raise FractalVersionMismatch(\n                f\"{server_fractal_server_version=} but \"\n                f\"{worker_fractal_server_version=}\"\n            )\n\n        # Get `worker_python_version` as a `list` since this is the type of\n        # `server_python_version` after a JSON dump/load round trip.\n        worker_python_version = list(sys.version_info[:3])\n\n        # Print a warning for Python version mismatch\n        server_python_version = input_data[\"python_version\"]\n        if worker_python_version != server_python_version:\n            if worker_python_version[:2] != server_python_version[:2]:\n                print(\n                    \"WARNING: \"\n                    f\"{server_python_version=} but {worker_python_version=}.\"\n                )\n\n        # Extract some useful paths\n        metadiff_file_remote = input_data[\"metadiff_file_remote\"]\n        log_path = input_data[\"log_file_remote\"]\n\n        # Execute command\n        full_command = input_data[\"full_command\"]\n        call_command_wrapper(cmd=full_command, log_path=log_path)\n\n        try:\n            with open(metadiff_file_remote) as f:\n                out_meta = json.load(f)\n            result = (True, out_meta)\n        except FileNotFoundError:\n            # Command completed, but it produced no metadiff file\n            result = (True, None)\n\n    except Exception as e:\n        # Exception objects are not serialisable. Here we save the relevant\n        # exception contents in a serializable dictionary. Note that whenever\n        # the task failed \"properly\", the exception is a `TaskExecutionError`\n        # and it has additional attributes.\n        import traceback\n\n        exc_type, exc_value, traceback_obj = sys.exc_info()\n        traceback_obj = traceback_obj.tb_next\n        traceback_list = traceback.format_exception(\n            exc_type,\n            exc_value,\n            traceback_obj,\n        )\n        traceback_string = \"\".join(traceback_list)\n        exc_proxy = dict(\n            exc_type_name=type(e).__name__,\n            traceback_string=traceback_string,\n        )\n        result = (False, exc_proxy)\n\n    # Write output file\n    with open(out_fname, \"w\") as f:\n        json.dump(result, f, indent=2)\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/slurm_config/","title":"slurm_config","text":"<p>Submodule to handle the SLURM configuration for a WorkflowTask</p>"},{"location":"reference/runner/executors/slurm_common/slurm_config/#fractal_server.runner.executors.slurm_common.slurm_config.SlurmConfig","title":"<code>SlurmConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Abstraction for SLURM parameters</p> <p>NOTE: <code>SlurmConfig</code> objects are created internally in <code>fractal-server</code>, and they are not meant to be initialized by the user; the same holds for <code>SlurmConfig</code> attributes (e.g. <code>mem_per_task_MB</code>), which are not meant to be part of the superuser-defined <code>resource.jobs_runner_config</code> JSON field.</p> <p>Part of the attributes map directly to some of the SLURM attributes (see https://slurm.schedmd.com/sbatch.html), e.g. <code>partition</code>. Other attributes are metaparameters which are needed in fractal-server to combine multiple tasks in the same SLURM job (e.g. <code>parallel_tasks_per_job</code> or <code>max_num_jobs</code>).</p> ATTRIBUTE DESCRIPTION <code>partition</code> <p>Corresponds to SLURM option.</p> <p> TYPE: <code>str</code> </p> <code>cpus_per_task</code> <p>Corresponds to SLURM option.</p> <p> TYPE: <code>int</code> </p> <code>mem_per_task_MB</code> <p>Corresponds to <code>mem</code> SLURM option.</p> <p> TYPE: <code>int</code> </p> <code>job_name</code> <p>Corresponds to <code>name</code> SLURM option.</p> <p> TYPE: <code>str | None</code> </p> <code>constraint</code> <p>Corresponds to SLURM option.</p> <p> TYPE: <code>str | None</code> </p> <code>gres</code> <p>Corresponds to SLURM option.</p> <p> TYPE: <code>str | None</code> </p> <code>account</code> <p>Corresponds to SLURM option.</p> <p> TYPE: <code>str | None</code> </p> <code>gpus</code> <p>Corresponds to SLURM option.</p> <p> TYPE: <code>str | None</code> </p> <code>time</code> <p>Corresponds to SLURM option (WARNING: not fully supported).</p> <p> TYPE: <code>str | None</code> </p> <code>nodelist</code> <p>Corresponds to SLURM option.</p> <p> TYPE: <code>str | None</code> </p> <code>nodes</code> <p>Corresponds to SLURM option.</p> <p> TYPE: <code>int | None</code> </p> <code>exclude</code> <p>Corresponds to SLURM option.</p> <p> TYPE: <code>str | None</code> </p> <code>prefix</code> <p>Prefix of configuration lines in SLURM submission scripts.</p> <p> TYPE: <code>str</code> </p> <code>shebang_line</code> <p>Shebang line for SLURM submission scripts.</p> <p> TYPE: <code>str</code> </p> <code>use_mem_per_cpu</code> <p>If <code>True</code>, use <code>--mem-per-cpu</code> rather than <code>--mem</code>, both at the job level and for <code>srun</code> statements.</p> <p> TYPE: <code>bool</code> </p> <code>extra_lines</code> <p>Additional lines to include in SLURM submission scripts.</p> <p> TYPE: <code>list[str]</code> </p> <code>tasks_per_job</code> <p>Number of tasks for each SLURM job.</p> <p> TYPE: <code>int | None</code> </p> <code>parallel_tasks_per_job</code> <p>Number of tasks to run in parallel for                     each SLURM job.</p> <p> TYPE: <code>int | None</code> </p> <code>target_cpus_per_job</code> <p>Optimal number of CPUs to be requested in each                  SLURM job.</p> <p> TYPE: <code>int</code> </p> <code>max_cpus_per_job</code> <p>Maximum number of CPUs that can be requested in each               SLURM job.</p> <p> TYPE: <code>int</code> </p> <code>target_mem_per_job</code> <p>Optimal amount of memory (in MB) to be requested in                 each SLURM job.</p> <p> TYPE: <code>int</code> </p> <code>max_mem_per_job</code> <p>Maximum amount of memory (in MB) that can be requested              in each SLURM job.</p> <p> TYPE: <code>int</code> </p> <code>target_num_jobs</code> <p>Optimal number of SLURM jobs for a given WorkflowTask.</p> <p> TYPE: <code>int</code> </p> <code>max_num_jobs</code> <p>Maximum number of SLURM jobs for a given WorkflowTask.</p> <p> TYPE: <code>int</code> </p> <code>user_local_exports</code> <p>Key-value pairs to be included as <code>export</code>-ed variables in SLURM submission script, after prepending values with the user's cache directory.</p> <p> TYPE: <code>dict[str, str]</code> </p> Source code in <code>fractal_server/runner/executors/slurm_common/slurm_config.py</code> <pre><code>class SlurmConfig(BaseModel):\n    \"\"\"\n    Abstraction for SLURM parameters\n\n    **NOTE**: `SlurmConfig` objects are created internally in `fractal-server`,\n    and they are not meant to be initialized by the user; the same holds for\n    `SlurmConfig` attributes (e.g. `mem_per_task_MB`), which are not meant to\n    be part of the superuser-defined `resource.jobs_runner_config` JSON field.\n\n    Part of the attributes map directly to some of the SLURM attributes (see\n    https://slurm.schedmd.com/sbatch.html), e.g. `partition`. Other attributes\n    are metaparameters which are needed in fractal-server to combine multiple\n    tasks in the same SLURM job (e.g. `parallel_tasks_per_job` or\n    `max_num_jobs`).\n\n    Attributes:\n        partition: Corresponds to SLURM option.\n        cpus_per_task: Corresponds to SLURM option.\n        mem_per_task_MB: Corresponds to `mem` SLURM option.\n        job_name: Corresponds to `name` SLURM option.\n        constraint: Corresponds to SLURM option.\n        gres: Corresponds to SLURM option.\n        account: Corresponds to SLURM option.\n        gpus: Corresponds to SLURM option.\n        time: Corresponds to SLURM option (WARNING: not fully supported).\n        nodelist: Corresponds to SLURM option.\n        nodes: Corresponds to SLURM option.\n        exclude: Corresponds to SLURM option.\n        prefix: Prefix of configuration lines in SLURM submission scripts.\n        shebang_line: Shebang line for SLURM submission scripts.\n        use_mem_per_cpu:\n            If `True`, use `--mem-per-cpu` rather than `--mem`, both at the job\n            level and for `srun` statements.\n        extra_lines: Additional lines to include in SLURM submission scripts.\n        tasks_per_job: Number of tasks for each SLURM job.\n        parallel_tasks_per_job: Number of tasks to run in parallel for\n                                each SLURM job.\n        target_cpus_per_job: Optimal number of CPUs to be requested in each\n                             SLURM job.\n        max_cpus_per_job: Maximum number of CPUs that can be requested in each\n                          SLURM job.\n        target_mem_per_job: Optimal amount of memory (in MB) to be requested in\n                            each SLURM job.\n        max_mem_per_job: Maximum amount of memory (in MB) that can be requested\n                         in each SLURM job.\n        target_num_jobs: Optimal number of SLURM jobs for a given WorkflowTask.\n        max_num_jobs: Maximum number of SLURM jobs for a given WorkflowTask.\n        user_local_exports:\n            Key-value pairs to be included as `export`-ed variables in SLURM\n            submission script, after prepending values with the user's cache\n            directory.\n\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    # Required SLURM parameters (note that the integer attributes are those\n    # that will need to scale up with the number of parallel tasks per job)\n    partition: str\n    cpus_per_task: int\n    mem_per_task_MB: int\n\n    prefix: str = \"#SBATCH\"\n    shebang_line: str = \"#!/bin/sh\"\n\n    use_mem_per_cpu: bool = False\n\n    # Optional SLURM parameters\n    job_name: str | None = None\n    constraint: str | None = None\n    gres: str | None = None\n    gpus: str | None = None\n    time: str | None = None\n    account: str | None = None\n    nodelist: str | None = None\n    nodes: int | None = None\n    exclude: str | None = None\n\n    # Free-field attribute for extra lines to be added to the SLURM job\n    # preamble\n    extra_lines: list[str] = Field(default_factory=list)\n\n    # Variables that will be `export`ed in the SLURM submission script\n    user_local_exports: dict[str, str] = Field(default_factory=dict)\n\n    # Metaparameters needed to combine multiple tasks in each SLURM job\n    tasks_per_job: int | None = None\n    parallel_tasks_per_job: int | None = None\n    target_cpus_per_job: int\n    max_cpus_per_job: int\n    target_mem_per_job: int\n    max_mem_per_job: int\n    target_num_jobs: int\n    max_num_jobs: int\n\n    def _sorted_extra_lines(self) -&gt; list[str]:\n        \"\"\"\n        Return a copy of `self.extra_lines`, where lines starting with\n        `self.prefix` are listed first.\n        \"\"\"\n\n        def _no_prefix(_line):\n            if _line.startswith(self.prefix):\n                return 0\n            else:\n                return 1\n\n        return sorted(self.extra_lines, key=_no_prefix)\n\n    def sort_script_lines(self, script_lines: list[str]) -&gt; list[str]:\n        \"\"\"\n        Return a copy of `script_lines`, where lines are sorted as in:\n\n        1. `self.shebang_line` (if present);\n        2. Lines starting with `self.prefix`;\n        3. Other lines.\n\n        Args:\n            script_lines:\n        \"\"\"\n\n        def _sorting_function(_line):\n            if _line == self.shebang_line:\n                return 0\n            elif _line.startswith(self.prefix):\n                return 1\n            else:\n                return 2\n\n        return sorted(script_lines, key=_sorting_function)\n\n    def to_sbatch_preamble(\n        self,\n        remote_export_dir: str,\n        use_mem_per_cpu: bool = False,\n    ) -&gt; list[str]:\n        \"\"\"\n        Compile `SlurmConfig` object into the preamble of a SLURM submission\n        script.\n\n        Args:\n            remote_export_dir:\n                Base directory for exports defined in\n                `self.user_local_exports`.\n            use_mem_per_cpu:\n                If `True`, use `--mem-per-cpu` rather than `--mem`.\n        \"\"\"\n        if self.parallel_tasks_per_job is None:\n            raise ValueError(\n                \"SlurmConfig.sbatch_preamble requires that \"\n                f\"{self.parallel_tasks_per_job=} is not None.\"\n            )\n        if len(self.extra_lines) != len(set(self.extra_lines)):\n            raise ValueError(f\"{self.extra_lines=} contains repetitions\")\n\n        if use_mem_per_cpu:\n            memory_line = f\"{self.prefix} --mem-per-cpu={self.mem_per_cpu_MB}M\"\n        else:\n            mem_per_job_MB = self.parallel_tasks_per_job * self.mem_per_task_MB\n            memory_line = f\"{self.prefix} --mem={mem_per_job_MB}M\"\n\n        lines = [\n            self.shebang_line,\n            f\"{self.prefix} --partition={self.partition}\",\n            f\"{self.prefix} --ntasks={self.parallel_tasks_per_job}\",\n            f\"{self.prefix} --cpus-per-task={self.cpus_per_task}\",\n            memory_line,\n        ]\n        for key in [\n            \"job_name\",\n            \"constraint\",\n            \"gres\",\n            \"gpus\",\n            \"time\",\n            \"account\",\n            \"exclude\",\n            \"nodelist\",\n            \"nodes\",\n        ]:\n            value = getattr(self, key)\n            if value is not None:\n                # Handle the `time` parameter\n                if key == \"time\" and self.parallel_tasks_per_job &gt; 1:\n                    # NOTE: see issue #1632\n                    logger.warning(\n                        f\"`time` SLURM parameter is set to {self.time}, \"\n                        \"but this does not take into account the number of \"\n                        f\"SLURM tasks ({self.parallel_tasks_per_job}).\"\n                    )\n                option = key.replace(\"_\", \"-\")\n                lines.append(f\"{self.prefix} --{option}={value}\")\n\n        for line in self._sorted_extra_lines():\n            lines.append(line)\n\n        if self.user_local_exports:\n            for key, value in self.user_local_exports.items():\n                tmp_value = str(Path(remote_export_dir) / value)\n                lines.append(f\"export {key}={tmp_value}\")\n\n        \"\"\"\n        FIXME export SRUN_CPUS_PER_TASK\n        # From https://slurm.schedmd.com/sbatch.html: Beginning with 22.05,\n        # srun will not inherit the --cpus-per-task value requested by salloc\n        # or sbatch.  It must be requested again with the call to srun or set\n        # with the SRUN_CPUS_PER_TASK environment variable if desired for the\n        # task(s).\n        if config.cpus_per_task:\n            #additional_setup_lines.append(\n                f\"export SRUN_CPUS_PER_TASK={config.cpus_per_task}\"\n            )\n        \"\"\"\n\n        return lines\n\n    @property\n    def batch_size(self) -&gt; int:\n        return self.tasks_per_job\n\n    @property\n    def mem_per_cpu_MB(self) -&gt; int:\n        return int(self.mem_per_task_MB / self.cpus_per_task)\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/slurm_config/#fractal_server.runner.executors.slurm_common.slurm_config.SlurmConfig._sorted_extra_lines","title":"<code>_sorted_extra_lines()</code>","text":"<p>Return a copy of <code>self.extra_lines</code>, where lines starting with <code>self.prefix</code> are listed first.</p> Source code in <code>fractal_server/runner/executors/slurm_common/slurm_config.py</code> <pre><code>def _sorted_extra_lines(self) -&gt; list[str]:\n    \"\"\"\n    Return a copy of `self.extra_lines`, where lines starting with\n    `self.prefix` are listed first.\n    \"\"\"\n\n    def _no_prefix(_line):\n        if _line.startswith(self.prefix):\n            return 0\n        else:\n            return 1\n\n    return sorted(self.extra_lines, key=_no_prefix)\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/slurm_config/#fractal_server.runner.executors.slurm_common.slurm_config.SlurmConfig.sort_script_lines","title":"<code>sort_script_lines(script_lines)</code>","text":"<p>Return a copy of <code>script_lines</code>, where lines are sorted as in:</p> <ol> <li><code>self.shebang_line</code> (if present);</li> <li>Lines starting with <code>self.prefix</code>;</li> <li>Other lines.</li> </ol> PARAMETER DESCRIPTION <code>script_lines</code> <p> TYPE: <code>list[str]</code> </p> Source code in <code>fractal_server/runner/executors/slurm_common/slurm_config.py</code> <pre><code>def sort_script_lines(self, script_lines: list[str]) -&gt; list[str]:\n    \"\"\"\n    Return a copy of `script_lines`, where lines are sorted as in:\n\n    1. `self.shebang_line` (if present);\n    2. Lines starting with `self.prefix`;\n    3. Other lines.\n\n    Args:\n        script_lines:\n    \"\"\"\n\n    def _sorting_function(_line):\n        if _line == self.shebang_line:\n            return 0\n        elif _line.startswith(self.prefix):\n            return 1\n        else:\n            return 2\n\n    return sorted(script_lines, key=_sorting_function)\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/slurm_config/#fractal_server.runner.executors.slurm_common.slurm_config.SlurmConfig.to_sbatch_preamble","title":"<code>to_sbatch_preamble(remote_export_dir, use_mem_per_cpu=False)</code>","text":"<p>Compile <code>SlurmConfig</code> object into the preamble of a SLURM submission script.</p> PARAMETER DESCRIPTION <code>remote_export_dir</code> <p>Base directory for exports defined in <code>self.user_local_exports</code>.</p> <p> TYPE: <code>str</code> </p> <code>use_mem_per_cpu</code> <p>If <code>True</code>, use <code>--mem-per-cpu</code> rather than <code>--mem</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>fractal_server/runner/executors/slurm_common/slurm_config.py</code> <pre><code>def to_sbatch_preamble(\n    self,\n    remote_export_dir: str,\n    use_mem_per_cpu: bool = False,\n) -&gt; list[str]:\n    \"\"\"\n    Compile `SlurmConfig` object into the preamble of a SLURM submission\n    script.\n\n    Args:\n        remote_export_dir:\n            Base directory for exports defined in\n            `self.user_local_exports`.\n        use_mem_per_cpu:\n            If `True`, use `--mem-per-cpu` rather than `--mem`.\n    \"\"\"\n    if self.parallel_tasks_per_job is None:\n        raise ValueError(\n            \"SlurmConfig.sbatch_preamble requires that \"\n            f\"{self.parallel_tasks_per_job=} is not None.\"\n        )\n    if len(self.extra_lines) != len(set(self.extra_lines)):\n        raise ValueError(f\"{self.extra_lines=} contains repetitions\")\n\n    if use_mem_per_cpu:\n        memory_line = f\"{self.prefix} --mem-per-cpu={self.mem_per_cpu_MB}M\"\n    else:\n        mem_per_job_MB = self.parallel_tasks_per_job * self.mem_per_task_MB\n        memory_line = f\"{self.prefix} --mem={mem_per_job_MB}M\"\n\n    lines = [\n        self.shebang_line,\n        f\"{self.prefix} --partition={self.partition}\",\n        f\"{self.prefix} --ntasks={self.parallel_tasks_per_job}\",\n        f\"{self.prefix} --cpus-per-task={self.cpus_per_task}\",\n        memory_line,\n    ]\n    for key in [\n        \"job_name\",\n        \"constraint\",\n        \"gres\",\n        \"gpus\",\n        \"time\",\n        \"account\",\n        \"exclude\",\n        \"nodelist\",\n        \"nodes\",\n    ]:\n        value = getattr(self, key)\n        if value is not None:\n            # Handle the `time` parameter\n            if key == \"time\" and self.parallel_tasks_per_job &gt; 1:\n                # NOTE: see issue #1632\n                logger.warning(\n                    f\"`time` SLURM parameter is set to {self.time}, \"\n                    \"but this does not take into account the number of \"\n                    f\"SLURM tasks ({self.parallel_tasks_per_job}).\"\n                )\n            option = key.replace(\"_\", \"-\")\n            lines.append(f\"{self.prefix} --{option}={value}\")\n\n    for line in self._sorted_extra_lines():\n        lines.append(line)\n\n    if self.user_local_exports:\n        for key, value in self.user_local_exports.items():\n            tmp_value = str(Path(remote_export_dir) / value)\n            lines.append(f\"export {key}={tmp_value}\")\n\n    \"\"\"\n    FIXME export SRUN_CPUS_PER_TASK\n    # From https://slurm.schedmd.com/sbatch.html: Beginning with 22.05,\n    # srun will not inherit the --cpus-per-task value requested by salloc\n    # or sbatch.  It must be requested again with the call to srun or set\n    # with the SRUN_CPUS_PER_TASK environment variable if desired for the\n    # task(s).\n    if config.cpus_per_task:\n        #additional_setup_lines.append(\n            f\"export SRUN_CPUS_PER_TASK={config.cpus_per_task}\"\n        )\n    \"\"\"\n\n    return lines\n</code></pre>"},{"location":"reference/runner/executors/slurm_common/slurm_job_task_models/","title":"slurm_job_task_models","text":""},{"location":"reference/runner/executors/slurm_ssh/","title":"slurm_ssh","text":""},{"location":"reference/runner/executors/slurm_ssh/run_subprocess/","title":"run_subprocess","text":""},{"location":"reference/runner/executors/slurm_ssh/runner/","title":"runner","text":""},{"location":"reference/runner/executors/slurm_ssh/runner/#fractal_server.runner.executors.slurm_ssh.runner.SlurmSSHRunner","title":"<code>SlurmSSHRunner</code>","text":"<p>               Bases: <code>BaseSlurmRunner</code></p> <p>Runner implementation for a computational <code>slurm_ssh</code> resource.</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/runner.py</code> <pre><code>class SlurmSSHRunner(BaseSlurmRunner):\n    \"\"\"\n    Runner implementation for a computational `slurm_ssh` resource.\n    \"\"\"\n\n    fractal_ssh: FractalSSH\n\n    def __init__(\n        self,\n        *,\n        # Common\n        root_dir_local: Path,\n        root_dir_remote: Path,\n        common_script_lines: list[str] | None = None,\n        resource: Resource,\n        # Specific\n        slurm_account: str | None = None,\n        profile: Profile,\n        user_cache_dir: str,\n        fractal_ssh: FractalSSH,\n    ) -&gt; None:\n        \"\"\"\n        Set parameters that are the same for different Fractal tasks and for\n        different SLURM jobs/tasks.\n        \"\"\"\n        self.fractal_ssh = fractal_ssh\n        self.shared_config = JobRunnerConfigSLURM(**resource.jobs_runner_config)\n        logger.warning(self.fractal_ssh)\n\n        # Check SSH connection and try to recover from a closed-socket error\n        self.fractal_ssh.check_connection()\n        super().__init__(\n            slurm_runner_type=\"ssh\",\n            root_dir_local=root_dir_local,\n            root_dir_remote=root_dir_remote,\n            common_script_lines=common_script_lines,\n            user_cache_dir=user_cache_dir,\n            poll_interval=resource.jobs_poll_interval,\n            python_worker_interpreter=resource.jobs_slurm_python_worker,\n            slurm_account=slurm_account,\n        )\n\n    @override\n    def _mkdir_local_folder(self, folder: str) -&gt; None:\n        Path(folder).mkdir(parents=True)\n\n    @override\n    def _mkdir_remote_folder(self, folder: str):\n        self.fractal_ssh.mkdir(\n            folder=folder,\n            parents=True,\n        )\n\n    def _fetch_artifacts(\n        self,\n        finished_slurm_jobs: list[SlurmJob],\n    ) -&gt; None:\n        \"\"\"\n        Fetch artifacts for a list of SLURM jobs.\n        \"\"\"\n\n        # Check length\n        if len(finished_slurm_jobs) == 0:\n            logger.debug(f\"[_fetch_artifacts] EXIT ({finished_slurm_jobs=}).\")\n            return None\n\n        t_0 = time.perf_counter()\n        logger.debug(f\"[_fetch_artifacts] START ({len(finished_slurm_jobs)=}).\")\n\n        # Extract `workdir_remote` and `workdir_local`\n        self.validate_slurm_jobs_workdirs(finished_slurm_jobs)\n        workdir_local = finished_slurm_jobs[0].workdir_local\n        workdir_remote = finished_slurm_jobs[0].workdir_remote\n\n        # Define local/remote tarfile paths\n        tarfile_path_local = workdir_local.with_suffix(\".tar.gz\").as_posix()\n        tarfile_path_remote = workdir_remote.with_suffix(\".tar.gz\").as_posix()\n\n        # Create file list\n        # NOTE: see issue 2483\n        filelist = []\n        for _slurm_job in finished_slurm_jobs:\n            _single_job_filelist = [\n                _slurm_job.slurm_stdout_remote_path.name,\n                _slurm_job.slurm_stderr_remote_path.name,\n            ]\n            for task in _slurm_job.tasks:\n                _single_job_filelist.extend(\n                    [\n                        task.output_file_remote_path.name,\n                        task.task_files.log_file_remote_path.name,\n                        task.task_files.metadiff_file_remote_path.name,\n                    ]\n                )\n            filelist.extend(_single_job_filelist)\n        filelist_string = \"\\n\".join(filelist)\n        elapsed = time.perf_counter() - t_0\n        logger.debug(\n            \"[_fetch_artifacts] Created filelist \"\n            f\"({len(filelist)=}, from start: {elapsed=:.3f} s).\"\n        )\n\n        # Write filelist to file remotely\n        tmp_filelist_path = workdir_remote / f\"filelist_{time.time()}.txt\"\n        self.fractal_ssh.write_remote_file(\n            path=tmp_filelist_path.as_posix(),\n            content=f\"{filelist_string}\\n\",\n        )\n        elapsed = time.perf_counter() - t_0\n        logger.debug(\n            f\"[_fetch_artifacts] File list written to {tmp_filelist_path} \"\n            f\"(from start: {elapsed=:.3f} s).\"\n        )\n\n        # Create remote tarfile\n        t_0_tar = time.perf_counter()\n        tar_command = get_tar_compression_cmd(\n            subfolder_path=workdir_remote,\n            filelist_path=tmp_filelist_path,\n        )\n        self.fractal_ssh.run_command(cmd=tar_command)\n        t_1_tar = time.perf_counter()\n        logger.info(\n            f\"[_fetch_artifacts] Remote archive {tarfile_path_remote} created\"\n            f\" - elapsed={t_1_tar - t_0_tar:.3f} s\"\n        )\n\n        # Fetch tarfile\n        t_0_get = time.perf_counter()\n        self.fractal_ssh.fetch_file(\n            remote=tarfile_path_remote,\n            local=tarfile_path_local,\n        )\n        t_1_get = time.perf_counter()\n        logger.info(\n            \"[_fetch_artifacts] Subfolder archive transferred back \"\n            f\"to {tarfile_path_local}\"\n            f\" - elapsed={t_1_get - t_0_get:.3f} s\"\n        )\n\n        # Extract tarfile locally\n        target_dir, cmd_tar = get_tar_extraction_cmd(Path(tarfile_path_local))\n        target_dir.mkdir(exist_ok=True)\n        run_subprocess(cmd=cmd_tar, logger_name=logger.name)\n        Path(tarfile_path_local).unlink(missing_ok=True)\n\n        t_1 = time.perf_counter()\n        logger.info(f\"[_fetch_artifacts] End - elapsed={t_1 - t_0:.3f} s\")\n\n    @override\n    def _run_remote_cmd(self, cmd: str) -&gt; str:\n        stdout = self.fractal_ssh.run_command(cmd=cmd)\n        return stdout\n\n    def _send_many_job_inputs(\n        self, *, workdir_local: Path, workdir_remote: Path\n    ) -&gt; None:\n        \"\"\"\n        Compress, transfer, and extract a local working directory onto a remote\n        host.\n\n        This method creates a temporary `.tar.gz` archive of the given\n        `workdir_local`, transfers it to the remote machine via the configured\n        SSH connection, extracts it into `workdir_remote`, and removes the\n        temporary archive from both local and remote filesystems.\n        \"\"\"\n\n        logger.debug(\"[_send_many_job_inputs] START\")\n\n        tar_path_local = workdir_local.with_suffix(\".tar.gz\")\n        tar_name = Path(tar_path_local).name\n        tar_path_remote = workdir_remote.parent / tar_name\n\n        tar_compression_cmd = get_tar_compression_cmd(\n            subfolder_path=workdir_local, filelist_path=None\n        )\n        _, tar_extraction_cmd = get_tar_extraction_cmd(\n            archive_path=tar_path_remote\n        )\n        rm_tar_cmd = f\"rm {tar_path_remote.as_posix()}\"\n\n        try:\n            run_subprocess(tar_compression_cmd, logger_name=logger.name)\n            logger.debug(\n                \"[_send_many_job_inputs] \"\n                f\"{workdir_local=} compressed to {tar_path_local=}.\"\n            )\n            self.fractal_ssh.send_file(\n                local=tar_path_local.as_posix(),\n                remote=tar_path_remote.as_posix(),\n            )\n            logger.debug(\n                \"[_send_many_job_inputs] \"\n                f\"{tar_path_local=} sent via SSH to {tar_path_remote=}.\"\n            )\n            self.fractal_ssh.run_command(cmd=tar_extraction_cmd)\n            logger.debug(\n                \"[_send_many_job_inputs] \"\n                f\"{tar_path_remote=} extracted to {workdir_remote=}.\"\n            )\n            self.fractal_ssh.run_command(cmd=rm_tar_cmd)\n            logger.debug(\n                \"[_send_many_job_inputs] \"\n                f\"{tar_path_remote=} removed from remote server.\"\n            )\n        except Exception as e:\n            raise e\n        finally:\n            Path(tar_path_local).unlink(missing_ok=True)\n            logger.debug(f\"[_send_many_job_inputs] {tar_path_local=} removed.\")\n\n        logger.debug(\"[_send_many_job_inputs] END.\")\n\n    @override\n    def run_squeue(self, *, job_ids: list[str]) -&gt; str:\n        \"\"\"\n        Run `squeue` for a set of SLURM job IDs.\n\n        Different scenarios:\n\n        1. When `squeue -j` succeeds (with exit code 0), return its stdout.\n        2. When `squeue -j` fails (typical example:\n           `squeue -j {invalid_job_id}` fails with exit code 1), re-raise.\n           The error will be handled upstream.\n        3. When the SSH command fails because another thread is keeping the\n           lock of the `FractalSSH` object for a long time, mock the standard\n           output of the `squeue` command so that it looks like jobs are not\n           completed yet.\n        4. When the SSH command fails for other reasons, despite a forgiving\n           setup (7 connection attempts with base waiting interval of 2\n           seconds, with a cumulative timeout of 126 seconds), return an empty\n           string. This will be treated upstream as an empty `squeu` output,\n           indirectly resulting in marking the job as completed.\n        \"\"\"\n\n        if len(job_ids) == 0:\n            return \"\"\n\n        job_id_single_str = \",\".join([str(j) for j in job_ids])\n        cmd = (\n            \"squeue --noheader --format='%i %T' --states=all \"\n            f\"--jobs={job_id_single_str}\"\n        )\n\n        try:\n            stdout = self.fractal_ssh.run_command(\n                cmd=cmd,\n            )\n            return stdout\n        except FractalSSHCommandError as e:\n            raise e\n        except FractalSSHTimeoutError:\n            logger.warning(\n                \"[run_squeue] Could not acquire lock, use stdout placeholder.\"\n            )\n            FAKE_STATUS = \"FRACTAL_STATUS_PLACEHOLDER\"\n            placeholder_stdout = \"\\n\".join(\n                [f\"{job_id} {FAKE_STATUS}\" for job_id in job_ids]\n            )\n            return placeholder_stdout\n        except Exception as e:\n            logger.error(f\"Ignoring `squeue` command failure {e}\")\n            return \"\"\n</code></pre>"},{"location":"reference/runner/executors/slurm_ssh/runner/#fractal_server.runner.executors.slurm_ssh.runner.SlurmSSHRunner.__init__","title":"<code>__init__(*, root_dir_local, root_dir_remote, common_script_lines=None, resource, slurm_account=None, profile, user_cache_dir, fractal_ssh)</code>","text":"<p>Set parameters that are the same for different Fractal tasks and for different SLURM jobs/tasks.</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/runner.py</code> <pre><code>def __init__(\n    self,\n    *,\n    # Common\n    root_dir_local: Path,\n    root_dir_remote: Path,\n    common_script_lines: list[str] | None = None,\n    resource: Resource,\n    # Specific\n    slurm_account: str | None = None,\n    profile: Profile,\n    user_cache_dir: str,\n    fractal_ssh: FractalSSH,\n) -&gt; None:\n    \"\"\"\n    Set parameters that are the same for different Fractal tasks and for\n    different SLURM jobs/tasks.\n    \"\"\"\n    self.fractal_ssh = fractal_ssh\n    self.shared_config = JobRunnerConfigSLURM(**resource.jobs_runner_config)\n    logger.warning(self.fractal_ssh)\n\n    # Check SSH connection and try to recover from a closed-socket error\n    self.fractal_ssh.check_connection()\n    super().__init__(\n        slurm_runner_type=\"ssh\",\n        root_dir_local=root_dir_local,\n        root_dir_remote=root_dir_remote,\n        common_script_lines=common_script_lines,\n        user_cache_dir=user_cache_dir,\n        poll_interval=resource.jobs_poll_interval,\n        python_worker_interpreter=resource.jobs_slurm_python_worker,\n        slurm_account=slurm_account,\n    )\n</code></pre>"},{"location":"reference/runner/executors/slurm_ssh/runner/#fractal_server.runner.executors.slurm_ssh.runner.SlurmSSHRunner._fetch_artifacts","title":"<code>_fetch_artifacts(finished_slurm_jobs)</code>","text":"<p>Fetch artifacts for a list of SLURM jobs.</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/runner.py</code> <pre><code>def _fetch_artifacts(\n    self,\n    finished_slurm_jobs: list[SlurmJob],\n) -&gt; None:\n    \"\"\"\n    Fetch artifacts for a list of SLURM jobs.\n    \"\"\"\n\n    # Check length\n    if len(finished_slurm_jobs) == 0:\n        logger.debug(f\"[_fetch_artifacts] EXIT ({finished_slurm_jobs=}).\")\n        return None\n\n    t_0 = time.perf_counter()\n    logger.debug(f\"[_fetch_artifacts] START ({len(finished_slurm_jobs)=}).\")\n\n    # Extract `workdir_remote` and `workdir_local`\n    self.validate_slurm_jobs_workdirs(finished_slurm_jobs)\n    workdir_local = finished_slurm_jobs[0].workdir_local\n    workdir_remote = finished_slurm_jobs[0].workdir_remote\n\n    # Define local/remote tarfile paths\n    tarfile_path_local = workdir_local.with_suffix(\".tar.gz\").as_posix()\n    tarfile_path_remote = workdir_remote.with_suffix(\".tar.gz\").as_posix()\n\n    # Create file list\n    # NOTE: see issue 2483\n    filelist = []\n    for _slurm_job in finished_slurm_jobs:\n        _single_job_filelist = [\n            _slurm_job.slurm_stdout_remote_path.name,\n            _slurm_job.slurm_stderr_remote_path.name,\n        ]\n        for task in _slurm_job.tasks:\n            _single_job_filelist.extend(\n                [\n                    task.output_file_remote_path.name,\n                    task.task_files.log_file_remote_path.name,\n                    task.task_files.metadiff_file_remote_path.name,\n                ]\n            )\n        filelist.extend(_single_job_filelist)\n    filelist_string = \"\\n\".join(filelist)\n    elapsed = time.perf_counter() - t_0\n    logger.debug(\n        \"[_fetch_artifacts] Created filelist \"\n        f\"({len(filelist)=}, from start: {elapsed=:.3f} s).\"\n    )\n\n    # Write filelist to file remotely\n    tmp_filelist_path = workdir_remote / f\"filelist_{time.time()}.txt\"\n    self.fractal_ssh.write_remote_file(\n        path=tmp_filelist_path.as_posix(),\n        content=f\"{filelist_string}\\n\",\n    )\n    elapsed = time.perf_counter() - t_0\n    logger.debug(\n        f\"[_fetch_artifacts] File list written to {tmp_filelist_path} \"\n        f\"(from start: {elapsed=:.3f} s).\"\n    )\n\n    # Create remote tarfile\n    t_0_tar = time.perf_counter()\n    tar_command = get_tar_compression_cmd(\n        subfolder_path=workdir_remote,\n        filelist_path=tmp_filelist_path,\n    )\n    self.fractal_ssh.run_command(cmd=tar_command)\n    t_1_tar = time.perf_counter()\n    logger.info(\n        f\"[_fetch_artifacts] Remote archive {tarfile_path_remote} created\"\n        f\" - elapsed={t_1_tar - t_0_tar:.3f} s\"\n    )\n\n    # Fetch tarfile\n    t_0_get = time.perf_counter()\n    self.fractal_ssh.fetch_file(\n        remote=tarfile_path_remote,\n        local=tarfile_path_local,\n    )\n    t_1_get = time.perf_counter()\n    logger.info(\n        \"[_fetch_artifacts] Subfolder archive transferred back \"\n        f\"to {tarfile_path_local}\"\n        f\" - elapsed={t_1_get - t_0_get:.3f} s\"\n    )\n\n    # Extract tarfile locally\n    target_dir, cmd_tar = get_tar_extraction_cmd(Path(tarfile_path_local))\n    target_dir.mkdir(exist_ok=True)\n    run_subprocess(cmd=cmd_tar, logger_name=logger.name)\n    Path(tarfile_path_local).unlink(missing_ok=True)\n\n    t_1 = time.perf_counter()\n    logger.info(f\"[_fetch_artifacts] End - elapsed={t_1 - t_0:.3f} s\")\n</code></pre>"},{"location":"reference/runner/executors/slurm_ssh/runner/#fractal_server.runner.executors.slurm_ssh.runner.SlurmSSHRunner._send_many_job_inputs","title":"<code>_send_many_job_inputs(*, workdir_local, workdir_remote)</code>","text":"<p>Compress, transfer, and extract a local working directory onto a remote host.</p> <p>This method creates a temporary <code>.tar.gz</code> archive of the given <code>workdir_local</code>, transfers it to the remote machine via the configured SSH connection, extracts it into <code>workdir_remote</code>, and removes the temporary archive from both local and remote filesystems.</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/runner.py</code> <pre><code>def _send_many_job_inputs(\n    self, *, workdir_local: Path, workdir_remote: Path\n) -&gt; None:\n    \"\"\"\n    Compress, transfer, and extract a local working directory onto a remote\n    host.\n\n    This method creates a temporary `.tar.gz` archive of the given\n    `workdir_local`, transfers it to the remote machine via the configured\n    SSH connection, extracts it into `workdir_remote`, and removes the\n    temporary archive from both local and remote filesystems.\n    \"\"\"\n\n    logger.debug(\"[_send_many_job_inputs] START\")\n\n    tar_path_local = workdir_local.with_suffix(\".tar.gz\")\n    tar_name = Path(tar_path_local).name\n    tar_path_remote = workdir_remote.parent / tar_name\n\n    tar_compression_cmd = get_tar_compression_cmd(\n        subfolder_path=workdir_local, filelist_path=None\n    )\n    _, tar_extraction_cmd = get_tar_extraction_cmd(\n        archive_path=tar_path_remote\n    )\n    rm_tar_cmd = f\"rm {tar_path_remote.as_posix()}\"\n\n    try:\n        run_subprocess(tar_compression_cmd, logger_name=logger.name)\n        logger.debug(\n            \"[_send_many_job_inputs] \"\n            f\"{workdir_local=} compressed to {tar_path_local=}.\"\n        )\n        self.fractal_ssh.send_file(\n            local=tar_path_local.as_posix(),\n            remote=tar_path_remote.as_posix(),\n        )\n        logger.debug(\n            \"[_send_many_job_inputs] \"\n            f\"{tar_path_local=} sent via SSH to {tar_path_remote=}.\"\n        )\n        self.fractal_ssh.run_command(cmd=tar_extraction_cmd)\n        logger.debug(\n            \"[_send_many_job_inputs] \"\n            f\"{tar_path_remote=} extracted to {workdir_remote=}.\"\n        )\n        self.fractal_ssh.run_command(cmd=rm_tar_cmd)\n        logger.debug(\n            \"[_send_many_job_inputs] \"\n            f\"{tar_path_remote=} removed from remote server.\"\n        )\n    except Exception as e:\n        raise e\n    finally:\n        Path(tar_path_local).unlink(missing_ok=True)\n        logger.debug(f\"[_send_many_job_inputs] {tar_path_local=} removed.\")\n\n    logger.debug(\"[_send_many_job_inputs] END.\")\n</code></pre>"},{"location":"reference/runner/executors/slurm_ssh/runner/#fractal_server.runner.executors.slurm_ssh.runner.SlurmSSHRunner.run_squeue","title":"<code>run_squeue(*, job_ids)</code>","text":"<p>Run <code>squeue</code> for a set of SLURM job IDs.</p> <p>Different scenarios:</p> <ol> <li>When <code>squeue -j</code> succeeds (with exit code 0), return its stdout.</li> <li>When <code>squeue -j</code> fails (typical example:    <code>squeue -j {invalid_job_id}</code> fails with exit code 1), re-raise.    The error will be handled upstream.</li> <li>When the SSH command fails because another thread is keeping the    lock of the <code>FractalSSH</code> object for a long time, mock the standard    output of the <code>squeue</code> command so that it looks like jobs are not    completed yet.</li> <li>When the SSH command fails for other reasons, despite a forgiving    setup (7 connection attempts with base waiting interval of 2    seconds, with a cumulative timeout of 126 seconds), return an empty    string. This will be treated upstream as an empty <code>squeu</code> output,    indirectly resulting in marking the job as completed.</li> </ol> Source code in <code>fractal_server/runner/executors/slurm_ssh/runner.py</code> <pre><code>@override\ndef run_squeue(self, *, job_ids: list[str]) -&gt; str:\n    \"\"\"\n    Run `squeue` for a set of SLURM job IDs.\n\n    Different scenarios:\n\n    1. When `squeue -j` succeeds (with exit code 0), return its stdout.\n    2. When `squeue -j` fails (typical example:\n       `squeue -j {invalid_job_id}` fails with exit code 1), re-raise.\n       The error will be handled upstream.\n    3. When the SSH command fails because another thread is keeping the\n       lock of the `FractalSSH` object for a long time, mock the standard\n       output of the `squeue` command so that it looks like jobs are not\n       completed yet.\n    4. When the SSH command fails for other reasons, despite a forgiving\n       setup (7 connection attempts with base waiting interval of 2\n       seconds, with a cumulative timeout of 126 seconds), return an empty\n       string. This will be treated upstream as an empty `squeu` output,\n       indirectly resulting in marking the job as completed.\n    \"\"\"\n\n    if len(job_ids) == 0:\n        return \"\"\n\n    job_id_single_str = \",\".join([str(j) for j in job_ids])\n    cmd = (\n        \"squeue --noheader --format='%i %T' --states=all \"\n        f\"--jobs={job_id_single_str}\"\n    )\n\n    try:\n        stdout = self.fractal_ssh.run_command(\n            cmd=cmd,\n        )\n        return stdout\n    except FractalSSHCommandError as e:\n        raise e\n    except FractalSSHTimeoutError:\n        logger.warning(\n            \"[run_squeue] Could not acquire lock, use stdout placeholder.\"\n        )\n        FAKE_STATUS = \"FRACTAL_STATUS_PLACEHOLDER\"\n        placeholder_stdout = \"\\n\".join(\n            [f\"{job_id} {FAKE_STATUS}\" for job_id in job_ids]\n        )\n        return placeholder_stdout\n    except Exception as e:\n        logger.error(f\"Ignoring `squeue` command failure {e}\")\n        return \"\"\n</code></pre>"},{"location":"reference/runner/executors/slurm_ssh/tar_commands/","title":"tar_commands","text":"<p>Prepare tar commands for task-subfolder compression/extraction.</p>"},{"location":"reference/runner/executors/slurm_ssh/tar_commands/#fractal_server.runner.executors.slurm_ssh.tar_commands.get_tar_compression_cmd","title":"<code>get_tar_compression_cmd(subfolder_path, filelist_path)</code>","text":"<p>Prepare command to compress e.g. <code>/path/dir</code> into <code>/path/dir.tar.gz</code>.</p> <p>Note that <code>/path/dir.tar.gz</code> may already exist. In this case, it will be overwritten.</p> PARAMETER DESCRIPTION <code>subfolder_path</code> <p>Absolute path to the folder to compress.</p> <p> TYPE: <code>Path</code> </p> <code>filelist_path</code> <p>If set, to be used in the <code>--files-from</code> option.</p> <p> TYPE: <code>Path | None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>tar command</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/tar_commands.py</code> <pre><code>def get_tar_compression_cmd(\n    subfolder_path: Path,\n    filelist_path: Path | None,\n) -&gt; str:\n    \"\"\"\n    Prepare command to compress e.g. `/path/dir` into `/path/dir.tar.gz`.\n\n    Note that `/path/dir.tar.gz` may already exist. In this case, it will\n    be overwritten.\n\n    Args:\n        subfolder_path: Absolute path to the folder to compress.\n        filelist_path: If set, to be used in the `--files-from` option.\n\n    Returns:\n        tar command\n    \"\"\"\n    tarfile_path = subfolder_path.with_suffix(\".tar.gz\")\n    if filelist_path is None:\n        cmd_tar = (\n            f\"tar -c -z \"\n            f\"-f {tarfile_path} \"\n            f\"--directory={subfolder_path.as_posix()} \"\n            \".\"\n        )\n    else:\n        cmd_tar = (\n            f\"tar -c -z -f {tarfile_path} \"\n            f\"--directory={subfolder_path.as_posix()} \"\n            f\"--files-from={filelist_path.as_posix()} --ignore-failed-read\"\n        )\n\n    return cmd_tar\n</code></pre>"},{"location":"reference/runner/executors/slurm_ssh/tar_commands/#fractal_server.runner.executors.slurm_ssh.tar_commands.get_tar_extraction_cmd","title":"<code>get_tar_extraction_cmd(archive_path)</code>","text":"<p>Prepare command to extract e.g. <code>/path/dir.tar.gz</code> into <code>/path/dir</code>.</p> PARAMETER DESCRIPTION <code>archive_path</code> <p>Absolute path to the tar.gz archive.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>tuple[Path, str]</code> <p>Target extraction folder and tar command</p> Source code in <code>fractal_server/runner/executors/slurm_ssh/tar_commands.py</code> <pre><code>def get_tar_extraction_cmd(archive_path: Path) -&gt; tuple[Path, str]:\n    \"\"\"\n    Prepare command to extract e.g. `/path/dir.tar.gz` into `/path/dir`.\n\n    Args:\n        archive_path: Absolute path to the tar.gz archive.\n\n    Returns:\n        Target extraction folder and tar command\n    \"\"\"\n\n    # Prepare subfolder path\n    if archive_path.suffixes[-2:] != [\".tar\", \".gz\"]:\n        raise ValueError(\n            \"Archive path must end with `.tar.gz` \"\n            f\"(given: {archive_path.as_posix()})\"\n        )\n    subfolder_path = archive_path.with_suffix(\"\").with_suffix(\"\")\n\n    cmd_tar = (\n        f\"tar -xzvf {archive_path} --directory={subfolder_path.as_posix()}\"\n    )\n    return subfolder_path, cmd_tar\n</code></pre>"},{"location":"reference/runner/executors/slurm_sudo/","title":"slurm_sudo","text":""},{"location":"reference/runner/executors/slurm_sudo/_subprocess_run_as_user/","title":"_subprocess_run_as_user","text":"<p>Run simple commands as another user</p> <p>This module provides a set of tools similar to <code>subprocess.run</code>, <code>glob.glob</code> or <code>os.path.exists</code>, but extended so that they can be executed on behalf of another user. Note that this requires appropriate sudo permissions.</p>"},{"location":"reference/runner/executors/slurm_sudo/_subprocess_run_as_user/#fractal_server.runner.executors.slurm_sudo._subprocess_run_as_user._mkdir_as_user","title":"<code>_mkdir_as_user(*, folder, user)</code>","text":"<p>Create a folder as a different user</p> PARAMETER DESCRIPTION <code>folder</code> <p>Absolute path to the folder</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>User to be impersonated</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if <code>user</code> is not correctly defined, or if subprocess           returncode is not 0.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/_subprocess_run_as_user.py</code> <pre><code>def _mkdir_as_user(*, folder: str, user: str) -&gt; None:\n    \"\"\"\n    Create a folder as a different user\n\n    Args:\n        folder: Absolute path to the folder\n        user: User to be impersonated\n\n    Raises:\n        RuntimeError: if `user` is not correctly defined, or if subprocess\n                      returncode is not 0.\n    \"\"\"\n    if not user:\n        raise RuntimeError(f\"{user=} not allowed in _mkdir_as_user\")\n\n    cmd = f\"mkdir -p {folder}\"\n    _run_command_as_user(cmd=cmd, user=user, check=True)\n</code></pre>"},{"location":"reference/runner/executors/slurm_sudo/_subprocess_run_as_user/#fractal_server.runner.executors.slurm_sudo._subprocess_run_as_user._run_command_as_user","title":"<code>_run_command_as_user(*, cmd, user=None, check=False)</code>","text":"<p>Use <code>sudo -u</code> to impersonate another user and run a command</p> PARAMETER DESCRIPTION <code>cmd</code> <p>Command to be run</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>User to be impersonated</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>check</code> <p>If <code>True</code>, check that <code>returncode=0</code> and fail otherwise.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if <code>check=True</code> and returncode is non-zero.</p> RETURNS DESCRIPTION <code>res</code> <p>The return value from <code>subprocess.run</code>.</p> <p> TYPE: <code>CompletedProcess</code> </p> Source code in <code>fractal_server/runner/executors/slurm_sudo/_subprocess_run_as_user.py</code> <pre><code>def _run_command_as_user(\n    *,\n    cmd: str,\n    user: str | None = None,\n    check: bool = False,\n) -&gt; subprocess.CompletedProcess:\n    \"\"\"\n    Use `sudo -u` to impersonate another user and run a command\n\n    Args:\n        cmd: Command to be run\n        user: User to be impersonated\n        check: If `True`, check that `returncode=0` and fail otherwise.\n\n    Raises:\n        RuntimeError: if `check=True` and returncode is non-zero.\n\n    Returns:\n        res: The return value from `subprocess.run`.\n    \"\"\"\n    validate_cmd(cmd)\n    logger.debug(f'[_run_command_as_user] {user=}, cmd=\"{cmd}\"')\n    if user:\n        new_cmd = f\"sudo --set-home --non-interactive -u {user} {cmd}\"\n    else:\n        new_cmd = cmd\n    res = subprocess.run(  # nosec\n        shlex.split(new_cmd),\n        capture_output=True,\n        encoding=\"utf-8\",\n    )\n    logger.debug(f\"[_run_command_as_user] {res.returncode=}\")\n    logger.debug(f\"[_run_command_as_user] {res.stdout=}\")\n    logger.debug(f\"[_run_command_as_user] {res.stderr=}\")\n\n    if check and not res.returncode == 0:\n        raise RuntimeError(\n            f\"{cmd=}\\n\\n{res.returncode=}\\n\\n{res.stdout=}\\n\\n{res.stderr=}\\n\"\n        )\n\n    return res\n</code></pre>"},{"location":"reference/runner/executors/slurm_sudo/runner/","title":"runner","text":""},{"location":"reference/runner/executors/slurm_sudo/runner/#fractal_server.runner.executors.slurm_sudo.runner.SlurmSudoRunner","title":"<code>SlurmSudoRunner</code>","text":"<p>               Bases: <code>BaseSlurmRunner</code></p> <p>Runner implementation for a computational <code>slurm_sudo</code> resource.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/runner.py</code> <pre><code>class SlurmSudoRunner(BaseSlurmRunner):\n    \"\"\"\n    Runner implementation for a computational `slurm_sudo` resource.\n    \"\"\"\n\n    slurm_user: str\n    slurm_account: str | None = None\n\n    def __init__(\n        self,\n        *,\n        # Common\n        root_dir_local: Path,\n        root_dir_remote: Path,\n        common_script_lines: list[str] | None = None,\n        resource: Resource,\n        # Specific\n        profile: Profile,\n        user_cache_dir: str,\n        slurm_account: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Set parameters that are the same for different Fractal tasks and for\n        different SLURM jobs/tasks.\n        \"\"\"\n\n        self.slurm_user = profile.username\n        self.shared_config = JobRunnerConfigSLURM(**resource.jobs_runner_config)\n\n        super().__init__(\n            slurm_runner_type=\"sudo\",\n            root_dir_local=root_dir_local,\n            root_dir_remote=root_dir_remote,\n            common_script_lines=common_script_lines,\n            user_cache_dir=user_cache_dir,\n            poll_interval=resource.jobs_poll_interval,\n            python_worker_interpreter=resource.jobs_slurm_python_worker,\n            slurm_account=slurm_account,\n        )\n\n    @override\n    def _mkdir_local_folder(self, folder: str) -&gt; None:\n        original_umask = os.umask(0)\n        Path(folder).mkdir(parents=True, mode=0o755)\n        os.umask(original_umask)\n\n    @override\n    def _mkdir_remote_folder(self, folder: str) -&gt; None:\n        _mkdir_as_user(folder=folder, user=self.slurm_user)\n\n    def _fetch_artifacts_single_job(self, job: SlurmJob) -&gt; None:\n        \"\"\"\n        Fetch artifacts for a single SLURM jobs.\n        \"\"\"\n        logger.debug(f\"[_fetch_artifacts_single_job] {job.slurm_job_id=} START\")\n        source_target_list = [\n            (job.slurm_stdout_remote, job.slurm_stdout_local),\n            (job.slurm_stderr_remote, job.slurm_stderr_local),\n        ]\n        for task in job.tasks:\n            source_target_list.extend(\n                [\n                    (\n                        task.output_file_remote,\n                        task.output_file_local,\n                    ),\n                    (\n                        task.task_files.log_file_remote,\n                        task.task_files.log_file_local,\n                    ),\n                    (\n                        task.task_files.metadiff_file_remote,\n                        task.task_files.metadiff_file_local,\n                    ),\n                ]\n            )\n\n        for source, target in source_target_list:\n            try:\n                res = _run_command_as_user(\n                    cmd=f\"cat {source}\",\n                    user=self.slurm_user,\n                    check=True,\n                )\n                # Write local file\n                with open(target, \"w\") as f:\n                    f.write(res.stdout)\n                logger.debug(\n                    f\"[_fetch_artifacts_single_job] Copied {source} into \"\n                    f\"{target}\"\n                )\n            except RuntimeError as e:\n                logger.warning(\n                    f\"SKIP copy {source} into {target}. \"\n                    f\"Original error: {str(e)}\"\n                )\n        logger.debug(f\"[_fetch_artifacts_single_job] {job.slurm_job_id=} END\")\n\n    def _fetch_artifacts(\n        self,\n        finished_slurm_jobs: list[SlurmJob],\n    ) -&gt; None:\n        \"\"\"\n        Fetch artifacts for a list of SLURM jobs.\n        \"\"\"\n        MAX_NUM_THREADS = 12\n        THREAD_NAME_PREFIX = \"fetch_artifacts\"\n        logger.debug(\n            \"[_fetch_artifacts] START \"\n            f\"({MAX_NUM_THREADS=}, {len(finished_slurm_jobs)=}).\"\n        )\n        with ThreadPoolExecutor(\n            max_workers=MAX_NUM_THREADS,\n            thread_name_prefix=THREAD_NAME_PREFIX,\n        ) as executor:\n            executor.map(\n                self._fetch_artifacts_single_job,\n                finished_slurm_jobs,\n            )\n        logger.debug(\"[_fetch_artifacts] END.\")\n\n    @override\n    def _run_remote_cmd(self, cmd: str) -&gt; str:\n        res = _run_command_as_user(\n            cmd=cmd,\n            user=self.slurm_user,\n            check=True,\n        )\n        return res.stdout\n\n    @override\n    def run_squeue(self, *, job_ids: list[str]) -&gt; str:\n        \"\"\"\n        Run `squeue` for a set of SLURM job IDs.\n        \"\"\"\n\n        if len(job_ids) == 0:\n            return \"\"\n\n        job_id_single_str = \",\".join([str(j) for j in job_ids])\n        cmd = (\n            \"squeue --noheader --format='%i %T' --states=all \"\n            f\"--jobs {job_id_single_str}\"\n        )\n        res = _subprocess_run_or_raise(cmd)\n        return res.stdout\n</code></pre>"},{"location":"reference/runner/executors/slurm_sudo/runner/#fractal_server.runner.executors.slurm_sudo.runner.SlurmSudoRunner.__init__","title":"<code>__init__(*, root_dir_local, root_dir_remote, common_script_lines=None, resource, profile, user_cache_dir, slurm_account=None)</code>","text":"<p>Set parameters that are the same for different Fractal tasks and for different SLURM jobs/tasks.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/runner.py</code> <pre><code>def __init__(\n    self,\n    *,\n    # Common\n    root_dir_local: Path,\n    root_dir_remote: Path,\n    common_script_lines: list[str] | None = None,\n    resource: Resource,\n    # Specific\n    profile: Profile,\n    user_cache_dir: str,\n    slurm_account: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Set parameters that are the same for different Fractal tasks and for\n    different SLURM jobs/tasks.\n    \"\"\"\n\n    self.slurm_user = profile.username\n    self.shared_config = JobRunnerConfigSLURM(**resource.jobs_runner_config)\n\n    super().__init__(\n        slurm_runner_type=\"sudo\",\n        root_dir_local=root_dir_local,\n        root_dir_remote=root_dir_remote,\n        common_script_lines=common_script_lines,\n        user_cache_dir=user_cache_dir,\n        poll_interval=resource.jobs_poll_interval,\n        python_worker_interpreter=resource.jobs_slurm_python_worker,\n        slurm_account=slurm_account,\n    )\n</code></pre>"},{"location":"reference/runner/executors/slurm_sudo/runner/#fractal_server.runner.executors.slurm_sudo.runner.SlurmSudoRunner._fetch_artifacts","title":"<code>_fetch_artifacts(finished_slurm_jobs)</code>","text":"<p>Fetch artifacts for a list of SLURM jobs.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/runner.py</code> <pre><code>def _fetch_artifacts(\n    self,\n    finished_slurm_jobs: list[SlurmJob],\n) -&gt; None:\n    \"\"\"\n    Fetch artifacts for a list of SLURM jobs.\n    \"\"\"\n    MAX_NUM_THREADS = 12\n    THREAD_NAME_PREFIX = \"fetch_artifacts\"\n    logger.debug(\n        \"[_fetch_artifacts] START \"\n        f\"({MAX_NUM_THREADS=}, {len(finished_slurm_jobs)=}).\"\n    )\n    with ThreadPoolExecutor(\n        max_workers=MAX_NUM_THREADS,\n        thread_name_prefix=THREAD_NAME_PREFIX,\n    ) as executor:\n        executor.map(\n            self._fetch_artifacts_single_job,\n            finished_slurm_jobs,\n        )\n    logger.debug(\"[_fetch_artifacts] END.\")\n</code></pre>"},{"location":"reference/runner/executors/slurm_sudo/runner/#fractal_server.runner.executors.slurm_sudo.runner.SlurmSudoRunner._fetch_artifacts_single_job","title":"<code>_fetch_artifacts_single_job(job)</code>","text":"<p>Fetch artifacts for a single SLURM jobs.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/runner.py</code> <pre><code>def _fetch_artifacts_single_job(self, job: SlurmJob) -&gt; None:\n    \"\"\"\n    Fetch artifacts for a single SLURM jobs.\n    \"\"\"\n    logger.debug(f\"[_fetch_artifacts_single_job] {job.slurm_job_id=} START\")\n    source_target_list = [\n        (job.slurm_stdout_remote, job.slurm_stdout_local),\n        (job.slurm_stderr_remote, job.slurm_stderr_local),\n    ]\n    for task in job.tasks:\n        source_target_list.extend(\n            [\n                (\n                    task.output_file_remote,\n                    task.output_file_local,\n                ),\n                (\n                    task.task_files.log_file_remote,\n                    task.task_files.log_file_local,\n                ),\n                (\n                    task.task_files.metadiff_file_remote,\n                    task.task_files.metadiff_file_local,\n                ),\n            ]\n        )\n\n    for source, target in source_target_list:\n        try:\n            res = _run_command_as_user(\n                cmd=f\"cat {source}\",\n                user=self.slurm_user,\n                check=True,\n            )\n            # Write local file\n            with open(target, \"w\") as f:\n                f.write(res.stdout)\n            logger.debug(\n                f\"[_fetch_artifacts_single_job] Copied {source} into \"\n                f\"{target}\"\n            )\n        except RuntimeError as e:\n            logger.warning(\n                f\"SKIP copy {source} into {target}. \"\n                f\"Original error: {str(e)}\"\n            )\n    logger.debug(f\"[_fetch_artifacts_single_job] {job.slurm_job_id=} END\")\n</code></pre>"},{"location":"reference/runner/executors/slurm_sudo/runner/#fractal_server.runner.executors.slurm_sudo.runner.SlurmSudoRunner.run_squeue","title":"<code>run_squeue(*, job_ids)</code>","text":"<p>Run <code>squeue</code> for a set of SLURM job IDs.</p> Source code in <code>fractal_server/runner/executors/slurm_sudo/runner.py</code> <pre><code>@override\ndef run_squeue(self, *, job_ids: list[str]) -&gt; str:\n    \"\"\"\n    Run `squeue` for a set of SLURM job IDs.\n    \"\"\"\n\n    if len(job_ids) == 0:\n        return \"\"\n\n    job_id_single_str = \",\".join([str(j) for j in job_ids])\n    cmd = (\n        \"squeue --noheader --format='%i %T' --states=all \"\n        f\"--jobs {job_id_single_str}\"\n    )\n    res = _subprocess_run_or_raise(cmd)\n    return res.stdout\n</code></pre>"},{"location":"reference/runner/v2/","title":"v2","text":""},{"location":"reference/runner/v2/_local/","title":"_local","text":""},{"location":"reference/runner/v2/_local/#fractal_server.runner.v2._local.process_workflow","title":"<code>process_workflow(*, job_id, workflow, dataset, workflow_dir_local, workflow_dir_remote=None, first_task_index=None, last_task_index=None, logger_name, job_attribute_filters, job_type_filters, user_id, resource, profile, user_cache_dir=None, fractal_ssh=None, slurm_account=None, worker_init=None)</code>","text":"<p>Run a workflow through a local backend.</p> PARAMETER DESCRIPTION <code>job_id</code> <p>Job ID.</p> <p> TYPE: <code>int</code> </p> <code>workflow</code> <p>Workflow to be run</p> <p> TYPE: <code>WorkflowV2</code> </p> <code>dataset</code> <p>Dataset to be used.</p> <p> TYPE: <code>DatasetV2</code> </p> <code>workflow_dir_local</code> <p>Local working directory for this job.</p> <p> TYPE: <code>Path</code> </p> <code>workflow_dir_remote</code> <p>Remote working directory for this job - only relevant for <code>slurm_sudo</code> and <code>slurm_ssh</code> backends.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>first_task_index</code> <p>Positional index of the first task to execute; if <code>None</code>, start from <code>0</code>.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>last_task_index</code> <p>Positional index of the last task to execute; if <code>None</code>, proceed until the last task.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>logger_name</code> <p>Logger name</p> <p> TYPE: <code>str</code> </p> <code>user_id</code> <p>User ID.</p> <p> TYPE: <code>int</code> </p> <code>resource</code> <p>Computational resource for running this job.</p> <p> TYPE: <code>Resource</code> </p> <code>profile</code> <p>Computational profile for running this job.</p> <p> TYPE: <code>Profile</code> </p> <code>user_cache_dir</code> <p>User-writeable folder (typically a subfolder of <code>project_dirs</code>). Only relevant for <code>slurm_sudo</code> and <code>slurm_ssh</code> backends.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>fractal_ssh</code> <p><code>FractalSSH</code> object, only relevant for the <code>slurm_ssh</code> backend.</p> <p> TYPE: <code>FractalSSH | None</code> DEFAULT: <code>None</code> </p> <code>slurm_account</code> <p>SLURM account to set. Only relevant for <code>slurm_sudo</code> and <code>slurm_ssh</code> backends.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>worker_init</code> <p>Additional preamble lines for SLURM submission script. Only relevant for <code>slurm_sudo</code> and <code>slurm_ssh</code> backends.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>fractal_server/runner/v2/_local.py</code> <pre><code>def process_workflow(\n    *,\n    job_id: int,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    workflow_dir_local: Path,\n    workflow_dir_remote: Path | None = None,\n    first_task_index: int | None = None,\n    last_task_index: int | None = None,\n    logger_name: str,\n    job_attribute_filters: AttributeFilters,\n    job_type_filters: dict[str, bool],\n    user_id: int,\n    resource: Resource,\n    profile: Profile,\n    user_cache_dir: str | None = None,\n    fractal_ssh: FractalSSH | None = None,\n    slurm_account: str | None = None,\n    worker_init: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Run a workflow through a local backend.\n\n    Args:\n        job_id: Job ID.\n        workflow: Workflow to be run\n        dataset: Dataset to be used.\n        workflow_dir_local: Local working directory for this job.\n        workflow_dir_remote:\n            Remote working directory for this job - only relevant for\n            `slurm_sudo` and `slurm_ssh` backends.\n        first_task_index:\n            Positional index of the first task to execute; if `None`, start\n            from `0`.\n        last_task_index:\n            Positional index of the last task to execute; if `None`, proceed\n            until the last task.\n        logger_name: Logger name\n        user_id: User ID.\n        resource: Computational resource for running this job.\n        profile: Computational profile for running this job.\n        user_cache_dir:\n            User-writeable folder (typically a subfolder of `project_dirs`).\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        fractal_ssh:\n            `FractalSSH` object, only relevant for the `slurm_ssh` backend.\n        slurm_account:\n            SLURM account to set.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        worker_init:\n            Additional preamble lines for SLURM submission script.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n    \"\"\"\n\n    if workflow_dir_remote and (workflow_dir_remote != workflow_dir_local):\n        raise NotImplementedError(\n            \"Local backend does not support different directories \"\n            f\"{workflow_dir_local=} and {workflow_dir_remote=}\"\n        )\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    with LocalRunner(\n        root_dir_local=workflow_dir_local,\n        resource=resource,\n        profile=profile,\n    ) as runner:\n        execute_tasks(\n            wf_task_list=workflow.task_list[\n                first_task_index : (last_task_index + 1)\n            ],\n            dataset=dataset,\n            job_id=job_id,\n            runner=runner,\n            workflow_dir_local=workflow_dir_local,\n            workflow_dir_remote=workflow_dir_local,\n            logger_name=logger_name,\n            get_runner_config=get_local_backend_config,\n            job_attribute_filters=job_attribute_filters,\n            job_type_filters=job_type_filters,\n            user_id=user_id,\n            resource_id=resource.id,\n        )\n</code></pre>"},{"location":"reference/runner/v2/_slurm_ssh/","title":"_slurm_ssh","text":"<p>Slurm Backend</p> <p>This backend runs fractal workflows in a SLURM cluster.</p>"},{"location":"reference/runner/v2/_slurm_ssh/#fractal_server.runner.v2._slurm_ssh.process_workflow","title":"<code>process_workflow(*, job_id, workflow, dataset, workflow_dir_local, workflow_dir_remote=None, first_task_index=None, last_task_index=None, logger_name, job_attribute_filters, job_type_filters, user_id, resource, profile, fractal_ssh=None, slurm_account=None, worker_init=None, user_cache_dir)</code>","text":"<p>Run a workflow through a <code>slurm_ssh</code> backend.</p> <pre><code>Args:\njob_id: Job ID.\nworkflow: Workflow to be run\ndataset: Dataset to be used.\nworkflow_dir_local: Local working directory for this job.\nworkflow_dir_remote:\n    Remote working directory for this job - only relevant for\n    `slurm_sudo` and `slurm_ssh` backends.\nfirst_task_index:\n    Positional index of the first task to execute; if `None`, start\n    from `0`.\nlast_task_index:\n    Positional index of the last task to execute; if `None`, proceed\n    until the last task.\nlogger_name: Logger name\nuser_id: User ID.\nresource: Computational resource for running this job.\nprofile: Computational profile for running this job.\nuser_cache_dir:\n    User-writeable folder (typically a subfolder of `project_dirs`).\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\nfractal_ssh:\n    `FractalSSH` object, only relevant for the `slurm_ssh` backend.\nslurm_account:\n    SLURM account to set.\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\nworker_init:\n    Additional preamble lines for SLURM submission script.\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n</code></pre> Source code in <code>fractal_server/runner/v2/_slurm_ssh.py</code> <pre><code>def process_workflow(\n    *,\n    job_id: int,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    workflow_dir_local: Path,\n    workflow_dir_remote: Path | None = None,\n    first_task_index: int | None = None,\n    last_task_index: int | None = None,\n    logger_name: str,\n    job_attribute_filters: AttributeFilters,\n    job_type_filters: dict[str, bool],\n    user_id: int,\n    resource: Resource,\n    profile: Profile,\n    fractal_ssh: FractalSSH | None = None,\n    slurm_account: str | None = None,\n    worker_init: str | None = None,\n    user_cache_dir: str,\n) -&gt; None:\n    \"\"\"\n    Run a workflow through a `slurm_ssh` backend.\n\n        Args:\n        job_id: Job ID.\n        workflow: Workflow to be run\n        dataset: Dataset to be used.\n        workflow_dir_local: Local working directory for this job.\n        workflow_dir_remote:\n            Remote working directory for this job - only relevant for\n            `slurm_sudo` and `slurm_ssh` backends.\n        first_task_index:\n            Positional index of the first task to execute; if `None`, start\n            from `0`.\n        last_task_index:\n            Positional index of the last task to execute; if `None`, proceed\n            until the last task.\n        logger_name: Logger name\n        user_id: User ID.\n        resource: Computational resource for running this job.\n        profile: Computational profile for running this job.\n        user_cache_dir:\n            User-writeable folder (typically a subfolder of `project_dirs`).\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        fractal_ssh:\n            `FractalSSH` object, only relevant for the `slurm_ssh` backend.\n        slurm_account:\n            SLURM account to set.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        worker_init:\n            Additional preamble lines for SLURM submission script.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n    \"\"\"\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    if isinstance(worker_init, str):\n        worker_init = worker_init.split(\"\\n\")\n\n    with SlurmSSHRunner(\n        fractal_ssh=fractal_ssh,\n        root_dir_local=workflow_dir_local,\n        root_dir_remote=workflow_dir_remote,\n        slurm_account=slurm_account,\n        resource=resource,\n        profile=profile,\n        common_script_lines=worker_init,\n        user_cache_dir=user_cache_dir,\n    ) as runner:\n        execute_tasks(\n            wf_task_list=workflow.task_list[\n                first_task_index : (last_task_index + 1)\n            ],\n            dataset=dataset,\n            job_id=job_id,\n            runner=runner,\n            workflow_dir_local=workflow_dir_local,\n            workflow_dir_remote=workflow_dir_remote,\n            logger_name=logger_name,\n            get_runner_config=get_slurm_config,\n            job_attribute_filters=job_attribute_filters,\n            job_type_filters=job_type_filters,\n            user_id=user_id,\n            resource_id=resource.id,\n        )\n</code></pre>"},{"location":"reference/runner/v2/_slurm_sudo/","title":"_slurm_sudo","text":"<p>Slurm Backend</p> <p>This backend runs fractal workflows in a SLURM cluster.</p>"},{"location":"reference/runner/v2/_slurm_sudo/#fractal_server.runner.v2._slurm_sudo.process_workflow","title":"<code>process_workflow(*, job_id, workflow, dataset, workflow_dir_local, workflow_dir_remote=None, first_task_index=None, last_task_index=None, logger_name, job_attribute_filters, job_type_filters, user_id, resource, profile, user_cache_dir, slurm_account=None, worker_init=None, fractal_ssh=None)</code>","text":"<p>Run a workflow through a <code>slurm_sudo</code> backend.</p> <pre><code>Args:\njob_id: Job ID.\nworkflow: Workflow to be run\ndataset: Dataset to be used.\nworkflow_dir_local: Local working directory for this job.\nworkflow_dir_remote:\n    Remote working directory for this job - only relevant for\n    `slurm_sudo` and `slurm_ssh` backends.\nfirst_task_index:\n    Positional index of the first task to execute; if `None`, start\n    from `0`.\nlast_task_index:\n    Positional index of the last task to execute; if `None`, proceed\n    until the last task.\nlogger_name: Logger name\nuser_id: User ID.\nresource: Computational resource for running this job.\nprofile: Computational profile for running this job.\nuser_cache_dir:\n    User-writeable folder (typically a subfolder of `project_dirs`).\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\nfractal_ssh:\n    `FractalSSH` object, only relevant for the `slurm_ssh` backend.\nslurm_account:\n    SLURM account to set.\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\nworker_init:\n    Additional preamble lines for SLURM submission script.\n    Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n</code></pre> Source code in <code>fractal_server/runner/v2/_slurm_sudo.py</code> <pre><code>def process_workflow(\n    *,\n    job_id: int,\n    workflow: WorkflowV2,\n    dataset: DatasetV2,\n    workflow_dir_local: Path,\n    workflow_dir_remote: Path | None = None,\n    first_task_index: int | None = None,\n    last_task_index: int | None = None,\n    logger_name: str,\n    job_attribute_filters: AttributeFilters,\n    job_type_filters: dict[str, bool],\n    user_id: int,\n    resource: Resource,\n    profile: Profile,\n    user_cache_dir: str,\n    slurm_account: str | None = None,\n    worker_init: str | None = None,\n    fractal_ssh: FractalSSH | None = None,\n) -&gt; None:\n    \"\"\"\n    Run a workflow through a `slurm_sudo` backend.\n\n        Args:\n        job_id: Job ID.\n        workflow: Workflow to be run\n        dataset: Dataset to be used.\n        workflow_dir_local: Local working directory for this job.\n        workflow_dir_remote:\n            Remote working directory for this job - only relevant for\n            `slurm_sudo` and `slurm_ssh` backends.\n        first_task_index:\n            Positional index of the first task to execute; if `None`, start\n            from `0`.\n        last_task_index:\n            Positional index of the last task to execute; if `None`, proceed\n            until the last task.\n        logger_name: Logger name\n        user_id: User ID.\n        resource: Computational resource for running this job.\n        profile: Computational profile for running this job.\n        user_cache_dir:\n            User-writeable folder (typically a subfolder of `project_dirs`).\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        fractal_ssh:\n            `FractalSSH` object, only relevant for the `slurm_ssh` backend.\n        slurm_account:\n            SLURM account to set.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n        worker_init:\n            Additional preamble lines for SLURM submission script.\n            Only relevant for `slurm_sudo` and `slurm_ssh` backends.\n    \"\"\"\n\n    # Set values of first_task_index and last_task_index\n    num_tasks = len(workflow.task_list)\n    first_task_index, last_task_index = set_start_and_last_task_index(\n        num_tasks,\n        first_task_index=first_task_index,\n        last_task_index=last_task_index,\n    )\n\n    if isinstance(worker_init, str):\n        worker_init = worker_init.split(\"\\n\")\n\n    with SlurmSudoRunner(\n        root_dir_local=workflow_dir_local,\n        root_dir_remote=workflow_dir_remote,\n        common_script_lines=worker_init,\n        resource=resource,\n        profile=profile,\n        user_cache_dir=user_cache_dir,\n        slurm_account=slurm_account,\n    ) as runner:\n        execute_tasks(\n            wf_task_list=workflow.task_list[\n                first_task_index : (last_task_index + 1)\n            ],\n            dataset=dataset,\n            job_id=job_id,\n            runner=runner,\n            workflow_dir_local=workflow_dir_local,\n            workflow_dir_remote=workflow_dir_remote,\n            logger_name=logger_name,\n            get_runner_config=get_slurm_config,\n            job_attribute_filters=job_attribute_filters,\n            job_type_filters=job_type_filters,\n            user_id=user_id,\n            resource_id=resource.id,\n        )\n</code></pre>"},{"location":"reference/runner/v2/db_tools/","title":"db_tools","text":""},{"location":"reference/runner/v2/db_tools/#fractal_server.runner.v2.db_tools.bulk_upsert_image_cache_fast","title":"<code>bulk_upsert_image_cache_fast(*, list_upsert_objects, db)</code>","text":"<p>Insert or update many objects into <code>HistoryImageCache</code> and commit</p> <p>This function is an optimized version of</p> <pre><code>for obj in list_upsert_objects:\n    db.merge(**obj)\ndb.commit()\n</code></pre> <p>See docs at https://docs.sqlalchemy.org/en/20/dialects/postgresql.html#insert-on-conflict-upsert</p> <p>NOTE: we tried to replace <code>index_elements</code> with <code>constraint=\"pk_historyimagecache\"</code>, but it did not work as expected.</p> PARAMETER DESCRIPTION <code>list_upsert_objects</code> <p>List of dictionaries for objects to be upsert-ed.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> <code>db</code> <p>A sync database session</p> <p> TYPE: <code>Session</code> </p> Source code in <code>fractal_server/runner/v2/db_tools.py</code> <pre><code>def bulk_upsert_image_cache_fast(\n    *,\n    list_upsert_objects: list[dict[str, Any]],\n    db: Session,\n) -&gt; None:\n    \"\"\"\n    Insert or update many objects into `HistoryImageCache` and commit\n\n    This function is an optimized version of\n\n    ```python\n    for obj in list_upsert_objects:\n        db.merge(**obj)\n    db.commit()\n    ```\n\n    See docs at\n    https://docs.sqlalchemy.org/en/20/dialects/postgresql.html#insert-on-conflict-upsert\n\n    NOTE: we tried to replace `index_elements` with\n    `constraint=\"pk_historyimagecache\"`, but it did not work as expected.\n\n    Args:\n        list_upsert_objects:\n            List of dictionaries for objects to be upsert-ed.\n        db: A sync database session\n    \"\"\"\n    len_list_upsert_objects = len(list_upsert_objects)\n\n    logger.debug(f\"[bulk_upsert_image_cache_fast] {len_list_upsert_objects=}.\")\n\n    if len_list_upsert_objects == 0:\n        return None\n\n    for ind in range(0, len_list_upsert_objects, _CHUNK_SIZE):\n        stmt = pg_insert(HistoryImageCache).values(\n            list_upsert_objects[ind : ind + _CHUNK_SIZE]\n        )\n        stmt = stmt.on_conflict_do_update(\n            index_elements=[\n                HistoryImageCache.zarr_url,\n                HistoryImageCache.dataset_id,\n                HistoryImageCache.workflowtask_id,\n            ],\n            set_=dict(\n                latest_history_unit_id=stmt.excluded.latest_history_unit_id\n            ),\n        )\n        db.execute(stmt)\n        db.commit()\n</code></pre>"},{"location":"reference/runner/v2/deduplicate_list/","title":"deduplicate_list","text":""},{"location":"reference/runner/v2/deduplicate_list/#fractal_server.runner.v2.deduplicate_list.deduplicate_list","title":"<code>deduplicate_list(this_list)</code>","text":"<p>Custom replacement for <code>set(this_list)</code>, when items are non-hashable.</p> Source code in <code>fractal_server/runner/v2/deduplicate_list.py</code> <pre><code>def deduplicate_list(\n    this_list: list[T],\n) -&gt; list[T]:\n    \"\"\"\n    Custom replacement for `set(this_list)`, when items are non-hashable.\n    \"\"\"\n    new_list_dict = []\n    new_list_objs = []\n    for this_obj in this_list:\n        this_dict = this_obj.model_dump()\n        if this_dict not in new_list_dict:\n            new_list_dict.append(this_dict)\n            new_list_objs.append(this_obj)\n    return new_list_objs\n</code></pre>"},{"location":"reference/runner/v2/merge_outputs/","title":"merge_outputs","text":""},{"location":"reference/runner/v2/runner/","title":"runner","text":""},{"location":"reference/runner/v2/runner/#fractal_server.runner.v2.runner._remove_status_from_attributes","title":"<code>_remove_status_from_attributes(images)</code>","text":"<p>Drop attribute <code>IMAGE_STATUS_KEY</code> from all images.</p> Source code in <code>fractal_server/runner/v2/runner.py</code> <pre><code>def _remove_status_from_attributes(\n    images: list[dict[str, Any]],\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Drop attribute `IMAGE_STATUS_KEY` from all images.\n    \"\"\"\n    images_copy = deepcopy(images)\n    [img[\"attributes\"].pop(IMAGE_STATUS_KEY, None) for img in images_copy]\n    return images_copy\n</code></pre>"},{"location":"reference/runner/v2/runner/#fractal_server.runner.v2.runner.execute_tasks","title":"<code>execute_tasks(*, wf_task_list, dataset, runner, user_id, workflow_dir_local, job_id, workflow_dir_remote=None, logger_name=None, get_runner_config, job_type_filters, job_attribute_filters, resource_id)</code>","text":"<p>Execute a list of task on a given dataset.</p> Source code in <code>fractal_server/runner/v2/runner.py</code> <pre><code>def execute_tasks(\n    *,\n    wf_task_list: list[WorkflowTaskV2],\n    dataset: DatasetV2,\n    runner: BaseRunner,\n    user_id: int,\n    workflow_dir_local: Path,\n    job_id: int,\n    workflow_dir_remote: Path | None = None,\n    logger_name: str | None = None,\n    get_runner_config: GetRunnerConfigType,\n    job_type_filters: dict[str, bool],\n    job_attribute_filters: AttributeFilters,\n    resource_id: int,\n) -&gt; None:\n    \"\"\"\n    Execute a list of task on a given dataset.\n    \"\"\"\n    logger = get_logger(logger_name=logger_name)\n\n    if not workflow_dir_local.exists():\n        logger.warning(\n            f\"Now creating {workflow_dir_local}, but it \"\n            \"should have already happened.\"\n        )\n        workflow_dir_local.mkdir()\n\n    # For local backend, remote and local folders are the same\n    if workflow_dir_remote is None:\n        workflow_dir_remote = workflow_dir_local\n\n    # Initialize local dataset attributes\n    zarr_dir = dataset.zarr_dir\n    tmp_images = deepcopy(dataset.images)\n    current_type_filters = copy(job_type_filters)\n\n    ENRICH_IMAGES_WITH_STATUS: bool = (\n        IMAGE_STATUS_KEY in job_attribute_filters.keys()\n    )\n\n    for ind_wftask, wftask in enumerate(wf_task_list):\n        task = wftask.task\n        with next(get_sync_db()) as db:\n            task_group = db.get(TaskGroupV2, task.taskgroupv2_id)\n        alias_string = f\"'{wftask.alias}', \" if wftask.alias else \"\"\n        task_log_description = (\n            f\"{wftask.order}-th task ({alias_string}'{task.name}', \"\n            f\"{task_group.pkg_name} {task_group.version})\"\n        )\n        logger.debug(f\"SUBMIT {task_log_description}\")\n\n        # PRE TASK EXECUTION\n\n        # Filter images by types and attributes (in two steps)\n        if wftask.task_type in [\n            TaskType.COMPOUND,\n            TaskType.PARALLEL,\n            TaskType.NON_PARALLEL,\n        ]:\n            # Non-converter task\n            type_filters = copy(current_type_filters)\n            type_filters_patch = merge_type_filters(\n                task_input_types=task.input_types,\n                wftask_type_filters=wftask.type_filters,\n            )\n            type_filters.update(type_filters_patch)\n\n            if ind_wftask == 0 and ENRICH_IMAGES_WITH_STATUS:\n                # FIXME: Could this be done on `type_filtered_images`?\n                tmp_images = enrich_images_unsorted_sync(\n                    images=tmp_images,\n                    dataset_id=dataset.id,\n                    workflowtask_id=wftask.id,\n                )\n            type_filtered_images = filter_image_list(\n                images=tmp_images,\n                type_filters=type_filters,\n            )\n            num_available_images = len(type_filtered_images)\n\n            filtered_images = filter_image_list(\n                images=type_filtered_images,\n                attribute_filters=job_attribute_filters,\n            )\n        else:\n            # Converter task\n            filtered_images = []\n            num_available_images = 0\n\n        with next(get_sync_db()) as db:\n            # Create dumps for workflowtask and taskgroup\n            workflowtask_dump = dict(\n                **wftask.model_dump(exclude={\"task\"}),\n                task=TaskDump(**wftask.task.model_dump()).model_dump(),\n            )\n            task_group = db.get(TaskGroupV2, wftask.task.taskgroupv2_id)\n            task_group_dump = TaskGroupDump(\n                **task_group.model_dump()\n            ).model_dump()\n            # Create HistoryRun\n            history_run = HistoryRun(\n                dataset_id=dataset.id,\n                workflowtask_id=wftask.id,\n                job_id=job_id,\n                task_id=wftask.task.id,\n                workflowtask_dump=workflowtask_dump,\n                task_group_dump=task_group_dump,\n                num_available_images=num_available_images,\n                status=HistoryUnitStatus.SUBMITTED,\n            )\n            db.add(history_run)\n            db.commit()\n            db.refresh(history_run)\n            history_run_id = history_run.id\n\n            # Refresh `job.executor_error_log`, to avoid a spurious value left\n            # over from a previous task\n            job_db = db.get(JobV2, job_id)\n            job_db.executor_error_log = None\n            db.merge(job_db)\n            db.commit()\n            db.expunge_all()\n\n        # Fail when running a non-converter task on an empty image list\n        if (\n            wftask.task_type\n            in [\n                TaskType.COMPOUND,\n                TaskType.PARALLEL,\n                TaskType.NON_PARALLEL,\n            ]\n            and len(filtered_images) == 0\n        ):\n            error_msg = (\n                f\"Cannot run task '{task.name}' for an empty image list \"\n                f\"(obtained after applying {type_filters=} and \"\n                f\"attribute_filters={job_attribute_filters}).\"\n            )\n            logger.info(error_msg)\n            with next(get_sync_db()) as db:\n                update_status_of_history_run(\n                    history_run_id=history_run_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n            raise JobExecutionError(error_msg)\n\n        # Fail if the resource is not open for new submissions\n        with next(get_sync_db()) as db:\n            resource = db.get(Resource, resource_id)\n            if resource.prevent_new_submissions:\n                error_msg = (\n                    f\"Cannot run '{task.name}', since the '{resource.name}' \"\n                    \"resource is not currently active.\"\n                )\n                logger.info(error_msg)\n                update_status_of_history_run(\n                    history_run_id=history_run_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n                raise JobExecutionError(error_msg)\n\n        # TASK EXECUTION\n        try:\n            if task.type in [\n                TaskType.NON_PARALLEL,\n                TaskType.CONVERTER_NON_PARALLEL,\n            ]:\n                outcomes_dict, num_tasks = run_task_non_parallel(\n                    images=filtered_images,\n                    zarr_dir=zarr_dir,\n                    wftask=wftask,\n                    task=task,\n                    workflow_dir_local=workflow_dir_local,\n                    workflow_dir_remote=workflow_dir_remote,\n                    runner=runner,\n                    get_runner_config=get_runner_config,\n                    history_run_id=history_run_id,\n                    dataset_id=dataset.id,\n                    task_type=task.type,\n                    user_id=user_id,\n                )\n            elif task.type == TaskType.PARALLEL:\n                outcomes_dict, num_tasks = run_task_parallel(\n                    images=filtered_images,\n                    wftask=wftask,\n                    task=task,\n                    workflow_dir_local=workflow_dir_local,\n                    workflow_dir_remote=workflow_dir_remote,\n                    runner=runner,\n                    get_runner_config=get_runner_config,\n                    history_run_id=history_run_id,\n                    dataset_id=dataset.id,\n                    user_id=user_id,\n                )\n            elif task.type in [\n                TaskType.COMPOUND,\n                TaskType.CONVERTER_COMPOUND,\n            ]:\n                outcomes_dict, num_tasks = run_task_compound(\n                    images=filtered_images,\n                    zarr_dir=zarr_dir,\n                    wftask=wftask,\n                    task=task,\n                    workflow_dir_local=workflow_dir_local,\n                    workflow_dir_remote=workflow_dir_remote,\n                    runner=runner,\n                    get_runner_config=get_runner_config,\n                    history_run_id=history_run_id,\n                    dataset_id=dataset.id,\n                    task_type=task.type,\n                    user_id=user_id,\n                )\n            else:\n                raise ValueError(f\"Unexpected error: Invalid {task.type=}.\")\n        except Exception as e:\n            outcomes_dict = {\n                0: SubmissionOutcome(\n                    task_output=None,\n                    exception=e,\n                )\n            }\n            num_tasks = 0\n\n        # POST TASK EXECUTION\n        try:\n            non_failed_task_outputs = [\n                value.task_output\n                for value in outcomes_dict.values()\n                if value.task_output is not None\n            ]\n            if len(non_failed_task_outputs) &gt; 0:\n                current_task_output = merge_outputs(non_failed_task_outputs)\n                # If `current_task_output` includes no images (to be created or\n                # removed), then flag all the input images as modified.\n                # See fractal-server issues #1374 and #2409.\n                if (\n                    current_task_output.image_list_updates == []\n                    and current_task_output.image_list_removals == []\n                ):\n                    current_task_output = TaskOutput(\n                        image_list_updates=[\n                            dict(zarr_url=img[\"zarr_url\"])\n                            for img in filtered_images\n                        ],\n                    )\n            else:\n                current_task_output = TaskOutput()\n\n            # Update image list\n            num_new_images = 0\n            current_task_output.check_zarr_urls_are_unique()\n            # NOTE: In principle we could make the task-output processing more\n            # granular, and also associate output-processing failures to\n            # history status.\n            for image_obj in current_task_output.image_list_updates:\n                image = image_obj.model_dump()\n                if image[\"zarr_url\"] in [img[\"zarr_url\"] for img in tmp_images]:\n                    img_search = find_image_by_zarr_url(\n                        images=tmp_images,\n                        zarr_url=image[\"zarr_url\"],\n                    )\n                    if img_search is None:\n                        raise ValueError(\n                            \"Unexpected error: \"\n                            f\"Image with zarr_url {image['zarr_url']} not \"\n                            \"found, while updating image list.\"\n                        )\n                    existing_image_index = img_search[\"index\"]\n\n                    if (\n                        image[\"origin\"] is None\n                        or image[\"origin\"] == image[\"zarr_url\"]\n                    ):\n                        # CASE 1: Edit existing image\n                        existing_image = img_search[\"image\"]\n                        new_attributes = copy(existing_image[\"attributes\"])\n                        new_types = copy(existing_image[\"types\"])\n                        new_image = dict(\n                            zarr_url=image[\"zarr_url\"],\n                        )\n                        if \"origin\" in existing_image.keys():\n                            new_image[\"origin\"] = existing_image[\"origin\"]\n                    else:\n                        # CASE 2: Re-create existing image based on `origin`\n                        # Propagate attributes and types from `origin` (if any)\n                        (\n                            new_attributes,\n                            new_types,\n                        ) = get_origin_attribute_and_types(\n                            origin_url=image[\"origin\"],\n                            images=tmp_images,\n                        )\n                        new_image = dict(\n                            zarr_url=image[\"zarr_url\"],\n                            origin=image[\"origin\"],\n                        )\n                    # Update attributes\n                    new_attributes.update(image[\"attributes\"])\n                    new_attributes = drop_none_attributes(new_attributes)\n                    new_image[\"attributes\"] = new_attributes\n                    # Update types\n                    new_types.update(image[\"types\"])\n                    new_types.update(task.output_types)\n                    new_image[\"types\"] = new_types\n                    # Validate new image\n                    SingleImage(**new_image)\n                    # Update image in the dataset image list\n                    tmp_images[existing_image_index] = new_image\n\n                else:\n                    # CASE 3: Add new image\n                    # Check that image['zarr_url'] is a subfolder of zarr_dir\n                    if (\n                        not image[\"zarr_url\"].startswith(zarr_dir)\n                        or image[\"zarr_url\"] == zarr_dir\n                    ):\n                        raise JobExecutionError(\n                            \"Cannot create image if zarr_url is not a \"\n                            \"subfolder of zarr_dir.\\n\"\n                            f\"zarr_dir: {zarr_dir}\\n\"\n                            f\"zarr_url: {image['zarr_url']}\"\n                        )\n\n                    # Propagate attributes and types from `origin` (if any)\n                    new_attributes, new_types = get_origin_attribute_and_types(\n                        origin_url=image[\"origin\"],\n                        images=tmp_images,\n                    )\n                    # Prepare new image\n                    new_attributes.update(image[\"attributes\"])\n                    new_attributes = drop_none_attributes(new_attributes)\n                    new_types.update(image[\"types\"])\n                    new_types.update(task.output_types)\n                    new_image = dict(\n                        zarr_url=image[\"zarr_url\"],\n                        origin=image[\"origin\"],\n                        attributes=new_attributes,\n                        types=new_types,\n                    )\n                    # Validate new image\n                    SingleImage(**new_image)\n                    # Add image into the dataset image list\n                    tmp_images.append(new_image)\n                    num_new_images += 1\n\n            # Remove images from tmp_images\n            for img_zarr_url in current_task_output.image_list_removals:\n                img_search = find_image_by_zarr_url(\n                    images=tmp_images, zarr_url=img_zarr_url\n                )\n                if img_search is None:\n                    raise JobExecutionError(\n                        \"Cannot remove missing image \"\n                        f\"(zarr_url={img_zarr_url}).\"\n                    )\n                else:\n                    tmp_images.pop(img_search[\"index\"])\n\n            # Update type_filters based on task-manifest output_types\n            type_filters_from_task_manifest = task.output_types\n            current_type_filters.update(type_filters_from_task_manifest)\n        except Exception as e:\n            logger.error(\n                \"Unexpected error in post-task-execution block. \"\n                f\"Original error: {str(e)}\"\n            )\n            with next(get_sync_db()) as db:\n                db.execute(\n                    update(HistoryUnit)\n                    .where(HistoryUnit.history_run_id == history_run_id)\n                    .values(status=HistoryUnitStatus.FAILED)\n                )\n                db.commit()\n            raise e\n\n        with next(get_sync_db()) as db:\n            # Write current dataset images into the database.\n            db_dataset = db.get(DatasetV2, dataset.id)\n            if ENRICH_IMAGES_WITH_STATUS:\n                db_dataset.images = _remove_status_from_attributes(tmp_images)\n            else:\n                db_dataset.images = tmp_images\n            flag_modified(db_dataset, \"images\")\n            db.merge(db_dataset)\n\n            db.execute(\n                delete(HistoryImageCache)\n                .where(HistoryImageCache.dataset_id == dataset.id)\n                .where(HistoryImageCache.workflowtask_id == wftask.id)\n                .where(\n                    HistoryImageCache.zarr_url.in_(\n                        current_task_output.image_list_removals\n                    )\n                )\n            )\n\n            db.commit()\n\n            # Store the SLURM error in the job database\n            job_db = db.get(JobV2, job_id)\n            job_db.executor_error_log = runner.executor_error_log\n            db.merge(job_db)\n            db.commit()\n\n            # Create accounting record\n            record = AccountingRecord(\n                user_id=user_id,\n                num_tasks=num_tasks,\n                num_new_images=num_new_images,\n            )\n            db.add(record)\n            db.commit()\n\n            # Update `HistoryRun` entry, and raise an error if task failed\n            try:\n                first_exception = next(\n                    value.exception\n                    for value in outcomes_dict.values()\n                    if value.exception is not None\n                )\n                # An exception was found\n                update_status_of_history_run(\n                    history_run_id=history_run_id,\n                    status=HistoryUnitStatus.FAILED,\n                    db_sync=db,\n                )\n                logger.warning(f\"END    {task_log_description} - ERROR.\")\n                # Raise first error\n                raise JobExecutionError(\n                    info=(\n                        f\"An error occurred.\\n\"\n                        f\"Original error:\\n{first_exception}\"\n                    )\n                )\n            except StopIteration:\n                # No exception was found\n                update_status_of_history_run(\n                    history_run_id=history_run_id,\n                    status=HistoryUnitStatus.DONE,\n                    db_sync=db,\n                )\n                db.commit()\n                logger.debug(f\"END    {task_log_description}\")\n</code></pre>"},{"location":"reference/runner/v2/runner/#fractal_server.runner.v2.runner.get_origin_attribute_and_types","title":"<code>get_origin_attribute_and_types(*, origin_url, images)</code>","text":"<p>Search for origin image and extract its attributes/types.</p> Source code in <code>fractal_server/runner/v2/runner.py</code> <pre><code>def get_origin_attribute_and_types(\n    *,\n    origin_url: str,\n    images: list[dict[str, Any]],\n) -&gt; tuple[dict[str, Any], dict[str, bool]]:\n    \"\"\"\n    Search for origin image and extract its attributes/types.\n    \"\"\"\n    origin_img_search = find_image_by_zarr_url(\n        images=images,\n        zarr_url=origin_url,\n    )\n    if origin_img_search is None:\n        updated_attributes = {}\n        updated_types = {}\n    else:\n        origin_image = origin_img_search[\"image\"]\n        updated_attributes = copy(origin_image[\"attributes\"])\n        updated_types = copy(origin_image[\"types\"])\n    return updated_attributes, updated_types\n</code></pre>"},{"location":"reference/runner/v2/runner_functions/","title":"runner_functions","text":""},{"location":"reference/runner/v2/runner_functions/#fractal_server.runner.v2.runner_functions.run_task_non_parallel","title":"<code>run_task_non_parallel(*, images, zarr_dir, task, wftask, workflow_dir_local, workflow_dir_remote, runner, get_runner_config, dataset_id, history_run_id, task_type, user_id)</code>","text":"<p>This runs server-side (see <code>executor</code> argument)</p> Source code in <code>fractal_server/runner/v2/runner_functions.py</code> <pre><code>def run_task_non_parallel(\n    *,\n    images: list[dict[str, Any]],\n    zarr_dir: str,\n    task: TaskV2,\n    wftask: WorkflowTaskV2,\n    workflow_dir_local: Path,\n    workflow_dir_remote: Path,\n    runner: BaseRunner,\n    get_runner_config: GetRunnerConfigType,\n    dataset_id: int,\n    history_run_id: int,\n    task_type: Literal[TaskType.NON_PARALLEL, TaskType.CONVERTER_NON_PARALLEL],\n    user_id: int,\n) -&gt; tuple[dict[int, SubmissionOutcome], int]:\n    \"\"\"\n    This runs server-side (see `executor` argument)\n    \"\"\"\n\n    if task_type not in [\n        TaskType.NON_PARALLEL,\n        TaskType.CONVERTER_NON_PARALLEL,\n    ]:\n        raise ValueError(f\"Invalid {task_type=} for `run_task_non_parallel`.\")\n\n    # Get TaskFiles object\n    task_files = TaskFiles(\n        root_dir_local=workflow_dir_local,\n        root_dir_remote=workflow_dir_remote,\n        task_order=wftask.order,\n        task_name=wftask.task.name,\n        component=\"\",\n        prefix=SUBMIT_PREFIX,\n    )\n\n    runner_config = get_runner_config(\n        shared_config=runner.shared_config,\n        wftask=wftask,\n        which_type=\"non_parallel\",\n        tot_tasks=1,\n    )\n\n    function_kwargs = {\n        \"zarr_dir\": zarr_dir,\n        **(wftask.args_non_parallel or {}),\n    }\n    if task_type == TaskType.NON_PARALLEL:\n        function_kwargs[\"zarr_urls\"] = [img[\"zarr_url\"] for img in images]\n\n    # Database History operations\n    with next(get_sync_db()) as db:\n        if task_type == TaskType.NON_PARALLEL:\n            zarr_urls = function_kwargs[\"zarr_urls\"]\n        elif task_type == TaskType.CONVERTER_NON_PARALLEL:\n            zarr_urls = []\n\n        history_unit = HistoryUnit(\n            history_run_id=history_run_id,\n            status=HistoryUnitStatus.SUBMITTED,\n            logfile=task_files.log_file_local,\n            zarr_urls=zarr_urls,\n        )\n        db.add(history_unit)\n        db.commit()\n        db.refresh(history_unit)\n        logger.debug(\n            \"[run_task_non_parallel] Created `HistoryUnit` with \"\n            f\"{history_run_id=}.\"\n        )\n        history_unit_id = history_unit.id\n        bulk_upsert_image_cache_fast(\n            db=db,\n            list_upsert_objects=[\n                dict(\n                    workflowtask_id=wftask.id,\n                    dataset_id=dataset_id,\n                    zarr_url=zarr_url,\n                    latest_history_unit_id=history_unit_id,\n                )\n                for zarr_url in history_unit.zarr_urls\n            ],\n        )\n\n    result, exception = runner.submit(\n        base_command=task.command_non_parallel,\n        workflow_task_order=wftask.order,\n        workflow_task_id=wftask.task_id,\n        task_name=wftask.task.name,\n        parameters=function_kwargs,\n        task_type=task_type,\n        task_files=task_files,\n        history_unit_id=history_unit_id,\n        config=runner_config,\n        user_id=user_id,\n    )\n\n    positional_index = 0\n    num_tasks = 1\n\n    outcome = {\n        positional_index: _process_task_output(\n            result=result,\n            exception=exception,\n        )\n    }\n    # NOTE: Here we don't have to handle the\n    # `outcome[0].exception is not None` branch, since for non_parallel\n    # tasks it was already handled within submit\n    if outcome[0].invalid_output:\n        with next(get_sync_db()) as db:\n            update_status_of_history_unit(\n                history_unit_id=history_unit_id,\n                status=HistoryUnitStatus.FAILED,\n                db_sync=db,\n            )\n    return outcome, num_tasks\n</code></pre>"},{"location":"reference/runner/v2/submit_workflow/","title":"submit_workflow","text":"<p>Runner backend subsystem root V2</p> <p>This module is the single entry point to the runner backend subsystem V2. Other subsystems should only import this module and not its submodules or the individual backends.</p>"},{"location":"reference/runner/v2/submit_workflow/#fractal_server.runner.v2.submit_workflow.submit_workflow","title":"<code>submit_workflow(*, workflow_id, dataset_id, job_id, user_id, user_cache_dir, resource, profile, worker_init=None, fractal_ssh=None)</code>","text":"<p>Prepares a workflow and applies it to a dataset</p> <p>This function wraps the process_workflow one, which is different for each backend (e.g. local or slurm backend).</p> PARAMETER DESCRIPTION <code>workflow_id</code> <p>ID of the workflow being applied</p> <p> TYPE: <code>int</code> </p> <code>dataset_id</code> <p>Dataset ID</p> <p> TYPE: <code>int</code> </p> <code>job_id</code> <p>Id of the job record which stores the state for the current workflow application.</p> <p> TYPE: <code>int</code> </p> <code>user_id</code> <p>User ID.</p> <p> TYPE: <code>int</code> </p> <code>worker_init</code> <p>Custom executor parameters that get parsed before the execution of each task.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>user_cache_dir</code> <p>Cache directory (namely a path where the user can write). For <code>slurm_sudo</code> backend, this is both a base directory for <code>job.working_dir_user</code>. For <code>slurm_sudo</code> and <code>slurm_ssh</code> backends, this is used for <code>user_local_exports</code>.</p> <p> TYPE: <code>str</code> </p> <code>resource</code> <p>Computational resource to be used for this job (e.g. a SLURM cluster).</p> <p> TYPE: <code>Resource</code> </p> <code>profile</code> <p>Computational profile to be used for this job.</p> <p> TYPE: <code>Profile</code> </p> <code>fractal_ssh</code> <p>SSH object, for when <code>resource.type = \"slurm_ssh\"</code>.</p> <p> TYPE: <code>FractalSSH | None</code> DEFAULT: <code>None</code> </p> Source code in <code>fractal_server/runner/v2/submit_workflow.py</code> <pre><code>def submit_workflow(\n    *,\n    workflow_id: int,\n    dataset_id: int,\n    job_id: int,\n    user_id: int,\n    user_cache_dir: str,\n    resource: Resource,\n    profile: Profile,\n    worker_init: str | None = None,\n    fractal_ssh: FractalSSH | None = None,\n) -&gt; None:\n    \"\"\"\n    Prepares a workflow and applies it to a dataset\n\n    This function wraps the process_workflow one, which is different for each\n    backend (e.g. local or slurm backend).\n\n    Args:\n        workflow_id:\n            ID of the workflow being applied\n        dataset_id:\n            Dataset ID\n        job_id:\n            Id of the job record which stores the state for the current\n            workflow application.\n        user_id:\n            User ID.\n        worker_init:\n            Custom executor parameters that get parsed before the execution of\n            each task.\n        user_cache_dir:\n            Cache directory (namely a path where the user can write). For\n            `slurm_sudo` backend, this is both a base directory for\n            `job.working_dir_user`. For `slurm_sudo` and `slurm_ssh` backends,\n            this is used for `user_local_exports`.\n        resource:\n            Computational resource to be used for this job (e.g. a SLURM\n            cluster).\n        profile:\n            Computational profile to be used for this job.\n        fractal_ssh: SSH object, for when `resource.type = \"slurm_ssh\"`.\n    \"\"\"\n    # Declare runner backend and set `process_workflow` function\n    logger_name = f\"WF{workflow_id}_job{job_id}\"\n    logger = set_logger(logger_name=logger_name)\n\n    with next(DB.get_sync_db()) as db_sync:\n        try:\n            job: JobV2 | None = db_sync.get(JobV2, job_id)\n            dataset: DatasetV2 | None = db_sync.get(DatasetV2, dataset_id)\n            workflow: WorkflowV2 | None = db_sync.get(WorkflowV2, workflow_id)\n        except Exception as e:\n            logger.error(\n                f\"Error connecting to the database. Original error: {str(e)}\"\n            )\n            reset_logger_handlers(logger)\n            return\n\n        if job is None:\n            logger.error(f\"JobV2 {job_id} does not exist\")\n            reset_logger_handlers(logger)\n            return\n        if dataset is None or workflow is None:\n            log_msg = \"\"\n            if dataset is None:\n                current_log_msg = (\n                    f\"Cannot fetch dataset {dataset_id} from database \"\n                    f\"(as part of job {job_id}).\"\n                )\n                logger.error(current_log_msg)\n                log_msg += f\"{current_log_msg}\\n\"\n            if workflow is None:\n                current_log_msg += (\n                    f\"Cannot fetch workflow {workflow_id} from database \"\n                    f\"(as part of job {job_id}).\"\n                )\n                logger.error(current_log_msg)\n                log_msg += f\"{current_log_msg}\\n\"\n\n            fail_job(\n                db=db_sync,\n                job=job,\n                log_msg=log_msg,\n                logger_name=logger_name,\n                emit_log=False,\n            )\n            return\n\n        try:\n            # Define local/remote folders, and create local folder\n            local_job_dir = Path(job.working_dir)\n            remote_job_dir = Path(job.working_dir_user)\n            match resource.type:\n                case ResourceType.LOCAL:\n                    local_job_dir.mkdir(parents=True, exist_ok=False)\n                case ResourceType.SLURM_SUDO:\n                    original_umask = os.umask(0)\n                    local_job_dir.mkdir(\n                        parents=True, mode=0o755, exist_ok=False\n                    )\n                    os.umask(original_umask)\n                case ResourceType.SLURM_SSH:\n                    local_job_dir.mkdir(parents=True, exist_ok=False)\n\n        except Exception as e:\n            error_type = type(e).__name__\n            fail_job(\n                db=db_sync,\n                job=job,\n                log_msg=(\n                    f\"{error_type} error while creating local job folder.\"\n                    f\" Original error: {str(e)}\"\n                ),\n                logger_name=logger_name,\n                emit_log=True,\n            )\n            return\n\n        # After Session.commit() is called, either explicitly or when using a\n        # context manager, all objects associated with the Session are expired.\n        # https://docs.sqlalchemy.org/en/14/orm/\n        #   session_basics.html#opening-and-closing-a-session\n        # https://docs.sqlalchemy.org/en/14/orm/\n        #   session_state_management.html#refreshing-expiring\n\n        # See issue #928:\n        # https://github.com/fractal-analytics-platform/\n        #   fractal-server/issues/928\n\n        db_sync.refresh(dataset)\n        db_sync.refresh(workflow)\n        for wftask in workflow.task_list:\n            db_sync.refresh(wftask)\n\n        # Write logs\n        log_file_path = local_job_dir / WORKFLOW_LOG_FILENAME\n        logger = set_logger(\n            logger_name=logger_name,\n            log_file_path=log_file_path,\n        )\n        logger.info(\n            f'Start execution of workflow \"{workflow.name}\"; '\n            f\"more logs at {str(log_file_path)}\"\n        )\n        logger.debug(f\"Resource name: {resource.name}\")\n        logger.debug(f\"Profile name: {profile.name}\")\n        logger.debug(f\"Username: {profile.username}\")\n        if resource.type in [ResourceType.SLURM_SUDO, ResourceType.SLURM_SSH]:\n            logger.debug(f\"slurm_account: {job.slurm_account}\")\n            logger.debug(f\"worker_init: {worker_init}\")\n        logger.debug(f\"job.id: {job.id}\")\n        logger.debug(f\"job.working_dir: {job.working_dir}\")\n        logger.debug(f\"job.working_dir_user: {job.working_dir_user}\")\n        logger.debug(f\"job.first_task_index: {job.first_task_index}\")\n        logger.debug(f\"job.last_task_index: {job.last_task_index}\")\n        logger.debug(f'START workflow \"{workflow.name}\"')\n        job_working_dir = job.working_dir\n\n    try:\n        process_workflow: ProcessWorkflowType\n        match resource.type:\n            case ResourceType.LOCAL:\n                process_workflow = local_process_workflow\n                backend_specific_kwargs = {}\n            case ResourceType.SLURM_SUDO:\n                process_workflow = slurm_sudo_process_workflow\n                backend_specific_kwargs = dict(\n                    slurm_account=job.slurm_account,\n                )\n            case ResourceType.SLURM_SSH:\n                process_workflow = slurm_ssh_process_workflow\n                backend_specific_kwargs = dict(\n                    fractal_ssh=fractal_ssh,\n                    slurm_account=job.slurm_account,\n                )\n\n        process_workflow(\n            workflow=workflow,\n            dataset=dataset,\n            job_id=job_id,\n            user_id=user_id,\n            workflow_dir_local=local_job_dir,\n            workflow_dir_remote=remote_job_dir,\n            logger_name=logger_name,\n            worker_init=worker_init,\n            first_task_index=job.first_task_index,\n            last_task_index=job.last_task_index,\n            job_attribute_filters=job.attribute_filters,\n            job_type_filters=job.type_filters,\n            resource=resource,\n            profile=profile,\n            user_cache_dir=user_cache_dir,\n            **backend_specific_kwargs,\n        )\n\n        logger.info(\n            f'End execution of workflow \"{workflow.name}\"; '\n            f\"more logs at {str(log_file_path)}\"\n        )\n        logger.debug(f'END workflow \"{workflow.name}\"')\n\n        # Update job DB entry\n        with next(DB.get_sync_db()) as db_sync:\n            job = db_sync.get(JobV2, job_id)\n            job.status = JobStatusType.DONE\n            job.end_timestamp = get_timestamp()\n            with log_file_path.open(\"r\") as f:\n                logs = f.read()\n            job.log = logs\n            db_sync.merge(job)\n            db_sync.commit()\n\n    except JobExecutionError as e:\n        logger.debug(f'FAILED workflow \"{workflow.name}\", JobExecutionError.')\n        logger.info(f'Workflow \"{workflow.name}\" failed (JobExecutionError).')\n        with next(DB.get_sync_db()) as db_sync:\n            job = db_sync.get(JobV2, job_id)\n            fail_job(\n                db=db_sync,\n                job=job,\n                log_msg=(\n                    f\"JOB ERROR in Fractal job {job.id}:\\n\"\n                    f\"TRACEBACK:\\n{e.assemble_error()}\"\n                ),\n                logger_name=logger_name,\n            )\n\n    except Exception:\n        logger.debug(f'FAILED workflow \"{workflow.name}\", unknown error.')\n        logger.info(f'Workflow \"{workflow.name}\" failed (unkwnon error).')\n\n        current_traceback = traceback.format_exc()\n        with next(DB.get_sync_db()) as db_sync:\n            job = db_sync.get(JobV2, job_id)\n            fail_job(\n                db=db_sync,\n                job=job,\n                log_msg=(\n                    f\"UNKNOWN ERROR in Fractal job {job.id}\\n\"\n                    f\"TRACEBACK:\\n{current_traceback}\"\n                ),\n                logger_name=logger_name,\n            )\n\n    finally:\n        reset_logger_handlers(logger)\n        _zip_folder_to_file_and_remove(folder=job_working_dir)\n</code></pre>"},{"location":"reference/runner/v2/task_interface/","title":"task_interface","text":""},{"location":"reference/runner/v2/task_interface/#fractal_server.runner.v2.task_interface.TaskOutput","title":"<code>TaskOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for task output.</p> ATTRIBUTE DESCRIPTION <code>image_list_updates</code> <p>List of image-list elements to be created or updated.</p> <p> TYPE: <code>list[SingleImageTaskOutput]</code> </p> <code>image_list_removals</code> <p>List of Zarr URLs to be removed from the dataset image list.</p> <p> TYPE: <code>list[ZarrUrlStr]</code> </p> Source code in <code>fractal_server/runner/v2/task_interface.py</code> <pre><code>class TaskOutput(BaseModel):\n    \"\"\"\n    Model for task output.\n\n    Attributes:\n        image_list_updates:\n            List of image-list elements to be created or updated.\n        image_list_removals:\n            List of Zarr URLs to be removed from the dataset image list.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    image_list_updates: list[SingleImageTaskOutput] = Field(\n        default_factory=list\n    )\n    image_list_removals: list[ZarrUrlStr] = Field(default_factory=list)\n\n    def check_zarr_urls_are_unique(self) -&gt; None:\n        zarr_urls = [img.zarr_url for img in self.image_list_updates]\n        zarr_urls.extend(self.image_list_removals)\n        if len(zarr_urls) != len(set(zarr_urls)):\n            duplicates = [\n                zarr_url\n                for zarr_url in set(zarr_urls)\n                if zarr_urls.count(zarr_url) &gt; 1\n            ]\n            msg = (\n                \"TaskOutput \"\n                f\"({len(self.image_list_updates)} image_list_updates and \"\n                f\"{len(self.image_list_removals)} image_list_removals) \"\n                \"has non-unique zarr_urls:\"\n            )\n            for duplicate in duplicates:\n                msg = f\"{msg}\\n{duplicate}\"\n            raise ValueError(msg)\n</code></pre>"},{"location":"reference/ssh/","title":"ssh","text":"<p>The <code>fractal_server.ssh</code> subpackage is meant as a layer in front of some SSH library (e.g. <code>fabric</code> or <code>asyncssh</code>).</p>"},{"location":"reference/ssh/_fabric/","title":"_fabric","text":""},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH","title":"<code>FractalSSH</code>","text":"<p>Wrapper of <code>fabric.Connection</code> object, enriched with locks.</p> <p>Note: methods marked as <code>_unsafe</code> should not be used directly, since they do not enforce locking.</p> ATTRIBUTE DESCRIPTION <code>_lock</code> <p> TYPE: <code>Lock</code> </p> <code>_connection</code> <p> TYPE: <code>Connection</code> </p> <code>default_lock_timeout</code> <p> TYPE: <code>float</code> </p> <code>sftp_get_prefetch</code> <p> TYPE: <code>bool</code> </p> <code>sftp_get_max_requests</code> <p> TYPE: <code>int</code> </p> <code>logger_name</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>class FractalSSH:\n    \"\"\"\n    Wrapper of `fabric.Connection` object, enriched with locks.\n\n    Note: methods marked as `_unsafe` should not be used directly,\n    since they do not enforce locking.\n\n    Attributes:\n        _lock:\n        _connection:\n        default_lock_timeout:\n        sftp_get_prefetch:\n        sftp_get_max_requests:\n        logger_name:\n    \"\"\"\n\n    _lock: Lock\n    _connection: Connection\n    default_lock_timeout: float\n    sftp_get_prefetch: bool\n    sftp_get_max_requests: int\n    logger_name: str\n    _pid: int\n\n    def __init__(\n        self,\n        connection: Connection,\n        default_timeout: float = 500.0,\n        sftp_get_prefetch: bool = False,\n        sftp_get_max_requests: int = 64,\n        logger_name: str = __name__,\n    ):\n        self._lock = Lock()\n        self._connection = connection\n        self.default_lock_timeout = default_timeout\n        self.sftp_get_prefetch = sftp_get_prefetch\n        self.sftp_get_max_requests = sftp_get_max_requests\n        self.logger_name = logger_name\n        set_logger(self.logger_name)\n        set_logger(SSH_MONITORING_LOGGER_NAME)\n        self._pid = os.getpid()\n\n    @property\n    def is_connected(self) -&gt; bool:\n        return self._connection.is_connected\n\n    @property\n    def logger(self) -&gt; logging.Logger:\n        return get_logger(self.logger_name)\n\n    def log_and_raise(self, *, e: Exception, message: str) -&gt; None:\n        \"\"\"\n        Log and re-raise an exception from a FractalSSH method.\n\n        Args:\n            message: Additional message to be logged.\n            e: Original exception\n        \"\"\"\n        try:\n            self.logger.error(message)\n            self.logger.error(f\"Original Error {type(e)} : \\n{str(e)}\")\n            # Handle the specific case of `NoValidConnectionsError`s from\n            # paramiko, which store relevant information in the `errors`\n            # attribute\n            if hasattr(e, \"errors\"):\n                self.logger.error(f\"{type(e)=}\")\n                for err in e.errors:\n                    self.logger.error(f\"{err}\")\n        except Exception as exception:\n            # Handle unexpected cases, e.g. (1) `e` has no `type`, or\n            # (2) `errors` is not iterable.\n            self.logger.error(\n                \"Unexpected Error while handling exception above: \"\n                f\"{str(exception)}\"\n            )\n\n        raise e\n\n    def _run(\n        self,\n        *args,\n        label: str,\n        lock_timeout: float | None = None,\n        **kwargs,\n    ) -&gt; Any:\n        actual_lock_timeout = self.default_lock_timeout\n        if lock_timeout is not None:\n            actual_lock_timeout = lock_timeout\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=label,\n            timeout=actual_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            return self._connection.run(*args, **kwargs)\n\n    def _sftp_unsafe(self) -&gt; paramiko.sftp_client.SFTPClient:\n        \"\"\"\n        This is marked as unsafe because you should only use its methods\n        after acquiring a lock.\n        \"\"\"\n        return self._connection.sftp()\n\n    @retry_if_socket_error\n    def read_remote_json_file(self, filepath: str) -&gt; dict[str, Any]:\n        self.logger.info(f\"START reading remote JSON file {filepath}.\")\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            timeout=self.default_lock_timeout,\n            logger_name=self.logger_name,\n            pid=self._pid,\n            label=f\"read_remote_json_file({filepath})\",\n        ):\n            try:\n                with self._sftp_unsafe().open(filepath, \"r\") as f:\n                    data = json.load(f)\n            except Exception as e:\n                self.log_and_raise(\n                    e=e,\n                    message=(\n                        f\"Error in `read_remote_json_file`, for {filepath=}.\"\n                    ),\n                )\n        self.logger.info(f\"END reading remote JSON file {filepath}.\")\n        return data\n\n    @retry_if_socket_error\n    def read_remote_text_file(self, filepath: str) -&gt; dict[str, Any]:\n        \"\"\"\n        Read a remote text file into a string.\n\n        Note from paramiko docs:\n        &gt; The Python 'b' flag is ignored, since SSH treats all files as binary.\n        \"\"\"\n        self.logger.info(f\"START reading remote text file {filepath}.\")\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=f\"read_remote_text_file({filepath})\",\n            timeout=self.default_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            try:\n                with self._sftp_unsafe().open(filepath, \"r\") as f:\n                    data = f.read().decode()\n            except Exception as e:\n                self.log_and_raise(\n                    e=e,\n                    message=(\n                        f\"Error in `read_remote_text_file`, for {filepath=}.\"\n                    ),\n                )\n        self.logger.info(f\"END reading remote text file {filepath}.\")\n        return data\n\n    def check_connection(self) -&gt; None:\n        \"\"\"\n        Open the SSH connection and handle exceptions.\n\n        This method should always be called at the beginning of background\n        operations that use FractalSSH, so that:\n\n        1. We try to restore unusable connections (e.g. due to closed socket).\n        2. We provide an informative error if connection cannot be established.\n        \"\"\"\n        self.logger.debug(\n            f\"[check_connection] {self._connection.is_connected=}\"\n        )\n        if self._connection.is_connected:\n            # Even if the connection appears open, it could be broken for\n            # external reasons (e.g. the socket is closed because the SSH\n            # server was restarted). In these cases, we catch the error and\n            # try to re-open the connection.\n            # NOTE: Several specific errors inherit from `SSHException`,\n            # including errors related to authentication.\n            try:\n                self.logger.info(\n                    \"[check_connection] Run dummy command to check connection.\"\n                )\n                # Run both an SFTP and an SSH command, as they correspond to\n                # different sockets\n                self.remote_exists(\"/dummy/path/\")\n                self.run_command(cmd=\"whoami\")\n                self.logger.info(\n                    \"[check_connection] SSH connection is already OK, exit.\"\n                )\n                return\n            except (OSError, EOFError, SSHException) as e:\n                self.logger.warning(\n                    f\"[check_connection] Detected error {str(e)}, re-open.\"\n                )\n        # Try opening the connection (if it was closed) or to re-open it (if\n        # an error happened).\n        self.refresh_connection()\n\n    def refresh_connection(self) -&gt; None:\n        try:\n            self.close()\n            with _acquire_lock_with_timeout(\n                lock=self._lock,\n                label=\"FractalSSH._connection.{open,open_sftp}()\",\n                timeout=self.default_lock_timeout,\n                logger_name=self.logger_name,\n                pid=self._pid,\n            ):\n                self._connection.open()\n                self._connection.client.open_sftp()\n                self.logger.info(\n                    \"[check_connection] SSH connection opened, exit.\"\n                )\n\n        except Exception as e:\n            raise RuntimeError(\n                f\"Cannot open SSH connection. Original error:\\n{str(e)}\"\n            )\n\n    def close(self) -&gt; None:\n        \"\"\"\n        Aggressively close `self._connection`.\n\n        When `Connection.is_connected` is `False`, `Connection.close()` does\n        not call `Connection.client.close()`. Thus we do this explicitly here,\n        because we observed cases where `is_connected=False` but the underlying\n        `Transport` object was not closed.\n        \"\"\"\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=\"FractalSSH._connection.close()\",\n            timeout=self.default_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            self._connection.close()\n            if self._connection.client is not None:\n                self._connection.client.close()\n        close_logger(get_logger(self.logger_name))\n        close_logger(get_logger(SSH_MONITORING_LOGGER_NAME))\n\n    @retry_if_socket_error\n    def run_command(\n        self,\n        *,\n        cmd: str,\n        allow_char: str | None = None,\n        lock_timeout: int | None = None,\n    ) -&gt; str:\n        \"\"\"\n        Run a command within an open SSH connection.\n\n        Args:\n            cmd: Command to be run\n            allow_char: Forbidden chars to allow for this command\n            lock_timeout:\n\n        Returns:\n            Standard output of the command, if successful.\n        \"\"\"\n\n        validate_cmd(cmd, allow_char=allow_char)\n\n        actual_lock_timeout = self.default_lock_timeout\n        if lock_timeout is not None:\n            actual_lock_timeout = lock_timeout\n\n        t_0 = time.perf_counter()\n        try:\n            # Case 1: Command runs successfully\n            res = self._run(\n                cmd,\n                label=cmd,\n                lock_timeout=actual_lock_timeout,\n                hide=True,\n                in_stream=False,\n            )\n            t_1 = time.perf_counter()\n            self.logger.info(\n                f\"END   running '{cmd}' over SSH, elapsed={t_1 - t_0:.3f}\"\n            )\n            self.logger.debug(\"STDOUT:\")\n            self.logger.debug(res.stdout)\n            self.logger.debug(\"STDERR:\")\n            self.logger.debug(res.stderr)\n            return res.stdout\n        # Case 2: Command fails with a connection error\n        except NoValidConnectionsError as e:\n            raise NoValidConnectionsError(errors=e.errors)\n        except UnexpectedExit as e:\n            # Case 3: Command fails with an actual error\n            error_msg = (\n                f\"Running command `{cmd}` over SSH failed.\\n\"\n                f\"Original error:\\n{str(e)}.\"\n            )\n            self.logger.error(error_msg)\n            raise FractalSSHCommandError(error_msg)\n        except FractalSSHTimeoutError as e:\n            raise e\n        except Exception as e:\n            self.logger.error(\n                f\"Running command `{cmd}` over SSH failed.\\n\"\n                f\"Original Error:\\n{str(e)}.\"\n            )\n            raise FractalSSHUnknownError(f\"{type(e)}: {str(e)}\")\n\n    @retry_if_socket_error\n    def send_file(\n        self,\n        *,\n        local: str,\n        remote: str,\n        lock_timeout: float | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Transfer a file via SSH\n\n        Args:\n            local: Local path to file.\n            remote: Target path on remote host.\n            lock_timeout: Timeout for lock acquisition (overrides default).\n        \"\"\"\n        try:\n            self.logger.info(\n                f\"[send_file] START transfer of '{local}' over SSH.\"\n            )\n            actual_lock_timeout = self.default_lock_timeout\n            if lock_timeout is not None:\n                actual_lock_timeout = lock_timeout\n            with _acquire_lock_with_timeout(\n                lock=self._lock,\n                label=f\"send_file({local},{remote})\",\n                timeout=actual_lock_timeout,\n                pid=self._pid,\n                logger_name=self.logger_name,\n            ):\n                self._sftp_unsafe().put(local, remote)\n            self.logger.info(f\"[send_file] END transfer of '{local}' over SSH.\")\n        except Exception as e:\n            self.log_and_raise(\n                e=e,\n                message=(\n                    \"Error in `send_file`, while \"\n                    f\"transferring {local=} to {remote=}.\"\n                ),\n            )\n\n    @retry_if_socket_error\n    def fetch_file(\n        self,\n        *,\n        local: str,\n        remote: str,\n        lock_timeout: float | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Transfer a file via SSH\n\n        Args:\n            local: Local path to file.\n            remote: Target path on remote host.\n            lock_timeout: Timeout for lock acquisition (overrides default).\n        \"\"\"\n        try:\n            prefix = \"[fetch_file] \"\n            self.logger.info(f\"{prefix} START fetching '{remote}' over SSH.\")\n            actual_lock_timeout = self.default_lock_timeout\n            if lock_timeout is not None:\n                actual_lock_timeout = lock_timeout\n            with _acquire_lock_with_timeout(\n                lock=self._lock,\n                label=f\"fetch_file({local},{remote})\",\n                timeout=actual_lock_timeout,\n                pid=self._pid,\n                logger_name=self.logger_name,\n            ):\n                self._sftp_unsafe().get(\n                    remote,\n                    local,\n                    prefetch=self.sftp_get_prefetch,\n                    max_concurrent_prefetch_requests=self.sftp_get_max_requests,  # noqa E501\n                )\n            self.logger.info(f\"{prefix} END fetching '{remote}' over SSH.\")\n        except Exception as e:\n            self.log_and_raise(\n                e=e,\n                message=(\n                    \"Error in `fetch_file`, while \"\n                    f\"Transferring {remote=} to {local=}.\"\n                ),\n            )\n\n    def mkdir(self, *, folder: str, parents: bool = True) -&gt; None:\n        \"\"\"\n        Create a folder remotely via SSH.\n\n        Args:\n            folder:\n            parents:\n        \"\"\"\n        if parents:\n            cmd = f\"mkdir -p {folder}\"\n        else:\n            cmd = f\"mkdir {folder}\"\n        self.run_command(cmd=cmd)\n\n    def remove_folder(\n        self,\n        *,\n        folder: str,\n        safe_root: str,\n    ) -&gt; None:\n        \"\"\"\n        Removes a folder remotely via SSH.\n\n        This functions calls `rm -r`, after a few checks on `folder`.\n\n        Args:\n            folder: Absolute path to a folder that should be removed.\n            safe_root: If `folder` is not a subfolder of the absolute\n                `safe_root` path, raise an error.\n        \"\"\"\n        validate_cmd(folder)\n        validate_cmd(safe_root)\n\n        if \" \" in folder:\n            raise ValueError(f\"folder='{folder}' includes whitespace.\")\n        elif \" \" in safe_root:\n            raise ValueError(f\"safe_root='{safe_root}' includes whitespace.\")\n        elif not Path(folder).is_absolute():\n            raise ValueError(f\"{folder=} is not an absolute path.\")\n        elif not Path(safe_root).is_absolute():\n            raise ValueError(f\"{safe_root=} is not an absolute path.\")\n        elif not (\n            Path(folder).resolve().is_relative_to(Path(safe_root).resolve())\n        ):\n            raise ValueError(f\"{folder=} is not a subfolder of {safe_root=}.\")\n        else:\n            cmd = f\"rm -r {folder}\"\n            self.run_command(cmd=cmd)\n\n    @retry_if_socket_error\n    def write_remote_file(\n        self,\n        *,\n        path: str,\n        content: str,\n        lock_timeout: float | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Open a remote file via SFTP and write it.\n\n        Args:\n            path: Absolute path of remote file.\n            content: Contents to be written to file.\n            lock_timeout: Timeout for lock acquisition (overrides default).\n        \"\"\"\n        t_start = time.perf_counter()\n        self.logger.info(f\"[write_remote_file] START ({path}).\")\n        actual_lock_timeout = self.default_lock_timeout\n        if lock_timeout is not None:\n            actual_lock_timeout = lock_timeout\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=f\"write_remote_file({path})\",\n            timeout=actual_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            try:\n                with self._sftp_unsafe().open(filename=path, mode=\"w\") as f:\n                    f.write(content)\n            except Exception as e:\n                self.log_and_raise(\n                    e=e, message=f\"Error in `write_remote_file`, for {path=}.\"\n                )\n\n        elapsed = time.perf_counter() - t_start\n        self.logger.info(f\"[write_remote_file] END, {elapsed=} s ({path}).\")\n\n    @retry_if_socket_error\n    def remote_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Return whether a remote file/folder exists\n        \"\"\"\n        self.logger.info(f\"START remote_file_exists {path}\")\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=f\"remote_file_exists({path})\",\n            timeout=self.default_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            try:\n                self._sftp_unsafe().stat(path)\n                self.logger.info(f\"END   remote_file_exists {path} / True\")\n                return True\n            except FileNotFoundError:\n                self.logger.info(f\"END   remote_file_exists {path} / False\")\n                return False\n            except Exception as e:\n                self.log_and_raise(\n                    e=e, message=f\"Error in `remote_exists`, for {path=}.\"\n                )\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH._sftp_unsafe","title":"<code>_sftp_unsafe()</code>","text":"<p>This is marked as unsafe because you should only use its methods after acquiring a lock.</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def _sftp_unsafe(self) -&gt; paramiko.sftp_client.SFTPClient:\n    \"\"\"\n    This is marked as unsafe because you should only use its methods\n    after acquiring a lock.\n    \"\"\"\n    return self._connection.sftp()\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.check_connection","title":"<code>check_connection()</code>","text":"<p>Open the SSH connection and handle exceptions.</p> <p>This method should always be called at the beginning of background operations that use FractalSSH, so that:</p> <ol> <li>We try to restore unusable connections (e.g. due to closed socket).</li> <li>We provide an informative error if connection cannot be established.</li> </ol> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def check_connection(self) -&gt; None:\n    \"\"\"\n    Open the SSH connection and handle exceptions.\n\n    This method should always be called at the beginning of background\n    operations that use FractalSSH, so that:\n\n    1. We try to restore unusable connections (e.g. due to closed socket).\n    2. We provide an informative error if connection cannot be established.\n    \"\"\"\n    self.logger.debug(\n        f\"[check_connection] {self._connection.is_connected=}\"\n    )\n    if self._connection.is_connected:\n        # Even if the connection appears open, it could be broken for\n        # external reasons (e.g. the socket is closed because the SSH\n        # server was restarted). In these cases, we catch the error and\n        # try to re-open the connection.\n        # NOTE: Several specific errors inherit from `SSHException`,\n        # including errors related to authentication.\n        try:\n            self.logger.info(\n                \"[check_connection] Run dummy command to check connection.\"\n            )\n            # Run both an SFTP and an SSH command, as they correspond to\n            # different sockets\n            self.remote_exists(\"/dummy/path/\")\n            self.run_command(cmd=\"whoami\")\n            self.logger.info(\n                \"[check_connection] SSH connection is already OK, exit.\"\n            )\n            return\n        except (OSError, EOFError, SSHException) as e:\n            self.logger.warning(\n                f\"[check_connection] Detected error {str(e)}, re-open.\"\n            )\n    # Try opening the connection (if it was closed) or to re-open it (if\n    # an error happened).\n    self.refresh_connection()\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.close","title":"<code>close()</code>","text":"<p>Aggressively close <code>self._connection</code>.</p> <p>When <code>Connection.is_connected</code> is <code>False</code>, <code>Connection.close()</code> does not call <code>Connection.client.close()</code>. Thus we do this explicitly here, because we observed cases where <code>is_connected=False</code> but the underlying <code>Transport</code> object was not closed.</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"\n    Aggressively close `self._connection`.\n\n    When `Connection.is_connected` is `False`, `Connection.close()` does\n    not call `Connection.client.close()`. Thus we do this explicitly here,\n    because we observed cases where `is_connected=False` but the underlying\n    `Transport` object was not closed.\n    \"\"\"\n    with _acquire_lock_with_timeout(\n        lock=self._lock,\n        label=\"FractalSSH._connection.close()\",\n        timeout=self.default_lock_timeout,\n        pid=self._pid,\n        logger_name=self.logger_name,\n    ):\n        self._connection.close()\n        if self._connection.client is not None:\n            self._connection.client.close()\n    close_logger(get_logger(self.logger_name))\n    close_logger(get_logger(SSH_MONITORING_LOGGER_NAME))\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.fetch_file","title":"<code>fetch_file(*, local, remote, lock_timeout=None)</code>","text":"<p>Transfer a file via SSH</p> PARAMETER DESCRIPTION <code>local</code> <p>Local path to file.</p> <p> TYPE: <code>str</code> </p> <code>remote</code> <p>Target path on remote host.</p> <p> TYPE: <code>str</code> </p> <code>lock_timeout</code> <p>Timeout for lock acquisition (overrides default).</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef fetch_file(\n    self,\n    *,\n    local: str,\n    remote: str,\n    lock_timeout: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Transfer a file via SSH\n\n    Args:\n        local: Local path to file.\n        remote: Target path on remote host.\n        lock_timeout: Timeout for lock acquisition (overrides default).\n    \"\"\"\n    try:\n        prefix = \"[fetch_file] \"\n        self.logger.info(f\"{prefix} START fetching '{remote}' over SSH.\")\n        actual_lock_timeout = self.default_lock_timeout\n        if lock_timeout is not None:\n            actual_lock_timeout = lock_timeout\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=f\"fetch_file({local},{remote})\",\n            timeout=actual_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            self._sftp_unsafe().get(\n                remote,\n                local,\n                prefetch=self.sftp_get_prefetch,\n                max_concurrent_prefetch_requests=self.sftp_get_max_requests,  # noqa E501\n            )\n        self.logger.info(f\"{prefix} END fetching '{remote}' over SSH.\")\n    except Exception as e:\n        self.log_and_raise(\n            e=e,\n            message=(\n                \"Error in `fetch_file`, while \"\n                f\"Transferring {remote=} to {local=}.\"\n            ),\n        )\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.log_and_raise","title":"<code>log_and_raise(*, e, message)</code>","text":"<p>Log and re-raise an exception from a FractalSSH method.</p> PARAMETER DESCRIPTION <code>message</code> <p>Additional message to be logged.</p> <p> TYPE: <code>str</code> </p> <code>e</code> <p>Original exception</p> <p> TYPE: <code>Exception</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def log_and_raise(self, *, e: Exception, message: str) -&gt; None:\n    \"\"\"\n    Log and re-raise an exception from a FractalSSH method.\n\n    Args:\n        message: Additional message to be logged.\n        e: Original exception\n    \"\"\"\n    try:\n        self.logger.error(message)\n        self.logger.error(f\"Original Error {type(e)} : \\n{str(e)}\")\n        # Handle the specific case of `NoValidConnectionsError`s from\n        # paramiko, which store relevant information in the `errors`\n        # attribute\n        if hasattr(e, \"errors\"):\n            self.logger.error(f\"{type(e)=}\")\n            for err in e.errors:\n                self.logger.error(f\"{err}\")\n    except Exception as exception:\n        # Handle unexpected cases, e.g. (1) `e` has no `type`, or\n        # (2) `errors` is not iterable.\n        self.logger.error(\n            \"Unexpected Error while handling exception above: \"\n            f\"{str(exception)}\"\n        )\n\n    raise e\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.mkdir","title":"<code>mkdir(*, folder, parents=True)</code>","text":"<p>Create a folder remotely via SSH.</p> PARAMETER DESCRIPTION <code>folder</code> <p> TYPE: <code>str</code> </p> <code>parents</code> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def mkdir(self, *, folder: str, parents: bool = True) -&gt; None:\n    \"\"\"\n    Create a folder remotely via SSH.\n\n    Args:\n        folder:\n        parents:\n    \"\"\"\n    if parents:\n        cmd = f\"mkdir -p {folder}\"\n    else:\n        cmd = f\"mkdir {folder}\"\n    self.run_command(cmd=cmd)\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.read_remote_text_file","title":"<code>read_remote_text_file(filepath)</code>","text":"<p>Read a remote text file into a string.</p> <p>Note from paramiko docs:</p> <p>The Python 'b' flag is ignored, since SSH treats all files as binary.</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef read_remote_text_file(self, filepath: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Read a remote text file into a string.\n\n    Note from paramiko docs:\n    &gt; The Python 'b' flag is ignored, since SSH treats all files as binary.\n    \"\"\"\n    self.logger.info(f\"START reading remote text file {filepath}.\")\n    with _acquire_lock_with_timeout(\n        lock=self._lock,\n        label=f\"read_remote_text_file({filepath})\",\n        timeout=self.default_lock_timeout,\n        pid=self._pid,\n        logger_name=self.logger_name,\n    ):\n        try:\n            with self._sftp_unsafe().open(filepath, \"r\") as f:\n                data = f.read().decode()\n        except Exception as e:\n            self.log_and_raise(\n                e=e,\n                message=(\n                    f\"Error in `read_remote_text_file`, for {filepath=}.\"\n                ),\n            )\n    self.logger.info(f\"END reading remote text file {filepath}.\")\n    return data\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.remote_exists","title":"<code>remote_exists(path)</code>","text":"<p>Return whether a remote file/folder exists</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef remote_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Return whether a remote file/folder exists\n    \"\"\"\n    self.logger.info(f\"START remote_file_exists {path}\")\n    with _acquire_lock_with_timeout(\n        lock=self._lock,\n        label=f\"remote_file_exists({path})\",\n        timeout=self.default_lock_timeout,\n        pid=self._pid,\n        logger_name=self.logger_name,\n    ):\n        try:\n            self._sftp_unsafe().stat(path)\n            self.logger.info(f\"END   remote_file_exists {path} / True\")\n            return True\n        except FileNotFoundError:\n            self.logger.info(f\"END   remote_file_exists {path} / False\")\n            return False\n        except Exception as e:\n            self.log_and_raise(\n                e=e, message=f\"Error in `remote_exists`, for {path=}.\"\n            )\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.remove_folder","title":"<code>remove_folder(*, folder, safe_root)</code>","text":"<p>Removes a folder remotely via SSH.</p> <p>This functions calls <code>rm -r</code>, after a few checks on <code>folder</code>.</p> PARAMETER DESCRIPTION <code>folder</code> <p>Absolute path to a folder that should be removed.</p> <p> TYPE: <code>str</code> </p> <code>safe_root</code> <p>If <code>folder</code> is not a subfolder of the absolute <code>safe_root</code> path, raise an error.</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def remove_folder(\n    self,\n    *,\n    folder: str,\n    safe_root: str,\n) -&gt; None:\n    \"\"\"\n    Removes a folder remotely via SSH.\n\n    This functions calls `rm -r`, after a few checks on `folder`.\n\n    Args:\n        folder: Absolute path to a folder that should be removed.\n        safe_root: If `folder` is not a subfolder of the absolute\n            `safe_root` path, raise an error.\n    \"\"\"\n    validate_cmd(folder)\n    validate_cmd(safe_root)\n\n    if \" \" in folder:\n        raise ValueError(f\"folder='{folder}' includes whitespace.\")\n    elif \" \" in safe_root:\n        raise ValueError(f\"safe_root='{safe_root}' includes whitespace.\")\n    elif not Path(folder).is_absolute():\n        raise ValueError(f\"{folder=} is not an absolute path.\")\n    elif not Path(safe_root).is_absolute():\n        raise ValueError(f\"{safe_root=} is not an absolute path.\")\n    elif not (\n        Path(folder).resolve().is_relative_to(Path(safe_root).resolve())\n    ):\n        raise ValueError(f\"{folder=} is not a subfolder of {safe_root=}.\")\n    else:\n        cmd = f\"rm -r {folder}\"\n        self.run_command(cmd=cmd)\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.run_command","title":"<code>run_command(*, cmd, allow_char=None, lock_timeout=None)</code>","text":"<p>Run a command within an open SSH connection.</p> PARAMETER DESCRIPTION <code>cmd</code> <p>Command to be run</p> <p> TYPE: <code>str</code> </p> <code>allow_char</code> <p>Forbidden chars to allow for this command</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>lock_timeout</code> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Standard output of the command, if successful.</p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef run_command(\n    self,\n    *,\n    cmd: str,\n    allow_char: str | None = None,\n    lock_timeout: int | None = None,\n) -&gt; str:\n    \"\"\"\n    Run a command within an open SSH connection.\n\n    Args:\n        cmd: Command to be run\n        allow_char: Forbidden chars to allow for this command\n        lock_timeout:\n\n    Returns:\n        Standard output of the command, if successful.\n    \"\"\"\n\n    validate_cmd(cmd, allow_char=allow_char)\n\n    actual_lock_timeout = self.default_lock_timeout\n    if lock_timeout is not None:\n        actual_lock_timeout = lock_timeout\n\n    t_0 = time.perf_counter()\n    try:\n        # Case 1: Command runs successfully\n        res = self._run(\n            cmd,\n            label=cmd,\n            lock_timeout=actual_lock_timeout,\n            hide=True,\n            in_stream=False,\n        )\n        t_1 = time.perf_counter()\n        self.logger.info(\n            f\"END   running '{cmd}' over SSH, elapsed={t_1 - t_0:.3f}\"\n        )\n        self.logger.debug(\"STDOUT:\")\n        self.logger.debug(res.stdout)\n        self.logger.debug(\"STDERR:\")\n        self.logger.debug(res.stderr)\n        return res.stdout\n    # Case 2: Command fails with a connection error\n    except NoValidConnectionsError as e:\n        raise NoValidConnectionsError(errors=e.errors)\n    except UnexpectedExit as e:\n        # Case 3: Command fails with an actual error\n        error_msg = (\n            f\"Running command `{cmd}` over SSH failed.\\n\"\n            f\"Original error:\\n{str(e)}.\"\n        )\n        self.logger.error(error_msg)\n        raise FractalSSHCommandError(error_msg)\n    except FractalSSHTimeoutError as e:\n        raise e\n    except Exception as e:\n        self.logger.error(\n            f\"Running command `{cmd}` over SSH failed.\\n\"\n            f\"Original Error:\\n{str(e)}.\"\n        )\n        raise FractalSSHUnknownError(f\"{type(e)}: {str(e)}\")\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.send_file","title":"<code>send_file(*, local, remote, lock_timeout=None)</code>","text":"<p>Transfer a file via SSH</p> PARAMETER DESCRIPTION <code>local</code> <p>Local path to file.</p> <p> TYPE: <code>str</code> </p> <code>remote</code> <p>Target path on remote host.</p> <p> TYPE: <code>str</code> </p> <code>lock_timeout</code> <p>Timeout for lock acquisition (overrides default).</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef send_file(\n    self,\n    *,\n    local: str,\n    remote: str,\n    lock_timeout: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Transfer a file via SSH\n\n    Args:\n        local: Local path to file.\n        remote: Target path on remote host.\n        lock_timeout: Timeout for lock acquisition (overrides default).\n    \"\"\"\n    try:\n        self.logger.info(\n            f\"[send_file] START transfer of '{local}' over SSH.\"\n        )\n        actual_lock_timeout = self.default_lock_timeout\n        if lock_timeout is not None:\n            actual_lock_timeout = lock_timeout\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=f\"send_file({local},{remote})\",\n            timeout=actual_lock_timeout,\n            pid=self._pid,\n            logger_name=self.logger_name,\n        ):\n            self._sftp_unsafe().put(local, remote)\n        self.logger.info(f\"[send_file] END transfer of '{local}' over SSH.\")\n    except Exception as e:\n        self.log_and_raise(\n            e=e,\n            message=(\n                \"Error in `send_file`, while \"\n                f\"transferring {local=} to {remote=}.\"\n            ),\n        )\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSH.write_remote_file","title":"<code>write_remote_file(*, path, content, lock_timeout=None)</code>","text":"<p>Open a remote file via SFTP and write it.</p> PARAMETER DESCRIPTION <code>path</code> <p>Absolute path of remote file.</p> <p> TYPE: <code>str</code> </p> <code>content</code> <p>Contents to be written to file.</p> <p> TYPE: <code>str</code> </p> <code>lock_timeout</code> <p>Timeout for lock acquisition (overrides default).</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@retry_if_socket_error\ndef write_remote_file(\n    self,\n    *,\n    path: str,\n    content: str,\n    lock_timeout: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Open a remote file via SFTP and write it.\n\n    Args:\n        path: Absolute path of remote file.\n        content: Contents to be written to file.\n        lock_timeout: Timeout for lock acquisition (overrides default).\n    \"\"\"\n    t_start = time.perf_counter()\n    self.logger.info(f\"[write_remote_file] START ({path}).\")\n    actual_lock_timeout = self.default_lock_timeout\n    if lock_timeout is not None:\n        actual_lock_timeout = lock_timeout\n    with _acquire_lock_with_timeout(\n        lock=self._lock,\n        label=f\"write_remote_file({path})\",\n        timeout=actual_lock_timeout,\n        pid=self._pid,\n        logger_name=self.logger_name,\n    ):\n        try:\n            with self._sftp_unsafe().open(filename=path, mode=\"w\") as f:\n                f.write(content)\n        except Exception as e:\n            self.log_and_raise(\n                e=e, message=f\"Error in `write_remote_file`, for {path=}.\"\n            )\n\n    elapsed = time.perf_counter() - t_start\n    self.logger.info(f\"[write_remote_file] END, {elapsed=} s ({path}).\")\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList","title":"<code>FractalSSHList</code>","text":"<p>Collection of <code>FractalSSH</code> objects</p> <p>Attributes are all private, and access to this collection must be through methods (mostly the <code>get</code> one).</p> ATTRIBUTE DESCRIPTION <code>_data</code> <p>Mapping of unique keys (the SSH-credentials tuples) to <code>FractalSSH</code> objects.</p> <p> TYPE: <code>dict[tuple[str, str, str], FractalSSH]</code> </p> <code>_lock</code> <p>A <code>threading.Lock object</code>, to be acquired when changing <code>_data</code>.</p> <p> TYPE: <code>Lock</code> </p> <code>_timeout</code> <p>Timeout for <code>_lock</code> acquisition.</p> <p> TYPE: <code>float</code> </p> <code>_logger_name</code> <p>Logger name.</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>class FractalSSHList:\n    \"\"\"\n    Collection of `FractalSSH` objects\n\n    Attributes are all private, and access to this collection must be\n    through methods (mostly the `get` one).\n\n    Attributes:\n        _data:\n            Mapping of unique keys (the SSH-credentials tuples) to\n            `FractalSSH` objects.\n        _lock:\n            A `threading.Lock object`, to be acquired when changing `_data`.\n        _timeout: Timeout for `_lock` acquisition.\n        _logger_name: Logger name.\n    \"\"\"\n\n    _data: dict[tuple[str, str, str], FractalSSH]\n    _lock: Lock\n    _timeout: float\n    _logger_name: str\n    _pid: int\n\n    def __init__(\n        self,\n        *,\n        timeout: float = 5.0,\n        logger_name: str = \"fractal_server.FractalSSHList\",\n    ):\n        self._lock = Lock()\n        self._data = {}\n        self._timeout = timeout\n        self._logger_name = logger_name\n        set_logger(self._logger_name)\n        self._pid = os.getpid()\n\n    @property\n    def logger(self) -&gt; logging.Logger:\n        \"\"\"\n        This property exists so that we never have to propagate the\n        `Logger` object.\n        \"\"\"\n        return get_logger(self._logger_name)\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"\n        Number of current key-value pairs in `self._data`.\n        \"\"\"\n        return len(self._data.values())\n\n    def get(self, *, host: str, user: str, key_path: str) -&gt; FractalSSH:\n        \"\"\"\n        Get the `FractalSSH` for the current credentials, or create one.\n\n        Note: Changing `_data` requires acquiring `_lock`.\n\n        Args:\n            host:\n            user:\n            key_path:\n        \"\"\"\n        key = (host, user, key_path)\n        fractal_ssh = self._data.get(key, None)\n        if fractal_ssh is not None:\n            self.logger.info(\n                f\"Return existing FractalSSH object for {user}@{host}\"\n            )\n            return fractal_ssh\n        else:\n            self.logger.info(f\"Add new FractalSSH object for {user}@{host}\")\n            connection = Connection(\n                host=host,\n                user=user,\n                forward_agent=False,\n                connect_kwargs={\n                    \"key_filename\": key_path,\n                    \"look_for_keys\": False,\n                    \"banner_timeout\": 30,\n                    \"auth_timeout\": 30,  # default value\n                    \"channel_timeout\": 60 * 60,  # default value\n                },\n            )\n            with _acquire_lock_with_timeout(\n                lock=self._lock,\n                label=\"FractalSSHList.get\",\n                timeout=self._timeout,\n                pid=self._pid,\n                logger_name=self._logger_name,\n            ):\n                self._data[key] = FractalSSH(connection=connection)\n                return self._data[key]\n\n    def contains(\n        self,\n        *,\n        host: str,\n        user: str,\n        key_path: str,\n    ) -&gt; bool:\n        \"\"\"\n        Return whether a given key is present in the collection.\n\n        Args:\n            host:\n            user:\n            key_path:\n        \"\"\"\n        key = (host, user, key_path)\n        return key in self._data.keys()\n\n    def remove(\n        self,\n        *,\n        host: str,\n        user: str,\n        key_path: str,\n    ) -&gt; None:\n        \"\"\"\n        Remove a key from `_data` and close the corresponding connection.\n\n        Note: Changing `_data` requires acquiring `_lock`.\n\n        Args:\n            host:\n            user:\n            key_path:\n        \"\"\"\n        key = (host, user, key_path)\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            timeout=self._timeout,\n            label=\"FractalSSHList.remove\",\n            pid=self._pid,\n            logger_name=self._logger_name,\n        ):\n            self.logger.info(\n                f\"Removing FractalSSH object for {user}@{host} from collection.\"\n            )\n            fractal_ssh_obj = self._data.pop(key)\n            self.logger.info(\n                f\"Closing FractalSSH object for {user}@{host} \"\n                f\"({fractal_ssh_obj.is_connected=}).\"\n            )\n            fractal_ssh_obj.close()\n\n    def close_all(self, *, timeout: float = 5.0):\n        \"\"\"\n        Close all `FractalSSH` objects in the collection.\n\n        Args:\n            timeout:\n                Timeout for `FractalSSH._lock` acquisition, to be obtained\n                before closing.\n        \"\"\"\n        for key, fractal_ssh_obj in self._data.items():\n            host, user, _ = key[:]\n            self.logger.info(\n                f\"Closing FractalSSH object for {user}@{host} \"\n                f\"({fractal_ssh_obj.is_connected=}).\"\n            )\n            fractal_ssh_obj.close()\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.logger","title":"<code>logger</code>  <code>property</code>","text":"<p>This property exists so that we never have to propagate the <code>Logger</code> object.</p>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.size","title":"<code>size</code>  <code>property</code>","text":"<p>Number of current key-value pairs in <code>self._data</code>.</p>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.close_all","title":"<code>close_all(*, timeout=5.0)</code>","text":"<p>Close all <code>FractalSSH</code> objects in the collection.</p> PARAMETER DESCRIPTION <code>timeout</code> <p>Timeout for <code>FractalSSH._lock</code> acquisition, to be obtained before closing.</p> <p> TYPE: <code>float</code> DEFAULT: <code>5.0</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def close_all(self, *, timeout: float = 5.0):\n    \"\"\"\n    Close all `FractalSSH` objects in the collection.\n\n    Args:\n        timeout:\n            Timeout for `FractalSSH._lock` acquisition, to be obtained\n            before closing.\n    \"\"\"\n    for key, fractal_ssh_obj in self._data.items():\n        host, user, _ = key[:]\n        self.logger.info(\n            f\"Closing FractalSSH object for {user}@{host} \"\n            f\"({fractal_ssh_obj.is_connected=}).\"\n        )\n        fractal_ssh_obj.close()\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.contains","title":"<code>contains(*, host, user, key_path)</code>","text":"<p>Return whether a given key is present in the collection.</p> PARAMETER DESCRIPTION <code>host</code> <p> TYPE: <code>str</code> </p> <code>user</code> <p> TYPE: <code>str</code> </p> <code>key_path</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def contains(\n    self,\n    *,\n    host: str,\n    user: str,\n    key_path: str,\n) -&gt; bool:\n    \"\"\"\n    Return whether a given key is present in the collection.\n\n    Args:\n        host:\n        user:\n        key_path:\n    \"\"\"\n    key = (host, user, key_path)\n    return key in self._data.keys()\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.get","title":"<code>get(*, host, user, key_path)</code>","text":"<p>Get the <code>FractalSSH</code> for the current credentials, or create one.</p> <p>Note: Changing <code>_data</code> requires acquiring <code>_lock</code>.</p> PARAMETER DESCRIPTION <code>host</code> <p> TYPE: <code>str</code> </p> <code>user</code> <p> TYPE: <code>str</code> </p> <code>key_path</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def get(self, *, host: str, user: str, key_path: str) -&gt; FractalSSH:\n    \"\"\"\n    Get the `FractalSSH` for the current credentials, or create one.\n\n    Note: Changing `_data` requires acquiring `_lock`.\n\n    Args:\n        host:\n        user:\n        key_path:\n    \"\"\"\n    key = (host, user, key_path)\n    fractal_ssh = self._data.get(key, None)\n    if fractal_ssh is not None:\n        self.logger.info(\n            f\"Return existing FractalSSH object for {user}@{host}\"\n        )\n        return fractal_ssh\n    else:\n        self.logger.info(f\"Add new FractalSSH object for {user}@{host}\")\n        connection = Connection(\n            host=host,\n            user=user,\n            forward_agent=False,\n            connect_kwargs={\n                \"key_filename\": key_path,\n                \"look_for_keys\": False,\n                \"banner_timeout\": 30,\n                \"auth_timeout\": 30,  # default value\n                \"channel_timeout\": 60 * 60,  # default value\n            },\n        )\n        with _acquire_lock_with_timeout(\n            lock=self._lock,\n            label=\"FractalSSHList.get\",\n            timeout=self._timeout,\n            pid=self._pid,\n            logger_name=self._logger_name,\n        ):\n            self._data[key] = FractalSSH(connection=connection)\n            return self._data[key]\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.FractalSSHList.remove","title":"<code>remove(*, host, user, key_path)</code>","text":"<p>Remove a key from <code>_data</code> and close the corresponding connection.</p> <p>Note: Changing <code>_data</code> requires acquiring <code>_lock</code>.</p> PARAMETER DESCRIPTION <code>host</code> <p> TYPE: <code>str</code> </p> <code>user</code> <p> TYPE: <code>str</code> </p> <code>key_path</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>def remove(\n    self,\n    *,\n    host: str,\n    user: str,\n    key_path: str,\n) -&gt; None:\n    \"\"\"\n    Remove a key from `_data` and close the corresponding connection.\n\n    Note: Changing `_data` requires acquiring `_lock`.\n\n    Args:\n        host:\n        user:\n        key_path:\n    \"\"\"\n    key = (host, user, key_path)\n    with _acquire_lock_with_timeout(\n        lock=self._lock,\n        timeout=self._timeout,\n        label=\"FractalSSHList.remove\",\n        pid=self._pid,\n        logger_name=self._logger_name,\n    ):\n        self.logger.info(\n            f\"Removing FractalSSH object for {user}@{host} from collection.\"\n        )\n        fractal_ssh_obj = self._data.pop(key)\n        self.logger.info(\n            f\"Closing FractalSSH object for {user}@{host} \"\n            f\"({fractal_ssh_obj.is_connected=}).\"\n        )\n        fractal_ssh_obj.close()\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric.SingleUseFractalSSH","title":"<code>SingleUseFractalSSH(*, ssh_config, logger_name)</code>","text":"<p>Get a new FractalSSH object (with a fresh connection).</p> PARAMETER DESCRIPTION <code>ssh_config</code> <p> TYPE: <code>SSHConfig</code> </p> <code>logger_name</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@contextmanager\ndef SingleUseFractalSSH(\n    *,\n    ssh_config: SSHConfig,\n    logger_name: str,\n) -&gt; Generator[FractalSSH, Any, None]:\n    \"\"\"\n    Get a new FractalSSH object (with a fresh connection).\n\n    Args:\n        ssh_config:\n        logger_name:\n    \"\"\"\n    _fractal_ssh_list = FractalSSHList(logger_name=logger_name)\n    _fractal_ssh = _fractal_ssh_list.get(**ssh_config.model_dump())\n    yield _fractal_ssh\n    _fractal_ssh.close()\n</code></pre>"},{"location":"reference/ssh/_fabric/#fractal_server.ssh._fabric._acquire_lock_with_timeout","title":"<code>_acquire_lock_with_timeout(lock, label, timeout, pid, logger_name)</code>","text":"<p>Given a <code>threading.Lock</code> object, try to acquire it within a given timeout.</p> PARAMETER DESCRIPTION <code>lock</code> <p> TYPE: <code>Lock</code> </p> <code>label</code> <p> TYPE: <code>str</code> </p> <code>timeout</code> <p> TYPE: <code>float</code> </p> <code>logger_name</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/ssh/_fabric.py</code> <pre><code>@contextmanager\ndef _acquire_lock_with_timeout(\n    lock: Lock,\n    label: str,\n    timeout: float,\n    pid: int,\n    logger_name: str,\n) -&gt; Generator[Literal[True], Any, None]:\n    \"\"\"\n    Given a `threading.Lock` object, try to acquire it within a given timeout.\n\n    Args:\n        lock:\n        label:\n        timeout:\n        logger_name:\n    \"\"\"\n    logger = get_logger(logger_name)\n    ssh_logger = get_logger(SSH_MONITORING_LOGGER_NAME)\n    logger.info(f\"Trying to acquire lock for '{label}', with {timeout=}\")\n    t_lock_request = time.perf_counter()\n    result = lock.acquire(timeout=timeout)\n    try:\n        if not result:\n            logger.error(f\"Lock for '{label}' was *not* acquired.\")\n            raise FractalSSHTimeoutError(\n                f\"Failed to acquire lock for '{label}' within {timeout} seconds\"\n            )\n        t_lock_acquisition = time.perf_counter()\n        elapsed = t_lock_acquisition - t_lock_request\n        logger.info(f\"Lock for '{label}' was acquired - {elapsed=:.4f} s\")\n        yield result\n    finally:\n        if result:\n            lock.release()\n            logger.info(f\"Lock for '{label}' was released.\")\n            t_lock_release = time.perf_counter()\n            lock_was_acquired = 1\n        else:\n            t_lock_release = time.perf_counter()\n            t_lock_acquisition = t_lock_release\n            lock_was_acquired = 0\n        lock_waiting_time = t_lock_acquisition - t_lock_request\n        lock_holding_time = t_lock_release - t_lock_acquisition\n        ssh_logger.info(\n            f\"{pid} {lock_waiting_time:.6e} {lock_holding_time:.6e} {lock_was_acquired} {label.replace(' ', '_')}\"  # noqa\n        )\n</code></pre>"},{"location":"reference/tasks/","title":"tasks","text":"<p><code>tasks</code> module</p>"},{"location":"reference/tasks/utils/","title":"utils","text":""},{"location":"reference/tasks/config/","title":"config","text":""},{"location":"reference/tasks/config/_pixi/","title":"_pixi","text":""},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig","title":"<code>PixiSLURMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters that are passed directly to a <code>sbatch</code> command.</p> <p>See https://slurm.schedmd.com/sbatch.html.</p> Source code in <code>fractal_server/tasks/config/_pixi.py</code> <pre><code>class PixiSLURMConfig(BaseModel):\n    \"\"\"\n    Parameters that are passed directly to a `sbatch` command.\n\n    See https://slurm.schedmd.com/sbatch.html.\n    \"\"\"\n\n    partition: NonEmptyStr\n    \"\"\"\n    `-p, --partition=&lt;partition_names&gt;`\n    \"\"\"\n    cpus: PositiveInt\n    \"\"\"\n    `-c, --cpus-per-task=&lt;ncpus&gt;\n    \"\"\"\n    mem: PixiMemoryStr | None = None\n    \"\"\"\n    `--mem=&lt;size&gt;[units]` (examples: `\"10M\"`, `\"10G\"`).\n    From `sbatch` docs: Specify the real memory required per node. Default\n    units are megabytes. Different units can be specified using the suffix\n    [K|M|G|T].\n    \"\"\"\n    mem_per_cpu: PixiMemoryStr | None = None\n    \"\"\"\n    `--mem-per-cpu=&lt;size&gt;[units]` (examples: `\"10M\"`, `\"10G\"`).\n    From `sbatch` docs: Minimum memory required per usable allocated CPU.\n    Default units are megabytes. [..] The --mem, --mem-per-cpu and\n    --mem-per-gpu options are mutually exclusive.\n    \"\"\"\n    time: NonEmptyStr\n    \"\"\"\n    `-t, --time=&lt;time&gt;`.\n    From `sbatch` docs: \"A time limit of zero requests that no time limit be\n    imposed. Acceptable time formats include \"minutes\", \"minutes:seconds\",\n    \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and\n    \"days-hours:minutes:seconds\".\n    \"\"\"\n    preamble: list[str] = Field(default_factory=list)\n    \"\"\"\n    Lines to be included at the beginning of the SLURM submission script.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def _memory_validator(self: Self) -&gt; Self:\n        if (self.mem is None and self.mem_per_cpu is None) or (\n            self.mem is not None and self.mem_per_cpu is not None\n        ):\n            raise ValueError(\"You must set either `mem` or `mem_per_cpu`.\")\n        return self\n</code></pre>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig.cpus","title":"<code>cpus</code>  <code>instance-attribute</code>","text":"<p>`-c, --cpus-per-task="},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig.mem","title":"<code>mem = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p><code>--mem=&lt;size&gt;[units]</code> (examples: <code>\"10M\"</code>, <code>\"10G\"</code>). From <code>sbatch</code> docs: Specify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T].</p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig.mem_per_cpu","title":"<code>mem_per_cpu = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p><code>--mem-per-cpu=&lt;size&gt;[units]</code> (examples: <code>\"10M\"</code>, <code>\"10G\"</code>). From <code>sbatch</code> docs: Minimum memory required per usable allocated CPU. Default units are megabytes. [..] The --mem, --mem-per-cpu and --mem-per-gpu options are mutually exclusive.</p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig.partition","title":"<code>partition</code>  <code>instance-attribute</code>","text":"<p><code>-p, --partition=&lt;partition_names&gt;</code></p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig.preamble","title":"<code>preamble = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Lines to be included at the beginning of the SLURM submission script.</p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.PixiSLURMConfig.time","title":"<code>time</code>  <code>instance-attribute</code>","text":"<p><code>-t, --time=&lt;time&gt;</code>. From <code>sbatch</code> docs: \"A time limit of zero requests that no time limit be imposed. Acceptable time formats include \"minutes\", \"minutes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-hours:minutes:seconds\".</p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings","title":"<code>TasksPixiSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for <code>pixi</code> Task collection.</p> Source code in <code>fractal_server/tasks/config/_pixi.py</code> <pre><code>class TasksPixiSettings(BaseModel):\n    \"\"\"\n    Configuration for `pixi` Task collection.\n    \"\"\"\n\n    versions: DictStrStr\n    \"\"\"\n    Dictionary mapping `pixi` versions (e.g. `0.47.0`) to the corresponding\n    folders (e.g. `/somewhere/pixi/0.47.0` - if the binary is\n    `/somewhere/pixi/0.47.0/bin/pixi`).\n    \"\"\"\n    default_version: str\n    \"\"\"\n    Default task-collection `pixi` version.\n    \"\"\"\n    PIXI_CONCURRENT_SOLVES: int = 4\n    \"\"\"\n    Value of\n    [`--concurrent-solves`](https://pixi.sh/latest/reference/cli/pixi/install/#arg---concurrent-solves)\n    for `pixi install`.\n    \"\"\"\n    PIXI_CONCURRENT_DOWNLOADS: int = 4\n    \"\"\"\n    Value of\n    [`--concurrent-downloads`](https://pixi.sh/latest/reference/cli/pixi/install/#arg---concurrent-downloads)\n    for `pixi install`.\n    \"\"\"\n    TOKIO_WORKER_THREADS: int = 2\n    \"\"\"\n    From\n    [Tokio documentation](\n    https://docs.rs/tokio/latest/tokio/#cpu-bound-tasks-and-blocking-code\n    )\n    :\n\n        The core threads are where all asynchronous code runs,\n        and Tokio will by default spawn one for each CPU core.\n        You can use the environment variable `TOKIO_WORKER_THREADS` to override\n        the default value.\n    \"\"\"\n    DEFAULT_ENVIRONMENT: str = \"default\"\n    \"\"\"\n    Default pixi environment name.\n    \"\"\"\n    DEFAULT_PLATFORM: str = \"linux-64\"\n    \"\"\"\n    Default platform for pixi.\n    \"\"\"\n    SLURM_CONFIG: PixiSLURMConfig | None = None\n    \"\"\"\n    Required when using `pixi` in a SSH/SLURM deployment.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def check_pixi_settings(self):\n        if self.default_version not in self.versions:\n            raise ValueError(\n                f\"Default version '{self.default_version}' not in \"\n                f\"available version {list(self.versions.keys())}.\"\n            )\n\n        pixi_base_dir = Path(self.versions[self.default_version]).parent\n\n        for key, value in self.versions.items():\n            pixi_path = Path(value)\n\n            if pixi_path.parent != pixi_base_dir:\n                raise ValueError(\n                    f\"{pixi_path=} is not located within the {pixi_base_dir=}.\"\n                )\n            if pixi_path.name != key:\n                raise ValueError(f\"{pixi_path.name=} is not equal to {key=}\")\n\n        return self\n</code></pre>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.DEFAULT_ENVIRONMENT","title":"<code>DEFAULT_ENVIRONMENT = 'default'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Default pixi environment name.</p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.DEFAULT_PLATFORM","title":"<code>DEFAULT_PLATFORM = 'linux-64'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Default platform for pixi.</p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.PIXI_CONCURRENT_DOWNLOADS","title":"<code>PIXI_CONCURRENT_DOWNLOADS = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Value of <code>--concurrent-downloads</code> for <code>pixi install</code>.</p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.PIXI_CONCURRENT_SOLVES","title":"<code>PIXI_CONCURRENT_SOLVES = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Value of <code>--concurrent-solves</code> for <code>pixi install</code>.</p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.SLURM_CONFIG","title":"<code>SLURM_CONFIG = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Required when using <code>pixi</code> in a SSH/SLURM deployment.</p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.TOKIO_WORKER_THREADS","title":"<code>TOKIO_WORKER_THREADS = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>From Tokio documentation :</p> <pre><code>The core threads are where all asynchronous code runs,\nand Tokio will by default spawn one for each CPU core.\nYou can use the environment variable `TOKIO_WORKER_THREADS` to override\nthe default value.\n</code></pre>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.default_version","title":"<code>default_version</code>  <code>instance-attribute</code>","text":"<p>Default task-collection <code>pixi</code> version.</p>"},{"location":"reference/tasks/config/_pixi/#fractal_server.tasks.config._pixi.TasksPixiSettings.versions","title":"<code>versions</code>  <code>instance-attribute</code>","text":"<p>Dictionary mapping <code>pixi</code> versions (e.g. <code>0.47.0</code>) to the corresponding folders (e.g. <code>/somewhere/pixi/0.47.0</code> - if the binary is <code>/somewhere/pixi/0.47.0/bin/pixi</code>).</p>"},{"location":"reference/tasks/config/_python/","title":"_python","text":""},{"location":"reference/tasks/config/_python/#fractal_server.tasks.config._python.TasksPythonSettings","title":"<code>TasksPythonSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the Python base interpreters to be used for task venvs.</p> <p>For task collection to work, there must be one or more base Python interpreters available on your system.</p> ATTRIBUTE DESCRIPTION <code>default_version</code> <p>Default task-collection Python version (must be a key of <code>versions</code>).</p> <p> TYPE: <code>NonEmptyStr</code> </p> <code>versions</code> <p>Dictionary mapping Python versions to the corresponding interpreters. Example: <pre><code>{\n  \"3.11\": \"/path/to/python3.11\",\n  \"3.13\": \"/path/to/python3.13\"\n}\n</code></pre></p> <p> TYPE: <code>dict[Literal['3.9', '3.10', '3.11', '3.12', '3.13', '3.14'], AbsolutePathStr]</code> </p> <code>pip_cache_dir</code> <p>Argument for <code>--cache-dir</code> option of <code>pip install</code>, if set.</p> <p> TYPE: <code>AbsolutePathStr | None</code> </p> Source code in <code>fractal_server/tasks/config/_python.py</code> <pre><code>class TasksPythonSettings(BaseModel):\n    \"\"\"\n    Configuration for the Python base interpreters to be used for task venvs.\n\n    For task collection to work, there must be one or more base Python\n    interpreters available on your system.\n\n    Attributes:\n        default_version:\n            Default task-collection Python version (must be a key of\n            `versions`).\n        versions:\n            Dictionary mapping Python versions to the corresponding\n            interpreters. Example:\n            ```json\n            {\n              \"3.11\": \"/path/to/python3.11\",\n              \"3.13\": \"/path/to/python3.13\"\n            }\n            ```\n        pip_cache_dir:\n            Argument for `--cache-dir` option of `pip install`, if set.\n    \"\"\"\n\n    default_version: NonEmptyStr\n    versions: dict[\n        Literal[\n            \"3.9\",\n            \"3.10\",\n            \"3.11\",\n            \"3.12\",\n            \"3.13\",\n            \"3.14\",\n        ],\n        AbsolutePathStr,\n    ]\n\n    pip_cache_dir: AbsolutePathStr | None = None\n\n    @model_validator(mode=\"after\")\n    def _validate_versions(self) -&gt; Self:\n        if self.default_version not in self.versions.keys():\n            raise ValueError(\n                f\"The default Python version ('{self.default_version}') is \"\n                f\"not available in {list(self.versions.keys())}.\"\n            )\n\n        return self\n</code></pre>"},{"location":"reference/tasks/v2/","title":"v2","text":""},{"location":"reference/tasks/v2/utils_background/","title":"utils_background","text":""},{"location":"reference/tasks/v2/utils_background/#fractal_server.tasks.v2.utils_background.prepare_tasks_metadata","title":"<code>prepare_tasks_metadata(*, package_manifest, package_root, python_bin=None, project_python_wrapper=None, package_version=None)</code>","text":"<p>Based on the package manifest and additional info, prepare the task list.</p> PARAMETER DESCRIPTION <code>package_manifest</code> <p> TYPE: <code>ManifestV2</code> </p> <code>package_root</code> <p> TYPE: <code>Path</code> </p> <code>package_version</code> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>python_bin</code> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>project_python_wrapper</code> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> Source code in <code>fractal_server/tasks/v2/utils_background.py</code> <pre><code>def prepare_tasks_metadata(\n    *,\n    package_manifest: ManifestV2,\n    package_root: Path,\n    python_bin: Path | None = None,\n    project_python_wrapper: Path | None = None,\n    package_version: str | None = None,\n) -&gt; list[TaskCreate]:\n    \"\"\"\n    Based on the package manifest and additional info, prepare the task list.\n\n    Args:\n        package_manifest:\n        package_root:\n        package_version:\n        python_bin:\n        project_python_wrapper:\n    \"\"\"\n\n    if bool(project_python_wrapper is None) == bool(python_bin is None):\n        raise UnreachableBranchError(\n            f\"Either {project_python_wrapper} or {python_bin} must be set.\"\n        )\n\n    if python_bin is not None:\n        actual_python = python_bin\n    else:\n        actual_python = project_python_wrapper\n\n    task_list = []\n    for _task in package_manifest.task_list:\n        # Set non-command attributes\n        task_attributes = {}\n        if package_version is not None:\n            task_attributes[\"version\"] = package_version\n        if package_manifest.has_args_schemas:\n            task_attributes[\"args_schema_version\"] = (\n                package_manifest.args_schema_version\n            )\n        # Set command attributes\n        if _task.executable_non_parallel is not None:\n            non_parallel_path = package_root / _task.executable_non_parallel\n            cmd_non_parallel = (\n                f\"{actual_python.as_posix()} {non_parallel_path.as_posix()}\"\n            )\n            task_attributes[\"command_non_parallel\"] = cmd_non_parallel\n        if _task.executable_parallel is not None:\n            parallel_path = package_root / _task.executable_parallel\n            cmd_parallel = (\n                f\"{actual_python.as_posix()} {parallel_path.as_posix()}\"\n            )\n            task_attributes[\"command_parallel\"] = cmd_parallel\n        # Create object\n        task_obj = TaskCreate(\n            **_task.model_dump(\n                exclude={\n                    \"executable_non_parallel\",\n                    \"executable_parallel\",\n                }\n            ),\n            **task_attributes,\n            authors=package_manifest.authors,\n        )\n        task_list.append(task_obj)\n    return task_list\n</code></pre>"},{"location":"reference/tasks/v2/utils_database/","title":"utils_database","text":""},{"location":"reference/tasks/v2/utils_database/#fractal_server.tasks.v2.utils_database.create_db_tasks_and_update_task_group_async","title":"<code>create_db_tasks_and_update_task_group_async(*, task_group_id, task_list, db)</code>  <code>async</code>","text":"<p>Create a <code>TaskGroupV2</code> with N <code>TaskV2</code>s, and insert them into the database.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p>ID of an existing <code>TaskGroupV2</code> object.</p> <p> TYPE: <code>int</code> </p> <code>task_list</code> <p>List of <code>TaskCreate</code> objects to be inserted into the db.</p> <p> TYPE: <code>list[TaskCreate]</code> </p> <code>db</code> <p>Synchronous database session</p> <p> TYPE: <code>AsyncSession</code> </p> RETURNS DESCRIPTION <code>TaskGroupV2</code> <p>Updated <code>TaskGroupV2</code> object.</p> Source code in <code>fractal_server/tasks/v2/utils_database.py</code> <pre><code>async def create_db_tasks_and_update_task_group_async(\n    *,\n    task_group_id: int,\n    task_list: list[TaskCreate],\n    db: AsyncSession,\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Create a `TaskGroupV2` with N `TaskV2`s, and insert them into the database.\n\n    Args:\n        task_group_id: ID of an existing `TaskGroupV2` object.\n        task_list: List of `TaskCreate` objects to be inserted into the db.\n        db: Synchronous database session\n\n    Returns:\n        Updated `TaskGroupV2` object.\n    \"\"\"\n    actual_task_list = [TaskV2(**task.model_dump()) for task in task_list]\n    task_group = await db.get(TaskGroupV2, task_group_id)\n    task_group.task_list = actual_task_list\n    db.add(task_group)\n    await db.commit()\n    await db.refresh(task_group)\n\n    return task_group\n</code></pre>"},{"location":"reference/tasks/v2/utils_database/#fractal_server.tasks.v2.utils_database.create_db_tasks_and_update_task_group_sync","title":"<code>create_db_tasks_and_update_task_group_sync(*, task_group_id, task_list, db)</code>","text":"<p>Create a <code>TaskGroupV2</code> with N <code>TaskV2</code>s, and insert them into the database.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p>ID of an existing <code>TaskGroupV2</code> object.</p> <p> TYPE: <code>int</code> </p> <code>task_list</code> <p>List of <code>TaskCreate</code> objects to be inserted into the db.</p> <p> TYPE: <code>list[TaskCreate]</code> </p> <code>db</code> <p>Synchronous database session</p> <p> TYPE: <code>Session</code> </p> RETURNS DESCRIPTION <code>TaskGroupV2</code> <p>Updated <code>TaskGroupV2</code> object.</p> Source code in <code>fractal_server/tasks/v2/utils_database.py</code> <pre><code>def create_db_tasks_and_update_task_group_sync(\n    *,\n    task_group_id: int,\n    task_list: list[TaskCreate],\n    db: DBSyncSession,\n) -&gt; TaskGroupV2:\n    \"\"\"\n    Create a `TaskGroupV2` with N `TaskV2`s, and insert them into the database.\n\n    Args:\n        task_group_id: ID of an existing `TaskGroupV2` object.\n        task_list: List of `TaskCreate` objects to be inserted into the db.\n        db: Synchronous database session\n\n    Returns:\n        Updated `TaskGroupV2` object.\n    \"\"\"\n    actual_task_list = [TaskV2(**task.model_dump()) for task in task_list]\n    task_group = db.get(TaskGroupV2, task_group_id)\n    task_group.task_list = actual_task_list\n    db.add(task_group)\n    db.commit()\n    db.refresh(task_group)\n\n    return task_group\n</code></pre>"},{"location":"reference/tasks/v2/utils_package_names/","title":"utils_package_names","text":""},{"location":"reference/tasks/v2/utils_package_names/#fractal_server.tasks.v2.utils_package_names._parse_wheel_filename","title":"<code>_parse_wheel_filename(wheel_filename)</code>","text":"<p>Extract distribution and version from a wheel filename.</p> <p>The structure of a wheel filename is fixed, and it must start with <code>{distribution}-{version}</code> (see https://packaging.python.org/en/latest/specifications/binary-distribution-format ).</p> <p>Note that we transform exceptions in <code>ValueError</code>s, since this function is also used within Pydantic validators.</p> Source code in <code>fractal_server/tasks/v2/utils_package_names.py</code> <pre><code>def _parse_wheel_filename(wheel_filename: str) -&gt; dict[str, str]:\n    \"\"\"\n    Extract distribution and version from a wheel filename.\n\n    The structure of a wheel filename is fixed, and it must start with\n    `{distribution}-{version}` (see\n    https://packaging.python.org/en/latest/specifications/binary-distribution-format\n    ).\n\n    Note that we transform exceptions in `ValueError`s, since this function is\n    also used within Pydantic validators.\n    \"\"\"\n    if \"/\" in wheel_filename:\n        raise ValueError(\n            \"[_parse_wheel_filename] Input must be a filename, not a full \"\n            f\"path (given: {wheel_filename}).\"\n        )\n    try:\n        parts = wheel_filename.split(\"-\")\n        return dict(distribution=parts[0], version=parts[1])\n    except Exception as e:\n        raise ValueError(\n            f\"Invalid {wheel_filename=}. Original error: {str(e)}.\"\n        )\n</code></pre>"},{"location":"reference/tasks/v2/utils_package_names/#fractal_server.tasks.v2.utils_package_names.compare_package_names","title":"<code>compare_package_names(*, pkg_name_pip_show, pkg_name_task_group, logger_name)</code>","text":"<p>Compare the package names from <code>pip show</code> and from the db.</p> Source code in <code>fractal_server/tasks/v2/utils_package_names.py</code> <pre><code>def compare_package_names(\n    *,\n    pkg_name_pip_show: str,\n    pkg_name_task_group: str,\n    logger_name: str,\n) -&gt; None:\n    \"\"\"\n    Compare the package names from `pip show` and from the db.\n    \"\"\"\n    logger = get_logger(logger_name)\n\n    if pkg_name_pip_show == pkg_name_task_group:\n        return\n\n    logger.warning(\n        f\"Package name mismatch: {pkg_name_task_group=}, {pkg_name_pip_show=}.\"\n    )\n    normalized_pkg_name_pip = normalize_package_name(pkg_name_pip_show)\n    normalized_pkg_name_taskgroup = normalize_package_name(pkg_name_task_group)\n    if normalized_pkg_name_pip != normalized_pkg_name_taskgroup:\n        error_msg = (\n            f\"Package name mismatch persists, after normalization: \"\n            f\"{pkg_name_task_group=}, \"\n            f\"{pkg_name_pip_show=}.\"\n        )\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/tasks/v2/utils_package_names/#fractal_server.tasks.v2.utils_package_names.normalize_package_name","title":"<code>normalize_package_name(name)</code>","text":"<p>Implement PyPa specifications for package-name normalization</p> <p>The name should be lowercased with all runs of the characters <code>.</code>, <code>-</code>, or <code>_</code> replaced with a single <code>-</code> character. This can be implemented in Python with the re module. (https://packaging.python.org/en/latest/specifications/name-normalization)</p> PARAMETER DESCRIPTION <code>name</code> <p>The non-normalized package name.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The normalized package name.</p> Source code in <code>fractal_server/tasks/v2/utils_package_names.py</code> <pre><code>def normalize_package_name(name: str) -&gt; str:\n    \"\"\"\n    Implement PyPa specifications for package-name normalization\n\n    The name should be lowercased with all runs of the characters `.`, `-`,\n    or `_` replaced with a single `-` character. This can be implemented in\n    Python with the re module.\n    (https://packaging.python.org/en/latest/specifications/name-normalization)\n\n    Args:\n        name: The non-normalized package name.\n\n    Returns:\n        The normalized package name.\n    \"\"\"\n    return re.sub(r\"[-_.]+\", \"-\", name).lower()\n</code></pre>"},{"location":"reference/tasks/v2/utils_pixi/","title":"utils_pixi","text":""},{"location":"reference/tasks/v2/utils_pixi/#fractal_server.tasks.v2.utils_pixi.parse_collect_stdout","title":"<code>parse_collect_stdout(stdout)</code>","text":"<p>Parse standard output of <code>pixi/1_collect.sh</code></p> Source code in <code>fractal_server/tasks/v2/utils_pixi.py</code> <pre><code>def parse_collect_stdout(stdout: str) -&gt; ParsedOutput:\n    \"\"\"\n    Parse standard output of `pixi/1_collect.sh`\n    \"\"\"\n    searches = [\n        (\"Package folder:\", \"package_root\"),\n        (\"Project Python wrapper:\", \"project_python_wrapper\"),\n    ]\n    stdout_lines = stdout.splitlines()\n    attributes = dict()\n    for search, attribute_name in searches:\n        matching_lines = [_line for _line in stdout_lines if search in _line]\n        if len(matching_lines) == 0:\n            raise ValueError(f\"String '{search}' not found in stdout.\")\n        elif len(matching_lines) &gt; 1:\n            raise ValueError(\n                f\"String '{search}' found too many times \"\n                f\"({len(matching_lines)}).\"\n            )\n        else:\n            actual_line = matching_lines[0]\n            attribute_value = actual_line.split(search)[-1].strip(\" \")\n            attributes[attribute_name] = attribute_value\n    return attributes\n</code></pre>"},{"location":"reference/tasks/v2/utils_pixi/#fractal_server.tasks.v2.utils_pixi.simplify_pyproject_toml","title":"<code>simplify_pyproject_toml(*, original_toml_string, pixi_environment, pixi_platform)</code>","text":"<p>Simplify <code>pyproject.toml</code> contents to make <code>pixi install</code> less heavy.</p> PARAMETER DESCRIPTION <code>original_toml_string</code> <p>Original <code>pyproject.toml</code> contents</p> <p> TYPE: <code>str</code> </p> <code>pixi_environment</code> <p>Name of the pixi environment to use (e.g. <code>default</code>).</p> <p> TYPE: <code>str</code> </p> <code>pixi_platform</code> <p>Name of the platform (e.g. <code>linux-64</code>)</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>New <code>pyproject.toml</code> contents</p> Source code in <code>fractal_server/tasks/v2/utils_pixi.py</code> <pre><code>def simplify_pyproject_toml(\n    *,\n    original_toml_string: str,\n    pixi_environment: str,\n    pixi_platform: str,\n) -&gt; str:\n    \"\"\"\n    Simplify `pyproject.toml` contents to make `pixi install` less heavy.\n\n    Args:\n        original_toml_string: Original `pyproject.toml` contents\n        pixi_environment: Name of the pixi environment to use (e.g. `default`).\n        pixi_platform: Name of the platform (e.g. `linux-64`)\n\n    Returns:\n        New `pyproject.toml` contents\n    \"\"\"\n\n    # Parse TOML string\n    data = tomllib.loads(original_toml_string)\n    try:\n        pixi_data = data[\"tool\"][\"pixi\"]\n    except KeyError:\n        logger.warning(\n            \"KeyError when looking for tool/pixi path. Return original value.\"\n        )\n        return original_toml_string\n\n    # Use a single platform (or skip, if not set)\n    # Note: we look both for `[workspace.platforms]` and `[project.platforms]`,\n    # even though `[project]` is deprecated as of\n    # https://pixi.sh/dev/CHANGELOG/#0570-2025-10-20.\n    try:\n        pixi_data[\"workspace\"][\"platforms\"] = [pixi_platform]\n    except KeyError:\n        logger.info(\"KeyError for workspace/platforms - skip.\")\n    try:\n        pixi_data[\"project\"][\"platforms\"] = [pixi_platform]\n    except KeyError:\n        logger.info(\"KeyError for project/platforms - skip.\")\n\n    # Keep a single environment (or skip, if not set)\n    try:\n        current_environments = pixi_data[\"environments\"]\n        pixi_data[\"environments\"] = {\n            key: value\n            for key, value in current_environments.items()\n            if key == pixi_environment\n        }\n        if pixi_data[\"environments\"] == {}:\n            raise ValueError(f\"No '{pixi_environment}' pixi environment found.\")\n    except KeyError:\n        logger.info(\"KeyError for workspace/platforms - skip.\")\n\n    # Drop pixi.tasks\n    pixi_data.pop(\"tasks\", None)\n\n    # Prepare and validate new `pyprojectl.toml` contents\n    data[\"tool\"][\"pixi\"] = pixi_data\n    new_toml_string = tomli_w.dumps(data)\n    tomllib.loads(new_toml_string)\n\n    return new_toml_string\n</code></pre>"},{"location":"reference/tasks/v2/utils_python_interpreter/","title":"utils_python_interpreter","text":""},{"location":"reference/tasks/v2/utils_python_interpreter/#fractal_server.tasks.v2.utils_python_interpreter.get_python_interpreter","title":"<code>get_python_interpreter(python_version, resource)</code>","text":"<p>Return the path to the Python interpreter</p> PARAMETER DESCRIPTION <code>python_version</code> <p>Python version</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the python version requested is not available on the         host.</p> RETURNS DESCRIPTION <code>interpreter</code> <p>string representing the python executable or its path</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/tasks/v2/utils_python_interpreter.py</code> <pre><code>def get_python_interpreter(\n    python_version: str,\n    resource: Resource,\n) -&gt; str:\n    \"\"\"\n    Return the path to the Python interpreter\n\n    Args:\n        python_version: Python version\n\n    Raises:\n        ValueError: If the python version requested is not available on the\n                    host.\n\n    Returns:\n        interpreter: string representing the python executable or its path\n    \"\"\"\n\n    python_path = resource.tasks_python_config[\"versions\"].get(python_version)\n    if python_path is None:\n        raise ValueError(f\"Requested {python_version=} is not available.\")\n    return python_path\n</code></pre>"},{"location":"reference/tasks/v2/utils_templates/","title":"utils_templates","text":""},{"location":"reference/tasks/v2/utils_templates/#fractal_server.tasks.v2.utils_templates.customize_template","title":"<code>customize_template(*, template_name, replacements, script_path)</code>","text":"<p>Customize a bash-script template and write it to disk.</p> PARAMETER DESCRIPTION <code>template_name</code> <p>Name of the template that will be customized.</p> <p> TYPE: <code>str</code> </p> <code>replacements</code> <p>List of replacements for template customization.</p> <p> TYPE: <code>set[tuple[str, str]]</code> </p> <code>script_path</code> <p>Local path where the customized template will be written.</p> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/tasks/v2/utils_templates.py</code> <pre><code>def customize_template(\n    *,\n    template_name: str,\n    replacements: set[tuple[str, str]],\n    script_path: str,\n) -&gt; None:\n    \"\"\"\n    Customize a bash-script template and write it to disk.\n\n    Args:\n        template_name: Name of the template that will be customized.\n        replacements: List of replacements for template customization.\n        script_path: Local path where the customized template will be written.\n    \"\"\"\n    _check_pixi_frozen_option(replacements=replacements)\n\n    # Read template\n    template_path = TEMPLATES_DIR / template_name\n    with template_path.open(\"r\") as f:\n        template_data = f.read()\n    # Customize template\n    script_data = template_data\n    for old_new in replacements:\n        script_data = script_data.replace(old_new[0], old_new[1])\n    # Create parent folder if needed\n    Path(script_path).parent.mkdir(exist_ok=True)\n    # Write script locally\n    with open(script_path, \"w\") as f:\n        f.write(script_data)\n</code></pre>"},{"location":"reference/tasks/v2/utils_templates/#fractal_server.tasks.v2.utils_templates.parse_script_pip_show_stdout","title":"<code>parse_script_pip_show_stdout(stdout)</code>","text":"<p>Parse standard output of 4_pip_show.sh</p> Source code in <code>fractal_server/tasks/v2/utils_templates.py</code> <pre><code>def parse_script_pip_show_stdout(stdout: str) -&gt; dict[str, str]:\n    \"\"\"\n    Parse standard output of 4_pip_show.sh\n    \"\"\"\n    searches = [\n        (\"Python interpreter:\", \"python_bin\"),\n        (\"Package name:\", \"package_name\"),\n        (\"Package version:\", \"package_version\"),\n        (\"Package parent folder:\", \"package_root_parent\"),\n        (\"Manifest absolute path:\", \"manifest_path\"),\n    ]\n    stdout_lines = stdout.splitlines()\n    attributes = dict()\n    for search, attribute_name in searches:\n        matching_lines = [_line for _line in stdout_lines if search in _line]\n        if len(matching_lines) == 0:\n            raise ValueError(f\"String '{search}' not found in stdout.\")\n        elif len(matching_lines) &gt; 1:\n            raise ValueError(\n                f\"String '{search}' found too many times \"\n                f\"({len(matching_lines)}).\"\n            )\n        else:\n            actual_line = matching_lines[0]\n            attribute_value = actual_line.split(search)[-1].strip(\" \")\n            attributes[attribute_name] = attribute_value\n    return attributes\n</code></pre>"},{"location":"reference/tasks/v2/local/","title":"local","text":""},{"location":"reference/tasks/v2/local/_utils/","title":"_utils","text":""},{"location":"reference/tasks/v2/local/_utils/#fractal_server.tasks.v2.local._utils._customize_and_run_template","title":"<code>_customize_and_run_template(template_filename, replacements, script_dir, logger_name, prefix)</code>","text":"<p>Customize one of the template bash scripts.</p> PARAMETER DESCRIPTION <code>template_filename</code> <p>Filename of the template file (ends with \".sh\").</p> <p> TYPE: <code>str</code> </p> <code>replacements</code> <p>Dictionary of replacements.</p> <p> TYPE: <code>list[tuple[str, str]]</code> </p> <code>script_dir</code> <p>Local folder where the script will be placed.</p> <p> TYPE: <code>str</code> </p> <code>prefix</code> <p>Prefix for the script filename.</p> <p> TYPE: <code>int</code> </p> Source code in <code>fractal_server/tasks/v2/local/_utils.py</code> <pre><code>def _customize_and_run_template(\n    template_filename: str,\n    replacements: list[tuple[str, str]],\n    script_dir: str,\n    logger_name: str,\n    prefix: int,\n) -&gt; str:\n    \"\"\"\n    Customize one of the template bash scripts.\n\n    Args:\n        template_filename: Filename of the template file (ends with \".sh\").\n        replacements: Dictionary of replacements.\n        script_dir: Local folder where the script will be placed.\n        prefix: Prefix for the script filename.\n    \"\"\"\n    logger = get_logger(logger_name=logger_name)\n    logger.debug(f\"_customize_and_run_template {template_filename} - START\")\n\n    # Prepare name and path of script\n    if not template_filename.endswith(\".sh\"):\n        raise ValueError(\n            f\"Invalid {template_filename=} (it must end with '.sh').\"\n        )\n\n    script_filename = f\"{prefix}_{template_filename}\"\n    script_path_local = Path(script_dir) / script_filename\n    # Read template\n    customize_template(\n        template_name=template_filename,\n        replacements=replacements,\n        script_path=script_path_local,\n    )\n    cmd = f\"bash {script_path_local}\"\n    logger.debug(f\"Now run '{cmd}' \")\n    stdout = execute_command_sync(command=cmd, logger_name=logger_name)\n    logger.debug(f\"_customize_and_run_template {template_filename} - END\")\n    return stdout\n</code></pre>"},{"location":"reference/tasks/v2/local/_utils/#fractal_server.tasks.v2.local._utils.check_task_files_exist","title":"<code>check_task_files_exist(task_list)</code>","text":"<p>Check that the modules listed in task commands point to existing files.</p> PARAMETER DESCRIPTION <code>task_list</code> <p> TYPE: <code>list[TaskCreate]</code> </p> Source code in <code>fractal_server/tasks/v2/local/_utils.py</code> <pre><code>def check_task_files_exist(task_list: list[TaskCreate]) -&gt; None:\n    \"\"\"\n    Check that the modules listed in task commands point to existing files.\n\n    Args:\n        task_list:\n    \"\"\"\n\n    for _task in task_list:\n        if _task.command_non_parallel is not None:\n            _task_path = _task.command_non_parallel.split()[-1]\n            if not Path(_task_path).exists():\n                raise FileNotFoundError(\n                    f\"Task `{_task.name}` has `command_non_parallel` \"\n                    f\"pointing to missing file `{_task_path}`.\"\n                )\n        if _task.command_parallel is not None:\n            _task_path = _task.command_parallel.split()[-1]\n            if not Path(_task_path).exists():\n                raise FileNotFoundError(\n                    f\"Task `{_task.name}` has `command_parallel` \"\n                    f\"pointing to missing file `{_task_path}`.\"\n                )\n</code></pre>"},{"location":"reference/tasks/v2/local/_utils/#fractal_server.tasks.v2.local._utils.edit_pyproject_toml_in_place_local","title":"<code>edit_pyproject_toml_in_place_local(pyproject_toml_path, resource)</code>","text":"<p>Wrapper of <code>simplify_pyproject_toml</code>, with I/O.</p> Source code in <code>fractal_server/tasks/v2/local/_utils.py</code> <pre><code>def edit_pyproject_toml_in_place_local(\n    pyproject_toml_path: Path,\n    resource: Resource,\n) -&gt; None:\n    \"\"\"\n    Wrapper of `simplify_pyproject_toml`, with I/O.\n    \"\"\"\n\n    # Read `pyproject.toml`\n    with pyproject_toml_path.open() as f:\n        pyproject_contents = f.read()\n\n    # Simplify contents\n    new_pyproject_contents = simplify_pyproject_toml(\n        original_toml_string=pyproject_contents,\n        pixi_environment=resource.tasks_pixi_config[\"DEFAULT_ENVIRONMENT\"],\n        pixi_platform=resource.tasks_pixi_config[\"DEFAULT_PLATFORM\"],\n    )\n    # Write new `pyproject.toml`\n    with pyproject_toml_path.open(\"w\") as f:\n        f.write(new_pyproject_contents)\n    logger.debug(\n        f\"Replaced local {pyproject_toml_path.as_posix()} \"\n        \"with simplified version.\"\n    )\n</code></pre>"},{"location":"reference/tasks/v2/local/collect/","title":"collect","text":""},{"location":"reference/tasks/v2/local/collect/#fractal_server.tasks.v2.local.collect.collect_local","title":"<code>collect_local(*, task_group_activity_id, task_group_id, resource, profile, wheel_file=None)</code>","text":"<p>Collect a task package.</p> <p>This function runs as a background task, therefore exceptions must be handled.</p> <p>NOTE:  since this function is sync, it runs within a thread - due to starlette/fastapi handling of background tasks (see https://github.com/encode/starlette/blob/master/starlette/background.py).</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>resource</code> <p>Resource</p> <p> TYPE: <code>Resource</code> </p> <code>wheel_file</code> <p> TYPE: <code>FractalUploadedFile | None</code> DEFAULT: <code>None</code> </p> Source code in <code>fractal_server/tasks/v2/local/collect.py</code> <pre><code>def collect_local(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n    wheel_file: FractalUploadedFile | None = None,\n) -&gt; None:\n    \"\"\"\n    Collect a task package.\n\n    This function runs as a background task, therefore exceptions must be\n    handled.\n\n    NOTE:  since this function is sync, it runs within a thread - due to\n    starlette/fastapi handling of background tasks (see\n    https://github.com/encode/starlette/blob/master/starlette/background.py).\n\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource: Resource\n        wheel_file:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.info(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            # Check that the (local) task_group path does exist\n            if Path(task_group.path).exists():\n                error_msg = f\"{task_group.path} already exists.\"\n                logger.error(error_msg)\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=FileExistsError(error_msg),\n                    db=db,\n                )\n                return\n\n            try:\n                # Create task_group.path folder\n                Path(task_group.path).mkdir(parents=True)\n                logger.info(f\"Created {task_group.path}\")\n\n                # Write wheel file and set task_group.archive_path\n                if wheel_file is not None:\n                    archive_path = (\n                        Path(task_group.path) / wheel_file.filename\n                    ).as_posix()\n                    logger.info(\n                        f\"Write wheel-file contents into {archive_path}\"\n                    )\n                    with open(archive_path, \"wb\") as f:\n                        f.write(wheel_file.contents)\n                    task_group.archive_path = archive_path\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n\n                # Prepare replacements for templates\n                python_bin = get_python_interpreter(\n                    python_version=task_group.python_version,\n                    resource=resource,\n                )\n                replacements = get_collection_replacements(\n                    task_group=task_group,\n                    python_bin=python_bin,\n                    resource=resource,\n                )\n\n                # Prepare common arguments for `_customize_and_run_template``\n                common_args = dict(\n                    replacements=replacements,\n                    script_dir=(\n                        Path(task_group.path) / SCRIPTS_SUBFOLDER\n                    ).as_posix(),\n                    prefix=(\n                        f\"{int(time.time())}_{TaskGroupActivityAction.COLLECT}\"\n                    ),\n                    logger_name=LOGGER_NAME,\n                )\n\n                # Set status to ONGOING and refresh logs\n                activity.status = TaskGroupActivityStatus.ONGOING\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 1\n                stdout = _customize_and_run_template(\n                    template_filename=\"1_create_venv.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 2\n                stdout = _customize_and_run_template(\n                    template_filename=\"2_pip_install.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 3\n                pip_freeze_stdout = _customize_and_run_template(\n                    template_filename=\"3_pip_freeze.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 4\n                stdout = _customize_and_run_template(\n                    template_filename=\"4_pip_show.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                pkg_attrs = parse_script_pip_show_stdout(stdout)\n                for key, value in pkg_attrs.items():\n                    logger.debug(f\"Parsed from pip-show: {key}={value}\")\n                # Check package_name match between pip show and task-group\n                task_group = db.get(TaskGroupV2, task_group_id)\n                package_name_pip_show = pkg_attrs.get(\"package_name\")\n                package_name_task_group = task_group.pkg_name\n                compare_package_names(\n                    pkg_name_pip_show=package_name_pip_show,\n                    pkg_name_task_group=package_name_task_group,\n                    logger_name=LOGGER_NAME,\n                )\n                # Extract/drop parsed attributes\n                package_name = package_name_task_group\n                python_bin = pkg_attrs.pop(\"python_bin\")\n                package_root_parent = pkg_attrs.pop(\"package_root_parent\")\n\n                # TODO : Use more robust logic to determine `package_root`.\n                # Examples: use `importlib.util.find_spec`, or parse the\n                # output of `pip show --files {package_name}`.\n                package_name_underscore = package_name.replace(\"-\", \"_\")\n                package_root = (\n                    Path(package_root_parent) / package_name_underscore\n                ).as_posix()\n\n                # Read and validate manifest file\n                manifest_path = pkg_attrs.pop(\"manifest_path\")\n                logger.info(f\"now loading {manifest_path=}\")\n                with open(manifest_path) as json_data:\n                    pkg_manifest_dict = json.load(json_data)\n                logger.info(f\"loaded {manifest_path=}\")\n                logger.info(\"now validating manifest content\")\n                pkg_manifest = ManifestV2(**pkg_manifest_dict)\n                logger.info(\"validated manifest content\")\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                logger.info(\"_prepare_tasks_metadata - start\")\n                task_list = prepare_tasks_metadata(\n                    package_manifest=pkg_manifest,\n                    package_version=task_group.version,\n                    package_root=Path(package_root),\n                    python_bin=Path(python_bin),\n                )\n                check_task_files_exist(task_list=task_list)\n                logger.info(\"_prepare_tasks_metadata - end\")\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                logger.info(\"create_db_tasks_and_update_task_group - start\")\n                create_db_tasks_and_update_task_group_sync(\n                    task_list=task_list,\n                    task_group_id=task_group.id,\n                    db=db,\n                )\n                logger.info(\"create_db_tasks_and_update_task_group - end\")\n\n                # Update task_group data\n                logger.info(\"Add env_info to TaskGroupV2 - start\")\n                task_group.env_info = pip_freeze_stdout\n                task_group = add_commit_refresh(obj=task_group, db=db)\n                logger.info(\"Add env_info to TaskGroupV2 - end\")\n\n                # Finalize (write metadata to DB)\n                logger.info(\"finalising - START\")\n                activity.status = TaskGroupActivityStatus.OK\n                activity.timestamp_ended = get_timestamp()\n                activity = add_commit_refresh(obj=activity, db=db)\n                logger.info(\"finalising - END\")\n                logger.info(\"END\")\n\n                reset_logger_handlers(logger)\n\n            except Exception as collection_e:\n                # Delete corrupted package dir\n                try:\n                    logger.info(f\"Now delete folder {task_group.path}\")\n                    shutil.rmtree(task_group.path)\n                    logger.info(f\"Deleted folder {task_group.path}\")\n                except Exception as rm_e:\n                    logger.error(\n                        f\"Removing folder failed.\\nOriginal error:\\n{str(rm_e)}\"\n                    )\n\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=collection_e,\n                    db=db,\n                )\n        return\n</code></pre>"},{"location":"reference/tasks/v2/local/collect_pixi/","title":"collect_pixi","text":""},{"location":"reference/tasks/v2/local/deactivate/","title":"deactivate","text":""},{"location":"reference/tasks/v2/local/deactivate/#fractal_server.tasks.v2.local.deactivate.deactivate_local","title":"<code>deactivate_local(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Deactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>resource</code> <p> TYPE: <code>Resource</code> </p> Source code in <code>fractal_server/tasks/v2/local/deactivate.py</code> <pre><code>def deactivate_local(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Deactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            # Check that the (local) task_group venv_path does exist\n            if not Path(task_group.venv_path).exists():\n                error_msg = f\"{task_group.venv_path} does not exist.\"\n                logger.error(error_msg)\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=FileNotFoundError(error_msg),\n                    db=db,\n                )\n                return\n\n            try:\n                activity.status = TaskGroupActivityStatus.ONGOING\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                if task_group.env_info is None:\n                    logger.warning(\n                        \"Recreate pip-freeze information, since \"\n                        f\"{task_group.env_info=}. NOTE: this should only \"\n                        \"happen for task groups created before 2.9.0.\"\n                    )\n                    # Prepare replacements for templates\n                    replacements = get_collection_replacements(\n                        task_group=task_group,\n                        python_bin=\"/not/applicable\",\n                        resource=resource,\n                    )\n\n                    # Prepare common arguments for _customize_and_run_template\n                    common_args = dict(\n                        replacements=replacements,\n                        script_dir=(\n                            Path(task_group.path) / SCRIPTS_SUBFOLDER\n                        ).as_posix(),\n                        prefix=(\n                            f\"{int(time.time())}_\"\n                            f\"{TaskGroupActivityAction.DEACTIVATE}\"\n                        ),\n                        logger_name=LOGGER_NAME,\n                    )\n\n                    # Update pip-freeze data\n                    pip_freeze_stdout = _customize_and_run_template(\n                        template_filename=\"3_pip_freeze.sh\",\n                        **common_args,\n                    )\n                    logger.info(\"Add pip freeze stdout to TaskGroupV2 - start\")\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n                    task_group.env_info = pip_freeze_stdout\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n                    logger.info(\"Add pip freeze stdout to TaskGroupV2 - end\")\n\n                # Handle some specific cases for wheel-file case\n                if task_group.origin == TaskGroupOriginEnum.WHEELFILE:\n                    logger.info(\n                        f\"Handle specific cases for {task_group.origin=}.\"\n                    )\n\n                    # Blocking situation: `archive_path` is not set or points\n                    # to a missing path\n                    if (\n                        task_group.archive_path is None\n                        or not Path(task_group.archive_path).exists()\n                    ):\n                        error_msg = (\n                            \"Invalid wheel path for task group with \"\n                            f\"{task_group_id=}. {task_group.archive_path=} is \"\n                            \"unset or does not exist.\"\n                        )\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileNotFoundError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    # Recoverable situation: `archive_path` was not yet copied\n                    # over to the correct server-side folder\n                    archive_path_parent_dir = Path(\n                        task_group.archive_path\n                    ).parent\n                    if archive_path_parent_dir != Path(task_group.path):\n                        logger.warning(\n                            f\"{archive_path_parent_dir.as_posix()} differs \"\n                            f\"from {task_group.path}. NOTE: this should only \"\n                            \"happen for task groups created before 2.9.0.\"\n                        )\n\n                        if task_group.archive_path not in task_group.env_info:\n                            raise ValueError(\n                                f\"Cannot find {task_group.archive_path=} in \"\n                                \"pip-freeze data. Exit.\"\n                            )\n\n                        logger.info(\n                            f\"Now copy wheel file into {task_group.path}.\"\n                        )\n                        new_archive_path = (\n                            Path(task_group.path)\n                            / Path(task_group.archive_path).name\n                        ).as_posix()\n                        shutil.copy(task_group.archive_path, new_archive_path)\n                        logger.info(f\"Copied wheel file to {new_archive_path}.\")\n\n                        task_group.archive_path = new_archive_path\n                        new_pip_freeze = task_group.env_info.replace(\n                            task_group.archive_path,\n                            new_archive_path,\n                        )\n                        task_group.env_info = new_pip_freeze\n                        task_group = add_commit_refresh(obj=task_group, db=db)\n                        logger.info(\n                            \"Updated `archive_path` and `env_info` \"\n                            \"task-group attributes.\"\n                        )\n\n                # Fail if `pip_freeze` includes \"github.com\", see\n                # https://github.com/fractal-analytics-platform/fractal-server/issues/2142\n                for forbidden_string in FORBIDDEN_DEPENDENCY_STRINGS:\n                    if forbidden_string in task_group.env_info:\n                        raise ValueError(\n                            \"Deactivation and reactivation of task packages \"\n                            f\"with direct {forbidden_string} dependencies \"\n                            \"are not currently supported. Exit.\"\n                        )\n\n                # We now have all required information for reactivating the\n                # virtual environment at a later point\n\n                # Actually mark the task group as non-active\n                logger.info(\"Now setting `active=False`.\")\n                task_group.active = False\n                task_group = add_commit_refresh(obj=task_group, db=db)\n\n                # Proceed with deactivation\n                logger.info(f\"Now removing {task_group.venv_path}.\")\n                shutil.rmtree(task_group.venv_path)\n                logger.info(f\"All good, {task_group.venv_path} removed.\")\n                activity.status = TaskGroupActivityStatus.OK\n                activity.log = get_current_log(log_file_path)\n                activity.timestamp_ended = get_timestamp()\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                reset_logger_handlers(logger)\n\n            except Exception as e:\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=e,\n                    db=db,\n                )\n        return\n</code></pre>"},{"location":"reference/tasks/v2/local/deactivate_pixi/","title":"deactivate_pixi","text":""},{"location":"reference/tasks/v2/local/deactivate_pixi/#fractal_server.tasks.v2.local.deactivate_pixi.deactivate_local_pixi","title":"<code>deactivate_local_pixi(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Deactivate a pixi task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> Source code in <code>fractal_server/tasks/v2/local/deactivate_pixi.py</code> <pre><code>def deactivate_local_pixi(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Deactivate a pixi task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            source_dir = Path(task_group.path, SOURCE_DIR_NAME)\n            if not source_dir.exists():\n                error_msg = f\"'{source_dir.as_posix()}' does not exist.\"\n                logger.error(error_msg)\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=FileNotFoundError(error_msg),\n                    db=db,\n                )\n                return\n\n            try:\n                activity.status = TaskGroupActivityStatus.ONGOING\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Actually mark the task group as non-active\n                logger.info(\"Now setting `active=False`.\")\n                task_group.active = False\n                task_group = add_commit_refresh(obj=task_group, db=db)\n\n                # Proceed with deactivation\n                logger.info(f\"Now removing '{source_dir.as_posix()}'.\")\n                shutil.rmtree(source_dir)\n                logger.info(f\"All good, '{source_dir.as_posix()}' removed.\")\n                activity.status = TaskGroupActivityStatus.OK\n                activity.log = get_current_log(log_file_path)\n                activity.timestamp_ended = get_timestamp()\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                reset_logger_handlers(logger)\n\n            except Exception as e:\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=e,\n                    db=db,\n                )\n        return\n</code></pre>"},{"location":"reference/tasks/v2/local/delete/","title":"delete","text":""},{"location":"reference/tasks/v2/local/reactivate/","title":"reactivate","text":""},{"location":"reference/tasks/v2/local/reactivate/#fractal_server.tasks.v2.local.reactivate.reactivate_local","title":"<code>reactivate_local(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Reactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>resource</code> <p> TYPE: <code>Resource</code> </p> Source code in <code>fractal_server/tasks/v2/local/reactivate.py</code> <pre><code>def reactivate_local(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Reactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            # Check that the (local) task_group venv_path does not exist\n            if Path(task_group.venv_path).exists():\n                error_msg = f\"{task_group.venv_path} already exists.\"\n                logger.error(error_msg)\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=FileExistsError(error_msg),\n                    db=db,\n                )\n                return\n\n            try:\n                activity.status = TaskGroupActivityStatus.ONGOING\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Prepare replacements for templates\n                python_bin = get_python_interpreter(\n                    python_version=task_group.python_version,\n                    resource=resource,\n                )\n                replacements = get_collection_replacements(\n                    task_group=task_group,\n                    python_bin=python_bin,\n                    resource=resource,\n                )\n                with open(f\"{tmpdir}/pip_freeze.txt\", \"w\") as f:\n                    f.write(task_group.env_info)\n                replacements.append(\n                    (\"__PIP_FREEZE_FILE__\", f\"{tmpdir}/pip_freeze.txt\")\n                )\n                # Prepare common arguments for `_customize_and_run_template`\n                common_args = dict(\n                    replacements=replacements,\n                    script_dir=(\n                        Path(task_group.path) / SCRIPTS_SUBFOLDER\n                    ).as_posix(),\n                    prefix=(\n                        f\"{int(time.time())}_\"\n                        f\"{TaskGroupActivityAction.REACTIVATE}\"\n                    ),\n                    logger_name=LOGGER_NAME,\n                )\n\n                logger.debug(\"start - create venv\")\n                _customize_and_run_template(\n                    template_filename=\"1_create_venv.sh\",\n                    **common_args,\n                )\n                logger.debug(\"end - create venv\")\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                logger.debug(\"start - install from pip freeze\")\n                _customize_and_run_template(\n                    template_filename=\"5_pip_install_from_freeze.sh\",\n                    **common_args,\n                )\n                logger.debug(\"end - install from pip freeze\")\n                activity.log = get_current_log(log_file_path)\n                activity.status = TaskGroupActivityStatus.OK\n                activity.timestamp_ended = get_timestamp()\n                activity = add_commit_refresh(obj=activity, db=db)\n                task_group.active = True\n                task_group = add_commit_refresh(obj=task_group, db=db)\n                logger.debug(\"END\")\n\n                reset_logger_handlers(logger)\n\n            except Exception as reactivate_e:\n                # Delete corrupted venv_path\n                try:\n                    logger.info(f\"Now delete folder {task_group.venv_path}\")\n                    shutil.rmtree(task_group.venv_path)\n                    logger.info(f\"Deleted folder {task_group.venv_path}\")\n                except Exception as rm_e:\n                    logger.error(\n                        f\"Removing folder failed.\\nOriginal error:\\n{str(rm_e)}\"\n                    )\n\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=reactivate_e,\n                    db=db,\n                )\n        return\n</code></pre>"},{"location":"reference/tasks/v2/local/reactivate_pixi/","title":"reactivate_pixi","text":""},{"location":"reference/tasks/v2/local/reactivate_pixi/#fractal_server.tasks.v2.local.reactivate_pixi.reactivate_local_pixi","title":"<code>reactivate_local_pixi(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Reactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>resource</code> <p> TYPE: <code>Resource</code> </p> Source code in <code>fractal_server/tasks/v2/local/reactivate_pixi.py</code> <pre><code>def reactivate_local_pixi(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Reactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            source_dir = Path(task_group.path, SOURCE_DIR_NAME).as_posix()\n            if Path(source_dir).exists():\n                error_msg = f\"{source_dir} already exists.\"\n                logger.error(error_msg)\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=FileExistsError(error_msg),\n                    db=db,\n                )\n                return\n\n            try:\n                activity.status = TaskGroupActivityStatus.ONGOING\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                common_args = dict(\n                    replacements={\n                        (\n                            \"__PIXI_HOME__\",\n                            resource.tasks_pixi_config[\"versions\"][\n                                task_group.pixi_version\n                            ],\n                        ),\n                        (\"__PACKAGE_DIR__\", task_group.path),\n                        (\"__TAR_GZ_PATH__\", task_group.archive_path),\n                        (\n                            \"__IMPORT_PACKAGE_NAME__\",\n                            task_group.pkg_name.replace(\"-\", \"_\"),\n                        ),\n                        (\"__SOURCE_DIR_NAME__\", SOURCE_DIR_NAME),\n                        (\"__FROZEN_OPTION__\", \"--frozen\"),\n                        (\n                            \"__TOKIO_WORKER_THREADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"TOKIO_WORKER_THREADS\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_SOLVES__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_SOLVES\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_DOWNLOADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_DOWNLOADS\"\n                                ]\n                            ),\n                        ),\n                    },\n                    script_dir=Path(\n                        task_group.path, SCRIPTS_SUBFOLDER\n                    ).as_posix(),\n                    prefix=(\n                        f\"{int(time.time())}_\"\n                        f\"{TaskGroupActivityAction.REACTIVATE}\"\n                    ),\n                    logger_name=LOGGER_NAME,\n                )\n\n                # Run script 1 - extract tar.gz into `source_dir`\n                _customize_and_run_template(\n                    template_filename=\"pixi_1_extract.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Simplify `pyproject.toml`\n                pyproject_toml_path = Path(source_dir, \"pyproject.toml\")\n                edit_pyproject_toml_in_place_local(\n                    pyproject_toml_path, resource=resource\n                )\n\n                # Write pixi.lock into `source_dir`\n                logger.debug(f\"start - writing {source_dir}/pixi.lock\")\n                with Path(source_dir, \"pixi.lock\").open(\"w\") as f:\n                    f.write(task_group.env_info)\n                logger.debug(f\"end - writing {source_dir}/pixi.lock\")\n\n                # Run script 2 - run pixi-install command\n                _customize_and_run_template(\n                    template_filename=\"pixi_2_install.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Run script 3 - post-install\n                _customize_and_run_template(\n                    template_filename=\"pixi_3_post_install.sh\",\n                    **common_args,\n                )\n                activity.log = get_current_log(log_file_path)\n                activity = add_commit_refresh(obj=activity, db=db)\n\n                # Make task folder 755\n                source_dir = Path(task_group.path, SOURCE_DIR_NAME).as_posix()\n                command = f\"chmod -R 755 {source_dir}\"\n                execute_command_sync(\n                    command=command,\n                    logger_name=LOGGER_NAME,\n                )\n\n                activity.log = get_current_log(log_file_path)\n                activity.status = TaskGroupActivityStatus.OK\n                activity.timestamp_ended = get_timestamp()\n                activity = add_commit_refresh(obj=activity, db=db)\n                task_group.active = True\n                task_group = add_commit_refresh(obj=task_group, db=db)\n                logger.debug(\"END\")\n\n                reset_logger_handlers(logger)\n\n            except Exception as reactivate_e:\n                # Delete corrupted source_dir\n                try:\n                    logger.info(f\"Now delete folder {source_dir}\")\n                    shutil.rmtree(source_dir)\n                    logger.info(f\"Deleted folder {source_dir}\")\n                except Exception as rm_e:\n                    logger.error(\n                        f\"Removing folder failed. Original error: {str(rm_e)}\"\n                    )\n\n                fail_and_cleanup(\n                    task_group=task_group,\n                    task_group_activity=activity,\n                    logger_name=LOGGER_NAME,\n                    log_file_path=log_file_path,\n                    exception=reactivate_e,\n                    db=db,\n                )\n        return\n</code></pre>"},{"location":"reference/tasks/v2/ssh/","title":"ssh","text":""},{"location":"reference/tasks/v2/ssh/_pixi_slurm_ssh/","title":"_pixi_slurm_ssh","text":""},{"location":"reference/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh._get_workdir_remote","title":"<code>_get_workdir_remote(script_paths)</code>","text":"<p>Check that there is one and only one <code>workdir</code>, and return it.</p> <p>Note: The <code>is_absolute</code> check is to filter out a <code>chmod</code> command.</p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def _get_workdir_remote(script_paths: list[str]) -&gt; str:\n    \"\"\"\n    Check that there is one and only one `workdir`, and return it.\n\n    Note: The `is_absolute` check is to filter out a `chmod` command.\n    \"\"\"\n    workdirs = [\n        Path(script_path).parent.as_posix()\n        for script_path in script_paths\n        if Path(script_path).is_absolute()\n    ]\n    if not len(set(workdirs)) == 1:\n        raise ValueError(f\"Invalid {script_paths=}.\")\n    return workdirs[0]\n</code></pre>"},{"location":"reference/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh._log_change_of_job_state","title":"<code>_log_change_of_job_state(*, old_state, new_state, logger_name)</code>","text":"<p>Emit a log for state changes.</p> PARAMETER DESCRIPTION <code>old_state</code> <p> TYPE: <code>str | None</code> </p> <code>new_state</code> <p> TYPE: <code>str</code> </p> <code>logger_name</code> <p> TYPE: <code>str</code> </p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def _log_change_of_job_state(\n    *,\n    old_state: str | None,\n    new_state: str,\n    logger_name: str,\n) -&gt; None:\n    \"\"\"\n    Emit a log for state changes.\n\n    Args:\n        old_state:\n        new_state:\n        logger_name:\n    \"\"\"\n    if new_state != old_state:\n        logger = get_logger(logger_name=logger_name)\n        logger.debug(\n            f\"SLURM-job state changed from {old_state=} to {new_state=}.\"\n        )\n</code></pre>"},{"location":"reference/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh._read_file_if_exists","title":"<code>_read_file_if_exists(*, fractal_ssh, path)</code>","text":"<p>Read a remote file if it exists, or return an empty string.</p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def _read_file_if_exists(\n    *,\n    fractal_ssh: FractalSSH,\n    path: str,\n) -&gt; str:\n    \"\"\"\n    Read a remote file if it exists, or return an empty string.\n    \"\"\"\n    if fractal_ssh.remote_exists(path=path):\n        return fractal_ssh.read_remote_text_file(path)\n    else:\n        return \"\"\n</code></pre>"},{"location":"reference/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh._run_squeue","title":"<code>_run_squeue(*, fractal_ssh, squeue_cmd, logger_name)</code>","text":"<p>Run a <code>squeue</code> command and handle exceptions.</p> PARAMETER DESCRIPTION <code>fractal_ssh</code> <p> TYPE: <code>FractalSSH</code> </p> <code>logger_name</code> <p> TYPE: <code>str</code> </p> <code>squeue_cmd</code> <p> TYPE: <code>str</code> </p> Return <p>state: The SLURM-job state.</p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def _run_squeue(\n    *,\n    fractal_ssh: FractalSSH,\n    squeue_cmd: str,\n    logger_name: str,\n) -&gt; str:\n    \"\"\"\n    Run a `squeue` command and handle exceptions.\n\n    Args:\n        fractal_ssh:\n        logger_name:\n        squeue_cmd:\n\n    Return:\n        state: The SLURM-job state.\n    \"\"\"\n    try:\n        cmd_stdout = fractal_ssh.run_command(cmd=squeue_cmd)\n        state = cmd_stdout.strip().split()[1]\n        return state\n    except Exception as e:\n        logger = get_logger(logger_name=logger_name)\n        logger.info(f\"`squeue` command failed (original error: {e})\")\n        return FRACTAL_SQUEUE_ERROR_STATE\n</code></pre>"},{"location":"reference/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh._verify_success_file_exists","title":"<code>_verify_success_file_exists(*, fractal_ssh, success_file_remote, logger_name, stderr_remote)</code>","text":"<p>Fail if the success sentinel file does not exist remotely.</p> <p>Note: the <code>FractalSSH</code> methods in this function may fail, and such failures are not handled in this function. Any such failure, however, will lead to a \"failed\" task-group lifecycle activity (because it will raise an exception from within <code>run_script_on_remote_slurm</code>, which will then be handled at the calling-function level.</p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def _verify_success_file_exists(\n    *,\n    fractal_ssh: FractalSSH,\n    success_file_remote: str,\n    logger_name: str,\n    stderr_remote: str,\n) -&gt; None:\n    \"\"\"\n    Fail if the success sentinel file does not exist remotely.\n\n    Note: the `FractalSSH` methods in this function may fail, and such failures\n    are not handled in this function. Any such failure, however, will lead to\n    a \"failed\" task-group lifecycle activity (because it will raise an\n    exception from within `run_script_on_remote_slurm`, which will then be\n    handled at the calling-function level.\n    \"\"\"\n    if not fractal_ssh.remote_exists(path=success_file_remote):\n        logger = get_logger(logger_name=logger_name)\n        error_msg = f\"{success_file_remote=} missing.\"\n        logger.info(error_msg)\n\n        stderr = _read_file_if_exists(\n            fractal_ssh=fractal_ssh, path=stderr_remote\n        )\n        if stderr:\n            logger.info(f\"SLURM-job stderr:\\n{stderr}\")\n        raise RuntimeError(error_msg)\n</code></pre>"},{"location":"reference/tasks/v2/ssh/_pixi_slurm_ssh/#fractal_server.tasks.v2.ssh._pixi_slurm_ssh.run_script_on_remote_slurm","title":"<code>run_script_on_remote_slurm(*, script_paths, slurm_config, fractal_ssh, logger_name, log_file_path, prefix, db, activity, poll_interval)</code>","text":"<p>Run a <code>pixi install</code> script as a SLURM job.</p> <p>NOTE: This is called from within a try/except, thus we can use exceptions as a mechanism to propagate failure/errors.</p> Source code in <code>fractal_server/tasks/v2/ssh/_pixi_slurm_ssh.py</code> <pre><code>def run_script_on_remote_slurm(\n    *,\n    script_paths: list[str],\n    slurm_config: dict[str, Any],\n    fractal_ssh: FractalSSH,\n    logger_name: str,\n    log_file_path: Path,\n    prefix: str,\n    db: Session,\n    activity: TaskGroupActivityV2,\n    poll_interval: int,\n):\n    \"\"\"\n    Run a `pixi install` script as a SLURM job.\n\n    NOTE: This is called from within a try/except, thus we can use exceptions\n    as a mechanism to propagate failure/errors.\n    \"\"\"\n\n    slurm_config_obj = PixiSLURMConfig(**slurm_config)\n\n    logger = get_logger(logger_name=logger_name)\n\n    # (1) Prepare remote submission script\n    workdir_remote = _get_workdir_remote(script_paths)\n    submission_script_remote = os.path.join(\n        workdir_remote, f\"{prefix}-submit.sh\"\n    )\n    stderr_remote = os.path.join(workdir_remote, f\"{prefix}-err.txt\")\n    stdout_remote = os.path.join(workdir_remote, f\"{prefix}-out.txt\")\n    success_file_remote = os.path.join(workdir_remote, f\"{prefix}-success.txt\")\n    if slurm_config_obj.mem is not None:\n        memory_line = f\"#SBATCH --mem={slurm_config_obj.mem}\"\n    else:\n        memory_line = f\"#SBATCH --mem-per-cpu={slurm_config_obj.mem_per_cpu}\"\n    script_lines = [\n        \"#!/bin/bash\",\n        f\"#SBATCH --partition={slurm_config_obj.partition}\",\n        f\"#SBATCH --cpus-per-task={slurm_config_obj.cpus}\",\n        memory_line,\n        f\"#SBATCH --time={slurm_config_obj.time}\",\n        f\"#SBATCH --err={stderr_remote}\",\n        f\"#SBATCH --out={stdout_remote}\",\n        f\"#SBATCH -D {workdir_remote}\",\n        \"\",\n    ]\n    script_lines.extend(slurm_config_obj.preamble)\n    for script_path in script_paths:\n        script_lines.append(f\"bash {script_path}\")\n    script_lines.append(f\"touch {success_file_remote}\")\n\n    script_contents = \"\\n\".join(script_lines)\n    fractal_ssh.write_remote_file(\n        path=submission_script_remote,\n        content=script_contents,\n    )\n    logger.debug(f\"Written {submission_script_remote=}.\")\n\n    activity.log = get_current_log(log_file_path)\n    activity = add_commit_refresh(obj=activity, db=db)\n\n    # (2) Submit SLURM job\n    logger.debug(\"Now submit SLURM job.\")\n    sbatch_cmd = f\"sbatch --parsable {submission_script_remote}\"\n    try:\n        stdout = fractal_ssh.run_command(cmd=sbatch_cmd)\n        job_id = int(stdout)\n        logger.debug(f\"SLURM-job submission successful ({job_id=}).\")\n    except Exception as e:\n        logger.error(\n            (\n                f\"Submission of {submission_script_remote} failed. \"\n                f\"Original error: {str(e)}\"\n            )\n        )\n        raise e\n    finally:\n        activity.log = get_current_log(log_file_path)\n        activity = add_commit_refresh(obj=activity, db=db)\n\n    # (3) Monitor job\n    squeue_cmd = (\n        f\"squeue --noheader --format='%i %T' --states=all --jobs={job_id}\"\n    )\n    logger.debug(f\"Start monitoring job with {squeue_cmd=}.\")\n    old_state = None\n    while True:\n        new_state = _run_squeue(\n            fractal_ssh=fractal_ssh,\n            squeue_cmd=squeue_cmd,\n            logger_name=logger_name,\n        )\n        _log_change_of_job_state(\n            old_state=old_state,\n            new_state=new_state,\n            logger_name=logger_name,\n        )\n        activity.log = get_current_log(log_file_path)\n        activity = add_commit_refresh(obj=activity, db=db)\n        if new_state in STATES_FINISHED:\n            logger.debug(f\"Exit retrieval loop (state={new_state}).\")\n            break\n        old_state = new_state\n        time.sleep(poll_interval)\n\n    _verify_success_file_exists(\n        fractal_ssh=fractal_ssh,\n        logger_name=logger_name,\n        success_file_remote=success_file_remote,\n        stderr_remote=stderr_remote,\n    )\n\n    stdout = _read_file_if_exists(\n        fractal_ssh=fractal_ssh,\n        path=stdout_remote,\n    )\n\n    logger.info(\"SLURM-job execution completed successfully, continue.\")\n    activity.log = get_current_log(log_file_path)\n    activity = add_commit_refresh(obj=activity, db=db)\n\n    return stdout\n</code></pre>"},{"location":"reference/tasks/v2/ssh/_utils/","title":"_utils","text":""},{"location":"reference/tasks/v2/ssh/_utils/#fractal_server.tasks.v2.ssh._utils._copy_wheel_file_ssh","title":"<code>_copy_wheel_file_ssh(*, task_group, fractal_ssh, logger_name)</code>","text":"<p>Handle the situation where <code>task_group.archive_path</code> is not part of <code>task_group.path</code>, by copying <code>archive_path</code> into <code>path</code>.</p> RETURNS DESCRIPTION <code>str</code> <p>The new <code>archive_path</code>.</p> Source code in <code>fractal_server/tasks/v2/ssh/_utils.py</code> <pre><code>def _copy_wheel_file_ssh(\n    *,\n    task_group: TaskGroupV2,\n    fractal_ssh: FractalSSH,\n    logger_name: str,\n) -&gt; str:\n    \"\"\"\n    Handle the situation where `task_group.archive_path` is not part of\n    `task_group.path`, by copying `archive_path` into `path`.\n\n    Returns:\n        The new `archive_path`.\n    \"\"\"\n    logger = get_logger(logger_name=logger_name)\n    source = task_group.archive_path\n    dest = (\n        Path(task_group.path) / Path(task_group.archive_path).name\n    ).as_posix()\n    cmd = f\"cp {source} {dest}\"\n    logger.debug(f\"[_copy_wheel_file_ssh] START {source=} {dest=}\")\n    fractal_ssh.run_command(cmd=cmd)\n    logger.debug(f\"[_copy_wheel_file_ssh] END {source=} {dest=}\")\n    return dest\n</code></pre>"},{"location":"reference/tasks/v2/ssh/_utils/#fractal_server.tasks.v2.ssh._utils._customize_and_run_template","title":"<code>_customize_and_run_template(*, template_filename, replacements, script_dir_local, script_dir_remote, prefix, fractal_ssh, logger_name)</code>","text":"<p>Customize one of the template bash scripts, transfer it to the remote host via SFTP and then run it via SSH.</p> PARAMETER DESCRIPTION <code>template_filename</code> <p>Filename of the template file (ends with \".sh\").</p> <p> TYPE: <code>str</code> </p> <code>replacements</code> <p>Dictionary of replacements.</p> <p> TYPE: <code>set[tuple[str, str]]</code> </p> <code>script_dir_remote</code> <p>Remote scripts directory</p> <p> TYPE: <code>str</code> </p> <code>script_dir_local</code> <p>Local folder where the script will be placed.</p> <p> TYPE: <code>str</code> </p> <code>prefix</code> <p>Prefix for the script filename.</p> <p> TYPE: <code>str</code> </p> <code>fractal_ssh</code> <p>FractalSSH object</p> <p> TYPE: <code>FractalSSH</code> </p> Source code in <code>fractal_server/tasks/v2/ssh/_utils.py</code> <pre><code>def _customize_and_run_template(\n    *,\n    template_filename: str,\n    replacements: set[tuple[str, str]],\n    script_dir_local: str,\n    script_dir_remote: str,\n    prefix: str,\n    fractal_ssh: FractalSSH,\n    logger_name: str,\n) -&gt; str:\n    \"\"\"\n    Customize one of the template bash scripts, transfer it to the remote host\n    via SFTP and then run it via SSH.\n\n    Args:\n        template_filename: Filename of the template file (ends with \".sh\").\n        replacements: Dictionary of replacements.\n        script_dir_remote: Remote scripts directory\n        script_dir_local: Local folder where the script will be placed.\n        prefix: Prefix for the script filename.\n        fractal_ssh: FractalSSH object\n    \"\"\"\n    logger = get_logger(logger_name=logger_name)\n    logger.debug(f\"_customize_and_run_template {template_filename} - START\")\n\n    script_path_remote = _customize_and_send_template(\n        template_filename=template_filename,\n        replacements=replacements,\n        script_dir_local=script_dir_local,\n        script_dir_remote=script_dir_remote,\n        prefix=prefix,\n        fractal_ssh=fractal_ssh,\n        logger_name=logger_name,\n    )\n\n    # Execute script remotely\n    cmd = f\"bash {script_path_remote}\"\n    logger.debug(f\"Now run '{cmd}' over SSH.\")\n    stdout = fractal_ssh.run_command(cmd=cmd)\n\n    logger.debug(f\"_customize_and_run_template {template_filename} - END\")\n    return stdout\n</code></pre>"},{"location":"reference/tasks/v2/ssh/_utils/#fractal_server.tasks.v2.ssh._utils._customize_and_send_template","title":"<code>_customize_and_send_template(*, template_filename, replacements, script_dir_local, script_dir_remote, prefix, fractal_ssh, logger_name)</code>","text":"<p>Customize a template bash scripts and transfer it to the remote host.</p> PARAMETER DESCRIPTION <code>template_filename</code> <p>Filename of the template file (ends with \".sh\").</p> <p> TYPE: <code>str</code> </p> <code>replacements</code> <p>Dictionary of replacements.</p> <p> TYPE: <code>set[tuple[str, str]]</code> </p> <code>script_dir_local</code> <p>Local folder where the script will be placed.</p> <p> TYPE: <code>str</code> </p> <code>script_dir_remote</code> <p>Remote scripts directory</p> <p> TYPE: <code>str</code> </p> <code>prefix</code> <p>Prefix for the script filename.</p> <p> TYPE: <code>str</code> </p> <code>fractal_ssh</code> <p>FractalSSH object</p> <p> TYPE: <code>FractalSSH</code> </p> Source code in <code>fractal_server/tasks/v2/ssh/_utils.py</code> <pre><code>def _customize_and_send_template(\n    *,\n    template_filename: str,\n    replacements: set[tuple[str, str]],\n    script_dir_local: str,\n    script_dir_remote: str,\n    prefix: str,\n    fractal_ssh: FractalSSH,\n    logger_name: str,\n) -&gt; str:\n    \"\"\"\n    Customize a template bash scripts and transfer it to the remote host.\n\n    Args:\n        template_filename: Filename of the template file (ends with \".sh\").\n        replacements: Dictionary of replacements.\n        script_dir_local: Local folder where the script will be placed.\n        script_dir_remote: Remote scripts directory\n        prefix: Prefix for the script filename.\n        fractal_ssh: FractalSSH object\n    \"\"\"\n    logger = get_logger(logger_name=logger_name)\n    logger.debug(f\"_customize_and_run_template {template_filename} - START\")\n    # Prepare name and path of script\n    if not template_filename.endswith(\".sh\"):\n        raise ValueError(\n            f\"Invalid {template_filename=} (it must end with '.sh').\"\n        )\n    script_filename = f\"{prefix}_{template_filename}\"\n    script_path_local = (Path(script_dir_local) / script_filename).as_posix()\n\n    customize_template(\n        template_name=template_filename,\n        replacements=replacements,\n        script_path=script_path_local,\n    )\n\n    # Transfer script to remote host\n    script_path_remote = os.path.join(\n        script_dir_remote,\n        script_filename,\n    )\n    logger.debug(f\"Now transfer {script_path_local=} over SSH.\")\n    fractal_ssh.send_file(\n        local=script_path_local,\n        remote=script_path_remote,\n    )\n    return script_path_remote\n</code></pre>"},{"location":"reference/tasks/v2/ssh/_utils/#fractal_server.tasks.v2.ssh._utils.check_ssh_or_fail_and_cleanup","title":"<code>check_ssh_or_fail_and_cleanup(*, fractal_ssh, task_group, task_group_activity, logger_name, log_file_path, db)</code>","text":"<p>Check SSH connection.</p> RETURNS DESCRIPTION <code>bool</code> <p>Whether SSH connection is OK.</p> Source code in <code>fractal_server/tasks/v2/ssh/_utils.py</code> <pre><code>def check_ssh_or_fail_and_cleanup(\n    *,\n    fractal_ssh: FractalSSH,\n    task_group: TaskGroupV2,\n    task_group_activity: TaskGroupActivityV2,\n    logger_name: str,\n    log_file_path: Path,\n    db: Session,\n) -&gt; bool:\n    \"\"\"\n    Check SSH connection.\n\n    Returns:\n        Whether SSH connection is OK.\n    \"\"\"\n    try:\n        fractal_ssh.check_connection()\n        return True\n    except Exception as e:\n        logger = get_logger(logger_name=logger_name)\n        logger.error(\n            f\"Cannot establish SSH connection. Original error: {str(e)}\"\n        )\n        fail_and_cleanup(\n            task_group=task_group,\n            task_group_activity=task_group_activity,\n            logger_name=logger_name,\n            log_file_path=log_file_path,\n            exception=e,\n            db=db,\n        )\n        return False\n</code></pre>"},{"location":"reference/tasks/v2/ssh/_utils/#fractal_server.tasks.v2.ssh._utils.edit_pyproject_toml_in_place_ssh","title":"<code>edit_pyproject_toml_in_place_ssh(*, fractal_ssh, pyproject_toml_path, resource)</code>","text":"<p>Wrapper of <code>simplify_pyproject_toml</code>, with I/O.</p> Source code in <code>fractal_server/tasks/v2/ssh/_utils.py</code> <pre><code>def edit_pyproject_toml_in_place_ssh(\n    *,\n    fractal_ssh: FractalSSH,\n    pyproject_toml_path: Path,\n    resource: Resource,\n) -&gt; None:\n    \"\"\"\n    Wrapper of `simplify_pyproject_toml`, with I/O.\n    \"\"\"\n\n    # Read `pyproject.toml`\n    pyproject_contents = fractal_ssh.read_remote_text_file(\n        pyproject_toml_path.as_posix()\n    )\n\n    # Simplify contents\n    new_pyproject_contents = simplify_pyproject_toml(\n        original_toml_string=pyproject_contents,\n        pixi_environment=resource.tasks_pixi_config[\"DEFAULT_ENVIRONMENT\"],\n        pixi_platform=resource.tasks_pixi_config[\"DEFAULT_PLATFORM\"],\n    )\n    # Write new `pyproject.toml`\n    fractal_ssh.write_remote_file(\n        path=pyproject_toml_path.as_posix(),\n        content=new_pyproject_contents,\n    )\n    logger.debug(\n        f\"Replaced remote {pyproject_toml_path.as_posix()} \"\n        \"with simplified version.\"\n    )\n</code></pre>"},{"location":"reference/tasks/v2/ssh/collect/","title":"collect","text":""},{"location":"reference/tasks/v2/ssh/collect/#fractal_server.tasks.v2.ssh.collect.collect_ssh","title":"<code>collect_ssh(*, task_group_id, task_group_activity_id, resource, profile, wheel_file=None)</code>","text":"<p>Collect a task package over SSH</p> <p>This function runs as a background task, therefore exceptions must be handled.</p> <p>NOTE: since this function is sync, it runs within a thread - due to starlette/fastapi handling of background tasks (see https://github.com/encode/starlette/blob/master/starlette/background.py).</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>resource</code> <p> TYPE: <code>Resource</code> </p> <code>profile</code> <p> TYPE: <code>Profile</code> </p> <code>wheel_file</code> <p> TYPE: <code>FractalUploadedFile | None</code> DEFAULT: <code>None</code> </p> Source code in <code>fractal_server/tasks/v2/ssh/collect.py</code> <pre><code>def collect_ssh(\n    *,\n    task_group_id: int,\n    task_group_activity_id: int,\n    resource: Resource,\n    profile: Profile,\n    wheel_file: FractalUploadedFile | None = None,\n) -&gt; None:\n    \"\"\"\n    Collect a task package over SSH\n\n    This function runs as a background task, therefore exceptions must be\n    handled.\n\n    NOTE: since this function is sync, it runs within a thread - due to\n    starlette/fastapi handling of background tasks (see\n    https://github.com/encode/starlette/blob/master/starlette/background.py).\n\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n        profile:\n        wheel_file:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    # Work within a temporary folder, where also logs will be placed\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = Path(tmpdir) / \"log\"\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.info(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (remote) task_group path does not exist\n                    # NOTE: this is not part of the try/except below, in order\n                    # to avoid removing the existing folder (as part of the\n                    # exception-handling).\n                    if fractal_ssh.remote_exists(task_group.path):\n                        error_msg = f\"{task_group.path} already exists.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileExistsError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    # Create remote `task_group.path` and `script_dir_remote`\n                    # folders\n                    script_dir_remote = (\n                        Path(task_group.path) / SCRIPTS_SUBFOLDER\n                    ).as_posix()\n                    fractal_ssh.mkdir(folder=task_group.path, parents=True)\n                    fractal_ssh.mkdir(folder=script_dir_remote, parents=True)\n\n                    # Write wheel file locally and send it to remote path,\n                    # and set task_group.archive_path\n                    if wheel_file is not None:\n                        wheel_filename = wheel_file.filename\n                        archive_path = (\n                            Path(task_group.path) / wheel_filename\n                        ).as_posix()\n                        tmp_archive_path = (\n                            Path(tmpdir) / wheel_filename\n                        ).as_posix()\n                        logger.info(f\"Write wheel file into {tmp_archive_path}\")\n                        with open(tmp_archive_path, \"wb\") as f:\n                            f.write(wheel_file.contents)\n                        fractal_ssh.send_file(\n                            local=tmp_archive_path,\n                            remote=archive_path,\n                        )\n                        task_group.archive_path = archive_path\n                        task_group = add_commit_refresh(obj=task_group, db=db)\n\n                    python_bin = get_python_interpreter(\n                        python_version=task_group.python_version,\n                        resource=resource,\n                    )\n                    replacements = get_collection_replacements(\n                        task_group=task_group,\n                        python_bin=python_bin,\n                        resource=resource,\n                    )\n\n                    # Prepare common arguments for _customize_and_run_template\n                    common_args = dict(\n                        replacements=replacements,\n                        script_dir_local=Path(\n                            tmpdir, SCRIPTS_SUBFOLDER\n                        ).as_posix(),\n                        script_dir_remote=script_dir_remote,\n                        prefix=(\n                            f\"{int(time.time())}_\"\n                            f\"{TaskGroupActivityAction.COLLECT}\"\n                        ),\n                        fractal_ssh=fractal_ssh,\n                        logger_name=LOGGER_NAME,\n                    )\n\n                    logger.info(\"installing - START\")\n\n                    # Set status to ONGOING and refresh logs\n                    activity.status = TaskGroupActivityStatus.ONGOING\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run script 1\n                    stdout = _customize_and_run_template(\n                        template_filename=\"1_create_venv.sh\",\n                        **common_args,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run script 2\n                    stdout = _customize_and_run_template(\n                        template_filename=\"2_pip_install.sh\",\n                        **common_args,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run script 3\n                    pip_freeze_stdout = _customize_and_run_template(\n                        template_filename=\"3_pip_freeze.sh\",\n                        **common_args,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run script 4\n                    stdout = _customize_and_run_template(\n                        template_filename=\"4_pip_show.sh\",\n                        **common_args,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    pkg_attrs = parse_script_pip_show_stdout(stdout)\n\n                    for key, value in pkg_attrs.items():\n                        logger.debug(f\"parsed from pip-show: {key}={value}\")\n                    # Check package_name match between pip show and task-group\n                    package_name_pip_show = pkg_attrs.get(\"package_name\")\n                    package_name_task_group = task_group.pkg_name\n                    compare_package_names(\n                        pkg_name_pip_show=package_name_pip_show,\n                        pkg_name_task_group=package_name_task_group,\n                        logger_name=LOGGER_NAME,\n                    )\n                    # Extract/drop parsed attributes\n                    package_name = package_name_task_group\n                    python_bin = pkg_attrs.pop(\"python_bin\")\n                    package_root_parent_remote = pkg_attrs.pop(\n                        \"package_root_parent\"\n                    )\n                    manifest_path_remote = pkg_attrs.pop(\"manifest_path\")\n\n                    # TODO SSH: Use more robust logic to determine\n                    # `package_root`. Examples: use `importlib.util.find_spec`\n                    # or parse the output of `pip show --files {package_name}`.\n                    package_name_underscore = package_name.replace(\"-\", \"_\")\n                    package_root_remote = (\n                        Path(package_root_parent_remote)\n                        / package_name_underscore\n                    ).as_posix()\n\n                    # Read and validate remote manifest file\n                    pkg_manifest_dict = fractal_ssh.read_remote_json_file(\n                        manifest_path_remote\n                    )\n                    logger.info(f\"Loaded {manifest_path_remote=}\")\n                    pkg_manifest = ManifestV2(**pkg_manifest_dict)\n                    logger.info(\"Manifest is a valid ManifestV2\")\n\n                    logger.info(\"_prepare_tasks_metadata - start\")\n                    task_list = prepare_tasks_metadata(\n                        package_manifest=pkg_manifest,\n                        package_version=task_group.version,\n                        package_root=Path(package_root_remote),\n                        python_bin=Path(python_bin),\n                    )\n                    logger.info(\"_prepare_tasks_metadata - end\")\n\n                    logger.info(\"create_db_tasks_and_update_task_group - start\")\n                    create_db_tasks_and_update_task_group_sync(\n                        task_list=task_list,\n                        task_group_id=task_group.id,\n                        db=db,\n                    )\n                    logger.info(\"create_db_tasks_and_update_task_group - end\")\n\n                    # Update task_group data\n                    logger.info(\"Add env_info to TaskGroupV2 - start\")\n                    task_group.env_info = pip_freeze_stdout\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n                    logger.info(\"Add env_info to TaskGroupV2 - end\")\n\n                    # Finalize (write metadata to DB)\n                    logger.info(\"finalising - START\")\n                    activity.status = TaskGroupActivityStatus.OK\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n                    logger.info(\"finalising - END\")\n                    logger.info(\"END\")\n                    reset_logger_handlers(logger)\n\n                except Exception as collection_e:\n                    # Delete corrupted package dir\n                    try:\n                        logger.info(\n                            f\"Now delete remote folder {task_group.path}\"\n                        )\n                        fractal_ssh.remove_folder(\n                            folder=task_group.path,\n                            safe_root=profile.tasks_remote_dir,\n                        )\n                        logger.info(f\"Deleted remoted folder {task_group.path}\")\n                    except Exception as e_rm:\n                        logger.error(\n                            \"Removing folder failed. \"\n                            f\"Original error: {str(e_rm)}\"\n                        )\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        log_file_path=log_file_path,\n                        logger_name=LOGGER_NAME,\n                        exception=collection_e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/tasks/v2/ssh/collect_pixi/","title":"collect_pixi","text":""},{"location":"reference/tasks/v2/ssh/collect_pixi/#fractal_server.tasks.v2.ssh.collect_pixi.collect_ssh_pixi","title":"<code>collect_ssh_pixi(*, task_group_id, task_group_activity_id, tar_gz_file, resource, profile)</code>","text":"<p>Collect a task package over SSH</p> <p>This function runs as a background task, therefore exceptions must be handled.</p> <p>NOTE: since this function is sync, it runs within a thread - due to starlette/fastapi handling of background tasks (see https://github.com/encode/starlette/blob/master/starlette/background.py).</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>tar_gz_file</code> <p> TYPE: <code>FractalUploadedFile</code> </p> <code>resource</code> <p> TYPE: <code>Resource</code> </p> <code>profile</code> <p> TYPE: <code>Profile</code> </p> Source code in <code>fractal_server/tasks/v2/ssh/collect_pixi.py</code> <pre><code>def collect_ssh_pixi(\n    *,\n    task_group_id: int,\n    task_group_activity_id: int,\n    tar_gz_file: FractalUploadedFile,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Collect a task package over SSH\n\n    This function runs as a background task, therefore exceptions must be\n    handled.\n\n    NOTE: since this function is sync, it runs within a thread - due to\n    starlette/fastapi handling of background tasks (see\n    https://github.com/encode/starlette/blob/master/starlette/background.py).\n\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        tar_gz_file:\n        resource:\n        profile:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    # Work within a temporary folder, where also logs will be placed\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = Path(tmpdir) / \"log\"\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.info(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (remote) task_group path does not exist\n                    if fractal_ssh.remote_exists(task_group.path):\n                        error_msg = f\"{task_group.path} already exists.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileExistsError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    # Create remote `task_group.path` and `script_dir_remote`\n                    # folders\n                    script_dir_remote = Path(\n                        task_group.path, SCRIPTS_SUBFOLDER\n                    ).as_posix()\n                    fractal_ssh.mkdir(folder=task_group.path, parents=True)\n                    fractal_ssh.mkdir(folder=script_dir_remote, parents=True)\n\n                    # Write tar.gz file locally and send it to remote path,\n                    # and set task_group.archive_path\n                    tar_gz_filename = tar_gz_file.filename\n                    archive_path = (\n                        Path(task_group.path) / tar_gz_filename\n                    ).as_posix()\n                    tmp_archive_path = Path(tmpdir, tar_gz_filename).as_posix()\n                    logger.info(f\"Write tar.gz file into {tmp_archive_path}\")\n                    with open(tmp_archive_path, \"wb\") as f:\n                        f.write(tar_gz_file.contents)\n                    fractal_ssh.send_file(\n                        local=tmp_archive_path,\n                        remote=archive_path,\n                    )\n                    task_group.archive_path = archive_path\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n\n                    replacements = {\n                        (\n                            \"__PIXI_HOME__\",\n                            resource.tasks_pixi_config[\"versions\"][\n                                task_group.pixi_version\n                            ],\n                        ),\n                        (\"__PACKAGE_DIR__\", task_group.path),\n                        (\"__TAR_GZ_PATH__\", task_group.archive_path),\n                        (\n                            \"__IMPORT_PACKAGE_NAME__\",\n                            task_group.pkg_name.replace(\"-\", \"_\"),\n                        ),\n                        (\"__SOURCE_DIR_NAME__\", SOURCE_DIR_NAME),\n                        (\"__FROZEN_OPTION__\", \"\"),\n                        (\n                            \"__TOKIO_WORKER_THREADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"TOKIO_WORKER_THREADS\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_SOLVES__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_SOLVES\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_DOWNLOADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_DOWNLOADS\"\n                                ]\n                            ),\n                        ),\n                    }\n\n                    logger.info(\"installing - START\")\n\n                    # Set status to ONGOING and refresh logs\n                    activity.status = TaskGroupActivityStatus.ONGOING\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    common_args = dict(\n                        script_dir_local=(\n                            Path(tmpdir, SCRIPTS_SUBFOLDER)\n                        ).as_posix(),\n                        script_dir_remote=script_dir_remote,\n                        prefix=(\n                            f\"{int(time.time())}_\"\n                            f\"{TaskGroupActivityAction.COLLECT}\"\n                        ),\n                        logger_name=LOGGER_NAME,\n                        fractal_ssh=fractal_ssh,\n                    )\n\n                    # Run the three pixi-related scripts\n                    stdout = _customize_and_run_template(\n                        template_filename=\"pixi_1_extract.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    logger.debug(f\"STDOUT: {stdout}\")\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Simplify `pyproject.toml`\n                    source_dir = Path(\n                        task_group.path, SOURCE_DIR_NAME\n                    ).as_posix()\n                    pyproject_toml_path = Path(source_dir, \"pyproject.toml\")\n                    edit_pyproject_toml_in_place_ssh(\n                        fractal_ssh=fractal_ssh,\n                        pyproject_toml_path=pyproject_toml_path,\n                        resource=resource,\n                    )\n\n                    # Prepare scripts 2 and 3\n                    remote_script2_path = _customize_and_send_template(\n                        template_filename=\"pixi_2_install.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    remote_script3_path = _customize_and_send_template(\n                        template_filename=\"pixi_3_post_install.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    logger.debug(\n                        \"Post-installation script written to \"\n                        f\"{remote_script3_path=}.\"\n                    )\n                    logger.debug(\n                        \"Installation script written to \"\n                        f\"{remote_script2_path=}.\"\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run scripts 2 and 3\n                    stdout = run_script_on_remote_slurm(\n                        script_paths=[\n                            remote_script2_path,\n                            remote_script3_path,\n                            f\"chmod -R 755 {source_dir}\",\n                        ],\n                        slurm_config=resource.tasks_pixi_config[\"SLURM_CONFIG\"],\n                        fractal_ssh=fractal_ssh,\n                        logger_name=LOGGER_NAME,\n                        prefix=common_args[\"prefix\"],\n                        db=db,\n                        activity=activity,\n                        log_file_path=log_file_path,\n                        poll_interval=resource.jobs_poll_interval,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Parse stdout\n                    parsed_output = parse_collect_stdout(stdout)\n                    package_root_remote = parsed_output[\"package_root\"]\n                    project_python_wrapper = parsed_output[\n                        \"project_python_wrapper\"\n                    ]\n\n                    # Read and validate remote manifest file\n                    manifest_path_remote = (\n                        f\"{package_root_remote}/__FRACTAL_MANIFEST__.json\"\n                    )\n                    pkg_manifest_dict = fractal_ssh.read_remote_json_file(\n                        manifest_path_remote\n                    )\n                    logger.info(f\"Loaded {manifest_path_remote=}\")\n                    pkg_manifest = ManifestV2(**pkg_manifest_dict)\n                    logger.info(\"Manifest is a valid ManifestV2\")\n\n                    logger.info(\"_prepare_tasks_metadata - start\")\n                    task_list = prepare_tasks_metadata(\n                        package_manifest=pkg_manifest,\n                        package_version=task_group.version,\n                        package_root=Path(package_root_remote),\n                        project_python_wrapper=Path(project_python_wrapper),\n                    )\n                    logger.info(\"_prepare_tasks_metadata - end\")\n\n                    logger.info(\"create_db_tasks_and_update_task_group - start\")\n                    create_db_tasks_and_update_task_group_sync(\n                        task_list=task_list,\n                        task_group_id=task_group.id,\n                        db=db,\n                    )\n                    logger.info(\"create_db_tasks_and_update_task_group - end\")\n\n                    # NOTE: see issue 2626 about whether to keep `pixi.lock`\n                    # files in the database\n                    remote_pixi_lock_file = Path(\n                        task_group.path,\n                        SOURCE_DIR_NAME,\n                        \"pixi.lock\",\n                    ).as_posix()\n                    pixi_lock_contents = fractal_ssh.read_remote_text_file(\n                        remote_pixi_lock_file\n                    )\n\n                    # Update task_group data\n                    logger.info(\"Add env_info to TaskGroupV2 - start\")\n                    task_group.env_info = pixi_lock_contents\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n                    logger.info(\"Add env_info to TaskGroupV2 - end\")\n\n                    # Finalize (write metadata to DB)\n                    logger.info(\"finalising - START\")\n                    activity.status = TaskGroupActivityStatus.OK\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n                    logger.info(\"finalising - END\")\n                    logger.info(\"END\")\n                    reset_logger_handlers(logger)\n\n                except Exception as collection_e:\n                    # Delete corrupted package dir\n                    try:\n                        logger.info(\n                            f\"Now delete remote folder {task_group.path}\"\n                        )\n                        fractal_ssh.remove_folder(\n                            folder=task_group.path,\n                            safe_root=profile.tasks_remote_dir,\n                        )\n                        logger.info(f\"Deleted remoted folder {task_group.path}\")\n                    except Exception as e_rm:\n                        logger.error(\n                            \"Removing folder failed. \"\n                            f\"Original error: {str(e_rm)}\"\n                        )\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        log_file_path=log_file_path,\n                        logger_name=LOGGER_NAME,\n                        exception=collection_e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/tasks/v2/ssh/deactivate/","title":"deactivate","text":""},{"location":"reference/tasks/v2/ssh/deactivate/#fractal_server.tasks.v2.ssh.deactivate.deactivate_ssh","title":"<code>deactivate_ssh(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Deactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>resource</code> <p> TYPE: <code>Resource</code> </p> <code>profile</code> <p> TYPE: <code>Profile</code> </p> Source code in <code>fractal_server/tasks/v2/ssh/deactivate.py</code> <pre><code>def deactivate_ssh(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Deactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n        profile:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    user=profile.username,\n                    host=resource.host,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (local) task_group venv_path does exist\n                    if not fractal_ssh.remote_exists(task_group.venv_path):\n                        error_msg = f\"{task_group.venv_path} does not exist.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileNotFoundError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    activity.status = TaskGroupActivityStatus.ONGOING\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    if task_group.env_info is None:\n                        logger.warning(\n                            \"Recreate pip-freeze information, since \"\n                            f\"{task_group.env_info=}. NOTE: this should \"\n                            \"only happen for task groups created before 2.9.0.\"\n                        )\n\n                        # Prepare replacements for templates\n                        replacements = get_collection_replacements(\n                            task_group=task_group,\n                            python_bin=\"/not/applicable\",\n                            resource=resource,\n                        )\n\n                        # Define script_dir_remote and create it if missing\n                        script_dir_remote = (\n                            Path(task_group.path) / SCRIPTS_SUBFOLDER\n                        ).as_posix()\n                        fractal_ssh.mkdir(\n                            folder=script_dir_remote, parents=True\n                        )\n\n                        # Prepare arguments for `_customize_and_run_template`\n                        common_args = dict(\n                            replacements=replacements,\n                            script_dir_local=(\n                                Path(tmpdir) / SCRIPTS_SUBFOLDER\n                            ).as_posix(),\n                            script_dir_remote=script_dir_remote,\n                            prefix=(\n                                f\"{int(time.time())}_\"\n                                f\"{TaskGroupActivityAction.DEACTIVATE}\"\n                            ),\n                            fractal_ssh=fractal_ssh,\n                            logger_name=LOGGER_NAME,\n                        )\n\n                        # Run `pip freeze`\n                        pip_freeze_stdout = _customize_and_run_template(\n                            template_filename=\"3_pip_freeze.sh\",\n                            **common_args,\n                        )\n\n                        # Update pip-freeze data\n                        logger.info(\n                            \"Add pip freeze stdout to TaskGroupV2 - start\"\n                        )\n                        activity.log = get_current_log(log_file_path)\n                        activity = add_commit_refresh(obj=activity, db=db)\n                        task_group.env_info = pip_freeze_stdout\n                        task_group = add_commit_refresh(obj=task_group, db=db)\n                        logger.info(\n                            \"Add pip freeze stdout to TaskGroupV2 - end\"\n                        )\n\n                    # Handle some specific cases for wheel-file case\n                    if task_group.origin == TaskGroupOriginEnum.WHEELFILE:\n                        logger.info(\n                            f\"Handle specific cases for {task_group.origin=}.\"\n                        )\n\n                        # Blocking situation: `archive_path` is not set or\n                        # points to a missing path\n                        if (\n                            task_group.archive_path is None\n                            or not fractal_ssh.remote_exists(\n                                task_group.archive_path\n                            )\n                        ):\n                            error_msg = (\n                                \"Invalid wheel path for task group with \"\n                                f\"{task_group_id=}. \"\n                                f\"{task_group.archive_path=}  is unset or \"\n                                \"does not exist.\"\n                            )\n                            logger.error(error_msg)\n                            fail_and_cleanup(\n                                task_group=task_group,\n                                task_group_activity=activity,\n                                logger_name=LOGGER_NAME,\n                                log_file_path=log_file_path,\n                                exception=FileNotFoundError(error_msg),\n                                db=db,\n                            )\n                            return\n\n                        # Recoverable situation: `archive_path` was not yet\n                        # copied over to the correct server-side folder\n                        archive_path_parent_dir = Path(\n                            task_group.archive_path\n                        ).parent\n                        if archive_path_parent_dir != Path(task_group.path):\n                            logger.warning(\n                                f\"{archive_path_parent_dir.as_posix()} \"\n                                f\"differs from {task_group.path}. \"\n                                \"NOTE: this should only happen for task \"\n                                \"groups created before 2.9.0.\"\n                            )\n\n                            if (\n                                task_group.archive_path\n                                not in task_group.env_info\n                            ):\n                                raise ValueError(\n                                    f\"Cannot find {task_group.archive_path=} \"\n                                    \"in pip-freeze data. Exit.\"\n                                )\n\n                            logger.info(\n                                f\"Now copy wheel file into {task_group.path}.\"\n                            )\n                            new_archive_path = _copy_wheel_file_ssh(\n                                task_group=task_group,\n                                fractal_ssh=fractal_ssh,\n                                logger_name=LOGGER_NAME,\n                            )\n                            logger.info(\n                                f\"Copied wheel file to {new_archive_path}.\"\n                            )\n\n                            task_group.archive_path = new_archive_path\n                            new_pip_freeze = task_group.env_info.replace(\n                                task_group.archive_path,\n                                new_archive_path,\n                            )\n                            task_group.env_info = new_pip_freeze\n                            task_group = add_commit_refresh(\n                                obj=task_group, db=db\n                            )\n                            logger.info(\n                                \"Updated `archive_path` and `env_info` \"\n                                \"task-group attributes.\"\n                            )\n\n                    # Fail if `env_info` includes \"github\", see\n                    # https://github.com/fractal-analytics-platform/fractal-server/issues/2142\n                    for forbidden_string in FORBIDDEN_DEPENDENCY_STRINGS:\n                        if forbidden_string in task_group.env_info:\n                            raise ValueError(\n                                \"Deactivation and reactivation of task \"\n                                f\"packages with direct {forbidden_string} \"\n                                \"dependencies are not currently supported. \"\n                                \"Exit.\"\n                            )\n\n                    # We now have all required information for reactivating the\n                    # virtual environment at a later point\n\n                    # Actually mark the task group as non-active\n                    logger.info(\"Now setting `active=False`.\")\n                    task_group.active = False\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n\n                    # Proceed with deactivation\n                    logger.info(f\"Now removing {task_group.venv_path}.\")\n                    fractal_ssh.remove_folder(\n                        folder=task_group.venv_path,\n                        safe_root=profile.tasks_remote_dir,\n                    )\n                    logger.info(f\"All good, {task_group.venv_path} removed.\")\n                    activity.status = TaskGroupActivityStatus.OK\n                    activity.log = get_current_log(log_file_path)\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    reset_logger_handlers(logger)\n\n                except Exception as e:\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        exception=e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/tasks/v2/ssh/deactivate_pixi/","title":"deactivate_pixi","text":""},{"location":"reference/tasks/v2/ssh/deactivate_pixi/#fractal_server.tasks.v2.ssh.deactivate_pixi.deactivate_ssh_pixi","title":"<code>deactivate_ssh_pixi(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Deactivate a pixi task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> PARAMETER DESCRIPTION <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>resource</code> <p> TYPE: <code>Resource</code> </p> <code>profile</code> <p> TYPE: <code>Profile</code> </p> Source code in <code>fractal_server/tasks/v2/ssh/deactivate_pixi.py</code> <pre><code>def deactivate_ssh_pixi(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Deactivate a pixi task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_activity_id:\n        task_group_id:\n        resource:\n        profile:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (remote) task_group venv_path does exist\n                    source_dir = Path(\n                        task_group.path, SOURCE_DIR_NAME\n                    ).as_posix()\n                    if not fractal_ssh.remote_exists(source_dir):\n                        error_msg = f\"{source_dir} does not exist.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileNotFoundError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    # Actually mark the task group as non-active\n                    logger.info(\"Now setting `active=False`.\")\n                    task_group.active = False\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n\n                    # Proceed with deactivation\n                    logger.info(f\"Now removing {source_dir}.\")\n                    fractal_ssh.remove_folder(\n                        folder=source_dir,\n                        safe_root=profile.tasks_remote_dir,\n                    )\n                    logger.info(f\"All good, {source_dir} removed.\")\n                    activity.status = TaskGroupActivityStatus.OK\n                    activity.log = get_current_log(log_file_path)\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    reset_logger_handlers(logger)\n\n                except Exception as e:\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        exception=e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/tasks/v2/ssh/delete/","title":"delete","text":""},{"location":"reference/tasks/v2/ssh/delete/#fractal_server.tasks.v2.ssh.delete.delete_ssh","title":"<code>delete_ssh(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Delete a task group.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>resource</code> <p> TYPE: <code>Resource</code> </p> <code>profile</code> <p> TYPE: <code>Profile</code> </p> Source code in <code>fractal_server/tasks/v2/ssh/delete.py</code> <pre><code>def delete_ssh(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Delete a task group.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n        profile:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n        logger.debug(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    activity.status = TaskGroupActivityStatus.ONGOING\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    db.delete(task_group)\n                    db.commit()\n                    logger.debug(\"Task group removed from database.\")\n\n                    if task_group.origin != TaskGroupOriginEnum.OTHER:\n                        logger.debug(\n                            f\"Removing remote {task_group.path=} \"\n                            f\"(with {profile.tasks_remote_dir=}).\"\n                        )\n                        fractal_ssh.remove_folder(\n                            folder=task_group.path,\n                            safe_root=profile.tasks_remote_dir,\n                        )\n                        logger.debug(f\"Remote {task_group.path=} removed.\")\n\n                    activity.status = TaskGroupActivityStatus.OK\n                    activity.log = get_current_log(log_file_path)\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    logger.debug(\"END\")\n\n                except Exception as e:\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        exception=e,\n                        db=db,\n                    )\n\n                finally:\n                    reset_logger_handlers(logger)\n</code></pre>"},{"location":"reference/tasks/v2/ssh/reactivate/","title":"reactivate","text":""},{"location":"reference/tasks/v2/ssh/reactivate/#fractal_server.tasks.v2.ssh.reactivate.reactivate_ssh","title":"<code>reactivate_ssh(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Reactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> PARAMETER DESCRIPTION <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>resource</code> <p> TYPE: <code>Resource</code> </p> <code>profile</code> <p> TYPE: <code>Profile</code> </p> Source code in <code>fractal_server/tasks/v2/ssh/reactivate.py</code> <pre><code>def reactivate_ssh(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Reactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_activity_id:\n        task_group_id:\n        resource:\n        profile:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.info(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (remote) task_group venv_path does not\n                    # exist\n                    if fractal_ssh.remote_exists(task_group.venv_path):\n                        error_msg = f\"{task_group.venv_path} already exists.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileExistsError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    activity.status = TaskGroupActivityStatus.ONGOING\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Prepare replacements for templates\n                    python_bin = get_python_interpreter(\n                        python_version=task_group.python_version,\n                        resource=resource,\n                    )\n                    replacements = get_collection_replacements(\n                        task_group=task_group,\n                        python_bin=python_bin,\n                        resource=resource,\n                    )\n\n                    # Prepare replacements for templates\n                    pip_freeze_file_local = f\"{tmpdir}/pip_freeze.txt\"\n                    pip_freeze_file_remote = (\n                        Path(task_group.path) / \"_tmp_pip_freeze.txt\"\n                    ).as_posix()\n                    with open(pip_freeze_file_local, \"w\") as f:\n                        f.write(task_group.env_info)\n                    fractal_ssh.send_file(\n                        local=pip_freeze_file_local,\n                        remote=pip_freeze_file_remote,\n                    )\n                    replacements.append(\n                        (\"__PIP_FREEZE_FILE__\", pip_freeze_file_remote)\n                    )\n\n                    # Define script_dir_remote and create it if missing\n                    script_dir_remote = (\n                        Path(task_group.path) / SCRIPTS_SUBFOLDER\n                    ).as_posix()\n                    fractal_ssh.mkdir(folder=script_dir_remote, parents=True)\n\n                    # Prepare common arguments for _customize_and_run_template\n                    common_args = dict(\n                        replacements=replacements,\n                        script_dir_local=(\n                            Path(tmpdir) / SCRIPTS_SUBFOLDER\n                        ).as_posix(),\n                        script_dir_remote=script_dir_remote,\n                        prefix=(\n                            f\"{int(time.time())}_\"\n                            f\"{TaskGroupActivityAction.REACTIVATE}\"\n                        ),\n                        fractal_ssh=fractal_ssh,\n                        logger_name=LOGGER_NAME,\n                    )\n\n                    # Create remote directory for scripts\n                    fractal_ssh.mkdir(folder=script_dir_remote)\n\n                    logger.info(\"start - create venv\")\n                    _customize_and_run_template(\n                        template_filename=\"1_create_venv.sh\",\n                        **common_args,\n                    )\n                    logger.info(\"end - create venv\")\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    logger.info(\"start - install from pip freeze\")\n                    _customize_and_run_template(\n                        template_filename=\"5_pip_install_from_freeze.sh\",\n                        **common_args,\n                    )\n                    logger.info(\"end - install from pip freeze\")\n                    activity.log = get_current_log(log_file_path)\n                    activity.status = TaskGroupActivityStatus.OK\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n                    task_group.active = True\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n                    logger.info(\"END\")\n\n                    reset_logger_handlers(logger)\n\n                except Exception as reactivate_e:\n                    # Delete corrupted venv_path\n                    try:\n                        logger.info(f\"Now delete folder {task_group.venv_path}\")\n                        fractal_ssh.remove_folder(\n                            folder=task_group.venv_path,\n                            safe_root=profile.tasks_remote_dir,\n                        )\n                        logger.info(f\"Deleted folder {task_group.venv_path}\")\n                    except Exception as rm_e:\n                        logger.error(\n                            \"Removing folder failed. \"\n                            f\"Original error: {str(rm_e)}\"\n                        )\n\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        exception=reactivate_e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/tasks/v2/ssh/reactivate_pixi/","title":"reactivate_pixi","text":""},{"location":"reference/tasks/v2/ssh/reactivate_pixi/#fractal_server.tasks.v2.ssh.reactivate_pixi.reactivate_ssh_pixi","title":"<code>reactivate_ssh_pixi(*, task_group_activity_id, task_group_id, resource, profile)</code>","text":"<p>Reactivate a task group venv.</p> <p>This function is run as a background task, therefore exceptions must be handled.</p> PARAMETER DESCRIPTION <code>task_group_id</code> <p> TYPE: <code>int</code> </p> <code>task_group_activity_id</code> <p> TYPE: <code>int</code> </p> <code>resource</code> <p> TYPE: <code>Resource</code> </p> <code>profile</code> <p> TYPE: <code>Profile</code> </p> Source code in <code>fractal_server/tasks/v2/ssh/reactivate_pixi.py</code> <pre><code>def reactivate_ssh_pixi(\n    *,\n    task_group_activity_id: int,\n    task_group_id: int,\n    resource: Resource,\n    profile: Profile,\n) -&gt; None:\n    \"\"\"\n    Reactivate a task group venv.\n\n    This function is run as a background task, therefore exceptions must be\n    handled.\n\n    Args:\n        task_group_id:\n        task_group_activity_id:\n        resource:\n        profile:\n    \"\"\"\n\n    LOGGER_NAME = f\"{__name__}.ID{task_group_activity_id}\"\n\n    with TemporaryDirectory() as tmpdir:\n        log_file_path = get_log_path(Path(tmpdir))\n        logger = set_logger(\n            logger_name=LOGGER_NAME,\n            log_file_path=log_file_path,\n        )\n\n        logger.info(\"START\")\n        with next(get_sync_db()) as db:\n            db_objects_ok, task_group, activity = get_activity_and_task_group(\n                task_group_activity_id=task_group_activity_id,\n                task_group_id=task_group_id,\n                db=db,\n                logger_name=LOGGER_NAME,\n            )\n            if not db_objects_ok:\n                return\n\n            with SingleUseFractalSSH(\n                ssh_config=SSHConfig(\n                    host=resource.host,\n                    user=profile.username,\n                    key_path=profile.ssh_key_path,\n                ),\n                logger_name=LOGGER_NAME,\n            ) as fractal_ssh:\n                try:\n                    # Check SSH connection\n                    ssh_ok = check_ssh_or_fail_and_cleanup(\n                        fractal_ssh=fractal_ssh,\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        db=db,\n                    )\n                    if not ssh_ok:\n                        return\n\n                    # Check that the (remote) task_group source_dir does not\n                    # exist\n                    source_dir = Path(\n                        task_group.path, SOURCE_DIR_NAME\n                    ).as_posix()\n                    if fractal_ssh.remote_exists(source_dir):\n                        error_msg = f\"{source_dir} already exists.\"\n                        logger.error(error_msg)\n                        fail_and_cleanup(\n                            task_group=task_group,\n                            task_group_activity=activity,\n                            logger_name=LOGGER_NAME,\n                            log_file_path=log_file_path,\n                            exception=FileExistsError(error_msg),\n                            db=db,\n                        )\n                        return\n\n                    replacements = {\n                        (\n                            \"__PIXI_HOME__\",\n                            resource.tasks_pixi_config[\"versions\"][\n                                task_group.pixi_version\n                            ],\n                        ),\n                        (\"__PACKAGE_DIR__\", task_group.path),\n                        (\"__TAR_GZ_PATH__\", task_group.archive_path),\n                        (\n                            \"__IMPORT_PACKAGE_NAME__\",\n                            task_group.pkg_name.replace(\"-\", \"_\"),\n                        ),\n                        (\"__SOURCE_DIR_NAME__\", SOURCE_DIR_NAME),\n                        (\"__FROZEN_OPTION__\", \"--frozen\"),\n                        (\n                            \"__TOKIO_WORKER_THREADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"TOKIO_WORKER_THREADS\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_SOLVES__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_SOLVES\"\n                                ]\n                            ),\n                        ),\n                        (\n                            \"__PIXI_CONCURRENT_DOWNLOADS__\",\n                            str(\n                                resource.tasks_pixi_config[\n                                    \"PIXI_CONCURRENT_DOWNLOADS\"\n                                ]\n                            ),\n                        ),\n                    }\n\n                    logger.info(\"installing - START\")\n\n                    # Set status to ONGOING and refresh logs\n                    activity.status = TaskGroupActivityStatus.ONGOING\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    script_dir_remote = Path(\n                        task_group.path, SCRIPTS_SUBFOLDER\n                    ).as_posix()\n                    common_args = dict(\n                        script_dir_local=(\n                            Path(tmpdir) / SCRIPTS_SUBFOLDER\n                        ).as_posix(),\n                        script_dir_remote=script_dir_remote,\n                        prefix=(\n                            f\"{int(time.time())}_\"\n                            f\"{TaskGroupActivityAction.REACTIVATE}\"\n                        ),\n                        logger_name=LOGGER_NAME,\n                        fractal_ssh=fractal_ssh,\n                    )\n\n                    # Run script 1 - extract tar.gz into `source_dir`\n                    stdout = _customize_and_run_template(\n                        template_filename=\"pixi_1_extract.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    logger.debug(f\"STDOUT: {stdout}\")\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Simplify `pyproject.toml`\n                    source_dir = Path(\n                        task_group.path, SOURCE_DIR_NAME\n                    ).as_posix()\n                    pyproject_toml_path = Path(source_dir, \"pyproject.toml\")\n                    edit_pyproject_toml_in_place_ssh(\n                        fractal_ssh=fractal_ssh,\n                        pyproject_toml_path=pyproject_toml_path,\n                        resource=resource,\n                    )\n                    # Write pixi.lock into `source_dir`\n                    pixi_lock_local = Path(tmpdir, \"pixi.lock\").as_posix()\n                    pixi_lock_remote = Path(\n                        task_group.path, SOURCE_DIR_NAME, \"pixi.lock\"\n                    ).as_posix()\n                    logger.info(\n                        f\"Write `env_info` contents into {pixi_lock_local}\"\n                    )\n                    with open(pixi_lock_local, \"w\") as f:\n                        f.write(task_group.env_info)\n                    fractal_ssh.send_file(\n                        local=pixi_lock_local,\n                        remote=pixi_lock_remote,\n                    )\n\n                    # Prepare scripts 2 and 3\n                    remote_script2_path = _customize_and_send_template(\n                        template_filename=\"pixi_2_install.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    remote_script3_path = _customize_and_send_template(\n                        template_filename=\"pixi_3_post_install.sh\",\n                        replacements=replacements,\n                        **common_args,\n                    )\n                    logger.debug(\n                        \"Post-installation script written to \"\n                        f\"{remote_script3_path=}.\"\n                    )\n                    logger.debug(\n                        \"Installation script written to \"\n                        f\"{remote_script2_path=}.\"\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Run scripts 2 and 3\n                    stdout = run_script_on_remote_slurm(\n                        script_paths=[\n                            remote_script2_path,\n                            remote_script3_path,\n                            f\"chmod -R 755 {source_dir}\",\n                        ],\n                        slurm_config=resource.tasks_pixi_config[\"SLURM_CONFIG\"],\n                        fractal_ssh=fractal_ssh,\n                        logger_name=LOGGER_NAME,\n                        prefix=common_args[\"prefix\"],\n                        db=db,\n                        activity=activity,\n                        log_file_path=log_file_path,\n                        poll_interval=resource.jobs_poll_interval,\n                    )\n                    activity.log = get_current_log(log_file_path)\n                    activity = add_commit_refresh(obj=activity, db=db)\n\n                    # Finalize (write metadata to DB)\n                    activity.status = TaskGroupActivityStatus.OK\n                    activity.timestamp_ended = get_timestamp()\n                    activity = add_commit_refresh(obj=activity, db=db)\n                    task_group.active = True\n                    task_group = add_commit_refresh(obj=task_group, db=db)\n                    logger.info(\"END\")\n\n                    reset_logger_handlers(logger)\n\n                except Exception as reactivate_e:\n                    # Delete corrupted source_dir\n                    try:\n                        logger.info(f\"Now delete folder {source_dir}\")\n                        fractal_ssh.remove_folder(\n                            folder=source_dir,\n                            safe_root=profile.tasks_remote_dir,\n                        )\n                        logger.info(f\"Deleted folder {source_dir}\")\n                    except Exception as rm_e:\n                        logger.error(\n                            \"Removing folder failed. \"\n                            f\"Original error: {str(rm_e)}\"\n                        )\n\n                    fail_and_cleanup(\n                        task_group=task_group,\n                        task_group_activity=activity,\n                        logger_name=LOGGER_NAME,\n                        log_file_path=log_file_path,\n                        exception=reactivate_e,\n                        db=db,\n                    )\n</code></pre>"},{"location":"reference/types/","title":"types","text":""},{"location":"reference/types/#fractal_server.types.AbsolutePathStr","title":"<code>AbsolutePathStr = Annotated[NonEmptyStr, AfterValidator(val_absolute_path), AfterValidator(val_no_dotdot_in_path), AfterValidator(val_os_path_normpath)]</code>  <code>module-attribute</code>","text":"<p>String representing an absolute path.</p> <p>Validation fails if the path is not absolute or if it contains a parent-directory reference \"/../\".</p>"},{"location":"reference/types/#fractal_server.types.AttributeFilters","title":"<code>AttributeFilters = Annotated[dict[str, list[ImageAttributeValue]], AfterValidator(validate_attribute_filters)]</code>  <code>module-attribute</code>","text":"<p>Image-attributes filters.</p>"},{"location":"reference/types/#fractal_server.types.DictStrAny","title":"<code>DictStrAny = Annotated[dict[str, Any], AfterValidator(valdict_keys)]</code>  <code>module-attribute</code>","text":"<p>Dictionary where keys are strings with no leading/trailing whitespaces.</p>"},{"location":"reference/types/#fractal_server.types.DictStrStr","title":"<code>DictStrStr = Annotated[dict[str, NonEmptyStr], AfterValidator(valdict_keys)]</code>  <code>module-attribute</code>","text":"<p>Dictionary where keys are strings with no leading/trailing whitespaces and values are non-empty strings.</p>"},{"location":"reference/types/#fractal_server.types.HttpUrlStr","title":"<code>HttpUrlStr = Annotated[NonEmptyStr, AfterValidator(val_http_url)]</code>  <code>module-attribute</code>","text":"<p>String representing an URL.</p>"},{"location":"reference/types/#fractal_server.types.ImageAttributeValue","title":"<code>ImageAttributeValue = Union[int, float, str, bool]</code>  <code>module-attribute</code>","text":"<p>Possible values for image attributes.</p>"},{"location":"reference/types/#fractal_server.types.ImageAttributes","title":"<code>ImageAttributes = Annotated[dict[str, ImageAttributeValue], AfterValidator(valdict_keys)]</code>  <code>module-attribute</code>","text":"<p>Image-attributes dictionary.</p>"},{"location":"reference/types/#fractal_server.types.ImageAttributesWithNone","title":"<code>ImageAttributesWithNone = Annotated[dict[str, ImageAttributeValue | None], AfterValidator(valdict_keys)]</code>  <code>module-attribute</code>","text":"<p>Image-attributes dictionary, including <code>None</code> attributes.</p>"},{"location":"reference/types/#fractal_server.types.ImageTypes","title":"<code>ImageTypes = Annotated[dict[str, bool], AfterValidator(valdict_keys)]</code>  <code>module-attribute</code>","text":"<p>Image types.</p>"},{"location":"reference/types/#fractal_server.types.ListUniqueAbsolutePathStr","title":"<code>ListUniqueAbsolutePathStr = Annotated[list[AbsolutePathStr], AfterValidator(val_unique_list)]</code>  <code>module-attribute</code>","text":"<p>List of unique absolute-path-string items.</p>"},{"location":"reference/types/#fractal_server.types.ListUniqueNonEmptyString","title":"<code>ListUniqueNonEmptyString = Annotated[list[NonEmptyStr], AfterValidator(val_unique_list)]</code>  <code>module-attribute</code>","text":"<p>List of unique non-empty-string items.</p>"},{"location":"reference/types/#fractal_server.types.ListUniqueNonNegativeInt","title":"<code>ListUniqueNonNegativeInt = Annotated[list[NonNegativeInt], AfterValidator(val_unique_list)]</code>  <code>module-attribute</code>","text":"<p>List of unique non-negative-integer items.</p>"},{"location":"reference/types/#fractal_server.types.NonEmptyStr","title":"<code>NonEmptyStr = Annotated[str, StringConstraints(min_length=1, strip_whitespace=True)]</code>  <code>module-attribute</code>","text":"<p>A non-empty string, with no leading/trailing whitespaces.</p>"},{"location":"reference/types/#fractal_server.types.SafeNonEmptyStr","title":"<code>SafeNonEmptyStr = Annotated[NonEmptyStr, StringConstraints(pattern='^([a-zA-Z0-9_. -]+)$')]</code>  <code>module-attribute</code>","text":"<p>A non-empty string restricted to the characters matching the regex, i.e. alphanumerics, underscores, dots, spaces, and hyphens, with no leading or trailing spaces.</p>"},{"location":"reference/types/#fractal_server.types.SafeRelativePathStr","title":"<code>SafeRelativePathStr = Annotated[NonEmptyStr, AfterValidator(val_no_dotdot_in_path), AfterValidator(val_os_path_normpath), AfterValidator(val_non_absolute_path), StringConstraints(pattern='^([a-zA-Z0-9_. /-]+)$')]</code>  <code>module-attribute</code>","text":"<p>String representing a relative path.</p> <p>Validation fails if the path is absolute, if it contains a parent-directory reference \"/../\" or if it doesn't match the regex, i.e. if it contains characters which are not alphanumerics, underscores, dots, spaces, hyphens or slashes.</p>"},{"location":"reference/types/#fractal_server.types.TypeFilters","title":"<code>TypeFilters = Annotated[dict[str, bool], AfterValidator(valdict_keys)]</code>  <code>module-attribute</code>","text":"<p>Image-type filters.</p>"},{"location":"reference/types/#fractal_server.types.WorkflowTaskArgument","title":"<code>WorkflowTaskArgument = Annotated[DictStrAny, AfterValidator(validate_wft_args)]</code>  <code>module-attribute</code>","text":"<p>Dictionary with no keys from a given forbid-list.</p>"},{"location":"reference/types/#fractal_server.types.ZarrDirStr","title":"<code>ZarrDirStr = Annotated[NonEmptyStr, AfterValidator(val_no_dotdot_in_path), AfterValidator(normalize_url)]</code>  <code>module-attribute</code>","text":"<p>String representing a <code>zarr_dir</code> path.</p> <p>Validation fails if the path is not absolute or if it contains a parent-directory reference \"/../\".</p>"},{"location":"reference/types/#fractal_server.types.ZarrUrlStr","title":"<code>ZarrUrlStr = Annotated[NonEmptyStr, AfterValidator(val_no_dotdot_in_path), AfterValidator(normalize_url)]</code>  <code>module-attribute</code>","text":"<p>String representing a zarr URL/path.</p> <p>Validation fails if the path is not absolute or if it contains a parent-directory reference \"/../\".</p>"},{"location":"reference/types/validators/","title":"validators","text":""},{"location":"reference/types/validators/_common_validators/","title":"_common_validators","text":""},{"location":"reference/types/validators/_common_validators/#fractal_server.types.validators._common_validators.val_absolute_path","title":"<code>val_absolute_path(path)</code>","text":"<p>Check that a string attribute is an absolute path</p> Source code in <code>fractal_server/types/validators/_common_validators.py</code> <pre><code>def val_absolute_path(path: str) -&gt; str:\n    \"\"\"\n    Check that a string attribute is an absolute path\n    \"\"\"\n    if not os.path.isabs(path):\n        raise ValueError(f\"String must be an absolute path (given '{path}').\")\n    return path\n</code></pre>"},{"location":"reference/types/validators/_common_validators/#fractal_server.types.validators._common_validators.val_no_dotdot_in_path","title":"<code>val_no_dotdot_in_path(path)</code>","text":"<p>Check that a string attribute has no '/../' in it</p> Source code in <code>fractal_server/types/validators/_common_validators.py</code> <pre><code>def val_no_dotdot_in_path(path: str) -&gt; str:\n    \"\"\"\n    Check that a string attribute has no '/../' in it\n    \"\"\"\n    if \"..\" in Path(path).parts:\n        raise ValueError(\"String must not contain '/../'.\")\n    return path\n</code></pre>"},{"location":"reference/types/validators/_common_validators/#fractal_server.types.validators._common_validators.val_non_absolute_path","title":"<code>val_non_absolute_path(path)</code>","text":"<p>Check that a string attribute is not an absolute path</p> Source code in <code>fractal_server/types/validators/_common_validators.py</code> <pre><code>def val_non_absolute_path(path: str) -&gt; str:\n    \"\"\"\n    Check that a string attribute is not an absolute path\n    \"\"\"\n    if os.path.isabs(path):\n        raise ValueError(\n            f\"String must not be an absolute path (given '{path}').\"\n        )\n    return path\n</code></pre>"},{"location":"reference/types/validators/_common_validators/#fractal_server.types.validators._common_validators.val_os_path_normpath","title":"<code>val_os_path_normpath(path)</code>","text":"<p>Apply <code>os.path.normpath</code> to <code>path</code>.</p> <p>Note: we keep this separate from <code>fractal_server.urls.normalize_url</code>, because this function only applies to on-disk paths, while <code>normalize_url</code> may apply to s3 URLs as well.</p> Source code in <code>fractal_server/types/validators/_common_validators.py</code> <pre><code>def val_os_path_normpath(path: str) -&gt; str:\n    \"\"\"\n    Apply `os.path.normpath` to `path`.\n\n    Note: we keep this separate from `fractal_server.urls.normalize_url`,\n    because this function only applies to on-disk paths, while `normalize_url`\n    may apply to s3 URLs as well.\n    \"\"\"\n    return os.path.normpath(path)\n</code></pre>"},{"location":"reference/types/validators/_common_validators/#fractal_server.types.validators._common_validators.valdict_keys","title":"<code>valdict_keys(d)</code>","text":"<p>Strip every key of the dictionary, and fail if there are identical keys</p> Source code in <code>fractal_server/types/validators/_common_validators.py</code> <pre><code>def valdict_keys(d: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Strip every key of the dictionary, and fail if there are identical keys\n    \"\"\"\n    old_keys = list(d.keys())\n    new_keys = [key.strip() for key in old_keys]\n    if any(k == \"\" for k in new_keys):\n        raise ValueError(f\"Empty string in {new_keys}.\")\n    if len(new_keys) != len(set(new_keys)):\n        raise ValueError(f\"Dictionary contains multiple identical keys: '{d}'.\")\n    for old_key, new_key in zip(old_keys, new_keys):\n        if new_key != old_key:\n            d[new_key] = d.pop(old_key)\n    return d\n</code></pre>"},{"location":"reference/types/validators/_filter_validators/","title":"_filter_validators","text":""},{"location":"reference/types/validators/_workflow_task_arguments_validators/","title":"_workflow_task_arguments_validators","text":""}]}